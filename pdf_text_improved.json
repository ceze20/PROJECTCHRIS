[
    {
        "page": 1,
        "text": "[OCR Extracted Text from Image 1]:\nJohn L. Hennessy | David A. Patterson\n\nCOMPUTER\nARCHITECTURE\n\nA Quantitative Approach"
    },
    {
        "page": 2,
        "text": "Computer Architecture Formulas\n1. CPU time = Instruction count  Clock cycles per instruction  Clock cycle time \n  \n2. X is n times faster than Y: n\n \n \n=\n \n3. Amdahl\u2019s Law: Speedupoverall \n \n \n=\n \n \n=\n4.\n5.\n6.\n7. Availability = Mean time to fail / (Mean time to fail  + Mean time to repair) \n8.\nwhere Wafer yield accounts for wafers that are so bad they need not be tested and \nis a parameter called \nthe process-complexity factor, a measure of manufacturing difficulty. \nranges from 11.5 to 15.5 in 2011.\n9. Means\u2014arithmetic (AM), weighted arithmetic (WAM), and geometric (GM):\nAM = \n \n WAM = \n GM = \n \nwhere Timei is the execution time for the ith program of a total of n in the workload, Weighti is the\nweighting of the ith program in the workload.\n10. Average memory-access time = Hit time + Miss rate  Miss penalty\n11. Misses per instruction = Miss rate  Memory access per instruction\n12. Cache index size: 2index = Cache size /(Block size  Set associativity)\n13. Power Utilization Effectiveness (PUE) of a Warehouse Scale Computer = \nRules of Thumb\n1. Amdahl/Case Rule: A balanced computer system needs about 1 MB of main memory capacity and 1\nmegabit per second of I/O bandwidth per MIPS of CPU performance.\n2. 90/10 Locality Rule: A program executes about 90% of its instructions in 10% of its code.\n3. Bandwidth Rule: Bandwidth grows by at least the square of the improvement in latency.\n4. 2:1 Cache Rule: The miss rate of a direct-mapped cache of size N is about the same as a two-way set-\nassociative cache of size N/2.\n5. Dependability Rule: Design with no single point of failure.\n6. Watt-Year Rule: The fully burdened cost of a Watt per year in a Warehouse Scale Computer in North\nAmerica in 2011, including the cost of amortizing the power and cooling infrastructure, is about $2.\nExecution timeY Execution timeX\n/\nPerformanceX PerformanceY\n/\n=\nExecution timeold\nExecution timenew\n-------------------------------------------\n1\n1\nFractionenhanced\n\u2013\n#\n)\nFractionenhanced\nSpeedupenhanced\n------------------------------------\n+\n---------------------------------------------------------------------------------------------\nEnergydynamic\n1 2\n/\nCapacitive load\nVoltage2\n\n\nPowerdynamic\n1 2\n/\nCapacitive load\n\nVoltage2\nFrequency switched\n \n\nPowerstatic\nCurrentstatic\nVoltage\n\nDie yield\nWafer yield\n1\n1\nDefects per unit area\nDie area\n\n+\n)\n(\n/\nN\n\n=\n1\nn---\nTimei\ni 1\n=\nn\nWeighti\nTimei\n\ni 1\n=\nn\nn\nTimei\ni\n1\n=\nn\nTotal Facility Power\nIT Equipment Power\n--------------------------------------------------\n(\nN\nN"
    },
    {
        "page": 3,
        "text": "In Praise of Computer Architecture: A Quantitative Approach\nSixth Edition\n\u201cAlthough important concepts of architecture are timeless, this edition has been\nthoroughly updated with the latest technology developments, costs, examples,\nand references. Keeping pace with recent developments in open-sourced architec-\nture, the instruction set architecture used in the book has been updated to use the\nRISC-V ISA.\u201d\n\u2014from the foreword by Norman P. Jouppi, Google\n\u201cComputer Architecture: A Quantitative Approach is a classic that, like fine wine,\njust keeps getting better. I bought my first copy as I finished up my undergraduate\ndegree and it remains one of my most frequently referenced texts today.\u201d\n\u2014James Hamilton, Amazon Web Service\n\u201cHennessy and Patterson wrote the first edition of this book when graduate stu-\ndents built computers with 50,000 transistors. Today, warehouse-size computers\ncontain that many servers, each consisting of dozens of independent processors\nand billions of transistors. The evolution of computer architecture has been rapid\nand relentless, but Computer Architecture: A Quantitative Approach has kept pace,\nwith each edition accurately explaining and analyzing the important emerging\nideas that make this field so exciting.\u201d\n\u2014James Larus, Microsoft Research\n\u201cAnother timely and relevant update to a classic, once again also serving as a win-\ndow into the relentless and exciting evolution of computer architecture! The new\ndiscussions in this edition on the slowing of Moore's law and implications for\nfuture systems are must-reads for both computer architects and practitioners\nworking on broader systems.\u201d\n\u2014Parthasarathy (Partha) Ranganathan, Google\n\u201cI love the \u2018Quantitative Approach\u2019 books because they are written by engineers,\nfor engineers. John Hennessy and Dave Patterson show the limits imposed by\nmathematics and the possibilities enabled by materials science. Then they teach\nthrough real-world examples how architects analyze, measure, and compromise\nto build working systems. This sixth edition comes at a critical time: Moore\u2019s\nLaw is fading just as deep learning demands unprecedented compute cycles.\nThe new chapter on domain-specific architectures documents a number of prom-\nising approaches and prophesies a rebirth in computer architecture. Like the\nscholars of the European Renaissance, computer architects must understand our\nown history, and then combine the lessons of that history with new techniques\nto remake the world.\u201d\n\u2014Cliff Young, Google"
    },
    {
        "page": 4,
        "text": "This page intentionally left blank"
    },
    {
        "page": 5,
        "text": "Computer Architecture\nA Quantitative Approach\nSixth Edition"
    },
    {
        "page": 6,
        "text": "John L. Hennessy is a Professor of Electrical Engineering and Computer Science at Stanford\nUniversity, where he has been a member of the faculty since 1977 and was, from 2000 to\n2016, its 10th President. He currently serves as the Director of the Knight-Hennessy Fellow-\nship, which provides graduate fellowships to potential future leaders. Hennessy is a Fellow of\nthe IEEE and ACM, a member of the National Academy of Engineering, the National Acad-\nemy of Science, and the American Philosophical Society, and a Fellow of the American Acad-\nemy of Arts and Sciences. Among his many awards are the 2001 Eckert-Mauchly Award for\nhis contributions to RISC technology, the 2001 Seymour Cray Computer Engineering Award,\nand the 2000 John von Neumann Award, which he shared with David Patterson. He has also\nreceived 10 honorary doctorates.\nIn 1981, he started the MIPS project at Stanford with a handful of graduate students. After\ncompleting the project in 1984, he took a leave from the university to cofound MIPS Com-\nputer Systems, which developed one of the first commercial RISC microprocessors. As of\n2017, over 5 billion MIPS microprocessors have been shipped in devices ranging from video\ngames and palmtop computers to laser printers and network switches. Hennessy subse-\nquently led the DASH (Director Architecture for Shared Memory) project, which prototyped\nthe first scalable cache coherent multiprocessor; many of the key ideas have been adopted\nin modern multiprocessors. In addition to his technical activities and university responsibil-\nities, he has continued to work with numerous start-ups, both as an early-stage advisor and\nan investor.\nDavid A. Patterson became a Distinguished Engineer at Google in 2016 after 40 years as a\nUC Berkeley professor. He joined UC Berkeley immediately after graduating from UCLA. He\nstill spends a day a week in Berkeley as an Emeritus Professor of Computer Science. His\nteaching has been honored by the Distinguished Teaching Award from the University of\nCalifornia, the Karlstrom Award from ACM, and the Mulligan Education Medal and Under-\ngraduate Teaching Award from IEEE. Patterson received the IEEE Technical Achievement\nAward and the ACM Eckert-Mauchly Award for contributions to RISC, and he shared the IEEE\nJohnson Information Storage Award for contributions to RAID. He also shared the IEEE John\nvon Neumann Medal and the C & C Prize with John Hennessy. Like his co-author, Patterson is\na Fellow of the American Academy of Arts and Sciences, the Computer History Museum,\nACM, and IEEE, and he was elected to the National Academy of Engineering, the National\nAcademy of Sciences, and the Silicon Valley Engineering Hall of Fame. He served on the\nInformation Technology Advisory Committee to the President of the United States, as chair\nof the CS division in the Berkeley EECS department, as chair of the Computing Research\nAssociation, and as President of ACM. This record led to Distinguished Service Awards from\nACM, CRA, and SIGARCH. He is currently Vice-Chair of the Board of Directors of the RISC-V\nFoundation.\nAt Berkeley, Patterson led the design and implementation of RISC I, likely the first VLSI\nreduced instruction set computer, and the foundation of the commercial SPARC architec-\nture. He was a leader of the Redundant Arrays of Inexpensive Disks (RAID) project, which led\nto dependable storage systems from many companies. He was also involved in the Network\nof Workstations (NOW) project, which led to cluster technology used by Internet companies\nand later to cloud computing. His current interests are in designing domain-specific archi-\ntectures for machine learning, spreading the word on the open RISC-V instruction set archi-\ntecture, and in helping the UC Berkeley RISELab (Real-time Intelligent Secure Execution)."
    },
    {
        "page": 7,
        "text": "Computer Architecture\nA Quantitative Approach\nSixth Edition\nJohn L. Hennessy\nStanford University\nDavid A. Patterson\nUniversity of California, Berkeley\nWith Contributions by\nKrste Asanovi\u0001c\nUniversity of California, Berkeley\nJason D. Bakos\nUniversity of South Carolina\nRobert P. Colwell\nR&E Colwell & Assoc. Inc.\nAbhishek Bhattacharjee\nRutgers University\nThomas M. Conte\nGeorgia Tech\nJos\u0001e Duato\nProemisa\nDiana Franklin\nUniversity of Chicago\nDavid Goldberg\neBay\nNorman P. Jouppi\nGoogle\nSheng Li\nIntel Labs\nNaveen Muralimanohar\nHP Labs\nGregory D. Peterson\nUniversity of Tennessee\nTimothy M. Pinkston\nUniversity of Southern California\nParthasarathy Ranganathan\nGoogle\nDavid A. Wood\nUniversity of Wisconsin\u2013Madison\nCliff Young\nGoogle\nAmr Zaky\nUniversity of Santa Clara"
    },
    {
        "page": 8,
        "text": "Morgan Kaufmann is an imprint of Elsevier\n50 Hampshire Street, 5th Floor, Cambridge, MA 02139, United States\n\u00a9 2019 Elsevier Inc. All rights reserved.\nNo part of this publication may be reproduced or transmitted in any form or by any means, electronic or mechanical,\nincluding photocopying, recording, or any information storage and retrieval system, without permission in writing\nfrom the publisher. Details on how to seek permission, further information about the Publisher\u2019s permissions\npolicies and our arrangements with organizations such as the Copyright Clearance Center and the Copyright\nLicensing Agency, can be found at our website: www.elsevier.com/permissions.\nThis book and the individual contributions contained in it are protected under copyright by the Publisher (other than\nas may be noted herein).\nNotices\nKnowledge and best practice in this field are constantly changing. As new research and experience broaden our\nunderstanding, changes in research methods, professional practices, or medical treatment may become necessary.\nPractitioners and researchers must always rely on their own experience and knowledge in evaluating and using any\ninformation, methods, compounds, or experiments described herein. In using such information or methods they\nshould be mindful of their own safety and the safety of others, including parties for whom they have a professional\nresponsibility.\nTo the fullest extent of the law, neither the Publisher nor the authors, contributors, or editors, assume any liability for\nany injury and/or damage to persons or property as a matter of products liability, negligence or otherwise, or from\nany use or operation of any methods, products, instructions, or ideas contained in the material herein.\nLibrary of Congress Cataloging-in-Publication Data\nA catalog record for this book is available from the Library of Congress\nBritish Library Cataloguing-in-Publication Data\nA catalogue record for this book is available from the British Library\nISBN: 978-0-12-811905-1\nFor information on all Morgan Kaufmann publications\nvisit our website at https://www.elsevier.com/books-and-journals\nPublisher: Katey Birtcher\nAcquisition Editor: Stephen Merken\nDevelopmental Editor: Nate McFadden\nProduction Project Manager: Stalin Viswanathan\nCover Designer: Christian J. Bilbow\nTypeset by SPi Global, India"
    },
    {
        "page": 9,
        "text": "To Andrea, Linda, and our four sons"
    },
    {
        "page": 10,
        "text": "This page intentionally left blank"
    },
    {
        "page": 11,
        "text": "Foreword\nby Norman P. Jouppi, Google\nMuch of the improvement in computer performance over the last 40 years has been\nprovided by computer architecture advancements that have leveraged Moore\u2019s\nLaw and Dennard scaling to build larger and more parallel systems. Moore\u2019s\nLaw is the observation that the maximum number of transistors in an integrated\ncircuit doubles approximately every two years. Dennard scaling refers to the reduc-\ntion of MOS supply voltage in concert with the scaling of feature sizes, so that as\ntransistors get smaller, their power density stays roughly constant. With the end of\nDennard scaling a decade ago, and the recent slowdown of Moore\u2019s Law due to a\ncombination of physical limitations and economic factors, the sixth edition of the\npreeminent textbook for our field couldn\u2019t be more timely. Here are some reasons.\nFirst, because domain-specific architectures can provide equivalent perfor-\nmance and power benefits of three or more historical generations of Moore\u2019s\nLaw and Dennard scaling, they now can provide better implementations than\nmay ever be possible with future scaling of general-purpose architectures. And\nwith the diverse application space of computers today, there are many potential\nareas for architectural innovation with domain-specific architectures. Second,\nhigh-quality implementations of open-source architectures now have a much lon-\nger lifetime due to the slowdown in Moore\u2019s Law. This gives them more oppor-\ntunities for continued optimization and refinement, and hence makes them more\nattractive. Third, with the slowing of Moore\u2019s Law, different technology compo-\nnents have been scaling heterogeneously. Furthermore, new technologies such as\n2.5D stacking, new nonvolatile memories, and optical interconnects have been\ndeveloped to provide more than Moore\u2019s Law can supply alone. To use these\nnew technologies and nonhomogeneous scaling effectively, fundamental design\ndecisions need to be reexamined from first principles. Hence it is important for\nstudents, professors, and practitioners in the industry to be skilled in a wide range\nof both old and new architectural techniques. All told, I believe this is the most\nexciting time in computer architecture since the industrial exploitation of\ninstruction-level parallelism in microprocessors 25 years ago.\nThe largest change in this edition is the addition of a new chapter on domain-\nspecific architectures. It\u2019s long been known that customized domain-specific archi-\ntectures can have higher performance, lower power, and require less silicon area\nthan general-purpose processor implementations. However when general-purpose\nix"
    },
    {
        "page": 12,
        "text": "processors were increasing in single-threaded performance by 40% per year (see\nFig. 1.11), the extra time to market required to develop a custom architecture vs.\nusing a leading-edge standard microprocessor could cause the custom architecture\nto lose much of its advantage. In contrast, today single-core performance is\nimproving very slowly, meaning that the benefits of custom architectures will\nnot be made obsolete by general-purpose processors for a very long time, if ever.\nChapter 7 covers several domain-specific architectures. Deep neural networks\nhave very high computation requirements but lower data precision requirements \u2013\nthis combination can benefit significantly from custom architectures. Two example\narchitectures and implementations for deep neural networks are presented: one\noptimized for inference and a second optimized for training. Image processing\nis another example domain; it also has high computation demands and benefits\nfrom lower-precision data types. Furthermore, since it is often found in mobile\ndevices, the power savings from custom architectures are also very valuable.\nFinally, by nature of their reprogrammability, FPGA-based accelerators can be\nused to implement a variety of different domain-specific architectures on a single\ndevice. They also can benefit more irregular applications that are frequently\nupdated, like accelerating internet search.\nAlthough important concepts of architecture are timeless, this edition has been\nthoroughly updated with the latest technology developments, costs, examples, and\nreferences. Keeping pace with recent developments in open-sourced architecture,\nthe instruction set architecture used in the book has been updated to use the\nRISC-V ISA.\nOn a personal note, after enjoying the privilege of working with John as a grad-\nuate student, I am now enjoying the privilege of working with Dave at Google.\nWhat an amazing duo!\nx\n\u25a0\nForeword"
    },
    {
        "page": 13,
        "text": "Contents\nForeword\nix\nPreface\nxvii\nAcknowledgments\nxxv\nChapter 1\nFundamentals of Quantitative Design and Analysis\n1.1\nIntroduction\n2\n1.2\nClasses of Computers\n6\n1.3\nDefining Computer Architecture\n11\n1.4\nTrends in Technology\n18\n1.5\nTrends in Power and Energy in Integrated Circuits\n23\n1.6\nTrends in Cost\n29\n1.7\nDependability\n36\n1.8\nMeasuring, Reporting, and Summarizing Performance\n39\n1.9\nQuantitative Principles of Computer Design\n48\n1.10 Putting It All Together: Performance, Price, and Power\n55\n1.11 Fallacies and Pitfalls\n58\n1.12 Concluding Remarks\n64\n1.13 Historical Perspectives and References\n67\nCase Studies and Exercises by Diana Franklin\n67\nChapter 2\nMemory Hierarchy Design\n2.1\nIntroduction\n78\n2.2\nMemory Technology and Optimizations\n84\n2.3\nTen Advanced Optimizations of Cache Performance\n94\n2.4\nVirtual Memory and Virtual Machines\n118\n2.5\nCross-Cutting Issues: The Design of Memory Hierarchies\n126\n2.6\nPutting It All Together: Memory Hierarchies in the ARM Cortex-A53\nand Intel Core i7 6700\n129\n2.7\nFallacies and Pitfalls\n142\n2.8\nConcluding Remarks: Looking Ahead\n146\n2.9\nHistorical Perspectives and References\n148\nxi"
    },
    {
        "page": 14,
        "text": "Case Studies and Exercises by Norman P. Jouppi, Rajeev\nBalasubramonian, Naveen Muralimanohar, and Sheng Li\n148\nChapter 3\nInstruction-Level Parallelism and Its Exploitation\n3.1\nInstruction-Level Parallelism: Concepts and Challenges\n168\n3.2\nBasic Compiler Techniques for Exposing ILP\n176\n3.3\nReducing Branch Costs With Advanced Branch Prediction\n182\n3.4\nOvercoming Data Hazards With Dynamic Scheduling\n191\n3.5\nDynamic Scheduling: Examples and the Algorithm\n201\n3.6\nHardware-Based Speculation\n208\n3.7\nExploiting ILP Using Multiple Issue and Static Scheduling\n218\n3.8\nExploiting ILP Using Dynamic Scheduling, Multiple Issue, and\nSpeculation\n222\n3.9\nAdvanced Techniques for Instruction Delivery and Speculation\n228\n3.10 Cross-Cutting Issues\n240\n3.11 Multithreading: Exploiting Thread-Level Parallelism to Improve\nUniprocessor Throughput\n242\n3.12 Putting It All Together: The Intel Core i7 6700 and ARM Cortex-A53\n247\n3.13 Fallacies and Pitfalls\n258\n3.14 Concluding Remarks: What\u2019s Ahead?\n264\n3.15 Historical Perspective and References\n266\nCase Studies and Exercises by Jason D. Bakos and Robert P. Colwell\n266\nChapter 4\nData-Level Parallelism in Vector, SIMD, and GPU Architectures\n4.1\nIntroduction\n282\n4.2\nVector Architecture\n283\n4.3\nSIMD Instruction Set Extensions for Multimedia\n304\n4.4\nGraphics Processing Units\n310\n4.5\nDetecting and Enhancing Loop-Level Parallelism\n336\n4.6\nCross-Cutting Issues\n345\n4.7\nPutting It All Together: Embedded Versus Server GPUs and\nTesla Versus Core i7\n346\n4.8\nFallacies and Pitfalls\n353\n4.9\nConcluding Remarks\n357\n4.10 Historical Perspective and References\n357\nCase Study and Exercises by Jason D. Bakos\n357\nChapter 5\nThread-Level Parallelism\n5.1\nIntroduction\n368\n5.2\nCentralized Shared-Memory Architectures\n377\n5.3\nPerformance of Symmetric Shared-Memory Multiprocessors\n393\nxii\n\u25a0\nContents"
    },
    {
        "page": 15,
        "text": "5.4\nDistributed Shared-Memory and Directory-Based Coherence\n404\n5.5\nSynchronization: The Basics\n412\n5.6\nModels of Memory Consistency: An Introduction\n417\n5.7\nCross-Cutting Issues\n422\n5.8\nPutting It All Together: Multicore Processors and Their Performance\n426\n5.9\nFallacies and Pitfalls\n438\n5.10 The Future of Multicore Scaling\n442\n5.11 Concluding Remarks\n444\n5.12 Historical Perspectives and References\n445\nCase Studies and Exercises by Amr Zaky and David A. Wood\n446\nChapter 6\nWarehouse-Scale Computers to Exploit Request-Level\nand Data-Level Parallelism\n6.1\nIntroduction\n466\n6.2\nProgramming Models and Workloads for Warehouse-Scale\nComputers\n471\n6.3\nComputer Architecture of Warehouse-Scale Computers\n477\n6.4\nThe Efficiency and Cost of Warehouse-Scale Computers\n482\n6.5\nCloud Computing: The Return of Utility Computing\n490\n6.6\nCross-Cutting Issues\n501\n6.7\nPutting It All Together: A Google Warehouse-Scale Computer\n503\n6.8\nFallacies and Pitfalls\n514\n6.9\nConcluding Remarks\n518\n6.10 Historical Perspectives and References\n519\nCase Studies and Exercises by Parthasarathy Ranganathan\n519\nChapter 7\nDomain-Specific Architectures\n7.1\nIntroduction\n540\n7.2\nGuidelines for DSAs\n543\n7.3\nExample Domain: Deep Neural Networks\n544\n7.4\nGoogle\u2019s Tensor Processing Unit, an Inference Data\nCenter Accelerator\n557\n7.5\nMicrosoft Catapult, a Flexible Data Center Accelerator\n567\n7.6\nIntel Crest, a Data Center Accelerator for Training\n579\n7.7\nPixel Visual Core, a Personal Mobile Device Image Processing Unit\n579\n7.8\nCross-Cutting Issues\n592\n7.9\nPutting It All Together: CPUs Versus GPUs Versus DNN Accelerators\n595\n7.10 Fallacies and Pitfalls\n602\n7.11 Concluding Remarks\n604\n7.12 Historical Perspectives and References\n606\nCase Studies and Exercises by Cliff Young\n606\nContents\n\u25a0\nxiii"
    },
    {
        "page": 16,
        "text": "Appendix A\nInstruction Set Principles\nA.1\nIntroduction\nA-2\nA.2\nClassifying Instruction Set Architectures\nA-3\nA.3\nMemory Addressing\nA-7\nA.4\nType and Size of Operands\nA-13\nA.5\nOperations in the Instruction Set\nA-15\nA.6\nInstructions for Control Flow\nA-16\nA.7\nEncoding an Instruction Set\nA-21\nA.8\nCross-Cutting Issues: The Role of Compilers\nA-24\nA.9\nPutting It All Together: The RISC-V Architecture\nA-33\nA.10 Fallacies and Pitfalls\nA-42\nA.11 Concluding Remarks\nA-46\nA.12 Historical Perspective and References\nA-47\nExercises by Gregory D. Peterson\nA-47\nAppendix B\nReview of Memory Hierarchy\nB.1\nIntroduction\nB-2\nB.2\nCache Performance\nB-15\nB.3\nSix Basic Cache Optimizations\nB-22\nB.4\nVirtual Memory\nB-40\nB.5\nProtection and Examples of Virtual Memory\nB-49\nB.6\nFallacies and Pitfalls\nB-57\nB.7\nConcluding Remarks\nB-59\nB.8\nHistorical Perspective and References\nB-59\nExercises by Amr Zaky\nB-60\nAppendix C\nPipelining: Basic and Intermediate Concepts\nC.1\nIntroduction\nC-2\nC.2\nThe Major Hurdle of Pipelining\u2014Pipeline Hazards\nC-10\nC.3\nHow Is Pipelining Implemented?\nC-26\nC.4\nWhat Makes Pipelining Hard to Implement?\nC-37\nC.5\nExtending the RISC V Integer Pipeline to Handle Multicycle\nOperations\nC-45\nC.6\nPutting It All Together: The MIPS R4000 Pipeline\nC-55\nC.7\nCross-Cutting Issues\nC-65\nC.8\nFallacies and Pitfalls\nC-70\nC.9\nConcluding Remarks\nC-71\nC.10 Historical Perspective and References\nC-71\nUpdated Exercises by Diana Franklin\nC-71\nxiv\n\u25a0\nContents"
    },
    {
        "page": 17,
        "text": "Online Appendices\nAppendix D\nStorage Systems\nAppendix E\nEmbedded Systems\nby Thomas M. Conte\nAppendix F\nInterconnection Networks\nby Timothy M. Pinkston and Jos\u0001e Duato\nAppendix G\nVector Processors in More Depth\nby Krste Asanovic\nAppendix H\nHardware and Software for VLIW and EPIC\nAppendix I\nLarge-Scale Multiprocessors and Scientific Applications\nAppendix J\nComputer Arithmetic\nby David Goldberg\nAppendix K\nSurvey of Instruction Set Architectures\nAppendix L\nAdvanced Concepts on Address Translation\nby Abhishek Bhattacharjee\nAppendix M\nHistorical Perspectives and References\nReferences\nR-1\nIndex\nI-1\nContents\n\u25a0\nxv"
    },
    {
        "page": 18,
        "text": "This page intentionally left blank"
    },
    {
        "page": 19,
        "text": "Preface\nWhy We Wrote This Book\nThrough six editions of this book, our goal has been to describe the basic principles\nunderlying what will be tomorrow\u2019s technological developments. Our excitement\nabout the opportunities in computer architecture has not abated, and we echo what\nwe said about the field in the first edition: \u201cIt is not a dreary science of paper\nmachines that will never work. No! It\u2019s a discipline of keen intellectual interest,\nrequiring the balance of marketplace forces to cost-performance-power, leading\nto glorious failures and some notable successes.\u201d\nOur primary objective in writing our first book was to change the way people\nlearn and think about computer architecture. We feel this goal is still valid and\nimportant. The field is changing daily and must be studied with real examples\nand measurements on real computers, rather than simply as a collection of defini-\ntions and designs that will never need to be realized. We offer an enthusiastic wel-\ncome to anyone who came along with us in the past, as well as to those who are\njoining us now. Either way, we can promise the same quantitative approach to, and\nanalysis of, real systems.\nAs with earlier versions, we have strived to produce a new edition that will\ncontinue to be as relevant for professional engineers and architects as it is for those\ninvolved in advanced computer architecture and design courses. Like the first edi-\ntion, this edition has a sharp focus on new platforms\u2014personal mobile devices and\nwarehouse-scale\ncomputers\u2014and\nnew\narchitectures\u2014specifically,\ndomain-\nspecific architectures. As much as its predecessors, this edition aims to demystify\ncomputer architecture through an emphasis on cost-performance-energy trade-offs\nand good engineering design. We believe that the field has continued to mature and\nmove toward the rigorous quantitative foundation of long-established scientific\nand engineering disciplines.\nxvii"
    },
    {
        "page": 20,
        "text": "This Edition\nThe ending of Moore\u2019s Law and Dennard scaling is having as profound effect on\ncomputer architecture as did the switch to multicore. We retain the focus on the\nextremes in size of computing, with personal mobile devices (PMDs) such as cell\nphones and tablets as the clients and warehouse-scale computers offering cloud\ncomputing as the server. We also maintain the other theme of parallelism in all\nits forms: data-level parallelism (DLP) in Chapters 1 and 4, instruction-level par-\nallelism (ILP) in Chapter 3, thread-level parallelism in Chapter 5, and request-\nlevel parallelism (RLP) in Chapter 6.\nThe most pervasive change in this edition is switching from MIPS to the RISC-\nV instruction set. We suspect this modern, modular, open instruction set may\nbecome a significant force in the information technology industry. It may become\nas important in computer architecture as Linux is for operating systems.\nThe newcomer in this edition is Chapter 7, which introduces domain-specific\narchitectures with several concrete examples from industry.\nAs before, the first three appendices in the book give basics on the RISC-V\ninstruction set, memory hierarchy, and pipelining for readers who have not read\na book like Computer Organization and Design. To keep costs down but still sup-\nply supplemental material that is of interest to some readers, available online at\nhttps://www.elsevier.com/books-and-journals/book-companion/9780128119051\nare nine more appendices. There are more pages in these appendices than there are\nin this book!\nThis edition continues the tradition of using real-world examples to demonstrate\nthe ideas, and the \u201cPuttingIt All Together\u201d sections are brand new. The\u201cPuttingItAll\nTogether\u201dsectionsofthiseditionincludethepipelineorganizationsandmemoryhier-\narchies of the ARM Cortex A8 processor, the Intel core i7 processor, the NVIDIA\nGTX-280 and GTX-480 GPUs, and one of the Google warehouse-scale computers.\nTopic Selection and Organization\nAs before, we have taken a conservative approach to topic selection, for there are\nmany more interesting ideas in the field than can reasonably be covered in a treat-\nment of basic principles. We have steered away from a comprehensive survey of\nevery architecture a reader might encounter. Instead, our presentation focuses on\ncore concepts likely to be found in any new machine. The key criterion remains\nthat of selecting ideas that have been examined and utilized successfully enough\nto permit their discussion in quantitative terms.\nOur intent has always been to focus on material that is not available in equiv-\nalent form from other sources, so we continue to emphasize advanced content\nwherever possible. Indeed, there are several systems here whose descriptions can-\nnot be found in the literature. (Readers interested strictly in a more basic introduc-\ntion to computer architecture should read Computer Organization and Design: The\nHardware/Software Interface.)\nxviii\n\u25a0\nPreface"
    },
    {
        "page": 21,
        "text": "An Overview of the Content\nChapter 1 includes formulas for energy, static power, dynamic power, integrated cir-\ncuit costs, reliability, and availability. (These formulas are also found on the front\ninside cover.) Our hope is that these topics can be used through the rest of the book.\nIn addition to the classic quantitative principles of computer design and performance\nmeasurement, it shows the slowing of performance improvement of general-purpose\nmicroprocessors, which is one inspiration for domain-specific architectures.\nOur view is that the instruction set architecture is playing less of a role today\nthan in 1990, so we moved this material to Appendix A. It now uses the RISC-V\narchitecture. (For quick review, a summary of the RISC-V ISA can be found on the\nback inside cover.) For fans of ISAs, Appendix K was revised for this edition and\ncovers 8 RISC architectures (5 for desktop and server use and 3 for embedded use),\nthe 80\u000186, the DEC VAX, and the IBM 360/370.\nWe then move onto memory hierarchy in Chapter 2, since it is easy to apply the\ncost-performance-energy principles to this material, and memory is a critical\nresource for the rest of the chapters. As in the past edition, Appendix B contains\nan introductory review of cache principles, which is available in case you need it.\nChapter 2 discusses 10 advanced optimizations of caches. The chapter includes\nvirtual machines, which offer advantages in protection, software management,\nand hardware management, and play an important role in cloud computing. In\naddition to covering SRAM and DRAM technologies, the chapter includes new\nmaterial both on Flash memory and on the use of stacked die packaging for extend-\ning the memory hierarchy. The PIAT examples are the ARM Cortex A8, which is\nused in PMDs, and the Intel Core i7, which is used in servers.\nChapter 3 covers the exploitation of instruction-level parallelism in high-\nperformance processors, including superscalar execution, branch prediction\n(including the new tagged hybrid predictors), speculation, dynamic scheduling,\nand simultaneous multithreading. As mentioned earlier, Appendix C is a review\nof pipelining in case you need it. Chapter 3 also surveys the limits of ILP. Like\nChapter 2, the PIAT examples are again the ARM Cortex A8 and the Intel Core\ni7. While the third edition contained a great deal on Itanium and VLIW, this mate-\nrial is now in Appendix H, indicating our view that this architecture did not live up\nto the earlier claims.\nThe increasing importance of multimedia applications such as games and video\nprocessing has also increased the importance of architectures that can exploit data\nlevel parallelism. In particular, there is a rising interest in computing using graph-\nical processing units (GPUs), yet few architects understand how GPUs really work.\nWe decided to write a new chapter in large part to unveil this new style of computer\narchitecture. Chapter 4 starts with an introduction to vector architectures, which\nacts as a foundation on which to build explanations of multimedia SIMD instruc-\ntion set extensions and GPUs. (Appendix G goes into even more depth on vector\narchitectures.) This chapter introduces the Roofline performance model and then\nuses it to compare the Intel Core i7 and the NVIDIA GTX 280 and GTX 480 GPUs.\nThe chapter also describes the Tegra 2 GPU for PMDs.\nPreface\n\u25a0\nxix"
    },
    {
        "page": 22,
        "text": "Chapter 5 describes multicore processors. It explores symmetric and\ndistributed-memory architectures, examining both organizational principles and\nperformance. The primary additions to this chapter include more comparison of\nmulticore organizations, including the organization of multicore-multilevel\ncaches, multicore coherence schemes, and on-chip multicore interconnect. Topics\nin synchronization and memory consistency models are next. The example is the\nIntel Core i7. Readers interested in more depth on interconnection networks should\nread Appendix F, and those interested in larger scale multiprocessors and scientific\napplications should read Appendix I.\nChapter 6 describes warehouse-scale computers (WSCs). It was extensively\nrevised based on help from engineers at Google and Amazon Web Services. This\nchapter integrates details on design, cost, and performance of WSCs that few archi-\ntects are aware of. It starts with the popular MapReduce programming model\nbefore describing the architecture and physical implementation of WSCs, includ-\ning cost. The costs allow us to explain the emergence of cloud computing, whereby\nit can be cheaper to compute using WSCs in the cloud than in your local datacenter.\nThe PIAT example is a description of a Google WSC that includes information\npublished for the first time in this book.\nThe new Chapter 7 motivates the need for Domain-Specific Architectures\n(DSAs). It draws guiding principles for DSAs based on the four examples of DSAs.\nEachDSA correspondsto chips that have been deployedincommercial settings.We\nalso explain why we expect a renaissance in computer architecture via DSAs given\nthat single-thread performance of general-purpose microprocessors has stalled.\nThis brings us to Appendices A through M. Appendix A covers principles of\nISAs, including RISC-V, and Appendix K describes 64-bit versions of RISC V,\nARM, MIPS, Power, and SPARC and their multimedia extensions. It also includes\nsome classic architectures (80x86, VAX, and IBM 360/370) and popular embed-\nded instruction sets (Thumb-2, microMIPS, and RISC V C). Appendix H is related,\nin that it covers architectures and compilers for VLIW ISAs.\nAs mentioned earlier, Appendix B and Appendix C are tutorials on basic cach-\ning and pipelining concepts. Readers relatively new to caching should read Appen-\ndix B before Chapter 2, and those new to pipelining should read Appendix C before\nChapter 3.\nAppendix D, \u201cStorage Systems,\u201d has an expanded discussion of reliability and\navailability, a tutorial on RAID with a description of RAID 6 schemes, and rarely\nfound failure statistics of real systems. It continues to provide an introduction to\nqueuing theory and I/O performance benchmarks. We evaluate the cost, perfor-\nmance, and reliability of a real cluster: the Internet Archive. The \u201cPutting It All\nTogether\u201d example is the NetApp FAS6000 filer.\nAppendix E, by Thomas M. Conte, consolidates the embedded material in\none place.\nAppendix F, on interconnection networks, is revised by Timothy M. Pinkston\nand Jos\u0001e Duato. Appendix G, written originally by Krste Asanovi\u0001c, includes a\ndescription of vector processors. We think these two appendices are some of\nthe best material we know of on each topic.\nxx\n\u25a0\nPreface"
    },
    {
        "page": 23,
        "text": "Appendix H describes VLIW and EPIC, the architecture of Itanium.\nAppendix I describes parallel processing applications and coherence protocols\nfor larger-scale, shared-memory multiprocessing. Appendix J, by David Goldberg,\ndescribes computer arithmetic.\nAppendix L, by Abhishek Bhattacharjee, is new and discusses advanced tech-\nniques for memory management, focusing on support for virtual machines and\ndesign of address translation for very large address spaces. With the growth in\nclouds processors, these architectural enhancements are becoming more important.\nAppendix M collects the \u201cHistorical Perspective and References\u201d from each\nchapter into a single appendix. It attempts to give proper credit for the ideas in each\nchapter and a sense of the history surrounding the inventions. We like to think of\nthis as presenting the human drama of computer design. It also supplies references\nthat the student of architecture may want to pursue. If you have time, we recom-\nmend reading some of the classic papers in the field that are mentioned in these\nsections. It is both enjoyable and educational to hear the ideas directly from the\ncreators. \u201cHistorical Perspective\u201d was one of the most popular sections of prior\neditions.\nNavigating the Text\nThere is no single best order in which to approach these chapters and appendices,\nexcept that all readers should start with Chapter 1. If you don\u2019t want to read every-\nthing, here are some suggested sequences:\n\u25a0\nMemory Hierarchy: Appendix B, Chapter 2, and Appendices D and M.\n\u25a0\nInstruction-Level Parallelism: Appendix C, Chapter 3, and Appendix H\n\u25a0\nData-Level Parallelism: Chapters 4, 6, and 7, Appendix G\n\u25a0\nThread-Level Parallelism: Chapter 5, Appendices F and I\n\u25a0\nRequest-Level Parallelism: Chapter 6\n\u25a0\nISA: Appendices A and K\nAppendix E can be read at any time, but it might work best if read after the ISA and\ncache sequences. Appendix J can be read whenever arithmetic moves you. You\nshould read the corresponding portion of Appendix M after you complete each\nchapter.\nChapter Structure\nThe material we have selected has been stretched upon a consistent framework that\nis followed in each chapter. We start by explaining the ideas of a chapter. These\nideas are followed by a \u201cCrosscutting Issues\u201d section, a feature that shows how the\nideas covered in one chapter interact with those given in other chapters. This is\nPreface\n\u25a0\nxxi"
    },
    {
        "page": 24,
        "text": "followed by a \u201cPutting It All Together\u201d section that ties these ideas together by\nshowing how they are used in a real machine.\nNext in the sequence is \u201cFallacies and Pitfalls,\u201d which lets readers learn from\nthe mistakes of others. We show examples of common misunderstandings and\narchitectural traps that are difficult to avoid even when you know they are lying\nin wait for you. The \u201cFallacies and Pitfalls\u201d sections is one of the most popular\nsections of the book. Each chapter ends with a \u201cConcluding Remarks\u201d section.\nCase Studies With Exercises\nEach chapter ends with case studies and accompanying exercises. Authored by\nexperts in industry and academia, the case studies explore key chapter concepts\nand verify understanding through increasingly challenging exercises. Instructors\nshould find the case studies sufficiently detailed and robust to allow them to create\ntheir own additional exercises.\nBrackets for each exercise (<chapter.section>) indicate the text sections of\nprimary relevance to completing the exercise. We hope this helps readers to avoid\nexercises for which they haven\u2019t read the corresponding section, in addition to pro-\nviding the source for review. Exercises are rated, to give the reader a sense of the\namount of time required to complete an exercise:\n[10] Less than 5 min (to read and understand)\n[15] 5\u201315 min for a full answer\n[20] 15\u201320 min for a full answer\n[25] 1 h for a full written answer\n[30] Short programming project: less than 1 full day of programming\n[40] Significant programming project: 2 weeks of elapsed time\n[Discussion] Topic for discussion with others\nSolutions to the case studies and exercises are available for instructors who\nregister at textbooks.elsevier.com.\nSupplemental Materials\nA variety of resources are available online at https://www.elsevier.com/books/\ncomputer-architecture/hennessy/978-0-12-811905-1, including the following:\n\u25a0\nReference appendices, some guest authored by subject experts, covering a\nrange of advanced topics\n\u25a0\nHistorical perspectives material that explores the development of the key ideas\npresented in each of the chapters in the text\nxxii\n\u25a0\nPreface"
    },
    {
        "page": 25,
        "text": "\u25a0\nInstructor slides in PowerPoint\n\u25a0\nFigures from the book in PDF, EPS, and PPT formats\n\u25a0\nLinks to related material on the Web\n\u25a0\nList of errata\nNew materials and links to other resources available on the Web will be added\non a regular basis.\nHelping Improve This Book\nFinally, it is possible to make money while reading this book. (Talk about cost per-\nformance!) If you read the Acknowledgments that follow, you will see that we\nwent to great lengths to correct mistakes. Since a book goes through many print-\nings, we have the opportunity to make even more corrections. If you uncover any\nremaining resilient bugs, please contact the publisher by electronic mail\n(ca6bugs@mkp.com).\nWe welcome general comments to the text and invite you to send them to a\nseparate email address at ca6comments@mkp.com.\nConcluding Remarks\nOnce again, this book is a true co-authorship, with each of us writing half the chap-\nters and an equal share of the appendices. We can\u2019t imagine how long it would have\ntaken without someone else doing half the work, offering inspiration when the task\nseemed hopeless, providing the key insight to explain a difficult concept, supply-\ning over-the-weekend reviews of chapters, and commiserating when the weight of\nour other obligations made it hard to pick up the pen.\nThus, once again, we share equally the blame for what you are about to read.\nJohn Hennessy\n\u25a0David Patterson\nPreface\n\u25a0\nxxiii"
    },
    {
        "page": 26,
        "text": "This page intentionally left blank"
    },
    {
        "page": 27,
        "text": "Acknowledgments\nAlthough this is only the sixth edition of this book, we have actually created ten\ndifferent versions of the text: three versions of the first edition (alpha, beta, and\nfinal) and two versions of the second, third, and fourth editions (beta and final).\nAlong the way, we have received help from hundreds of reviewers and users. Each\nof these people has helped make this book better. Thus, we have chosen to list all of\nthe people who have made contributions to some version of this book.\nContributors to the Sixth Edition\nLike prior editions, this is a community effort that involves scores of volunteers.\nWithout their help, this edition would not be nearly as polished.\nReviewers\nJason D. Bakos, University of South Carolina; Rajeev Balasubramonian, Univer-\nsity of Utah; Jose Delgado-Frias, Washington State University; Diana Franklin,\nThe University of Chicago; Norman P. Jouppi, Google; Hugh C. Lauer, Worcester\nPolytechnic Institute; Gregory Peterson, University of Tennessee; Bill Pierce,\nHood College; Parthasarathy Ranganathan, Google; William H. Robinson, Van-\nderbilt University; Pat Stakem, Johns Hopkins University; Cliff Young, Google;\nAmr Zaky, University of Santa Clara; Gerald Zarnett, Ryerson University;\nHuiyang Zhou, North Carolina State University.\nMembers of the University of California-Berkeley Par Lab and RAD Lab who gave\nfrequent reviews of Chapters 1, 4, and 6 and shaped the explanation of GPUs and\nWSCs: Krste Asanovi\u0001c, Michael Armbrust, Scott Beamer, Sarah Bird, Bryan Catan-\nzaro, Jike Chong, Henry Cook, Derrick Coetzee, Randy Katz, Yunsup Lee, Leo\nMeyervich, Mark Murphy, Zhangxi Tan, Vasily Volkov, and Andrew Waterman.\nAppendices\nKrste Asanovi\u0001c, University of California, Berkeley (Appendix G); Abhishek\nBhattacharjee, Rutgers University (Appendix L); Thomas M. Conte, North Caro-\nlina State University (Appendix E); Jos\u0001e Duato, Universitat Polit\u00e8cnica de\nxxv"
    },
    {
        "page": 28,
        "text": "Val\u00e8ncia and Simula (Appendix F); David Goldberg, Xerox PARC (Appendix J);\nTimothy M. Pinkston, University of Southern California (Appendix F).\nJos\u0001e Flich of the Universidad Polit\u0001ecnica de Valencia provided significant contri-\nbutions to the updating of Appendix F.\nCase Studies With Exercises\nJason D. Bakos, University of South Carolina (Chapters 3 and 4); Rajeev Balasu-\nbramonian, University of Utah (Chapter 2); Diana Franklin, The University of\nChicago (Chapter 1 and Appendix C); Norman P. Jouppi, Google, (Chapter 2);\nNaveen Muralimanohar, HP Labs (Chapter 2); Gregory Peterson, University of\nTennessee (Appendix A); Parthasarathy Ranganathan, Google (Chapter 6); Cliff\nYoung, Google (Chapter 7); Amr Zaky, University of Santa Clara (Chapter 5\nand Appendix B).\nJichuan Chang, Junwhan Ahn, Rama Govindaraju, and Milad Hashemi assisted in\nthe development and testing of the case studies and exercises for Chapter 6.\nAdditional Material\nJohn Nickolls, Steve Keckler, and Michael Toksvig of NVIDIA (Chapter 4 NVI-\nDIA GPUs); Victor Lee, Intel (Chapter 4 comparison of Core i7 and GPU); John\nShalf, LBNL (Chapter 4 recent vector architectures); Sam Williams, LBNL (Roof-\nline model for computers in Chapter 4); Steve Blackburn of Australian National\nUniversity and Kathryn McKinley of University of Texas at Austin (Intel perfor-\nmance and power measurements in Chapter 5); Luiz Barroso, Urs H\u20acolzle, Jimmy\nClidaris, Bob Felderman, and Chris Johnson of Google (the Google WSC in\nChapter 6); James Hamilton of Amazon Web Services (power distribution and cost\nmodel in Chapter 6).\nJason D. Bakos of the University of South Carolina updated the lecture slides\nfor this edition.\nThis book could not have been published without a publisher, of course. We\nwish to thank all the Morgan Kaufmann/Elsevier staff for their efforts and support.\nFor this fifth edition, we particularly want to thank our editors Nate McFadden and\nSteve Merken, who coordinated surveys, development of the case studies and exer-\ncises, manuscript reviews, and the updating of the appendices.\nWe must also thank our university staff, Margaret Rowland and Roxana\nInfante, for countless express mailings, as well as for holding down the fort at Stan-\nford and Berkeley while we worked on the book.\nOur final thanks go to our wives for their suffering through increasingly early\nmornings of reading, thinking, and writing.\nxxvi\n\u25a0\nAcknowledgments"
    },
    {
        "page": 29,
        "text": "Contributors to Previous Editions\nReviewers\nGeorge Adams, Purdue University; Sarita Adve, University of Illinois at Urbana-\nChampaign; Jim Archibald, Brigham Young University; Krste Asanovi\u0001c, Massa-\nchusetts Institute of Technology; Jean-Loup Baer, University of Washington; Paul\nBarr, Northeastern University; Rajendra V. Boppana, University of Texas, San\nAntonio; Mark Brehob, University of Michigan; Doug Burger, University of\nTexas, Austin; John Burger, SGI; Michael Butler; Thomas Casavant; Rohit Chan-\ndra; Peter Chen, University of Michigan; the classes at SUNY Stony Brook, Car-\nnegie\nMellon,\nStanford,\nClemson,\nand\nWisconsin;\nTim\nCoe,\nVitesse\nSemiconductor; Robert P. Colwell; David Cummings; Bill Dally; David Douglas;\nJos\u0001e Duato, Universitat Polit\u00e8cnica de Val\u00e8ncia and Simula; Anthony Duben,\nSoutheast Missouri State University; Susan Eggers, University of Washington;\nJoel Emer; Barry Fagin, Dartmouth; Joel Ferguson, University of California, Santa\nCruz; Carl Feynman; David Filo; Josh Fisher, Hewlett-Packard Laboratories; Rob\nFowler, DIKU; Mark Franklin, Washington University (St. Louis); Kourosh Ghar-\nachorloo; Nikolas Gloy, Harvard University; David Goldberg, Xerox Palo Alto\nResearch Center; Antonio Gonz\u00e1lez, Intel and Universitat Polit\u00e8cnica de Catalu-\nnya; James Goodman, University of Wisconsin-Madison; Sudhanva Gurumurthi,\nUniversity of Virginia; David Harris, Harvey Mudd College; John Heinlein; Mark\nHeinrich, Stanford; Daniel Helman, University of California, Santa Cruz; Mark D.\nHill, University of Wisconsin-Madison; Martin Hopkins, IBM; Jerry Huck,\nHewlett-Packard Laboratories; Wen-mei Hwu, University of Illinois at Urbana-\nChampaign; Mary Jane Irwin, Pennsylvania State University; Truman Joe; Norm\nJouppi; David Kaeli, Northeastern University; Roger Kieckhafer, University of\nNebraska; Lev G. Kirischian, Ryerson University; Earl Killian; Allan Knies, Pur-\ndue University; Don Knuth; Jeff Kuskin, Stanford; James R. Larus, Microsoft\nResearch; Corinna Lee, University of Toronto; Hank Levy; Kai Li, Princeton Uni-\nversity; Lori Liebrock, University of Alaska, Fairbanks; Mikko Lipasti, University\nof Wisconsin-Madison; Gyula A. Mago, University of North Carolina, Chapel\nHill; Bryan Martin; Norman Matloff; David Meyer; William Michalson, Worcester\nPolytechnic Institute; James Mooney; Trevor Mudge, University of Michigan;\nRamadass Nagarajan, University of Texas at Austin; David Nagle, Carnegie Mel-\nlon University; Todd Narter; Victor Nelson; Vojin Oklobdzija, University of Cal-\nifornia,\nBerkeley;\nKunle\nOlukotun,\nStanford\nUniversity;\nBob\nOwens,\nPennsylvania State University; Greg Papadapoulous, Sun Microsystems; Joseph\nPfeiffer; Keshav Pingali, Cornell University; Timothy M. Pinkston, University\nof Southern California; Bruno Preiss, University of Waterloo; Steven Przybylski;\nJim Quinlan; Andras Radics; Kishore Ramachandran, Georgia Institute of Tech-\nnology; Joseph Rameh, University of Texas, Austin; Anthony Reeves, Cornell\nUniversity; Richard Reid, Michigan State University; Steve Reinhardt, University\nof Michigan; David Rennels, University of California, Los Angeles; Arnold L.\nRosenberg, University of Massachusetts, Amherst; Kaushik Roy, Purdue\nAcknowledgments\n\u25a0\nxxvii"
    },
    {
        "page": 30,
        "text": "University; Emilio Salgueiro, Unysis; Karthikeyan Sankaralingam, University of\nTexas at Austin; Peter Schnorf; Margo Seltzer; Behrooz Shirazi, Southern Meth-\nodist University; Daniel Siewiorek, Carnegie Mellon University; J. P. Singh, Prin-\nceton; Ashok Singhal; Jim Smith, University of Wisconsin-Madison; Mike Smith,\nHarvard University; Mark Smotherman, Clemson University; Gurindar Sohi, Uni-\nversity of Wisconsin-Madison; Arun Somani, University of Washington; Gene\nTagliarin, Clemson University; Shyamkumar Thoziyoor, University of Notre\nDame; Evan Tick, University of Oregon; Akhilesh Tyagi, University of North Car-\nolina, Chapel Hill; Dan Upton, University of Virginia; Mateo Valero, Universidad\nPolit\u0001ecnica de Catalu\u00f1a, Barcelona; Anujan Varma, University of California, Santa\nCruz; Thorsten von Eicken, Cornell University; Hank Walker, Texas A&M; Roy\nWant, Xerox Palo Alto Research Center; David Weaver, Sun Microsystems;\nShlomo Weiss, Tel Aviv University; David Wells; Mike Westall, Clemson Univer-\nsity; Maurice Wilkes; Eric Williams; Thomas Willis, Purdue University; Malcolm\nWing; Larry Wittie, SUNY Stony Brook; Ellen Witte Zegura, Georgia Institute of\nTechnology; Sotirios G. Ziavras, New Jersey Institute of Technology.\nAppendices\nThe vector appendix was revised by Krste Asanovi\u0001c of the Massachusetts Institute\nof Technology. The floating-point appendix was written originally by David Gold-\nberg of Xerox PARC.\nExercises\nGeorge Adams, Purdue University; Todd M. Bezenek, University of Wisconsin-\nMadison (in remembrance of his grandmother Ethel Eshom); Susan Eggers;\nAnoop Gupta; David Hayes; Mark Hill; Allan Knies; Ethan L. Miller, University\nof California, Santa Cruz; Parthasarathy Ranganathan, Compaq Western Research\nLaboratory; Brandon Schwartz, University of Wisconsin-Madison; Michael Scott;\nDan Siewiorek; Mike Smith; Mark Smotherman; Evan Tick; Thomas Willis.\nCase Studies With Exercises\nAndrea C. Arpaci-Dusseau, University of Wisconsin-Madison; Remzi H. Arpaci-\nDusseau, University of Wisconsin-Madison; Robert P. Colwell, R&E Colwell &\nAssoc., Inc.; Diana Franklin, California Polytechnic State University, San Luis\nObispo; Wen-mei W. Hwu, University of Illinois at Urbana-Champaign; Norman\nP. Jouppi, HP Labs; John W. Sias, University of Illinois at Urbana-Champaign;\nDavid A. Wood, University of Wisconsin-Madison.\nSpecial Thanks\nDuane Adams, Defense Advanced Research Projects Agency; Tom Adams; Sarita\nAdve, University of Illinois at Urbana-Champaign; Anant Agarwal; Dave\nxxviii\n\u25a0\nAcknowledgments"
    },
    {
        "page": 31,
        "text": "Albonesi, University of Rochester; Mitch Alsup; Howard Alt; Dave Anderson;\nPeter Ashenden; David Bailey; Bill Bandy, Defense Advanced Research Projects\nAgency; Luiz Barroso, Compaq\u2019s Western Research Lab; Andy Bechtolsheim; C.\nGordon Bell; Fred Berkowitz; John Best, IBM; Dileep Bhandarkar; Jeff Bier,\nBDTI; Mark Birman; David Black; David Boggs; Jim Brady; Forrest Brewer;\nAaron Brown, University of California, Berkeley; E. Bugnion, Compaq\u2019s Western\nResearch Lab; Alper Buyuktosunoglu, University of Rochester; Mark Callaghan;\nJason F. Cantin; Paul Carrick; Chen-Chung Chang; Lei Chen, University of Roch-\nester; Pete Chen; Nhan Chu; Doug Clark, Princeton University; Bob Cmelik; John\nCrawford; Zarka Cvetanovic; Mike Dahlin, University of Texas, Austin; Merrick\nDarley; the staff of the DEC Western Research Laboratory; John DeRosa; Lloyd\nDickman; J. Ding; Susan Eggers, University of Washington; Wael El-Essawy,\nUniversity of Rochester; Patty Enriquez, Mills; Milos Ercegovac; Robert Garner;\nK. Gharachorloo, Compaq\u2019s Western Research Lab; Garth Gibson; Ronald Green-\nberg; Ben Hao; John Henning, Compaq; Mark Hill, University of Wisconsin-\nMadison; Danny Hillis; David Hodges; Urs H\u20acolzle, Google; David Hough; Ed\nHudson; Chris Hughes, University of Illinois at Urbana-Champaign; Mark John-\nson; Lewis Jordan; Norm Jouppi; William Kahan; Randy Katz; Ed Kelly; Richard\nKessler; Les Kohn; John Kowaleski, Compaq Computer Corp; Dan Lambright;\nGary Lauterbach, Sun Microsystems; Corinna Lee; Ruby Lee; Don Lewine;\nChao-Huang Lin; Paul Losleben, Defense Advanced Research Projects Agency;\nYung-Hsiang Lu; Bob Lucas, Defense Advanced Research Projects Agency;\nKen Lutz; Alan Mainwaring, Intel Berkeley Research Labs; Al Marston; Rich\nMartin, Rutgers; John Mashey; Luke McDowell; Sebastian Mirolo, Trimedia Cor-\nporation; Ravi Murthy; Biswadeep Nag; Lisa Noordergraaf, Sun Microsystems;\nBob Parker, Defense Advanced Research Projects Agency; Vern Paxson, Center\nfor Internet Research; Lawrence Prince; Steven Przybylski; Mark Pullen, Defense\nAdvanced Research Projects Agency; Chris Rowen; Margaret Rowland; Greg\nSemeraro, University of Rochester; Bill Shannon; Behrooz Shirazi; Robert Shom-\nler; Jim Slager; Mark Smotherman, Clemson University; the SMT research group\nat the University of Washington; Steve Squires, Defense Advanced Research Pro-\njects Agency; Ajay Sreekanth; Darren Staples; Charles Stapper; Jorge Stolfi; Peter\nStoll; the students at Stanford and Berkeley who endured our first attempts at cre-\nating this book; Bob Supnik; Steve Swanson; Paul Taysom; Shreekant Thakkar;\nAlexander Thomasian, New Jersey Institute of Technology; John Toole, Defense\nAdvanced Research Projects Agency; Kees A. Vissers, Trimedia Corporation;\nWilla Walker; David Weaver; Ric Wheeler, EMC; Maurice Wilkes; Richard\nZimmerman.\nJohn Hennessy\n\u25a0David Patterson\nAcknowledgments\n\u25a0\nxxix"
    },
    {
        "page": 32,
        "text": "1.1\nIntroduction\n2\n1.2\nClasses of Computers\n6\n1.3\nDefining Computer Architecture\n11\n1.4\nTrends in Technology\n18\n1.5\nTrends in Power and Energy in Integrated Circuits\n23\n1.6\nTrends in Cost\n29\n1.7\nDependability\n36\n1.8\nMeasuring, Reporting, and Summarizing Performance\n39\n1.9\nQuantitative Principles of Computer Design\n48\n1.10\nPutting It All Together: Performance, Price, and Power\n55\n1.11\nFallacies and Pitfalls\n58\n1.12\nConcluding Remarks\n64\n1.13\nHistorical Perspectives and References\n67\nCase Studies and Exercises by Diana Franklin\n67"
    },
    {
        "page": 33,
        "text": "1\nFundamentals of Quantitative\nDesign and Analysis\nAn iPod, a phone, an Internet mobile communicator\u2026 these are\nNOT three separate devices! And we are calling it iPhone! Today\nApple is going to reinvent the phone. And here it is.\nSteve Jobs, January 9, 2007\nNew information and communications technologies, in particular\nhigh-speed Internet, are changing the way companies do business,\ntransforming public service delivery and democratizing innovation.\nWith 10 percent increase in high speed Internet connections,\neconomic growth increases by 1.3 percent.\nThe World Bank, July 28, 2009\nComputer Architecture. https://doi.org/10.1016/B978-0-12-811905-1.00001-8\n\u00a9 2019 Elsevier Inc. All rights reserved."
    },
    {
        "page": 34,
        "text": "1.1\nIntroduction\nComputer technology has made incredible progress in the roughly 70 years since\nthe first general-purpose electronic computer was created. Today, less than $500\nwill purchase a cell phone that has as much performance as the world\u2019s fastest\ncomputer bought in 1993 for $50 million. This rapid improvement has come both\nfrom advances in the technology used to build computers and from innovations in\ncomputer design.\nAlthough technological improvements historically have been fairly steady,\nprogress arising from better computer architectures has been much less consistent.\nDuring the first 25 years of electronic computers, both forces made a major con-\ntribution, delivering performance improvement of about 25% per year. The late\n1970s saw the emergence of the microprocessor. The ability of the microprocessor\nto ride the improvements in integrated circuit technology led to a higher rate of\nperformance improvement\u2014roughly 35% growth per year.\nThis growth rate, combined with the cost advantages of a mass-produced\nmicroprocessor, led to an increasing fraction of the computer business being based\non microprocessors. In addition, two significant changes in the computer market-\nplace made it easier than ever before to succeed commercially with a new archi-\ntecture. First, the virtual elimination of assembly language programming reduced\nthe need for object-code compatibility. Second, the creation of standardized,\nvendor-independent operating systems, such as UNIX and its clone, Linux, low-\nered the cost and risk of bringing out a new architecture.\nThese changes made it possible to develop successfully a new set of architec-\ntures with simpler instructions, called RISC (Reduced Instruction Set Computer)\narchitectures, in the early 1980s. The RISC-based machines focused the attention\nof designers on two critical performance techniques, the exploitation of instruc-\ntion-level parallelism (initially through pipelining and later through multiple\ninstruction issue) and the use of caches (initially in simple forms and later using\nmore sophisticated organizations and optimizations).\nThe RISC-based computers raised the performance bar, forcing prior architec-\ntures to keep up or disappear. The Digital Equipment Vax could not, and so it was\nreplaced by a RISC architecture. Intel rose to the challenge, primarily by translat-\ning 80x86 instructions into RISC-like instructions internally, allowing it to adopt\nmany of the innovations first pioneered in the RISC designs. As transistor counts\nsoared in the late 1990s, the hardware overhead of translating the more complex\nx86 architecture became negligible. In low-end applications, such as cell phones,\nthe cost in power and silicon area of the x86-translation overhead helped lead to a\nRISC architecture, ARM, becoming dominant.\nFigure 1.1 shows that the combination of architectural and organizational\nenhancements led to 17 years of sustained growth in performance at an annual rate\nof over 50%\u2014a rate that is unprecedented in the computer industry.\nThe effect of this dramatic growth rate during the 20th century was fourfold.\nFirst, it has significantly enhanced the capability available to computer users. For\nmany applications, the highest-performance microprocessors outperformed the\nsupercomputer of less than 20 years earlier.\n2\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 35,
        "text": "1\n5\n9\n13\n18\n24\n51\n80\n117\n183\n280\n481\n649\n993\n1,267\n1,779\n3,016\n4,195\n6,043\n6,681 7,108\n11,86514,387 19,484\n21,871\n24,129\n31,999\n34,967\n39,419\n40,967\n49,935\n49,935\n49,870\n1\n10\n100\n1000\n10,000\n100,000\n1978 1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 2008 2010 2012 2014 2016 2018\nPerformance (vs. VAX-11/780)\n25%/year\n52%/year\n23%/year\n12%/year\n3.5%/year\n IBM POWERstation 100, 150 MHz\nDigital Alphastation 4/266, 266 MHz\nDigital Alphastation 5/300, 300 MHz\nDigital Alphastation 5/500, 500 MHz \nAlphaServer 4000 5/600, 600 MHz 21164\nDigital AlphaServer 8400 6/575, 575 MHz 21264\nProfessional Workstation XP1000, 667 MHz 21264A\nIntel VC820 motherboard, 1.0 GHz Pentium III processor\n IBM Power4, 1.3 GHz\n Intel Xeon EE 3.2 GHz\n AMD Athlon, 2.6 GHz\n Intel Core 2 Extreme 2 cores, 2.9 GHz \n Intel Core Duo Extreme 2 cores, 3.0 GHz\n Intel Core i7 Extreme 4 cores 3.2 GHz (boost to 3.5 GHz)\n Intel Core i7 4 cores 3.4 GHz (boost to 3.8 GHz)\nIntel Xeon 4 cores 3.6 GHz (Boost to 4.0 GHz)\nIntel Xeon 4 cores 3.6 GHz (Boost to 4.0 GHz)\nIntel Xeon 4 cores 3.7 GHz (Boost to 4.1 GHz)\nIntel Core i7 4 cores 4.0 GHz (Boost to 4.2 GHz)\nIntel Core i7 4 cores 4.0 GHz (Boost to 4.2 GHz)\nIntel Core i7 4 cores 4.2 GHz (Boost to 4.5 GHz)\n Intel Xeon 4 cores, 3.3 GHz (boost to 3.6 GHz)\n Intel Xeon 6 cores, 3.3 GHz (boost to 3.6 GHz)\nIntel D850EMVR motherboard (3.06 GHz, Pentium 4 processor with Hyper-Threading Technology)\n AMD Athlon 64, 2.8 GHz\nDigital 3000 AXP/500, 150 MHz\nHP 9000/750, 66 MHz\nIBM RS6000/540, 30 MHz\nMIPS M2000, 25 MHz \nMIPS M/120, 16.7 MHz\nSun-4/260, 16.7 MHz\nVAX 8700, 22 MHz\nAX-11/780, 5 MHz\nFigure 1.1 Growth in processor performance over 40 years. This chart plots program performance relative to the VAX 11/780 as measured by\nthe SPEC integer benchmarks (see Section 1.8). Prior to the mid-1980s, growth in processor performance was largely technology-driven and\naveraged about 22% per year, or doubling performance every 3.5 years. The increase in growth to about 52% starting in 1986, or doubling every\n2 years, is attributable to more advanced architectural and organizational ideas typified in RISC architectures. By 2003 this growth led to a dif-\nference in performance of an approximate factor of 25 versus the performance that would have occurred if it had continued at the 22% rate. In\n2003 the limits of power due to the end of Dennard scaling and the available instruction-level parallelism slowed uniprocessor performance to\n23% per year until 2011, or doubling every 3.5 years. (The fastest SPECintbase performance since 2007 has had automatic parallelization turned\non, so uniprocessor speed is harder to gauge. These results are limited to single-chip systems with usually four cores per chip.) From 2011 to 2015,\nthe annual improvement was less than 12%, or doubling every 8 years in part due to the limits of parallelism of Amdahl\u2019s Law. Since 2015, with the\nend of Moore\u2019s Law, improvement has been just 3.5% per year, or doubling every 20 years! Performance for floating-point-oriented calculations\nfollows the same trends, but typically has 1% to 2% higher annual growth in each shaded region. Figure 1.11 on page 27 shows the improvement\nin clock rates for these same eras. Because SPEC has changed over the years, performance of newer machines is estimated by a scaling factor that\nrelates the performance for different versions of SPEC: SPEC89, SPEC92, SPEC95, SPEC2000, and SPEC2006. There are too few results for SPEC2017\nto plot yet.\n1.1\nIntroduction\n\u25a0\n3"
    },
    {
        "page": 36,
        "text": "Second, this dramatic improvement in cost-performance led to new classes of\ncomputers. Personal computers and workstations emerged in the 1980s with the\navailability of the microprocessor. The past decade saw the rise of smart cell\nphones and tablet computers, which many people are using as their primary com-\nputing platforms instead of PCs. These mobile client devices are increasingly using\nthe Internet to access warehouses containing 100,000 servers, which are being\ndesigned as if they were a single gigantic computer.\nThird, improvement of semiconductor manufacturing as predicted by Moore\u2019s\nlaw has led to the dominance of microprocessor-based computers across the entire\nrange of computer design. Minicomputers, which were traditionally made from\noff-the-shelf logic or from gate arrays, were replaced by servers made by using\nmicroprocessors. Even mainframe computers and high-performance supercom-\nputers are all collections of microprocessors.\nThe preceding hardware innovations led to a renaissance in computer design,\nwhich emphasized both architectural innovation and efficient use of technology\nimprovements. This rate of growth compounded so that by 2003, high-\nperformance microprocessors were 7.5 times as fast as what would have been\nobtained by relying solely on technology, including improved circuit design, that\nis, 52% per year versus 35% per year.\nThis hardware renaissance led to the fourth impact, which was on software\ndevelopment. This 50,000-fold performance improvement since 1978 (see\nFigure 1.1) allowed modern programmers to trade performance for productivity.\nIn place of performance-oriented languages like C and C++, much more program-\nming today is done in managed programming languages like Java and Scala. More-\nover, scripting languages like JavaScript and Python, which are even more\nproductive, are gaining in popularity along with programming frameworks like\nAngularJS and Django. To maintain productivity and try to close the performance\ngap, interpreters with just-in-time compilers and trace-based compiling are repla-\ncing the traditional compiler and linker of the past. Software deployment is chang-\ning as well, with Software as a Service (SaaS) used over the Internet replacing\nshrink-wrapped software that must be installed and run on a local computer.\nThe nature of applications is also changing. Speech, sound, images, and video\nare becoming increasingly important, along with predictable response time that is\nso critical to the user experience. An inspiring example is Google Translate. This\napplication lets you hold up your cell phone to point its camera at an object, and the\nimage is sent wirelessly over the Internet to a warehouse-scale computer (WSC)\nthat recognizes the text in the photo and translates it into your native language.\nYou can also speak into it, and it will translate what you said into audio output\nin another language. It translates text in 90 languages and voice in 15 languages.\nAlas, Figure 1.1 also shows that this 17-year hardware renaissance is over. The\nfundamental reason is that two characteristics of semiconductor processes that\nwere true for decades no longer hold.\nIn 1974 Robert Dennard observed that power density was constant for a given\narea of silicon even as you increased the number of transistors because of smaller\ndimensions of each transistor. Remarkably, transistors could go faster but use less\n4\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 37,
        "text": "power. Dennard scaling ended around 2004 because current and voltage couldn\u2019t\nkeep dropping and still maintain the dependability of integrated circuits.\nThis change forced the microprocessor industry to use multiple efficient pro-\ncessors or cores instead of a single inefficient processor. Indeed, in 2004 Intel can-\nceled its high-performance uniprocessor projects and joined others in declaring\nthat the road to higher performance would be via multiple processors per chip\nrather than via faster uniprocessors. This milestone signaled a historic switch from\nrelying solely on instruction-level parallelism (ILP), the primary focus of the first\nthree editions of this book, to data-level parallelism (DLP) and thread-level par-\nallelism (TLP), which were featured in the fourth edition and expanded in the fifth\nedition. The fifth edition also added WSCs and request-level parallelism (RLP),\nwhich is expanded in this edition. Whereas the compiler and hardware conspire\nto exploit ILP implicitly without the programmer\u2019s attention, DLP, TLP, and\nRLP are explicitly parallel, requiring the restructuring of the application so that\nit can exploit explicit parallelism. In some instances, this is easy; in many, it is\na major new burden for programmers.\nAmdahl\u2019s Law (Section 1.9) prescribes practical limits to the number of useful\ncores per chip. If 10% of the task is serial, then the maximum performance benefit\nfrom parallelism is 10 no matter how many cores you put on the chip.\nThe second observation that ended recently is Moore\u2019s Law. In 1965 Gordon\nMoore famously predicted that the number of transistors per chip would double\nevery year, which was amended in 1975 to every two years. That prediction lasted\nfor about 50 years, but no longer holds. For example, in the 2010 edition of this\nbook, the most recent Intel microprocessor had 1,170,000,000 transistors. If\nMoore\u2019s Law had continued, we could have expected microprocessors in 2016\nto have 18,720,000,000 transistors. Instead, the equivalent Intel microprocessor\nhas just 1,750,000,000 transistors, or off by a factor of 10 from what Moore\u2019s\nLaw would have predicted.\nThe combination of\n\u25a0\ntransistors no longer getting much better because of the slowing of Moore\u2019s\nLaw and the end of Dinnard scaling,\n\u25a0\nthe unchanging power budgets for microprocessors,\n\u25a0\nthe replacement of the single power-hungry processor with several energy-\nefficient processors, and\n\u25a0\nthe limits to multiprocessing to achieve Amdahl\u2019s Law\ncaused improvements in processor performance to slow down, that is, to double\nevery 20 years, rather than every 1.5 years as it did between 1986 and 2003\n(see Figure 1.1).\nThe only path left to improve energy-performance-cost is specialization. Future\nmicroprocessors will include several domain-specific cores that perform only one\nclass of computations well, but they do so remarkably better than general-purpose\ncores. The new Chapter 7 in this edition introduces domain-specific architectures.\n1.1\nIntroduction\n\u25a0\n5"
    },
    {
        "page": 38,
        "text": "This text is about the architectural ideas and accompanying compiler improve-\nments that made the incredible growth rate possible over the past century, the rea-\nsons for the dramatic change, and the challenges and initial promising approaches\nto architectural ideas, compilers, and interpreters for the 21st century. At the core is\na quantitative approach to computer design and analysis that uses empirical obser-\nvations of programs, experimentation, and simulation as its tools. It is this style and\napproach to computer design that is reflected in this text. The purpose of this chap-\nter is to lay the quantitative foundation on which the following chapters and appen-\ndices are based.\nThis book was written not only to explain this design style but also to stimulate\nyou to contribute to this progress. We believe this approach will serve the computers\nof the future just as it worked for the implicitly parallel computers of the past.\n1.2\nClasses of Computers\nThese changes have set the stage for a dramatic change in how we view computing,\ncomputing applications, and the computer markets in this new century. Not since\nthe creation of the personal computer have we seen such striking changes in the way\ncomputers appear and in how they are used. These changes in computer use have led\nto five diverse computing markets, each characterized by different applications,\nrequirements, and computing technologies. Figure 1.2 summarizes these main-\nstream classes of computing environments and their important characteristics.\nInternet of Things/Embedded Computers\nEmbedded computers are found in everyday machines: microwaves, washing\nmachines, most printers, networking switches, and all automobiles. The phrase\nFeature\nPersonal\nmobile device\n(PMD)\nDesktop\nServer\nClusters/warehouse-\nscale computer\nInternet of\nthings/\nembedded\nPrice of system\n$100\u2013$1000\n$300\u2013$2500\n$5000\u2013$10,000,000\n$100,000\u2013$200,000,000\n$10\u2013$100,000\nPrice of\nmicroprocessor\n$10\u2013$100\n$50\u2013$500\n$200\u2013$2000\n$50\u2013$250\n$0.01\u2013$100\nCritical system\ndesign issues\nCost, energy,\nmedia\nperformance,\nresponsiveness\nPrice-\nperformance,\nenergy, graphics\nperformance\nThroughput,\navailability,\nscalability, energy\nPrice-performance,\nthroughput, energy\nproportionality\nPrice, energy,\napplication-\nspecific\nperformance\nFigure 1.2 A summary of the five mainstream computing classes and their system characteristics. Sales in 2015\nincluded about 1.6 billion PMDs (90% cell phones), 275 million desktop PCs, and 15 million servers. The total number\nof embedded processors sold was nearly 19 billion. In total, 14.8 billion ARM-technology-based chips were shipped in\n2015. Note the wide range in system price for servers and embedded systems, which go from USB keys to network\nrouters. For servers, this range arises from the need for very large-scale multiprocessor systems for high-end trans-\naction processing.\n6\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 39,
        "text": "Internet of Things (IoT) refers to embedded computers that are connected to the\nInternet, typically wirelessly. When augmented with sensors and actuators, IoT\ndevices collect useful data and interact with the physical world, leading to a wide\nvariety of \u201csmart\u201d applications, such as smart watches, smart thermostats, smart\nspeakers, smart cars, smart homes, smart grids, and smart cities.\nEmbedded computers have the widest spread of processing power and cost.\nThey include 8-bit to 32-bit processors that may cost one penny, and high-end\n64-bit processors for cars and network switches that cost $100. Although the range\nof computing power in the embedded computing market is very large, price is a key\nfactor in the design of computers for this space. Performance requirements do exist,\nof course, but the primary goal often meets the performance need at a minimum\nprice, rather than achieving more performance at a higher price. The projections\nfor the number of IoT devices in 2020 range from 20 to 50 billion.\nMost of this book applies to the design, use, and performance of embedded\nprocessors, whether they are off-the-shelf microprocessors or microprocessor\ncores that will be assembled with other special-purpose hardware.\nUnfortunately, the data that drive the quantitative design and evaluation of\nother classes of computers have not yet been extended successfully to embedded\ncomputing (see the challenges with EEMBC, for example, in Section 1.8). Hence\nwe are left for now with qualitative descriptions, which do not fit well with the rest\nof the book. As a result, the embedded material is concentrated in Appendix E. We\nbelieve a separate appendix improves the flow of ideas in the text while allowing\nreaders to see how the differing requirements affect embedded computing.\nPersonal Mobile Device\nPersonal mobile device (PMD) is the term we apply to a collection of wireless\ndevices with multimedia user interfaces such as cell phones, tablet computers,\nand so on. Cost is a prime concern given the consumer price for the whole\nproduct is a few hundred dollars. Although the emphasis on energy efficiency\nis frequently driven by the use of batteries, the need to use less expensive packag-\ning\u2014plastic versus ceramic\u2014and the absence of a fan for cooling also limit total\npower consumption. We examine the issue of energy and power in more detail\nin Section 1.5. Applications on PMDs are often web-based and media-oriented,\nlike the previously mentioned Google Translate example. Energy and size\nrequirements lead to use of Flash memory for storage (Chapter 2) instead of\nmagnetic disks.\nThe processors in a PMD are often considered embedded computers, but we\nare keeping them as a separate category because PMDs are platforms that can\nrun externally developed software, and they share many of the characteristics of\ndesktop computers. Other embedded devices are more limited in hardware and\nsoftware sophistication. We use the ability to run third-party software as the divid-\ning line between nonembedded and embedded computers.\nResponsiveness and predictability are key characteristics for media applica-\ntions. A real-time performance requirement means a segment of the application\nhas an absolute maximum execution time. For example, in playing a video on a\n1.2\nClasses of Computers\n\u25a0\n7"
    },
    {
        "page": 40,
        "text": "PMD, the time to process each video frame is limited, since the processor must\naccept and process the next frame shortly. In some applications, a more nuanced\nrequirement exists: the average time for a particular task is constrained as well as\nthe number of instances when some maximum time is exceeded. Such\napproaches\u2014sometimes called soft real-time\u2014arise when it is possible to miss\nthe time constraint on an event occasionally, as long as not too many are missed.\nReal-time performance tends to be highly application-dependent.\nOther key characteristics in many PMD applications are the need to minimize\nmemory and the need to use energy efficiently. Energy efficiency is driven by both\nbattery power and heat dissipation. The memory can be a substantial portion of the\nsystem cost, and it is important to optimize memory size in such cases. The impor-\ntance of memory size translates to an emphasis on code size, since data size is dic-\ntated by the application.\nDesktop Computing\nThe first, and possibly still the largest market in dollar terms, is desktop computing.\nDesktop computing spans from low-end netbooks that sell for under $300 to high-\nend, heavily configured workstations that may sell for $2500. Since 2008, more\nthan half of the desktop computers made each year have been battery operated lap-\ntop computers. Desktop computing sales are declining.\nThroughout this range in price and capability, the desktop market tends to\nbe driven to optimize price-performance. This combination of performance\n(measured primarily in terms of compute performance and graphics perfor-\nmance) and price of a system is what matters most to customers in this market,\nand hence to computer designers. As a result, the newest, highest-performance\nmicroprocessors and cost-reduced microprocessors often appear first in desktop\nsystems (see Section 1.6 for a discussion of the issues affecting the cost of\ncomputers).\nDesktop computing also tends to be reasonably well characterized in terms of\napplications and benchmarking, though the increasing use of web-centric, interac-\ntive applications poses new challenges in performance evaluation.\nServers\nAs the shift to desktop computing occurred in the 1980s, the role of servers grew to\nprovide larger-scale and more reliable file and computing services. Such servers\nhave become the backbone of large-scale enterprise computing, replacing the tra-\nditional mainframe.\nFor servers, different characteristics are important. First, availability is critical.\n(We discuss availability in Section 1.7.) Consider the servers running ATM\nmachines for banks or airline reservation systems. Failure of such server systems\nis far more catastrophic than failure of a single desktop, since these servers must\noperate seven days a week, 24 hours a day. Figure 1.3 estimates revenue costs of\ndowntime for server applications.\n8\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 41,
        "text": "A second key feature of server systems is scalability. Server systems often\ngrow in response to an increasing demand for the services they support or an\nexpansion in functional requirements. Thus the ability to scale up the computing\ncapacity, the memory, the storage, and the I/O bandwidth of a server is crucial.\nFinally, servers are designed for efficient throughput. That is, the overall per-\nformance of the server\u2014in terms of transactions per minute or web pages served\nper second\u2014is what is crucial. Responsiveness to an individual request remains\nimportant, but overall efficiency and cost-effectiveness, as determined by how\nmany requests can be handled in a unit time, are the key metrics for most servers.\nWe return to the issue of assessing performance for different types of computing\nenvironments in Section 1.8.\nClusters/Warehouse-Scale Computers\nThe growth of Software as a Service (SaaS) for applications like search, social net-\nworking, video viewing and sharing, multiplayer games, online shopping, and so\non has led to the growth of a class of computers called clusters. Clusters are col-\nlections of desktop computers or servers connected by local area networks to act as\na single larger computer. Each node runs its own operating system, and nodes com-\nmunicate using a networking protocol. WSCs are the largest of the clusters, in that\nthey are designed so that tens of thousands of servers can act as one. Chapter 6\ndescribes this class of extremely large computers.\nPrice-performance and power are critical to WSCs since they are so large. As\nChapter 6 explains, the majority of the cost of a warehouse is associated with\npower and cooling of the computers inside the warehouse. The annual amortized\ncomputers themselves and the networking gear cost for a WSC is $40 million,\nbecause they are usually replaced every few years. When you are buying that\nApplication\nCost of downtime\nper hour\nAnnual losses with downtime of\n1%\n(87.6 h/year)\n0.5%\n(43.8 h/year)\n0.1%\n(8.8 h/year)\nBrokerage service\n$4,000,000\n$350,400,000\n$175,200,000\n$35,000,000\nEnergy\n$1,750,000\n$153,300,000\n$76,700,000\n$15,300,000\nTelecom\n$1,250,000\n$109,500,000\n$54,800,000\n$11,000,000\nManufacturing\n$1,000,000\n$87,600,000\n$43,800,000\n$8,800,000\nRetail\n$650,000\n$56,900,000\n$28,500,000\n$5,700,000\nHealth care\n$400,000\n$35,000,000\n$17,500,000\n$3,500,000\nMedia\n$50,000\n$4,400,000\n$2,200,000\n$400,000\nFigure 1.3 Costs rounded to nearest $100,000 of an unavailable system are shown by analyzing the cost of down-\ntime (in terms of immediately lost revenue), assuming three different levels of availability, and that downtime is\ndistributed uniformly. These data are from Landstrom (2014) and were collected and analyzed by Contingency Plan-\nning Research.\n1.2\nClasses of Computers\n\u25a0\n9"
    },
    {
        "page": 42,
        "text": "much computing, you need to buy wisely, because a 10% improvement in price-\nperformance means an annual savings of $4 million (10% of $40 million) per\nWSC; a company like Amazon might have 100 WSCs!\nWSCs are related to servers in that availability is critical. For example, Ama-\nzon.com had $136 billion in sales in 2016. As there are about 8800 hours in a year,\nthe average revenue per hour was about $15 million. During a peak hour for Christ-\nmas shopping, the potential loss would be many times higher. As Chapter 6\nexplains, the difference between WSCs and servers is that WSCs use redundant,\ninexpensive components as the building blocks, relying on a software layer to\ncatch and isolate the many failures that will happen with computing at this scale\nto deliver the availability needed for such applications. Note that scalability for a\nWSC is handled by the local area network connecting the computers and not by\nintegrated computer hardware, as in the case of servers.\nSupercomputers are related to WSCs in that they are equally expensive,\ncosting hundreds of millions of dollars, but supercomputers differ by emphasi-\nzing floating-point performance and by running large, communication-intensive\nbatch programs that can run for weeks at a time. In contrast, WSCs emphasize\ninteractive applications, large-scale storage, dependability, and high Internet\nbandwidth.\nClasses of Parallelism and Parallel Architectures\nParallelism at multiple levels is now the driving force of computer design across all\nfour classes of computers, with energy and cost being the primary constraints.\nThere are basically two kinds of parallelism in applications:\n1. Data-level parallelism (DLP) arises because there are many data items that can\nbe operated on at the same time.\n2. Task-level parallelism (TLP) arises because tasks of work are created that can\noperate independently and largely in parallel.\nComputer hardware in turn can exploit these two kinds of application parallelism in\nfour major ways:\n1. Instruction-level parallelism exploits data-level parallelism at modest levels\nwith compiler help using ideas like pipelining and at medium levels using ideas\nlike speculative execution.\n2. Vector architectures, graphic processor units (GPUs), and multimedia instruc-\ntion sets exploit data-level parallelism by applying a single instruction to a col-\nlection of data in parallel.\n3. Thread-level parallelism exploits either data-level parallelism or task-level par-\nallelism in a tightly coupled hardware model that allows for interaction between\nparallel threads.\n4. Request-level parallelism exploits parallelism among largely decoupled tasks\nspecified by the programmer or the operating system.\n10\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 43,
        "text": "When Flynn (1966) studied the parallel computing efforts in the 1960s, he\nfound a simple classification whose abbreviations we still use today. They target\ndata-level parallelism and task-level parallelism. He looked at the parallelism in the\ninstruction and data streams called for by the instructions at the most constrained\ncomponent of the multiprocessor and placed all computers in one of four\ncategories:\n1. Single instruction stream, single data stream (SISD)\u2014This category is the uni-\nprocessor. The programmer thinks of it as the standard sequential computer, but\nit can exploit ILP. Chapter 3 covers SISD architectures that use ILP techniques\nsuch as superscalar and speculative execution.\n2. Single instruction stream, multiple data streams (SIMD)\u2014The same instruc-\ntion is executed by multiple processors using different data streams. SIMD com-\nputers exploit data-level parallelism by applying the same operations to\nmultiple items of data in parallel. Each processor has its own data memory\n(hence, the MD of SIMD), but there is a single instruction memory and control\nprocessor, which fetches and dispatches instructions. Chapter 4 covers DLP and\nthree different architectures that exploit it: vector architectures, multimedia\nextensions to standard instruction sets, and GPUs.\n3. Multiple instruction streams, single data stream (MISD)\u2014No commercial mul-\ntiprocessor of this type has been built to date, but it rounds out this simple\nclassification.\n4. Multiple instruction streams, multiple data streams (MIMD)\u2014Each processor\nfetches its own instructions and operates on its own data, and it targets task-level\nparallelism. In general, MIMD is more flexible than SIMD and thus more gen-\nerally applicable, but it is inherently more expensive than SIMD. For example,\nMIMD computers can also exploit data-level parallelism, although the overhead\nis likely to be higher than would be seen in an SIMD computer. This overhead\nmeans that grain size must be sufficiently large to exploit the parallelism effi-\nciently. Chapter 5 covers tightly coupled MIMD architectures, which exploit\nthread-level parallelism because multiple cooperating threads operate in paral-\nlel. Chapter 6 covers loosely coupled MIMD architectures\u2014specifically, clus-\nters and warehouse-scale computers\u2014that exploit request-level parallelism,\nwhere many independent tasks can proceed in parallel naturally with little need\nfor communication or synchronization.\nThis taxonomy is a coarse model, as many parallel processors are hybrids of the\nSISD, SIMD, and MIMD classes. Nonetheless, it is useful to put a framework on\nthe design space for the computers we will see in this book.\n1.3\nDefining Computer Architecture\nThe task the computer designer faces is a complex one: determine what attributes\nare important for a new computer, then design a computer to maximize\n1.3\nDefining Computer Architecture\n\u25a0\n11"
    },
    {
        "page": 44,
        "text": "performance and energy efficiency while staying within cost, power, and availabil-\nity constraints. This task has many aspects, including instruction set design, func-\ntional organization, logic design, and implementation. The implementation may\nencompass integrated circuit design, packaging, power, and cooling. Optimizing\nthe design requires familiarity with a very wide range of technologies, from com-\npilers and operating systems to logic design and packaging.\nA few decades ago, the term computer architecture generally referred to only\ninstruction set design. Other aspects of computer design were called implementa-\ntion, often insinuating that implementation is uninteresting or less challenging.\nWe believe this view is incorrect. The architect\u2019s or designer\u2019s job is much\nmore than instruction set design, and the technical hurdles in the other aspects\nof the project are likely more challenging than those encountered in instruction\nset design. We\u2019ll quickly review instruction set architecture before describing\nthe larger challenges for the computer architect.\nInstruction Set Architecture: The Myopic View\nof Computer Architecture\nWe use the term instruction set architecture (ISA) to refer to the actual\nprogrammer-visible instruction set in this book. The ISA serves as the boundary\nbetween the software and hardware. This quick review of ISA will use examples\nfrom 80x86, ARMv8, and RISC-V to illustrate the seven dimensions of an ISA.\nThe most popular RISC processors come from ARM (Advanced RISC Machine),\nwhich were in 14.8 billion chips shipped in 2015, or roughly 50 times as many\nchips that shipped with 80x86 processors. Appendices A and K give more details\non the three ISAs.\nRISC-V (\u201cRISC Five\u201d) is a modern RISC instruction set developed at the\nUniversity of California, Berkeley, which was made free and openly adoptable\nin response to requests from industry. In addition to a full software stack (com-\npilers, operating systems, and simulators), there are several RISC-V implementa-\ntions freely available for use in custom chips or in field-programmable gate arrays.\nDeveloped 30 years after the first RISC instruction sets, RISC-V inherits its ances-\ntors\u2019 good ideas\u2014a large set of registers, easy-to-pipeline instructions, and a lean\nset of operations\u2014while avoiding their omissions or mistakes. It is a free and\nopen, elegant example of the RISC architectures mentioned earlier, which is\nwhy more than 60 companies have joined the RISC-V foundation, including\nAMD, Google, HP Enterprise, IBM, Microsoft, Nvidia, Qualcomm, Samsung,\nand Western Digital. We use the integer core ISA of RISC-V as the example\nISA in this book.\n1. Class of ISA\u2014Nearly all ISAs today are classified as general-purpose register\narchitectures, where the operands are either registers or memory locations. The\n80x86 has 16 general-purpose registers and 16 that can hold floating-point data,\nwhile RISC-V has 32 general-purpose and 32 floating-point registers (see\nFigure 1.4). The two popular versions of this class are register-memory ISAs,\n12\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 45,
        "text": "such as the 80x86, which can access memory as part of many instructions, and\nload-store ISAs, such as ARMv8 and RISC-V, which can access memory\nonly with load or store instructions. All ISAs announced since 1985 are\nload-store.\n2. Memory addressing\u2014Virtually all desktop and server computers, including the\n80x86, ARMv8, and RISC-V, use byte addressing to access memory operands.\nSome architectures, like ARMv8, require that objects must be aligned. An\naccess to an object of size s bytes at byte address A is aligned if A mod\ns\u00bc0. (See Figure A.5 on page A-8.) The 80x86 and RISC-V do not require\nalignment, but accesses are generally faster if operands are aligned.\n3. Addressing modes\u2014In addition to specifying registers and constant operands,\naddressing modes specify the address of a memory object. RISC-V addressing\nmodes are Register, Immediate (for constants), and Displacement, where a con-\nstant offset is added to a register to form the memory address. The 80x86\nsupports those three modes, plus three variations of displacement: no register\n(absolute), two registers (based indexed with displacement), and two registers\nRegister\nName\nUse\nSaver\nx0\nzero\nThe constant value 0\nN.A.\nx1\nra\nReturn address\nCaller\nx2\nsp\nStack pointer\nCallee\nx3\ngp\nGlobal pointer\n\u2013\nx4\ntp\nThread pointer\n\u2013\nx5\u2013x7\nt0\u2013t2\nTemporaries\nCaller\nx8\ns0/fp\nSaved register/frame pointer\nCallee\nx9\ns1\nSaved register\nCallee\nx10\u2013x11\na0\u2013a1\nFunction arguments/return values\nCaller\nx12\u2013x17\na2\u2013a7\nFunction arguments\nCaller\nx18\u2013x27\ns2\u2013s11\nSaved registers\nCallee\nx28\u2013x31\nt3\u2013t6\nTemporaries\nCaller\nf0\u2013f7\nft0\u2013ft7\nFP temporaries\nCaller\nf8\u2013f9\nfs0\u2013fs1\nFP saved registers\nCallee\nf10\u2013f11\nfa0\u2013fa1\nFP function arguments/return values\nCaller\nf12\u2013f17\nfa2\u2013fa7\nFP function arguments\nCaller\nf18\u2013f27\nfs2\u2013fs11\nFP saved registers\nCallee\nf28\u2013f31\nft8\u2013ft11\nFP temporaries\nCaller\nFigure 1.4 RISC-V registers, names, usage, and calling conventions. In addition to the\n32 general-purpose registers (x0\u2013x31), RISC-V has 32 floating-point registers (f0\u2013f31)\nthat can hold either a 32-bit single-precision number or a 64-bit double-precision num-\nber. The registers that are preserved across a procedure call are labeled \u201cCallee\u201d saved.\n1.3\nDefining Computer Architecture\n\u25a0\n13"
    },
    {
        "page": 46,
        "text": "where one register is multiplied by the size of the operand in bytes (based with\nscaled index and displacement). It has more like the last three modes, minus the\ndisplacement field, plus register indirect, indexed, and based with scaled index.\nARMv8 has the three RISC-V addressing modes plus PC-relative addressing,\nthe sum of two registers, and the sum of two registers where one register is\nmultiplied by the size of the operand in bytes. It also has autoincrement and\nautodecrement addressing, where the calculated address replaces the contents\nof one of the registers used in forming the address.\n4. Types and sizes of operands\u2014Like most ISAs, 80x86, ARMv8, and RISC-V\nsupport operand sizes of 8-bit (ASCII character), 16-bit (Unicode character\nor half word), 32-bit (integer or word), 64-bit (double word or long integer),\nand IEEE 754 floating point in 32-bit (single precision) and 64-bit (double\nprecision). The 80x86 also supports 80-bit floating point (extended double\nprecision).\n5. Operations\u2014The general categories of operations are data transfer, arithmetic\nlogical, control (discussed next), and floating point. RISC-V is a simple and\neasy-to-pipeline instruction set architecture, and it is representative of the RISC\narchitectures being used in 2017. Figure 1.5 summarizes the integer RISC-V\nISA, and Figure 1.6 lists the floating-point ISA. The 80x86 has a much richer\nand larger set of operations (see Appendix K).\n6. Control flow instructions\u2014Virtually all ISAs, including these three, support\nconditional branches, unconditional jumps, procedure calls, and returns. All\nthree use PC-relative addressing, where the branch address is specified by an\naddress field that is added to the PC. There are some small differences.\nRISC-V conditional branches (BE, BNE, etc.) test the contents of registers,\nand the 80x86 and ARMv8 branches test condition code bits set as side effects\nof arithmetic/logic operations. The ARMv8 and RISC-V procedure call places\nthe return address in a register, whereas the 80x86 call (CALLF) places the\nreturn address on a stack in memory.\n7. Encoding an ISA\u2014There are two basic choices on encoding: fixed length and\nvariable length. All ARMv8 and RISC-V instructions are 32 bits long, which\nsimplifies instruction decoding. Figure 1.7 shows the RISC-V instruction for-\nmats. The 80x86 encoding is variable length, ranging from 1 to 18 bytes.\nVariable-length instructions can take less space than fixed-length instructions,\nso a program compiled for the 80x86 is usually smaller than the same program\ncompiled for RISC-V. Note that choices mentioned previously will affect how\nthe instructions are encoded into a binary representation. For example, the num-\nber of registers and the number of addressing modes both have a significant\nimpact on the size of instructions, because the register field and addressing\nmode field can appear many times in a single instruction. (Note that ARMv8\nand RISC-V later offered extensions, called Thumb-2 and RV64IC, that\nprovide a mix of 16-bit and 32-bit length instructions, respectively, to reduce\nprogram size. Code size for these compact versions of RISC architectures\nare smaller than that of the 80x86. See Appendix K.)\n14\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 47,
        "text": "Instruction type/opcode\nInstruction meaning\nData transfers\nMove data between registers and memory, or between the integer and FP or special\nregisters; only memory address mode is 12-bit displacement+contents of a GPR\nlb, lbu, sb\nLoad byte, load byte unsigned, store byte (to/from integer registers)\nlh, lhu, sh\nLoad half word, load half word unsigned, store half word (to/from integer registers)\nlw, lwu, sw\nLoad word, load word unsigned, store word (to/from integer registers)\nld, sd\nLoad double word, store double word (to/from integer registers)\nflw, fld, fsw, fsd\nLoad SP float, load DP float, store SP float, store DP float\nfmv._.x, fmv.x._\nCopy from/to integer register to/from floating-point register; \u201c__\u201d\u00bcS for single-\nprecision, D for double-precision\ncsrrw, csrrwi, csrrs,\ncsrrsi, csrrc, csrrci\nRead counters and write status registers, which include counters: clock cycles, time,\ninstructions retired\nArithmetic/logical\nOperations on integer or logical data in GPRs\nadd, addi, addw, addiw\nAdd, add immediate (all immediates are 12 bits), add 32-bits only & sign-extend to 64\nbits, add immediate 32-bits only\nsub, subw\nSubtract, subtract 32-bits only\nmul, mulw, mulh, mulhsu,\nmulhu\nMultiply, multiply 32-bits only, multiply upper half, multiply upper half signed-\nunsigned, multiply upper half unsigned\ndiv, divu, rem, remu\nDivide, divide unsigned, remainder, remainder unsigned\ndivw, divuw, remw, remuw\nDivide and remainder: as previously, but divide only lower 32-bits, producing 32-bit\nsign-extended result\nand, andi\nAnd, and immediate\nor, ori, xor, xori\nOr, or immediate, exclusive or, exclusive or immediate\nlui\nLoad upper immediate; loads bits 31-12 of register with immediate, then sign-extends\nauipc\nAdds immediate in bits 31\u201312 with zeros in lower bits to PC; used with JALR to\ntransfer control to any 32-bit address\nsll, slli, srl, srli, sra,\nsrai\nShifts: shift left logical, right logical, right arithmetic; both variable and immediate\nforms\nsllw, slliw, srlw, srliw,\nsraw, sraiw\nShifts: as previously, but shift lower 32-bits, producing 32-bit sign-extended result\nslt, slti, sltu, sltiu\nSet less than, set less than immediate, signed and unsigned\nControl\nConditional branches and jumps; PC-relative or through register\nbeq, bne, blt, bge, bltu,\nbgeu\nBranch GPR equal/not equal; less than; greater than or equal, signed and unsigned\njal, jalr\nJump and link: save PC+4, target is PC-relative (JAL) or a register (JALR); if specify\nx0 as destination register, then acts as a simple jump\necall\nMake a request to the supporting execution environment, which is usually an OS\nebreak\nDebuggers used to cause control to be transferred back to a debugging environment\nfence, fence.i\nSynchronize threads to guarantee ordering of memory accesses; synchronize\ninstructions and data for stores to instruction memory\nFigure 1.5 Subset of the instructions in RISC-V. RISC-V has a base set of instructions (R64I) and offers optional exten-\nsions: multiply-divide (RVM), single-precision floating point (RVF), double-precision floating point (RVD). This figure\nincludes RVM and the next one shows RVF and RVD. Appendix A gives much more detail on RISC-V.\n1.3\nDefining Computer Architecture\n\u25a0\n15"
    },
    {
        "page": 48,
        "text": "Instruction type/opcode\nInstruction meaning\nFloating point\nFP operations on DP and SP formats\nfadd.d, fadd.s\nAdd DP, SP numbers\nfsub.d, fsub.s\nSubtract DP, SP numbers\nfmul.d, fmul.s\nMultiply DP, SP floating point\nfmadd.d, fmadd.s, fnmadd.d,\nfnmadd.s\nMultiply-add DP, SP numbers; negative multiply-add DP, SP numbers\nfmsub.d, fmsub.s, fnmsub.d,\nfnmsub.s\nMultiply-sub DP, SP numbers; negative multiply-sub DP, SP numbers\nfdiv.d, fdiv.s\nDivide DP, SP floating point\nfsqrt.d, fsqrt.s\nSquare root DP, SP floating point\nfmax.d, fmax.s, fmin.d,\nfmin.s\nMaximum and minimum DP, SP floating point\nfcvt._._, fcvt._._u,\nfcvt._u._\nConvert instructions: FCVT.x.y converts from type x to type y, where x and y are\nL (64-bit integer), W (32-bit integer), D (DP), or S (SP). Integers can be unsigned (U)\nfeq._, flt._,fle._\nFloating-point compare between floating-point registers and record the Boolean\nresult in integer register; \u201c__\u201d\u00bcS for single-precision, D for double-precision\nfclass.d, fclass.s\nWrites to integer register a 10-bit mask that indicates the class of the floating-point\nnumber (\u221e, +\u221e, 0, +0, NaN, \u2026)\nfsgnj._, fsgnjn._,\nfsgnjx._\nSign-injection instructions that changes only the sign bit: copy sign bit from other\nsource, the oppositive of sign bit of other source, XOR of the 2 sign bits\nFigure 1.6 Floating point instructions for RISC-V. RISC-V has a base set of instructions (R64I) and offers optional\nextensions for single-precision floating point (RVF) and double-precision floating point (RVD). SP\u00bcsingle precision;\nDP\u00bcdouble precision.\nR-type\n0\n7 6\n12 11\n15 14\n20 19\n25 24\n31\nI-type\nS-type\nU-type\nopcode\nrd\nrs1\nrs2\nfunct3\nfunct7\nopcode\nimm [4:0]\nimm [4:1|11]\nrs1\nrs1\nrs2\nrs2\nfunct3\nfunct3\nimm [11:5]\nopcode\nrd\nrs1\nfunct3\nimm [11:0]\nopcode\nrd\nimm [31:12]\nJ-type\nopcode\nrd\nimm [20|10:1|11|19:12]\nB-type\nopcode\nimm [10:5]\nimm [12]\nFigure 1.7 The base RISC-V instruction set architecture formats. All instructions are 32 bits long. The R format is for\ninteger register-to-register operations, such as ADD, SUB, and so on. The I format is for loads and immediate oper-\nations, such as LD and ADDI. The B format is for branches and the J format is for jumps and link. The S format is for\nstores. Having a separate format for stores allows the three register specifiers (rd, rs1, rs2) to always be in the same\nlocation in all formats. The U format is for the wide immediate instructions (LUI, AUIPC).\n16\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 49,
        "text": "The other challenges facing the computer architect beyond ISA design are par-\nticularly acute at the present, when the differences among instruction sets are small\nand when there are distinct application areas. Therefore, starting with the fourth\nedition of this book, beyond this quick review, the bulk of the instruction set mate-\nrial is found in the appendices (see Appendices A and K).\nGenuine Computer Architecture: Designing the Organization\nand Hardware to Meet Goals and Functional Requirements\nThe implementation of a computer has two components: organization and hard-\nware. The term organization includes the high-level aspects of a computer\u2019s\ndesign, such as the memory system, the memory interconnect, and the design of\nthe internal processor or CPU (central processing unit\u2014where arithmetic, logic,\nbranching, and data transfer are implemented). The term microarchitecture is also\nused instead of organization. For example, two processors with the same instruc-\ntion set architectures but different organizations are the AMD Opteron and the Intel\nCore i7. Both processors implement the 80x86 instruction set, but they have very\ndifferent pipeline and cache organizations.\nThe switch to multiple processors per microprocessor led to the term core also\nbeing used for processors. Instead of saying multiprocessor microprocessor, the\nterm multicore caught on. Given that virtually all chips have multiple processors,\nthe term central processing unit, or CPU, is fading in popularity.\nHardware refers to the specifics of a computer, including the detailed logic\ndesign and the packaging technology of the computer. Often a line of computers\ncontains computers with identical instruction set architectures and very similar\norganizations, but they differ in the detailed hardware implementation. For exam-\nple, the Intel Core i7 (see Chapter 3) and the Intel Xeon E7 (see Chapter 5) are\nnearly identical but offer different clock rates and different memory systems, mak-\ning the Xeon E7 more effective for server computers.\nIn this book, the word architecture covers all three aspects of computer\ndesign\u2014instruction set architecture, organization or microarchitecture, and\nhardware.\nComputer architects must design a computer to meet functional requirements\nas well as price, power, performance, and availability goals. Figure 1.8 summarizes\nrequirements to consider in designing a new computer. Often, architects also must\ndetermine what the functional requirements are, which can be a major task. The\nrequirements may be specific features inspired by the market. Application software\ntypically drives the choice of certain functional requirements by determining how\nthe computer will be used. If a large body of software exists for a particular instruc-\ntion set architecture, the architect may decide that a new computer should imple-\nment an existing instruction set. The presence of a large market for a particular\nclass of applications might encourage the designers to incorporate requirements\nthat would make the computer competitive in that market. Later chapters examine\nmany of these requirements and features in depth.\n1.3\nDefining Computer Architecture\n\u25a0\n17"
    },
    {
        "page": 50,
        "text": "Architects must also be aware of important trends in both the technology and\nthe use of computers because such trends affect not only the future cost but also the\nlongevity of an architecture.\n1.4\nTrends in Technology\nIf an instruction set architecture is to prevail, it must be designed to survive rapid\nchanges in computer technology. After all, a successful new instruction set\nFunctional requirements\nTypical features required or supported\nApplication area\nTarget of computer\nPersonal mobile device\nReal-time performance for a range of tasks, including interactive performance for\ngraphics, video, and audio; energy efficiency (Chapters 2\u20135 and 7; Appendix A)\nGeneral-purpose desktop\nBalanced performance for a range of tasks, including interactive performance for\ngraphics, video, and audio (Chapters 2\u20135; Appendix A)\nServers\nSupport for databases and transaction processing; enhancements for reliability and\navailability; support for scalability (Chapters 2, 5, and 7; Appendices A, D, and F)\nClusters/warehouse-scale\ncomputers\nThroughput performance for many independent tasks; error correction for memory;\nenergy proportionality (Chapters 2, 6, and 7; Appendix F)\nInternet of things/embedded\ncomputing\nOften requires special support for graphics or video (or other application-specific\nextension); power limitations and power control may be required; real-time constraints\n(Chapters 2, 3, 5, and 7; Appendices A and E)\nLevel of software compatibility\nDetermines amount of existing software for computer\nAt programming language\nMost flexible for designer; need new compiler (Chapters 3, 5, and 7; Appendix A)\nObject code or binary\ncompatible\nInstruction set architecture is completely defined\u2014little flexibility\u2014but no investment\nneeded in software or porting programs (Appendix A)\nOperating system requirements\nNecessary features to support chosen OS (Chapter 2; Appendix B)\nSize of address space\nVery important feature (Chapter 2); may limit applications\nMemory management\nRequired for modern OS; may be paged or segmented (Chapter 2)\nProtection\nDifferent OS and application needs: page versus segment; virtual machines (Chapter 2)\nStandards\nCertain standards may be required by marketplace\nFloating point\nFormat and arithmetic: IEEE 754 standard (Appendix J), special arithmetic for graphics\nor signal processing\nI/O interfaces\nFor I/O devices: Serial ATA, Serial Attached SCSI, PCI Express (Appendices D and F)\nOperating systems\nUNIX, Windows, Linux, CISCO IOS\nNetworks\nSupport required for different networks: Ethernet, Infiniband (Appendix F)\nProgramming languages\nLanguages (ANSI C, C++, Java, Fortran) affect instruction set (Appendix A)\nFigure 1.8 Summary of some of the most important functional requirements an architect faces. The left-hand\ncolumn describes the class of requirement, while the right-hand column gives specific examples. The right-hand col-\numn also contains references to chapters and appendices that deal with the specific issues.\n18\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 51,
        "text": "architecture may last decades\u2014for example, the core of the IBM mainframe has\nbeen in use for more than 50 years. An architect must plan for technology changes\nthat can increase the lifetime of a successful computer.\nTo plan for the evolution of a computer, the designer must be aware of rapid\nchanges in implementation technology. Five implementation technologies, which\nchange at a dramatic pace, are critical to modern implementations:\n\u25a0\nIntegrated circuit logic technology\u2014Historically, transistor density increased\nby about 35% per year, quadrupling somewhat over four years. Increases in\ndie size are less predictable and slower, ranging from 10% to 20% per year.\nThe combined effect was a traditional growth rate in transistor count on a chip\nof about 40%\u201355% per year, or doubling every 18\u201324 months. This trend is\npopularly known as Moore\u2019s Law. Device speed scales more slowly, as we\ndiscuss below. Shockingly, Moore\u2019s Law is no more. The number of devices\nper chip is still increasing, but at a decelerating rate. Unlike in the Moore\u2019s\nLaw era, we expect the doubling time to be stretched with each new technol-\nogy generation.\n\u25a0\nSemiconductor DRAM (dynamic random-access memory)\u2014This technology\nis the foundation of main memory, and we discuss it in Chapter 2. The growth\nof DRAM has slowed dramatically, from quadrupling every three years as in\nthe past. The 8-gigabit DRAM was shipping in 2014, but the 16-gigabit\nDRAM won\u2019t reach that state until 2019, and it looks like there will be no\n32-gigabit DRAM (Kim, 2005). Chapter 2 mentions several other technologies\nthat may replace DRAM when it hits its capacity wall.\n\u25a0\nSemiconductor Flash (electrically erasable programmable read-only mem-\nory)\u2014This nonvolatile semiconductor memory is the standard storage device\nin PMDs, and its rapidly increasing popularity has fueled its rapid growth\nrate in capacity. In recent years, the capacity per Flash chip increased by about\n50%\u201360% per year, doubling roughly every 2 years. Currently, Flash\nmemory is 8\u201310 times cheaper per bit than DRAM. Chapter 2 describes Flash\nmemory.\n\u25a0\nMagnetic disk technology\u2014Prior to 1990, density increased by about 30% per\nyear, doubling in three years. It rose to 60% per year thereafter, and increased to\n100% per year in 1996. Between 2004 and 2011, it dropped back to about 40%\nper year, or doubled every two years. Recently, disk improvement has slowed\nto less than 5% per year. One way to increase disk capacity is to add more plat-\nters at the same areal density, but there are already seven platters within the\none-inch depth of the 3.5-inch form factor disks. There is room for at most\none or two more platters. The last hope for real density increase is to use a small\nlaser on each disk read-write head to heat a 30 nm spot to 400\u00b0C so that it can\nbe written magnetically before it cools. It is unclear whether Heat Assisted\nMagnetic Recording can be manufactured economically and reliably, although\nSeagate announced plans to ship HAMR in limited production in 2018. HAMR\nis the last chance for continued improvement in areal density of hard disk\n1.4\nTrends in Technology\n\u25a0\n19"
    },
    {
        "page": 52,
        "text": "drives, which are now 8\u201310 times cheaper per bit than Flash and 200\u2013300 times\ncheaper per bit than DRAM. This technology is central to server- and\nwarehouse-scale storage, and we discuss the trends in detail in Appendix D.\n\u25a0\nNetwork technology\u2014Network performance depends both on the performance\nof switches and on the performance of the transmission system. We discuss the\ntrends in networking in Appendix F.\nThese rapidly changing technologies shape the design of a computer that, with\nspeed and technology enhancements, may have a lifetime of 3\u20135 years. Key tech-\nnologies such as Flash change sufficiently that the designer must plan for these\nchanges. Indeed, designers often design for the next technology, knowing that,\nwhen a product begins shipping in volume, the following technology may be\nthe most cost-effective or may have performance advantages. Traditionally, cost\nhas decreased at about the rate at which density increases.\nAlthough technology improves continuously, the impact of these increases can\nbe in discrete leaps, as a threshold that allows a new capability is reached. For\nexample, when MOS technology reached a point in the early 1980s where between\n25,000 and 50,000 transistors could fit on a single chip, it became possible to build\na single-chip, 32-bit microprocessor. By the late 1980s, first-level caches could go\non a chip. By eliminating chip crossings within the processor and between the pro-\ncessor and the cache, a dramatic improvement in cost-performance and energy-\nperformance was possible. This design was simply unfeasible until the technology\nreached a certain point. With multicore microprocessors and increasing numbers of\ncores each generation, even server computers are increasingly headed toward a sin-\ngle chip for all processors. Such technology thresholds are not rare and have a sig-\nnificant impact on a wide variety of design decisions.\nPerformance Trends: Bandwidth Over Latency\nAs we shall see in Section 1.8, bandwidth or throughput is the total amount of work\ndone in a given time, such as megabytes per second for a disk transfer. In contrast,\nlatency or response time is the time between the start and the completion of an\nevent, such as milliseconds for a disk access. Figure 1.9 plots the relative improve-\nment in bandwidth and latency for technology milestones for microprocessors,\nmemory, networks, and disks. Figure 1.10 describes the examples and milestones\nin more detail.\nPerformance is the primary differentiator for microprocessors and networks, so\nthey have seen the greatest gains: 32,000\u201340,000\u0003 in bandwidth and 50\u201390\u0003 in\nlatency. Capacity is generally more important than performance for memory and\ndisks, so capacity has improved more, yet bandwidth advances of 400\u20132400\u0003 are\nstill much greater than gains in latency of 8\u20139\u0003.\nClearly,bandwidthhasoutpacedlatencyacrossthesetechnologiesandwilllikely\ncontinue to do so. A simple rule of thumb is that bandwidth grows by at least the\nsquare of the improvement in latency. Computer designers should plan accordingly.\n20\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 53,
        "text": "Scaling of Transistor Performance and Wires\nIntegrated circuit processes are characterized by the feature size, which is the min-\nimum size of a transistor or a wire in either the x or y dimension. Feature sizes\ndecreased from 10 \u03bcm in 1971 to 0.016 \u03bcm in 2017; in fact, we have switched\nunits, so production in 2017 is referred to as \u201c16 nm,\u201d and 7 nm chips are under-\nway. Since the transistor count per square millimeter of silicon is determined by the\nsurface area of a transistor, the density of transistors increases quadratically with a\nlinear decrease in feature size.\n1\n10\n100\n1000\n10,000\n100,000\n1\n10\n100\nRelative bandwidth improvement\nRelative Latency Improvement\nMicroprocessor\nMemory\nNetwork\nDisk\n(Latency improvement \n= Bandwidth improvement)\nFigure 1.9 Log-log plot of bandwidth and latency milestones in Figure 1.10 relative to the first milestone. Note\nthat latency improved 8\u201391\u0003, while bandwidth improved about 400\u201332,000\u0003. Except for networking, we note that\nthere were modest improvements in latency and bandwidth in the other three technologies in the six years since the\nlast edition: 0%\u201323% in latency and 23%\u201370% in bandwidth. Updated from Patterson, D., 2004. Latency lags band-\nwidth. Commun. ACM 47 (10), 71\u201375.\n1.4\nTrends in Technology\n\u25a0\n21"
    },
    {
        "page": 54,
        "text": "Microprocessor\n16-Bit\naddress/\nbus,\nmicrocoded\n32-Bit\naddress/\nbus,\nmicrocoded\n5-Stage\npipeline,\non-chip I & D\ncaches, FPU\n2-Way\nsuperscalar,\n64-bit bus\nOut-of-order\n3-way\nsuperscalar\nOut-of-order\nsuperpipelined,\non-chip L2\ncache\nMulticore\nOOO 4-way\non chip L3\ncache, Turbo\nProduct\nIntel 80286 Intel 80386\nIntel 80486\nIntel Pentium Intel Pentium Pro Intel Pentium 4 Intel Core i7\nYear\n1982\n1985\n1989\n1993\n1997\n2001\n2015\nDie size (mm2)\n47\n43\n81\n90\n308\n217\n122\nTransistors\n134,000\n275,000\n1,200,000\n3,100,000\n5,500,000\n42,000,000\n1,750,000,000\nProcessors/chip\n1\n1\n1\n1\n1\n1\n4\nPins\n68\n132\n168\n273\n387\n423\n1400\nLatency (clocks)\n6\n5\n5\n5\n10\n22\n14\nBus width (bits)\n16\n32\n32\n64\n64\n64\n196\nClock rate (MHz)\n12.5\n16\n25\n66\n200\n1500\n4000\nBandwidth (MIPS)\n2\n6\n25\n132\n600\n4500\n64,000\nLatency (ns)\n320\n313\n200\n76\n50\n15\n4\nMemory module\nDRAM\nPage mode\nDRAM\nFast page\nmode DRAM\nFast page\nmode DRAM\nSynchronous\nDRAM\nDouble data\nrate SDRAM\nDDR4\nSDRAM\nModule width (bits)\n16\n16\n32\n64\n64\n64\n64\nYear\n1980\n1983\n1986\n1993\n1997\n2000\n2016\nMbits/DRAM chip\n0.06\n0.25\n1\n16\n64\n256\n4096\nDie size (mm2)\n35\n45\n70\n130\n170\n204\n50\nPins/DRAM chip\n16\n16\n18\n20\n54\n66\n134\nBandwidth (MBytes/s)\n13\n40\n160\n267\n640\n1600\n27,000\nLatency (ns)\n225\n170\n125\n75\n62\n52\n30\nLocal area network\nEthernet\nFast\nEthernet\nGigabit\nEthernet\n10 Gigabit\nEthernet\n100 Gigabit\nEthernet\n400 Gigabit\nEthernet\nIEEE standard\n802.3\n803.3u\n802.3ab\n802.3ac\n802.3ba\n802.3bs\nYear\n1978\n1995\n1999\n2003\n2010\n2017\nBandwidth (Mbits/seconds)\n10\n100\n1000\n10,000\n100,000\n400,000\nLatency (\u03bcs)\n3000\n500\n340\n190\n100\n60\nHard disk\n3600 RPM\n5400 RPM\n7200 RPM\n10,000 RPM\n15,000 RPM\n15,000 RPM\nProduct\nCDC WrenI\n94145-36\nSeagate\nST41600\nSeagate\nST15150\nSeagate\nST39102\nSeagate\nST373453\nSeagate\nST600MX0062\nYear\n1983\n1990\n1994\n1998\n2003\n2016\nCapacity (GB)\n0.03\n1.4\n4.3\n9.1\n73.4\n600\nDisk form factor\n5.25 in.\n5.25 in.\n3.5 in.\n3.5 in.\n3.5 in.\n3.5 in.\nMedia diameter\n5.25 in.\n5.25 in.\n3.5 in.\n3.0 in.\n2.5 in.\n2.5 in.\nInterface\nST-412\nSCSI\nSCSI\nSCSI\nSCSI\nSAS\nBandwidth (MBytes/s)\n0.6\n4\n9\n24\n86\n250\nLatency (ms)\n48.3\n17.1\n12.7\n8.8\n5.7\n3.6\nFigure 1.10 Performance milestones over 25\u201340 years for microprocessors, memory, networks, and disks. The\nmicroprocessor milestones are several generations of IA-32 processors, going from a 16-bit bus, microcoded 80286 to\na 64-bit bus, multicore, out-of-order execution, superpipelined Core i7. Memory module milestones go from 16-bit-\nwide, plain DRAM to 64-bit-wide double data rate version 3 synchronous DRAM. Ethernet advanced from 10 Mbits/s\nto 400 Gbits/s. Disk milestones are based on rotation speed, improving from 3600 to 15,000 RPM. Each case is best-\ncase bandwidth, and latency is the time for a simple operation assuming no contention. Updated from Patterson, D.,\n2004. Latency lags bandwidth. Commun. ACM 47 (10), 71\u201375."
    },
    {
        "page": 55,
        "text": "The increase in transistor performance, however, is more complex. As feature\nsizes shrink, devices shrink quadratically in the horizontal dimension and also\nshrink in the vertical dimension. The shrink in the vertical dimension requires a\nreduction in operating voltage to maintain correct operation and reliability of the\ntransistors. This combination of scaling factors leads to a complex interrelationship\nbetween transistor performance and process feature size. To a first approximation, in\nthe past the transistor performance improved linearly with decreasing feature size.\nThefactthattransistorcountimprovesquadraticallywithalinearincreaseintran-\nsistor performance is both the challenge and the opportunity for which computer\narchitects were created! In the early days of microprocessors, the higher rate of\nimprovement in density was used to move quickly from 4-bit, to 8-bit, to 16-bit,\nto 32-bit, to 64-bit microprocessors. More recently, density improvements have sup-\nported the introduction of multiple processors per chip, wider SIMD units, and many\nof the innovations in speculative execution and caches found in Chapters 2\u20135.\nAlthough transistors generally improve in performance with decreased feature\nsize, wires in an integrated circuit do not. In particular, the signal delay for a wire\nincreases in proportion to the product of its resistance and capacitance. Of course,\nas feature size shrinks, wires get shorter, but the resistance and capacitance per unit\nlength get worse. This relationship is complex, since both resistance and capaci-\ntance depend on detailed aspects of the process, the geometry of a wire, the loading\non a wire, and even the adjacency to other structures. There are occasional process\nenhancements, such as the introduction of copper, which provide one-time\nimprovements in wire delay.\nIn general, however, wire delay scales poorly compared to transistor perfor-\nmance, creating additional challenges for the designer. In addition to the power\ndissipation limit, wire delay has become a major design obstacle for large inte-\ngrated circuits and is often more critical than transistor switching delay. Larger\nand larger fractions of the clock cycle have been consumed by the propagation\ndelay of signals on wires, but power now plays an even greater role than wire delay.\n1.5\nTrends in Power and Energy in Integrated Circuits\nToday, energy is the biggest challenge facing the computer designer for nearly\nevery class of computer. First, power must be brought in and distributed around\nthe chip, and modern microprocessors use hundreds of pins and multiple intercon-\nnect layers just for power and ground. Second, power is dissipated as heat and must\nbe removed.\nPower and Energy: A Systems Perspective\nHow should a system architect or a user think about performance, power, and\nenergy? From the viewpoint of a system designer, there are three primary concerns.\nFirst, what is the maximum power a processor ever requires? Meeting this\ndemand can be important to ensuring correct operation. For example, if a processor\n1.5\nTrends in Power and Energy in Integrated Circuits\n\u25a0\n23"
    },
    {
        "page": 56,
        "text": "attempts to draw more power than a power-supply system can provide (by drawing\nmore current than the system can supply), the result is typically a voltage drop,\nwhich can cause devices to malfunction. Modern processors can vary widely in\npower consumption with high peak currents; hence they provide voltage indexing\nmethods that allow the processor to slow down and regulate voltage within a wider\nmargin. Obviously, doing so decreases performance.\nSecond, what is the sustained power consumption? This metric is widely called\nthe thermal design power (TDP) because it determines the cooling requirement.\nTDP is neither peak power, which is often 1.5 times higher, nor is it the actual aver-\nage power that will be consumed during a given computation, which is likely to be\nlower still. A typical power supply for a system is typically sized to exceed the\nTDP, and a cooling system is usually designed to match or exceed TDP. Failure\nto provide adequate cooling will allow the junction temperature in the processor to\nexceed its maximum value, resulting in device failure and possibly permanent\ndamage. Modern processors provide two features to assist in managing heat, since\nthe highest power (and hence heat and temperature rise) can exceed the long-term\naverage specified by the TDP. First, as the thermal temperature approaches the\njunction temperature limit, circuitry lowers the clock rate, thereby reducing power.\nShould this technique not be successful, a second thermal overload trap is activated\nto power down the chip.\nThe third factor that designers and users need to consider is energy and energy\nefficiency. Recall that power is simply energy per unit time: 1 watt\u00bc1 joule per\nsecond. Which metric is the right one for comparing processors: energy or power?\nIn general, energy is always a better metric because it is tied to a specific task and\nthe time required for that task. In particular, the energy to complete a workload is\nequal to the average power times the execution time for the workload.\nThus, if we want to know which of two processors is more efficient for a given\ntask, we need to compare energy consumption (not power) for executing the task.\nFor example, processor A may have a 20% higher average power consumption\nthan processor B, but if A executes the task in only 70% of the time needed by\nB, its energy consumption will be 1.2\u00030.7\u00bc0.84, which is clearly better.\nOne might argue that in a large server or cloud, it is sufficient to consider the\naverage power, since the workload is often assumed to be infinite, but this is mis-\nleading. If our cloud were populated with processor Bs rather than As, then the\ncloud would do less work for the same amount of energy expended. Using energy\nto compare the alternatives avoids this pitfall. Whenever we have a fixed workload,\nwhether for a warehouse-size cloud or a smartphone, comparing energy will be the\nright way to compare computer alternatives, because the electricity bill for the\ncloud and the battery lifetime for the smartphone are both determined by the energy\nconsumed.\nWhen is power consumption a useful measure? The primary legitimate use is as\na constraint: for example, an air-cooled chip might be limited to 100 W. It can be\nused as a metric if the workload is fixed, but then it\u2019s just a variation of the true\nmetric of energy per task.\n24\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 57,
        "text": "Energy and Power Within a Microprocessor\nFor CMOS chips, the traditional primary energy consumption has been in switch-\ning transistors, also called dynamic energy. The energy required per transistor is\nproportional to the product of the capacitive load driven by the transistor and\nthe square of the voltage:\nEnergydynamic / Capacitive load\u0003Voltage2\nThis equation is the energy of pulse of the logic transition of 0!1!0 or\n1!0!1. The energy of a single transition (0!1 or 1!0) is then:\nEnergydynamic / 1=2\u0003Capacitive load\u0003Voltage2\nThe power required per transistor is just the product of the energy of a transition\nmultiplied by the frequency of transitions:\nPowerdynamic / 1=2\u0003Capacitive load\u0003Voltage2 \u0003Frequency switched\nFor a fixed task, slowing clock rate reduces power, but not energy.\nClearly, dynamic power and energy are greatly reduced by lowering the volt-\nage, so voltages have dropped from 5 V to just under 1 V in 20 years. The capac-\nitive load is a function of the number of transistors connected to an output and the\ntechnology, which determines the capacitance of the wires and the transistors.\nExample\nSome microprocessors today are designed to have adjustable voltage, so a 15%\nreduction in voltage may result in a 15% reduction in frequency. What would\nbe the impact on dynamic energy and on dynamic power?\nAnswer\nBecause the capacitance is unchanged, the answer for energy is the ratio of the\nvoltages\nEnergynew\nEnergyold\n\u00bc Voltage\u00030:85\n\u00f0\n\u00de2\nVoltage2\n\u00bc 0:852 \u00bc 0:72\nwhich reduces energy to about 72% of the original. For power, we add the ratio of\nthe frequencies\nPowernew\nPowerold\n\u00bc 0:72\u0003 Frequency switched\u00030:85\n\u00f0\n\u00de\nFrequency switched\n\u00bc 0:61\nshrinking power to about 61% of the original.\nAs we move from one process to the next, the increase in the number of tran-\nsistors switching and the frequency with which they change dominate the decrease\nin load capacitance and voltage, leading to an overall growth in power consump-\ntion and energy. The first microprocessors consumed less than a watt, and the first\n1.5\nTrends in Power and Energy in Integrated Circuits\n\u25a0\n25"
    },
    {
        "page": 58,
        "text": "32-bit microprocessors (such as the Intel 80386) used about 2 W, whereas a\n4.0 GHz Intel Core i7-6700K consumes 95 W. Given that this heat must be dissi-\npated from a chip that is about 1.5 cm on a side, we are near the limit of what can be\ncooled by air, and this is where we have been stuck for nearly a decade.\nGiven the preceding equation, you would expect clock frequency growth to\nslow down if we can\u2019t reduce voltage or increase power per chip. Figure 1.11\nshows that this has indeed been the case since 2003, even for the microprocessors\nin Figure 1.1 that were the highest performers each year. Note that this period of\nflatter clock rates corresponds to the period of slow performance improvement\nrange in Figure 1.1.\nDistributing the power, removing the heat, and preventing hot spots have\nbecome increasingly difficult challenges. Energy is now the major constraint to\nusing transistors; in the past, it was the raw silicon area. Therefore modern\n1\n10\n100\n1000\n10,000\n1978\n1980\n1982\n1984\n1986\n1988\n1990\n1992\n1994\n1996\n1998\n2000\n2002\n2004\n2006\n2008\n2010\n2012\n2014\n2016\n2018\nClock rate (MHz)\nIntel Pentium4 Xeon\n3200 MHz in 2003\nIntel Skylake Core i7\n4200 MHz in 2017\nIntel Pentium III\n1000 MHz in 2000\nDigital Alpha 21164A\n500 MHz in 1996\nDigital Alpha 21064\n150 MHz in 1992\nMIPS M2000\n25 MHz in 1989\nDigital VAX-11/780\n5 MHz in 1978\nSun-4 SPARC\n16.7 MHz in 1986 \n15%/year\n40%/year\n2%/year\nFigure 1.11 Growth in clock rate of microprocessors in Figure 1.1. Between 1978 and 1986, the clock rate improved\nless than 15% per year while performance improved by 22% per year. During the \u201crenaissance period\u201d of 52% per-\nformance improvement per year between 1986 and 2003, clock rates shot up almost 40% per year. Since then, the\nclock rate has been nearly flat, growing at less than 2% per year, while single processor performance improved\nrecently at just 3.5% per year.\n26\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 59,
        "text": "microprocessors offer many techniques to try to improve energy efficiency despite\nflat clock rates and constant supply voltages:\n1. Do nothing well. Most microprocessors today turn off the clock of inactive\nmodules to save energy and dynamic power. For example, if no floating-point\ninstructions are executing, the clock of the floating-point unit is disabled. If\nsome cores are idle, their clocks are stopped.\n2. Dynamic voltage-frequency scaling (DVFS). The second technique comes\ndirectly from the preceding formulas. PMDs, laptops, and even servers have\nperiods of low activity where there is no need to operate at the highest clock\nfrequency and voltages. Modern microprocessors typically offer a few clock\nfrequencies and voltages in which to operate that use lower power and energy.\nFigure 1.12 plots the potential power savings via DVFS for a server as the work-\nload shrinks for three different clock rates: 2.4, 1.8, and 1 GHz. The overall\nserver power savings is about 10%\u201315% for each of the two steps.\n3. Design for the typical case. Given that PMDs and laptops are often idle, mem-\nory and storage offer low power modes to save energy. For example, DRAMs\nhave a series of increasingly lower power modes to extend battery life in PMDs\nand laptops, and there have been proposals for disks that have a mode that spins\nmore slowly when unused to save power. However, you cannot access DRAMs\nor disks in these modes, so you must return to fully active mode to read or write,\nno matter how low the access rate. As mentioned, microprocessors for PCs have\nbeen designed instead for heavy use at high operating temperatures, relying on\non-chip temperature sensors to detect when activity should be reduced automat-\nically to avoid overheating. This \u201cemergency slowdown\u201d allows manufacturers\nto design for a more typical case and then rely on this safety mechanism if some-\none really does run programs that consume much more power than is typical.\n100\nPower (% of peak) \n80\n60\n40\n20\n0\n1 GHz \nDVS savings (%) \n1.8 GHz \n2.4 GHz\nIdle\n7\n14\n21\n29\n36\n43\n50\n57\n64\n71\n79\n86\n93\n100\nCompute load (%) \nFigure 1.12 Energy savings for a server using an AMD Opteron microprocessor, 8 GB\nof DRAM, and one ATA disk. At 1.8 GHz, the server can handle at most up to two-thirds\nof the workload without causing service-level violations, and at 1 GHz, it can safely han-\ndle only one-third of the workload (Figure 5.11 in Barroso and H\u20acolzle, 2009).\n1.5\nTrends in Power and Energy in Integrated Circuits\n\u25a0\n27"
    },
    {
        "page": 60,
        "text": "4. Overclocking. Intel started offering Turbo mode in 2008, where the chip decides\nthat it is safe to run at a higher clock rate for a short time, possibly on just a few\ncores, until temperature starts to rise. For example, the 3.3 GHz Core i7 can run\nin short bursts for 3.6 GHz. Indeed, the highest-performing microprocessors\neach year since 2008 shown in Figure 1.1 have all offered temporary overclock-\ning of about 10% over the nominal clock rate. For single-threaded code, these\nmicroprocessors can turn off all cores but one and run it faster. Note that,\nalthough the operating system can turn off Turbo mode, there is no notification\nonce it is enabled, so the programmers may be surprised to see their programs\nvary in performance because of room temperature!\nAlthough dynamic power is traditionally thought of as the primary source of\npower dissipation in CMOS, static power is becoming an important issue because\nleakage current flows even when a transistor is off:\nPowerstatic / Currentstatic \u0003Voltage\nThat is, static power is proportional to the number of devices.\nThus increasing the number of transistors increases power even if they are idle,\nand current leakage increases in processors with smaller transistor sizes. As a\nresult, very low-power systems are even turning off the power supply (power gat-\ning) to inactive modules in order to control loss because of leakage. In 2011 the\ngoal for leakage was 25% of the total power consumption, with leakage in\nhigh-performance designs sometimes far exceeding that goal. Leakage can be as\nhigh as 50% for such chips, in part because of the large SRAM caches that need\npower to maintain the storage values. (The S in SRAM is for static.) The only hope\nto stop leakage is to turn off power to the chips\u2019 subsets.\nFinally, because the processor is just a portion of the whole energy cost of a sys-\ntem,it can make sense to use a faster, less energy-efficient processor to allow the rest\nof the system to go into a sleep mode. This strategy is known as race-to-halt.\nThe importance of power and energy has increased the scrutiny on the effi-\nciency of an innovation, so the primary evaluation now is tasks per joule or per-\nformance per watt, contrary to performance per mm2 of silicon as in the past. This\nnew metric affects approaches to parallelism, as we will see in Chapters 4 and 5.\nThe Shift in Computer Architecture Because of Limits of Energy\nAs transistor improvement decelerates, computer architects must look elsewhere\nfor improved energy efficiency. Indeed, given the energy budget, it is easy today\nto design a microprocessor with so many transistors that they cannot all be turned\non at the same time. This phenomenon has been called dark silicon, in that much of\na chip cannot be unused (\u201cdark\u201d) at any moment in time because of thermal con-\nstraints. This observation has led architects to reexamine the fundamentals of pro-\ncessors\u2019 design in the search for a greater energy-cost performance.\nFigure 1.13, which lists the energy cost and area cost of the building blocks of\na modern computer, reveals surprisingly large ratios. For example, a 32-bit\n28\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 61,
        "text": "floating-point addition uses 30 times as much energy as an 8-bit integer add. The\narea difference is even larger, by 60 times. However, the biggest difference is in\nmemory; a 32-bit DRAM access takes 20,000 times as much energy as an 8-bit\naddition. A small SRAM is 125 times more energy-efficient than DRAM, which\ndemonstrates the importance of careful uses of caches and memory buffers.\nThe new design principle of minimizing energy per task combined with the\nrelative energy and area costs in Figure 1.13 have inspired a new direction for com-\nputer architecture, which we describe in Chapter 7. Domain-specific processors\nsave energy by reducing wide floating-point operations and deploying special-pur-\npose memories to reduce accesses to DRAM. They use those saving to provide\n10\u2013100 more (narrower) integer arithmetic units than a traditional processor.\nAlthough such processors perform only a limited set of tasks, they perform them\nremarkably faster and more energy efficiently than a general-purpose processor.\nLike a hospital with general practitioners and medical specialists, computers in\nthis energy-aware world will likely be combinations of general-purpose cores that\ncan perform any task and special-purpose cores that do a few things extremely well\nand even more cheaply.\n1.6\nTrends in Cost\nAlthough costs tend to be less important in some computer designs\u2014specifically\nsupercomputers\u2014cost-sensitive designs are of growing significance. Indeed, in\nthe past 35 years, the use of technology improvements to lower cost, as well as\nincrease performance, has been a major theme in the computer industry.\nRelative energy cost\n1\nEnergy numbers are from Mark Horowitz *Computing\u2019s Energy problem (and what we can do about it)*. ISSCC 2014\nArea numbers are from synthesized result using Design compiler under TSMC 45nm tech node. FP units used DesignWare Library.\n10\n100\n1000\n10000\n1\n10\n100\n1000\nRelative area cost\nOperation:\n8b Add\n0.03\n0.05\n0.1\n0.4\n0.9\n0.2\n3.1\n1.1\n3.7\n5\n640\n16b Add\n16b FB Add\n32b FB Add\n32b Add\n8b Mult\n32b Mult\n16b FB Mult\n32b FB Mult\n32b SRAM Read (8KB)\n32b DRAM Read\nEnergy (pJ)\nArea (\u00b5m2)\n36\n67\n137\n1360\n4184\n282\n3495\n1640\n7700\nN/A\nN/A\nFigure 1.13 Comparison of the energy and die area of arithmetic operations and energy cost of accesses to SRAM\nand DRAM. [Azizi][Dally]. Area is for TSMC 45 nm technology node.\n1.6\nTrends in Cost\n\u25a0\n29"
    },
    {
        "page": 62,
        "text": "Textbooks often ignore the cost half of cost-performance because costs change,\nthereby dating books, and because the issues are subtle and differ across industry\nsegments. Nevertheless, it\u2019s essential for computer architects to have an under-\nstanding of cost and its factors in order to make intelligent decisions about whether\na new feature should be included in designs where cost is an issue. (Imagine archi-\ntects designing skyscrapers without any information on costs of steel beams and\nconcrete!)\nThis section discusses the major factors that influence the cost of a computer\nand how these factors are changing over time.\nThe Impact of Time, Volume, and Commoditization\nThe cost of a manufactured computer component decreases over time even without\nsignificant improvements in the basic implementation technology. The underlying\nprinciple that drives costs down is the learning curve\u2014manufacturing costs\ndecrease over time. The learning curve itself is best measured by change in\nyield\u2014the percentage of manufactured devices that survives the testing procedure.\nWhether it is a chip, a board, or a system, designs that have twice the yield will have\nhalf the cost.\nUnderstanding how the learning curve improves yield is critical to projecting\ncosts over a product\u2019s life. One example is that the price per megabyte of DRAM\nhas dropped over the long term. Since DRAMs tend to be priced in close relation-\nship to cost\u2014except for periods when there is a shortage or an oversupply\u2014price\nand cost of DRAM track closely.\nMicroprocessor prices also drop over time, but because they are less standard-\nized than DRAMs, the relationship between price and cost is more complex. In a\nperiod of significant competition, price tends to track cost closely, although micro-\nprocessor vendors probably rarely sell at a loss.\nVolume is a second key factor in determining cost. Increasing volumes affect\ncost in several ways. First, they decrease the time needed to get through the learn-\ning curve, which is partly proportional to the number of systems (or chips) man-\nufactured. Second, volume decreases cost because it increases purchasing and\nmanufacturing efficiency. As a rule of thumb, some designers have estimated that\ncosts decrease about 10% for each doubling of volume. Moreover, volume\ndecreases the amount of development costs that must be amortized by each com-\nputer, thus allowing cost and selling price to be closer and still make a profit.\nCommodities are products that are sold by multiple vendors in large volumes\nand are essentially identical. Virtually all the products sold on the shelves of gro-\ncery stores are commodities, as are standard DRAMs, Flash memory, monitors,\nand keyboards. In the past 30 years, much of the personal computer industry\nhas become a commodity business focused on building desktop and laptop com-\nputers running Microsoft Windows.\nBecause many vendors ship virtually identical products, the market is highly\ncompetitive. Of course, this competition decreases the gap between cost and selling\n30\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 63,
        "text": "price, but it also decreases cost. Reductions occur because a commodity market has\nboth volume and a clear product definition, which allows multiple suppliers to\ncompete in building components for the commodity product. As a result, the over-\nall product cost is lower because of the competition among the suppliers of the\ncomponents and the volume efficiencies the suppliers can achieve. This rivalry\nhas led to the low end of the computer business being able to achieve better\nprice-performance than other sectors and has yielded greater growth at the low\nend, although with very limited profits (as is typical in any commodity business).\nCost of an Integrated Circuit\nWhy would a computer architecture book have a section on integrated circuit\ncosts? In an increasingly competitive computer marketplace where standard\nparts\u2014disks, Flash memory, DRAMs, and so on\u2014are becoming a significant por-\ntion of any system\u2019s cost, integrated circuit costs are becoming a greater portion of\nthe cost that varies between computers, especially in the high-volume, cost-\nsensitive portion of the market. Indeed, with PMDs\u2019 increasing reliance of whole\nsystems on a chip (SOC), the cost of the integrated circuits is much of the cost of the\nPMD. Thus computer designers must understand the costs of chips in order to\nunderstand the costs of current computers.\nAlthough the costs of integrated circuits have dropped exponentially, the basic\nprocess of silicon manufacture is unchanged: A wafer is still tested and chopped\ninto dies that are packaged (see Figures 1.14\u20131.16). Therefore the cost of a pack-\naged integrated circuit is\nCost of integrated circuit \u00bc Cost of die + Cost of testing die + Cost of packaging and final test\nFinal test yield\nIn this section, we focus on the cost of dies, summarizing the key issues in testing\nand packaging at the end.\nLearning how to predict the number of good chips per wafer requires first learn-\ning how many dies fit on a wafer and then learning how to predict the percentage of\nthose that will work. From there it is simple to predict cost:\nCost of die \u00bc\nCost of wafer\nDies per wafer \u0003Die yield\nThe most interesting feature of this initial term of the chip cost equation is its sen-\nsitivity to die size, shown below.\nThe number of dies per wafer is approximately the area of the wafer divided by\nthe area of the die. It can be more accurately estimated by\nDies per wafer \u00bc \u03c0 \u0003 Wafer diameter=2\n\u00f0\n\u00de2\nDie area\n\u03c0 \u0003Wafer diameter\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\n2\u0003Die area\np\nThe first term is the ratio of wafer area (\u03c0r2) to die area. The second compensates\nfor the \u201csquare peg in a round hole\u201d problem\u2014rectangular dies near the periphery\n1.6\nTrends in Cost\n\u25a0\n31"
    },
    {
        "page": 64,
        "text": "Core\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nMemory\nController\nCore\nCore\nCore\nCore\nMemory \nController\nDDRIO\nDDRIO\n3x Intel\u00ae UPI, 3x16 PCIe Gen3, 1x4 DMI3\nFigure 1.15 The components of the microprocessor die in Figure 1.14 are labeled with their functions.\nFigure 1.14 Photograph of an Intel Skylake microprocessor die, which is evaluated\nin Chapter 4.\n32\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 65,
        "text": "of round wafers. Dividing the circumference (\u03c0d) by the diagonal of a square die is\napproximately the number of dies along the edge.\nExample\nFind the number of dies per 300 mm (30 cm) wafer for a die that is 1.5 cm on a side\nand for a die that is 1.0 cm on a side.\nAnswer\nWhen die area is 2.25 cm2:\nDies per wafer \u00bc \u03c0 \u0003 30=2\n\u00f0\n\u00de2\n2:25\n\n\u03c0 \u000330\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\n2\u00032:25\np\n\u00bc 706:9\n2:25 94:2\n2:12 \u00bc 270\nBecause the area of the larger die is 2.25 times bigger, there are roughly 2.25 as\nmany smaller dies per wafer:\nDies per wafer \u00bc \u03c0 \u0003 30=2\n\u00f0\n\u00de2\n1:00\n\n\u03c0 \u000330\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\n2\u00031:00\np\n\u00bc 706:9\n1:00 94:2\n1:41 \u00bc 640\nHowever, this formula gives only the maximum number of dies per wafer. The\ncritical question is: What is the fraction of good dies on a wafer, or the die yield? A\nsimple model of integrated circuit yield, which assumes that defects are randomly\nFigure 1.16 This 200 mm diameter wafer of RISC-V dies was designed by SiFive. It has\ntwo types of RISC-V dies using an older, larger processing line. An FE310 die is 2.65\nmm \u0003 2.72 mm and an SiFive test die that is 2.89 mm \u0003 2.72 mm. The wafer contains\n1846 of the former and 1866 of the latter, totaling 3712 chips.\n1.6\nTrends in Cost\n\u25a0\n33"
    },
    {
        "page": 66,
        "text": "distributed over the wafer and that yield is inversely proportional to the complexity\nof the fabrication process, leads to the following:\nDie yield \u00bc Wafer yield\u00031= 1 + Defects per unit area\u0003Die area\n\u00f0\n\u00deN\nThis Bose-Einstein formula is an empirical model developed by looking at the\nyield of many manufacturing lines (Sydow, 2006), and it still applies today. Wafer\nyield accounts for wafers that are completely bad and so need not be tested. For\nsimplicity, we\u2019ll just assume the wafer yield is 100%. Defects per unit area is a\nmeasure of the random manufacturing defects that occur. In 2017 the value was\ntypically 0.08\u20130.10 defects per square inch for a 28-nm node and 0.10\u20130.30 for\nthe newer 16 nm node because it depends on the maturity of the process (recall\nthe learning curve mentioned earlier). The metric versions are 0.012\u20130.016 defects\nper square centimeter for 28 nm and 0.016\u20130.047 for 16 nm. Finally, N is a\nparameter called the process-complexity factor, a measure of manufacturing\ndifficulty. For 28 nm processes in 2017, N is 7.5\u20139.5. For a 16 nm process,\nN ranges from 10 to 14.\nExample\nFind the die yield for dies that are 1.5 cm on a side and 1.0 cm on a side, assuming a\ndefect density of 0.047 per cm2 and N is 12.\nAnswer\nThe total die areas are 2.25 and 1.00 cm2. For the larger die, the yield is\nDie yield \u00bc 1= 1 + 0:047\u00032:25\n\u00f0\n\u00de12 \u0003270 \u00bc 120\nFor the smaller die, the yield is\nDie yield \u00bc 1= 1 + 0:047\u00031:00\n\u00f0\n\u00de12 \u0003640 \u00bc 444\nThe bottom line is the number of good dies per wafer. Less than half of all the large\ndies are good, but nearly 70% of the small dies are good.\nAlthough many microprocessors fall between 1.00 and 2.25 cm2, low-end\nembedded 32-bit processors are sometimes as small as 0.05 cm2, processors used\nfor embedded control (for inexpensive IoT devices) are often less than 0.01 cm2,\nand high-end server and GPU chips can be as large as 8 cm2.\nGiven the tremendous price pressures on commodity products such as DRAM\nand SRAM, designers have included redundancy as a way to raise yield. For a\nnumber of years, DRAMs have regularly included some redundant memory cells\nso that a certain number of flaws can be accommodated. Designers have used sim-\nilar techniques in both standard SRAMs and in large SRAM arrays used for caches\nwithin microprocessors. GPUs have 4 redundant processors out of 84 for the same\nreason. Obviously, the presence of redundant entries can be used to boost the yield\nsignificantly.\n34\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 67,
        "text": "In 2017 processing of a 300 mm (12-inch) diameter wafer in a 28-nm technol-\nogy costs between $4000 and $5000, and a 16-nm wafer costs about $7000.\nAssuming a processed wafer cost of $7000, the cost of the 1.00 cm2 die would\nbe around $16, but the cost per die of the 2.25 cm2 die would be about $58, or\nalmost four times the cost of a die that is a little over twice as large.\nWhat should a computer designer remember about chip costs? The manufactur-\ning process dictates the wafer cost, wafer yield, and defects per unit area, so the sole\ncontrol of the designer is die area. In practice, because the number of defects per\nunit area is small, the number of good dies per wafer, and therefore the cost per die,\ngrows roughly as the square of the die area. The computer designer affects die size,\nand thus cost, both by what functions are included on or excluded from the die and\nby the number of I/O pins.\nBefore we have a part that is ready for use in a computer, the die must be tested\n(to separate the good dies from the bad), packaged, and tested again after packag-\ning. These steps all add significant costs, increasing the total by half.\nThe preceding analysis focused on the variable costs of producing a functional\ndie, which is appropriate for high-volume integrated circuits. There is, however,\none very important part of the fixed costs that can significantly affect the cost\nof an integrated circuit for low volumes (less than 1 million parts), namely, the cost\nof a mask set. Each step in the integrated circuit process requires a separate mask.\nTherefore, for modern high-density fabrication processes with up to 10 metal\nlayers, mask costs are about $4 million for 16 nm and $1.5 million for 28 nm.\nThe good news is that semiconductor companies offer \u201cshuttle runs\u201d to dramat-\nically lower the costs of tiny test chips. They lower costs by putting many small\ndesigns onto a single die to amortize the mask costs, and then later split the dies\ninto smaller pieces for each project. Thus TSMC delivers 80\u2013100 untested dies that\nare 1.57\u00031.57 mm in a 28 nm process for $30,000 in 2017. Although these die are\ntiny, they offer the architect millions of transistors to play with. For example, sev-\neral RISC-V processors would fit on such a die.\nAlthough shuttle runs help with prototyping and debugging runs, they don\u2019t\naddress small-volume production of tens to hundreds of thousands of parts.\nBecause mask costs are likely to continue to increase, some designers are incorpo-\nrating reconfigurable logic to enhance the flexibility of a part and thus reduce the\ncost implications of masks.\nCost Versus Price\nWith the commoditization of computers, the margin between the cost to manufac-\nture a product and the price the product sells for has been shrinking. Those margins\npay for a company\u2019s research and development (R&D), marketing, sales,\nmanufacturing equipment maintenance, building rental, cost of financing, pretax\nprofits, and taxes. Many engineers are surprised to find that most companies spend\nonly 4% (in the commodity PC business) to 12% (in the high-end server business)\nof their income on R&D, which includes all engineering.\n1.6\nTrends in Cost\n\u25a0\n35"
    },
    {
        "page": 68,
        "text": "Cost of Manufacturing Versus Cost of Operation\nFor the first four editions of this book, cost meant the cost to build a computer and\nprice meant price to purchase a computer. With the advent of WSCs, which contain\ntens of thousands of servers, the cost to operate the computers is significant in addi-\ntion to the cost of purchase. Economists refer to these two costs as capital expenses\n(CAPEX) and operational expenses (OPEX).\nAs Chapter 6 shows, the amortized purchase price of servers and networks\nis about half of the monthly cost to operate a WSC, assuming a short lifetime\nof the IT equipment of 3\u20134 years. About 40% of the monthly operational costs\nare for power use and the amortized infrastructure to distribute power and to cool\nthe IT equipment, despite this infrastructure being amortized over 10\u201315 years.\nThus, to lower operational costs in a WSC, computer architects need to use energy\nefficiently.\n1.7\nDependability\nHistorically, integrated circuits were one of the most reliable components of a com-\nputer. Although their pins may be vulnerable, and faults may occur over commu-\nnication channels, the failure rate inside the chip was very low. That conventional\nwisdom is changing as we head to feature sizes of 16 nm and smaller, because both\ntransient faults and permanent faults are becoming more commonplace, so archi-\ntects must design systems to cope with these challenges. This section gives a quick\noverview of the issues in dependability, leaving the official definition of the terms\nand approaches to Section D.3 in Appendix D.\nComputers are designed and constructed at different layers of abstraction. We\ncan descend recursively down through a computer seeing components enlarge\nthemselves to full subsystems until we run into individual transistors. Although\nsome faults are widespread, like the loss of power, many can be limited to a single\ncomponent in a module. Thus utter failure of a module at one level may be con-\nsidered merely a component error in a higher-level module. This distinction is\nhelpful in trying to find ways to build dependable computers.\nOne difficult question is deciding when a system is operating properly. This\ntheoretical point became concrete with the popularity of Internet services. Infra-\nstructure providers started offering service level agreements (SLAs) or service\nlevel objectives (SLOs) to guarantee that their networking or power service would\nbe dependable. For example, they would pay the customer a penalty if they did not\nmeet an agreement of some hours per month. Thus an SLA could be used to decide\nwhether the system was up or down.\nSystems alternate between two states of service with respect to an SLA:\n1. Service accomplishment, where the service is delivered as specified.\n2. Service interruption, where the delivered service is different from the SLA.\n36\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 69,
        "text": "Transitions between these two states are caused by failures (from state 1 to state 2)\nor restorations (2 to 1). Quantifying these transitions leads to the two main mea-\nsures of dependability:\n\u25a0\nModule reliability is a measure of the continuous service accomplishment (or,\nequivalently, of the time to failure) from a reference initial instant. Therefore the\nmean time to failure (MTTF) is a reliability measure. The reciprocal of MTTF is\na rate of failures, generally reported as failures per billion hours of operation, or\nFIT (for failures in time). Thus an MTTF of 1,000,000 hours equals 109/106 or\n1000 FIT. Service interruption is measured as mean time to repair (MTTR).\nMean time between failures (MTBF) is simply the sum of MTTF+MTTR.\nAlthough MTBF is widely used, MTTF is often the more appropriate term. If\na collection of modules has exponentially distributed lifetimes\u2014meaning that\nthe age of a module is not important in probability of failure\u2014the overall failure\nrate of the collection is the sum of the failure rates of the modules.\n\u25a0\nModule availability is a measure of the service accomplishment with respect to\nthe alternation between the two states of accomplishment and interruption. For\nnonredundant systems with repair, module availability is\nModule availability \u00bc\nMTTF\nMTTF + MTTR\n\u00f0\n\u00de\nNote that reliability and availability are now quantifiable metrics, rather than syn-\nonyms for dependability. From these definitions, we can estimate reliability of a\nsystem quantitatively if we make some assumptions about the reliability of com-\nponents and that failures are independent.\nExample\nAssume a disk subsystem with the following components and MTTF:\n\u25a0\n10 disks, each rated at 1,000,000-hour MTTF\n\u25a0\n1 ATA controller, 500,000-hour MTTF\n\u25a0\n1 power supply, 200,000-hour MTTF\n\u25a0\n1 fan, 200,000-hour MTTF\n\u25a0\n1 ATA cable, 1,000,000-hour MTTF\nUsing the simplifying assumptions that the lifetimes are exponentially distributed\nand that failures are independent, compute the MTTF of the system as a whole.\nAnswer\nThe sum of the failure rates is\nFailure ratesystem \u00bc 10\u0003\n1\n1,000,000 +\n1\n500,000 +\n1\n200,000 +\n1\n200,000 +\n1\n1,000,000\n\u00bc 10 + 2 + 5 + 5 + 1\n1,000,000 hours \u00bc\n23\n1,000,000 \u00bc\n23,000\n1,000,000,000 hours\n1.7\nDependability\n\u25a0\n37"
    },
    {
        "page": 70,
        "text": "or 23,000 FIT. The MTTF for the system is just the inverse of the failure rate\nMTTFsystem \u00bc\n1\nFailure ratesystem\n\u00bc 1,000,000,000 hours\n23,000\n\u00bc 43,500 hours\nor just under 5 years.\nThe primary way to cope with failure is redundancy, either in time (repeat the\noperation to see if it still is erroneous) or in resources (have other components to\ntake over from the one that failed). Once the component is replaced and the system\nis fully repaired, the dependability of the system is assumed to be as good as new.\nLet\u2019s quantify the benefits of redundancy with an example.\nExample\nDisk subsystems often have redundant power supplies to improve dependability.\nUsing the preceding components and MTTFs, calculate the reliability of redundant\npower supplies. Assume that one power supply is sufficient to run the disk subsys-\ntem and that we are adding one redundant power supply.\nAnswer\nWe need a formula to show what to expect when we can tolerate a failure and still\nprovide service. To simplify the calculations, we assume that the lifetimes of the\ncomponents are exponentially distributed and that there is no dependency between\nthe component failures. MTTF for our redundant power supplies is the mean time\nuntil one power supply fails divided by the chance that the other will fail before the\nfirst one is replaced. Thus, if the chance of a second failure before repair is small,\nthen the MTTF of the pair is large.\nSince we have two power supplies and independent failures, the mean time until\none supply fails is MTTFpower supply/2. A good approximation of the probability of\na second failure is MTTR over the mean time until the other power supply fails.\nTherefore a reasonable approximation for a redundant pair of power supplies is\nMTTFpower supply pair \u00bc MTTFpower supply=2\nMTTRpower supply\nMTTFpower supply\n\u00bc\nMTTF2\npower supply=2\nMTTRpower supply\n\u00bc\nMTTF2\npower supply\n2\u0003MTTRpower supply\nUsing the preceding MTTF numbers, if we assume it takes on average 24 hours for\na human operator to notice that a power supply has failed and to replace it, the reli-\nability of the fault tolerant pair of power supplies is\nMTTFpower supply pair \u00bc\nMTTF2\npower supply\n2\u0003MTTRpower supply\n\u00bc 200,0002\n2\u000324 \ufb03830,000,000\nmaking the pair about 4150 times more reliable than a single power supply.\nHaving quantified the cost, power, and dependability of computer technology, we\nare ready to quantify performance.\n38\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 71,
        "text": "1.8\nMeasuring, Reporting, and Summarizing Performance\nWhen we say one computer is faster than another one is, what do we mean? The\nuser of a cell phone may say a computer is faster when a program runs in less time,\nwhile an Amazon.com administrator may say a computer is faster when it com-\npletes more transactions per hour. The cell phone user wants to reduce response\ntime\u2014the time between the start and the completion of an event\u2014also referred\nto as execution time. The operator of a WSC wants to increase throughput\u2014the\ntotal amount of work done in a given time.\nIn comparing design alternatives, we often want to relate the performance of\ntwo different computers, say, X and Y. The phrase \u201cX is faster than Y\u201d is used\nhere to mean that the response time or execution time is lower on X than on Y\nfor the given task. In particular, \u201cX is n times as fast as Y\u201d will mean\nExecution timeY\nExecution timeX\n\u00bc n\nSince execution time is the reciprocal of performance, the following relationship\nholds:\nn \u00bc Execution timeY\nExecution timeX\n\u00bc\n1\nPerformanceY\n1\nPerformanceX\n\u00bc PerformanceX\nPerformanceY\nThe phrase \u201cthe throughput of X is 1.3 times as fast as Y\u201d signifies here that the\nnumber of tasks completed per unit time on computer X is 1.3 times the number\ncompleted on Y.\nUnfortunately, time is not always the metric quoted in comparing the perfor-\nmance of computers. Our position is that the only consistent and reliable measure\nof performance is the execution time of real programs, and that all proposed alter-\nnatives to time as the metric or to real programs as the items measured have even-\ntually led to misleading claims or even mistakes in computer design.\nEven execution time can be defined in different ways depending on what we\ncount. The most straightforward definition of time is called wall-clock time,\nresponse time, or elapsed time, which is the latency to complete a task, including\nstorage accesses, memory accesses, input/output activities, operating system over-\nhead\u2014everything. With multiprogramming, the processor works on another pro-\ngram while waiting for I/O and may not necessarily minimize the elapsed time of\none program. Thus we need a term to consider this activity. CPU time recognizes\nthis distinction and means the time the processor is computing, not including the\ntime waiting for I/O or running other programs. (Clearly, the response time seen by\nthe user is the elapsed time of the program, not the CPU time.)\nComputer users who routinely run the same programs would be the perfect can-\ndidates to evaluate a new computer. To evaluate a new system, these users would\nsimply compare the execution time of their workloads\u2014the mixture of programs\n1.8\nMeasuring, Reporting, and Summarizing Performance\n\u25a0\n39"
    },
    {
        "page": 72,
        "text": "and operating system commands that users run on a computer. Few are in this\nhappy situation, however. Most must rely on other methods to evaluate computers,\nand often other evaluators, hoping that these methods will predict performance for\ntheir usage of the new computer. One approach is benchmark programs, which are\nprograms that many companies use to establish the relative performance of their\ncomputers.\nBenchmarks\nThe best choice of benchmarks to measure performance is real applications, such as\nGoogle Translate mentioned in Section 1.1. Attempts at running programs that are\nmuch simpler than a real application have led to performance pitfalls. Examples\ninclude\n\u25a0\nKernels, which are small, key pieces of real applications.\n\u25a0\nToy programs, which are 100-line programs from beginning programming\nassignments, such as Quicksort.\n\u25a0\nSynthetic benchmarks, which are fake programs invented to try to match the\nprofile and behavior of real applications, such as Dhrystone.\nAll three are discredited today, usually because the compiler writer and architect\ncan conspire to make the computer appear faster on these stand-in programs than\non real applications. Regrettably for your authors\u2014who dropped the fallacy about\nusing synthetic benchmarks to characterize performance in the fourth edition of\nthis book since we thought all computer architects agreed it was disreputable\u2014\nthe synthetic program Dhrystone is still the most widely quoted benchmark for\nembedded processors in 2017!\nAnother issue is the conditions under which the benchmarks are run. One way\nto improve the performance of a benchmark has been with benchmark-specific\ncompiler flags; these flags often caused transformations that would be illegal on\nmany programs or would slow down performance on others. To restrict this pro-\ncess and increase the significance of the results, benchmark developers typically\nrequire the vendor to use one compiler and one set of flags for all the programs\nin the same language (such as C++ or C). In addition to the question of compiler\nflags, another question is whether source code modifications are allowed. There are\nthree different approaches to addressing this question:\n1. No source code modifications are allowed.\n2. Source code modifications are allowed but are essentially impossible. For\nexample, database benchmarks rely on standard database programs that are tens\nof millions of lines of code. The database companies are highly unlikely to make\nchanges to enhance the performance for one particular computer.\n3. Source modifications are allowed, as long as the altered version produces the\nsame output.\n40\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 73,
        "text": "Thekey issue thatbenchmark designers faceindecidingtoallowmodification ofthe\nsource is whether such modifications will reflect real practice and provide useful\ninsight to users, or whether these changes simply reduce the accuracy of the bench-\nmarks as predictors of real performance. As we will see in Chapter 7, domain-\nspecific architects often follow the third option when creating processors for\nwell-defined tasks.\nTo overcome the danger of placing too many eggs in one basket, collections of\nbenchmark applications, called benchmark suites, are a popular measure of perfor-\nmance of processors with a variety of applications. Of course, such collections are\nonly as good as the constituent individual benchmarks. Nonetheless, a key advan-\ntage of such suites is that the weakness of any one benchmark is lessened by the\npresence of the other benchmarks. The goal of a benchmark suite is that it will char-\nacterize the real relative performance of two computers, particularly for programs\nnot in the suite that customers are likely to run.\nA cautionary example is the Electronic Design News Embedded Microproces-\nsor Benchmark Consortium (or EEMBC, pronounced \u201cembassy\u201d) benchmarks.\nIt is a set of 41 kernels used to predict performance of different embedded\napplications: automotive/industrial, consumer, networking, office automation,\nand telecommunications. EEMBC reports unmodified performance and \u201cfull fury\u201d\nperformance, where almost anything goes. Because these benchmarks use small\nkernels, and because of the reporting options, EEMBC does not have the reputation\nof being a good predictor of relative performance of different embedded computers\nin the field. This lack of success is why Dhrystone, which EEMBC was trying to\nreplace, is sadly still used.\nOne of the most successful attempts to create standardized benchmark appli-\ncation suites has been the SPEC (Standard Performance Evaluation Corporation),\nwhich had its roots in efforts in the late 1980s to deliver better benchmarks for\nworkstations. Just as the computer industry has evolved over time, so has the need\nfor different benchmark suites, and there are now SPEC benchmarks to cover many\napplication classes. All the SPEC benchmark suites and their reported results are\nfound at http://www.spec.org.\nAlthough we focus our discussion on the SPEC benchmarks in many of the\nfollowing sections, many benchmarks have also been developed for PCs running\nthe Windows operating system.\nDesktop Benchmarks\nDesktop benchmarks divide into two broad classes: processor-intensive bench-\nmarks and graphics-intensive benchmarks, although many graphics benchmarks\ninclude intensive processor activity. SPEC originally created a benchmark set\nfocusing on processor performance (initially called SPEC89), which has evolved\ninto its sixth generation: SPEC CPU2017, which follows SPEC2006, SPEC2000,\nSPEC95 SPEC92, and SPEC89. SPEC CPU2017 consists of a set of 10 integer\nbenchmarks\n(CINT2017)\nand\n17\nfloating-point\nbenchmarks\n(CFP2017).\nFigure 1.17 describes the current SPEC CPU benchmarks and their ancestry.\n1.8\nMeasuring, Reporting, and Summarizing Performance\n\u25a0\n41"
    },
    {
        "page": 74,
        "text": "GNU C compiler\nPerl interpreter\nRoute planning\nGeneral data compression\nDiscrete Event simulation - computer network\nXML to HTML conversion via XSLT\nVideo compression\nArtificial Intelligence: alpha-beta tree search (Chess)\nArtificial Intelligence: Monte Carlo tree search (Go)\nArtificial Intelligence: recursive solution generator (Sudoku)\nExplosion modeling\nXZ\nomnetpp\nxalancbmk\nh264ref\nsjeng\ngobmk\nastar\nhmmer\nlibquantum\nbwaves\ncactuBSSN\nnamd\npovray\nlbm\nwrf\ngamess\nwupwise\napply\ngalgel\nmesa\nart\nequake\nfacerec\nammp\nlucas\nfma3d\nsixtrack\nparest\nblender\ncam4\nimagick\nnab\nfotonik3d\nroms\nmilc\nzeusmp\ngromacs\nleslie3d\ndealII\nsoplex\ncalculix\nGemsFDTD\ntonto\nsphinx3\nmcf\nbzip2\nvortex\ngzip\neon\ntwolf\nvortex\nvpr\ncrafty\nparser\nperl\ngcc\nespresso\nli\neqntott\ncompress\nsc\ngo\nijpeg\nm88ksim\nfpppp\ntomcatv\ndoduc\nnasa7\nspice\nmatrix300\nswim\nsu2cor\nwave5\napsi\nmgrid\napplu\nturb3d\nhydro2d\nX264\ndeepsjeng\nleela\nexchange2\nPhysics: relativity\nMolecular dynamics\nRay tracing\nFluid dynamics\nWeather forecasting\n3D rendering and animation\nAtmosphere modeling\nImage manipulation\nMolecular dynamics\nComputational Electromagnetics\nRegional ocean modeling\nBiomedical imaging: optical tomography with finite elements\nSPEC89\nSPEC95\nBenchmark name by SPEC generation\nSPEC92\nSPEC2000\nSPEC2006\nSPEC2017\nFigure 1.17 SPEC2017 programs and the evolution of the SPEC benchmarks over time, with integer programs above the line and floating-\npoint programs below the line. Of the 10 SPEC2017 integer programs, 5 are written in C, 4 in C++., and 1 in Fortran. For the floating-point\nprograms, the split is 3 in Fortran, 2 in C++, 2 in C, and 6 in mixed C, C++, and Fortran. The figure shows all 82 of the programs in the 1989,\n1992, 1995, 2000, 2006, and 2017 releases. Gcc is the senior citizen of the group. Only 3 integer programs and 3 floating-point programs survived\nthree or more generations. Although a few are carried over from generation to generation, the version of the program changes and either the\ninput or the size of the benchmark is often expanded to increase its running time and to avoid perturbation in measurement or domination of the\nexecution time by some factor other than CPU time. The benchmark descriptions on the left are for SPEC2017 only and do not apply to earlier\nversions. Programs in the same row from different generations of SPEC are generally not related; for example, fpppp is not a CFD code like\nbwaves.\n42\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 75,
        "text": "SPEC benchmarks are real programs modified to be portable and to minimize\nthe effect of I/O on performance. The integer benchmarks vary from part of a C\ncompiler to a go program to a video compression. The floating-point benchmarks\ninclude molecular dynamics, ray tracing, and weather forecasting. The SPEC\nCPU suite is useful for processor benchmarking for both desktop systems and\nsingle-processor servers. We will see data on many of these programs throughout\nthis book. However, these programs share little with modern programming lan-\nguages and environments and the Google Translate application that Section 1.1\ndescribes. Nearly half of them are written at least partially in Fortran! They are\neven statically linked instead of being dynamically linked like most real pro-\ngrams. Alas, the SPEC2017 applications themselves may be real, but they are\nnot inspiring. It\u2019s not clear that SPECINT2017 and SPECFP2017 capture what\nis exciting about computing in the 21st century.\nIn Section 1.11, we describe pitfalls that have occurred in developing the SPEC\nCPUbenchmark suite, as well as the challenges in maintaining a useful and pre-\ndictive benchmark suite.\nSPECCPU2017isaimedatprocessorperformance,butSPECoffersmanyother\nbenchmarks. Figure 1.18 lists the 17 SPEC benchmarks that are active in 2017.\nServer Benchmarks\nJust as servers have multiple functions, so are there multiple types of benchmarks.\nThe simplest benchmark is perhaps a processor throughput-oriented benchmark.\nSPEC CPU2017 uses the SPEC CPU benchmarks to construct a simple throughput\nbenchmark where the processing rate of a multiprocessor can be measured by run-\nning multiple copies (usually as many as there are processors) of each SPEC CPU\nbenchmark and converting the CPU time into a rate. This leads to a measurement\ncalled the SPECrate, and it is a measure of request-level parallelism from Section\n1.2. To measure thread-level parallelism, SPEC offers what they call high-\nperformance computing benchmarks around OpenMP and MPI as well as for\naccelerators such as GPUs (see Figure 1.18).\nOther than SPECrate, most server applications and benchmarks have signifi-\ncant I/O activity arising from either storage or network traffic, including bench-\nmarks for file server systems, for web servers, and for database and transaction-\nprocessing systems. SPEC offers both a file server benchmark (SPECSFS) and\na Java server benchmark. (Appendix D discusses some file and I/O system bench-\nmarks in detail.) SPECvirt_Sc2013 evaluates end-to-end performance of virtua-\nlized data center servers. Another SPEC benchmark measures power, which we\nexamine in Section 1.10.\nTransaction-processing (TP) benchmarks measure the ability of a system to\nhandle transactions that consist of database accesses and updates. Airline reserva-\ntion systems and bank ATM systems are typical simple examples of TP; more\nsophisticated TP systems involve complex databases and decision-making.\n1.8\nMeasuring, Reporting, and Summarizing Performance\n\u25a0\n43"
    },
    {
        "page": 76,
        "text": "In the mid-1980s, a group of concerned engineers formed the vendor-independent\nTransaction Processing Council (TPC) to try to create realistic and fair benchmarks\nfor TP. The TPC benchmarks are described at http://www.tpc.org.\nThe first TPC benchmark, TPC-A, was published in 1985 and has since been\nreplaced and enhanced by several different benchmarks. TPC-C, initially created in\n1992, simulates a complex query environment. TPC-H models ad hoc decision\nsupport\u2014the queries are unrelated and knowledge of past queries cannot be used\nto optimize future queries. The TPC-DI benchmark, a new data integration (DI)\ntask also known as ETL, is an important part of data warehousing. TPC-E is an\nonline transaction processing (OLTP) workload that simulates a brokerage firm\u2019s\ncustomer accounts.\nCategory\nName\nMeasures performance of\nCloud\nCloud_IaaS 2016\nCloud using NoSQL database transaction and K-Means\nclustering using map/reduce\nCPU\nCPU2017\nCompute-intensive integer and floating-point workloads\nGraphics and\nworkstation\nperformance\nSPECviewperf\u00ae 12\n3D graphics in systems running OpenGL and Direct X\nSPECwpc V2.0\nWorkstations running professional apps under the\nWindows OS\nSPECapcSM for 3ds Max 2015\u2122\n3D graphics running the proprietary Autodesk 3ds Max\n2015 app\nSPECapcSM for Maya\u00ae 2012\n3D graphics running the proprietary Autodesk 3ds Max\n2012 app\nSPECapcSM for PTC Creo 3.0\n3D graphics running the proprietary PTC Creo 3.0 app\nSPECapcSM for Siemens NX 9.0\nand 10.0\n3D graphics running the proprietary Siemens NX 9.0 or\n10.0 app\nSPECapcSM for SolidWorks 2015\n3D graphics of systems running the proprietary SolidWorks\n2015 CAD/CAM app\nHigh performance\ncomputing\nACCEL\nAccelerator and host CPU running parallel applications\nusing OpenCL and OpenACC\nMPI2007\nMPI-parallel, floating-point, compute-intensive programs\nrunning on clusters and SMPs\nOMP2012\nParallel apps running OpenMP\nJava client/server\nSPECjbb2015\nJava servers\nPower\nSPECpower_ssj2008\nPower of volume server class computers running\nSPECjbb2015\nSolution File\nServer (SFS)\nSFS2014\nFile server throughput and response time\nSPECsfs2008\nFile servers utilizing the NFSv3 and CIFS protocols\nVirtualization\nSPECvirt_sc2013\nDatacenter servers used in virtualized server consolidation\nFigure 1.18 Active benchmarks from SPEC as of 2017.\n44\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 77,
        "text": "Recognizing the controversy between traditional relational databases and \u201cNo\nSQL\u201d storage solutions, TPCx-HS measures systems using the Hadoop file system\nrunning MapReduce programs, and TPC-DS measures a decision support system\nthat uses either a relational database or a Hadoop-based system. TPC-VMS and\nTPCx-V measure database performance for virtualized systems, and TPC-Energy\nadds energy metrics to all the existing TPC benchmarks.\nAll the TPC benchmarks measure performance in transactions per second. In\naddition, they include a response time requirement so that throughput performance\nis measured only when the response time limit is met. To model real-world sys-\ntems, higher transaction rates are also associated with larger systems, in terms\nof both users and the database to which the transactions are applied. Finally, the\nsystem cost for a benchmark system must be included as well to allow accurate\ncomparisons of cost-performance. TPC modified its pricing policy so that there\nis a single specification for all the TPC benchmarks and to allow verification of\nthe prices that TPC publishes.\nReporting Performance Results\nThe guiding principle of reporting performance measurements should be repro-\nducibility\u2014list everything another experimenter would need to duplicate the\nresults. A SPEC benchmark report requires an extensive description of the com-\nputer and the compiler flags, as well as the publication of both the baseline and\nthe optimized results. In addition to hardware, software, and baseline tuning\nparameter descriptions, a SPEC report contains the actual performance times,\nshown both in tabular form and as a graph. A TPC benchmark report is even more\ncomplete, because it must include results of a benchmarking audit and cost\ninformation. These reports are excellent sources for finding the real costs of com-\nputing systems, since manufacturers compete on high performance and cost-\nperformance.\nSummarizing Performance Results\nIn practical computer design, one must evaluate myriad design choices for their\nrelative quantitative benefits across a suite of benchmarks believed to be relevant.\nLikewise, consumers trying to choose a computer will rely on performance mea-\nsurements from benchmarks, which ideally are similar to the users\u2019 applications. In\nboth cases, it is useful to have measurements for a suite of benchmarks so that the\nperformance of important applications is similar to that of one or more benchmarks\nin the suite and so that variability in performance can be understood. In the best\ncase, the suite resembles a statistically valid sample of the application space,\nbut such a sample requires more benchmarks than are typically found in most suites\nand requires a randomized sampling, which essentially no benchmark suite uses.\n1.8\nMeasuring, Reporting, and Summarizing Performance\n\u25a0\n45"
    },
    {
        "page": 78,
        "text": "Once we have chosen to measure performance with a benchmark suite, we\nwant to be able to summarize the performance results of the suite in a unique num-\nber. A simple approach to computing a summary result would be to compare the\narithmetic means of the execution times of the programs in the suite. An alternative\nwould be to add a weighting factor to each benchmark and use the weighted arith-\nmetic mean as the single number to summarize performance. One approach is to\nuse weights that make all programs execute an equal time on some reference com-\nputer, but this biases the results toward the performance characteristics of the ref-\nerence computer.\nRather than pick weights, we could normalize execution times to a reference\ncomputer by dividing the time on the reference computer by the time on the\ncomputer being rated, yielding a ratio proportional to performance. SPEC uses this\napproach, calling the ratio the SPECRatio. It has a particularly useful property\nthat matches the way we benchmark computer performance throughout this\ntext\u2014namely, comparing performance ratios. For example, suppose that the\nSPECRatio of computer A on a benchmark is 1.25 times as fast as computer B;\nthen we know\n1:25 \u00bc SPECRatioA\nSPECRatioB\n\u00bc\nExecution timereference\nExecution timeA\nExecution timereference\nExecution timeB\n\u00bc Execution timeB\nExecution timeA\n\u00bc PerformanceA\nPerformanceB\nNotice that the execution times on the reference computer drop out and the choice\nof the reference computer is irrelevant when the comparisons are made as a ratio,\nwhich is the approach we consistently use. Figure 1.19 gives an example.\nBecause a SPECRatio is a ratio rather than an absolute execution time,\nthe mean must be computed using the geometric mean. (Because SPECRatios\nhave no units, comparing SPECRatios arithmetically is meaningless.) The\nformula is\nGeometric mean \u00bc\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\nY\nn\ni\u00bc1\nsamplei\nn\ns\nIn the case of SPEC, samplei is the SPECRatio for program i. Using the geometric\nmean ensures two important properties:\n1. The geometric mean of the ratios is the same as the ratio of the geometric means.\n2. The ratio of the geometric means is equal to the geometric mean of the perfor-\nmance ratios, which implies that the choice of the reference computer is\nirrelevant.\nTherefore the motivations to use the geometric mean are substantial, especially\nwhen we use performance ratios to make comparisons.\n46\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 79,
        "text": "Example\nShow that the ratio of the geometric means is equal to the geometric mean of the\nperformance ratios and that the reference computer of SPECRatio does not matter.\nAnswer\nAssume two computers A and B and a set of SPECRatios for each.\nGeometric meanA\nGeometric meanB\n\u00bc\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\nY\nn\ni\u00bc1\nSPECRatio Ai\nn\ns\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\nY\nn\ni\u00bc1\nSPECRatio Bi\nn\ns\n\u00bc\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\nY\nn\ni\u00bc1\nSPECRatio Ai\nSPECRatio Bi\nn\ns\n\u00bc\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\nY\nn\ni\u00bc1\nExecution timereferencei\nExecution timeAi\nExecution timereferencei\nExecution timeBi\nn\nv\nu\nu\nu\nu\nu\nt\n\u00bc\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\nY\nn\ni\u00bc1\nExecution timeBi\nExecution timeAi\nn\ns\n\u00bc\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\nY\nn\ni\u00bc1\nPerformanceAi\nPerformanceBi\nn\ns\nThat is, the ratio of the geometric means of the SPECRatios of A and B is the geo-\nmetric mean of the performance ratios of A to B of all the benchmarks in the suite.\nFigure 1.19 demonstrates this validity using examples from SPEC.\nBenchmarks\nSun Ultra\nEnterprise\n2 time\n(seconds)\nAMD\nA10-\n6800K\ntime\n(seconds)\nSPEC\n2006Cint\nratio\nIntel Xeon\nE5-2690\ntime\n(seconds)\nSPEC\n2006Cint\nratio\nAMD/Intel\ntimes\n(seconds)\nIntel/AMD\nSPEC\nratios\nperlbench\n9770\n401\n24.36\n261\n37.43\n1.54\n1.54\nbzip2\n9650\n505\n19.11\n422\n22.87\n1.20\n1.20\ngcc\n8050\n490\n16.43\n227\n35.46\n2.16\n2.16\nmcf\n9120\n249\n36.63\n153\n59.61\n1.63\n1.63\ngobmk\n10,490\n418\n25.10\n382\n27.46\n1.09\n1.09\nhmmer\n9330\n182\n51.26\n120\n77.75\n1.52\n1.52\nsjeng\n12,100\n517\n23.40\n383\n31.59\n1.35\n1.35\nlibquantum\n20,720\n84\n246.08\n3\n7295.77\n29.65\n29.65\nh264ref\n22,130\n611\n36.22\n425\n52.07\n1.44\n1.44\nomnetpp\n6250\n313\n19.97\n153\n40.85\n2.05\n2.05\nastar\n7020\n303\n23.17\n209\n33.59\n1.45\n1.45\nxalancbmk\n6900\n215\n32.09\n98\n70.41\n2.19\n2.19\nGeometric mean\n31.91\n63.72\n2.00\n2.00\nFigure 1.19 SPEC2006Cint execution times (in seconds) for the Sun Ultra 5\u2014the reference computer of\nSPEC2006\u2014andexecutiontimesandSPECRatios for the AMD A10andIntelXeonE5-2690. Thefinal twocolumnsshow\ntheratiosofexecutiontimesandSPECratios.Thisfiguredemonstratestheirrelevanceofthereferencecomputerinrelative\nperformance. The ratio of the execution times is identical to the ratio of the SPEC ratios, and the ratio of the geometric\nmeans (63.7231.91/20.86\u00bc2.00) is identical to the geometric mean of the ratios (2.00). Section 1.11discusses libquantum,\nwhose performance is orders of magnitude higher than the other SPEC benchmarks.\n1.8\nMeasuring, Reporting, and Summarizing Performance\n\u25a0\n47"
    },
    {
        "page": 80,
        "text": "1.9\nQuantitative Principles of Computer Design\nNow that we have seen how to define, measure, and summarize performance, cost,\ndependability, energy, and power, we can explore guidelines and principles that are\nuseful in the design and analysis of computers. This section introduces important\nobservations about design, as well as two equations to evaluate alternatives.\nTake Advantage of Parallelism\nUsing parallelism is one of the most important methods for improving perfor-\nmance. Every chapter in this book has an example of how performance is enhanced\nthrough the exploitation of parallelism. We give three brief examples here, which\nare expounded on in later chapters.\nOur first example is the use of parallelism at the system level. To improve the\nthroughput performance on a typical server benchmark, such as SPECSFS or TPC-\nC, multiple processors and multiple storage devices can be used. The workload of\nhandling requests can then be spread among the processors and storage devices,\nresulting in improved throughput. Being able to expand memory and the number\nof processors and storage devices is called scalability, and it is a valuable asset for\nservers. Spreading of data across many storage devices for parallel reads and writes\nenables data-level parallelism. SPECSFS also relies on request-level parallelism to\nuse many processors, whereas TPC-C uses thread-level parallelism for faster pro-\ncessing of database queries.\nAt the level of an individual processor, taking advantage of parallelism among\ninstructions is critical to achieving high performance. One of the simplest ways to\ndo this is through pipelining. (Pipelining is explained in more detail in Appendix C\nand is a major focus of Chapter 3.) The basic idea behind pipelining is to overlap\ninstruction execution to reduce the total time to complete an instruction sequence.\nA key insight into pipelining is that not every instruction depends on its immediate\npredecessor, so executing the instructions completely or partially in parallel may be\npossible. Pipelining is the best-known example of ILP.\nParallelism can also be exploited at the level of detailed digital design. For\nexample, set-associative caches use multiple banks of memory that are typically\nsearched in parallel to find a desired item. Arithmetic-logical units use carry-\nlookahead, which uses parallelism to speed the process of computing sums from\nlinear to logarithmic in the number of bits per operand. These are more examples of\ndata-level parallelism.\nPrinciple of Locality\nImportant fundamental observations have come from properties of programs. The\nmost important program property that we regularly exploit is the principle of local-\nity: programs tend to reuse data and instructions they have used recently. A widely\nheld rule of thumb is that a program spends 90% of its execution time in only 10%\nof the code. An implication of locality is that we can predict with reasonable\n48\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 81,
        "text": "accuracy what instructions and data a program will use in the near future based on\nits accesses in the recent past. The principle of locality also applies to data accesses,\nthough not as strongly as to code accesses.\nTwo different types of locality have been observed. Temporal locality states\nthat recently accessed items are likely to be accessed soon. Spatial locality says\nthat items whose addresses are near one another tend to be referenced close\ntogether in time. We will see these principles applied in Chapter 2.\nFocus on the Common Case\nPerhaps the most important and pervasive principle of computer design is to focus\non the common case: in making a design trade-off, favor the frequent case over the\ninfrequent case. This principle applies when determining how to spend resources,\nbecause the impact of the improvement is higher if the occurrence is commonplace.\nFocusing on the common case works for energy as well as for resource allo-\ncation and performance. The instruction fetch and decode unit of a processor\nmay be used much more frequently than a multiplier, so optimize it first. It works\non dependability as well. If a database server has 50 storage devices for every pro-\ncessor, storage dependability will dominate system dependability.\nIn addition, the common case is often simpler and can be done faster than the\ninfrequent case. For example, when adding two numbers in the processor, we can\nexpect overflow to be a rare circumstance and can therefore improve performance\nby optimizing the more common case of no overflow. This emphasis may slow\ndown the case when overflow occurs, but if that is rare, then overall performance\nwill be improved by optimizing for the normal case.\nWe will see many cases of this principle throughout this text. In applying this\nsimple principle, we have to decide what the frequent case is and how much per-\nformance can be improved by making that case faster. A fundamental law, called\nAmdahl\u2019s Law, can be used to quantify this principle.\nAmdahl\u2019s Law\nThe performance gain that can be obtained by improving some portion of a com-\nputer can be calculated using Amdahl\u2019s Law. Amdahl\u2019s Law states that the perfor-\nmance improvement to be gained from using some faster mode of execution is\nlimited by the fraction of the time the faster mode can be used.\nAmdahl\u2019s Law defines the speedup that can be gained by using a particular\nfeature. What is speedup? Suppose that we can make an enhancement to a com-\nputer that will improve performance when it is used. Speedup is the ratio\nSpeedup \u00bc Performance for entire task using the enhancement when possible\nPerformance for entire task without using the enhancement\nAlternatively,\nSpeedup \u00bc\nExecution time for entire task without using the enhancement\nExecution time for entire task using the enhancement when possible\n1.9\nQuantitative Principles of Computer Design\n\u25a0\n49"
    },
    {
        "page": 82,
        "text": "Speedup tells us how much faster a task will run using the computer with the enhance-\nment contrary to the original computer.\nAmdahl\u2019s Law gives us a quick way to find the speedup from some enhance-\nment, which depends on two factors:\n1. The fraction of the computation time in the original computer that can be con-\nverted to take advantage of the enhancement\u2014For example, if 40 seconds of\nthe execution time of a program that takes 100 seconds in total can use an\nenhancement, the fraction is 40/100. This value, which we call Fractionenhanced,\nis always less than or equal to 1.\n2. The improvement gained by the enhanced execution mode, that is, how much\nfaster the task would run if the enhanced mode were used for the entire pro-\ngram\u2014This value is the time of the original mode over the time of the enhanced\nmode. If the enhanced mode takes, say, 4 seconds for a portion of the program,\nwhile it is 40 seconds in the original mode, the improvement is 40/4 or 10. We\ncall this value, which is always greater than 1, Speedupenhanced.\nThe execution time using the original computer with the enhanced mode will be the\ntime spent using the unenhanced portion of the computer plus the time spent using\nthe enhancement:\nExecution timenew \u00bc Execution timeold \u0003\n1Fractionenhanced\n\u00f0\n\u00de + Fractionenhanced\nSpeedupenhanced\n\u0003\n\u0004\nThe overall speedup is the ratio of the execution times:\nSpeedupoverall \u00bc Execution timeold\nExecution timenew\n\u00bc\n1\n1Fractionenhanced\n\u00f0\n\u00de + Fractionenhanced\nSpeedupenhanced\nExample\nSuppose that we want to enhance the processor used for web serving. The new\nprocessor is 10 times faster on computation in the web serving application than\nthe old processor. Assuming that the original processor is busy with computation\n40% of the time and is waiting for I/O 60% of the time, what is the overall speedup\ngained by incorporating the enhancement?\nAnswer\nFractionenhanced \u00bc 0:4; Speedupenhanced \u00bc 10; Speedupoverall \u00bc\n1\n0:6 + 0:4\n10\n\u00bc\n1\n0:64 \u0005 1:56\nAmdahl\u2019s Law expresses the law of diminishing returns: The incremental improve-\nment in speedup gained by an improvement of just a portion of the computation\ndiminishes as improvements are added. An important corollary of Amdahl\u2019s\nLaw is that if an enhancement is usable only for a fraction of a task, then we can\u2019t\nspeed up the task by more than the reciprocal of 1 minus that fraction.\n50\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 83,
        "text": "A common mistake in applying Amdahl\u2019s Law is to confuse \u201cfraction of time con-\nverted to use an enhancement\u201d and \u201cfraction of time after enhancement is in use.\u201d\nIf, instead of measuring the time that we could use the enhancement in a compu-\ntation, we measure the time after the enhancement is in use, the results will be\nincorrect!\nAmdahl\u2019s Law can serve as a guide to how much an enhancement will improve\nperformance and how to distribute resources to improve cost-performance. The\ngoal, clearly, is to spend resources proportional to where time is spent. Amdahl\u2019s\nLaw is particularly useful for comparing the overall system performance of two\nalternatives, but it can also be applied to compare two processor design alterna-\ntives, as the following example shows.\nExample\nA common transformation required in graphics processors is square root. Imple-\nmentations of floating-point (FP) square root vary significantly in performance,\nespecially among processors designed for graphics. Suppose FP square root\n(FSQRT) is responsible for 20% of the execution time of a critical graphics bench-\nmark. One proposal is to enhance the FSQRT hardware and speed up this operation\nby a factor of 10. The other alternative is just to try to make all FP instructions in the\ngraphics processor run faster by a factor of 1.6; FP instructions are responsible for\nhalf of the execution time for the application. The design team believes that they\ncan make all FP instructions run 1.6 times faster with the same effort as required for\nthe fast square root. Compare these two design alternatives.\nAnswer\nWe can compare these two alternatives by comparing the speedups:\nSpeedupFSQRT \u00bc\n1\n10:2\n\u00f0\n\u00de + 0:2\n10\n\u00bc 1\n0:82 \u00bc 1:22\nSpeedupFP \u00bc\n1\n10:5\n\u00f0\n\u00de + 0:5\n1:6\n\u00bc\n1\n0:8125 \u00bc 1:23\nImproving the performance of the FP operations overall is slightly better because\nof the higher frequency.\nAmdahl\u2019s Law is applicable beyond performance. Let\u2019s redo the reliability\nexample from page 39 after improving the reliability of the power supply via\nredundancy from 200,000-hour to 830,000,000-hour MTTF, or 4150\u0003 better.\nExample\nThe calculation of the failure rates of the disk subsystem was\nFailure ratesystem \u00bc 10\u0003\n1\n1,000,000 +\n1\n500,000 +\n1\n200,000 +\n1\n200,000 +\n1\n1,000,000\n\u00bc 10 + 2 + 5 + 5 + 1\n1,000,000 hours \u00bc\n23\n1,000,000 hours\n1.9\nQuantitative Principles of Computer Design\n\u25a0\n51"
    },
    {
        "page": 84,
        "text": "Therefore the fraction of the failure rate that could be improved is 5 per million\nhours out of 23 for the whole system, or 0.22.\nAnswer\nThe reliability improvement would be\nImprovementpower supply pair \u00bc\n1\n10:22\n\u00f0\n\u00de + 0:22\n4150\n\u00bc\n1\n0:78 \u00bc 1:28\nDespite an impressive 4150\u0003 improvement in reliability of one module, from the\nsystem\u2019s perspective, the change has a measurable but small benefit.\nIn the preceding examples, we needed the fraction consumed by the new and\nimproved version; often it is difficult to measure these times directly. In the next\nsection, we will see another way of doing such comparisons based on the use\nof an equation that decomposes the CPU execution time into three separate\ncomponents. If we know how an alternative affects these three components,\nwe can determine its overall performance. Furthermore, it is often possible to\nbuild simulators that measure these components before the hardware is actually\ndesigned.\nThe Processor Performance Equation\nEssentially all computers are constructed using a clock running at a constant rate.\nThese discrete time events are called clock periods, clocks, cycles, or clock cycles.\nComputer designers refer to the time of a clock period by its duration (e.g., 1 ns) or\nby its rate (e.g., 1 GHz). CPU time for a program can then be expressed two ways:\nCPU time \u00bc CPU clock cycles for a program\u0003Clock cycle time\nor\nCPU time \u00bc CPU clock cycles for a program\nClock rate\nIn addition to the number of clock cycles needed to execute a program, we can\nalso count the number of instructions executed\u2014the instruction path length or\ninstruction count (IC). If we know the number of clock cycles and the instruction\ncount, we can calculate the average number of clock cycles per instruction (CPI).\nBecause it is easier to work with, and because we will deal with simple processors\nin this chapter, we use CPI. Designers sometimes also use instructions per clock\n(IPC), which is the inverse of CPI.\nCPI is computed as\nCPI \u00bc CPU clock cycles for a program\nInstruction count\nThis processor figure of merit provides insight into different styles of instruction\nsets and implementations, and we will use it extensively in the next four\nchapters.\n52\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 85,
        "text": "By transposing the instruction count in the preceding formula, clock cycles\ncan be defined as IC\u0003CPI. This allows us to use CPI in the execution time\nformula:\nCPU time \u00bc Instruction count\u0003Cycles per instruction\u0003Clock cycle time\nExpanding the first formula into the units of measurement shows how the pieces fit\ntogether:\nInstructions\nProgram \u0003Clock cycles\nInstruction \u0003\nSeconds\nClock cycle \u00bc Seconds\nProgram \u00bc CPU time\nAs this formula demonstrates, processor performance is dependent upon three\ncharacteristics: clock cycle (or rate), clock cycles per instruction, and instruction\ncount. Furthermore, CPU time is equally dependent on these three characteristics;\nfor example, a 10% improvement in any one of them leads to a 10% improvement\nin CPU time.\nUnfortunately, it is difficult to change one parameter in complete isolation from\nothers because the basic technologies involved in changing each characteristic are\ninterdependent:\n\u25a0\nClock cycle time\u2014Hardware technology and organization\n\u25a0\nCPI\u2014Organization and instruction set architecture\n\u25a0\nInstruction count\u2014Instruction set architecture and compiler technology\nLuckily, many potential performance improvement techniques primarily enhance\none component of processor performance with small or predictable impacts on the\nother two.\nIn designing the processor, sometimes it is useful to calculate the number of\ntotal processor clock cycles as\nCPU clock cycles \u00bc\nX\nn\ni\u00bc1\nICi \u0003CPIi\nwhere ICi represents the number of times instruction i is executed in a program and\nCPIi represents the average number of clocks per instruction for instruction i. This\nform can be used to express CPU time as\nCPU time \u00bc\nX\nn\ni\u00bc1\nICi \u0003CPIi\n \n!\n\u0003Clock cycle time\nand overall CPI as\nCPI \u00bc\nX\nn\ni\u00bc1\nICi \u0003CPIi\nInstruction count \u00bc\nX\nn\ni\u00bc1\nICi\nInstruction count\u0003CPIi\nThe latter form of the CPI calculation uses each individual CPIi and the fraction of\noccurrences of that instruction in a program (i.e., ICi \u0006 Instruction count). Because\nit must include pipeline effects, cache misses, and any other memory system\n1.9\nQuantitative Principles of Computer Design\n\u25a0\n53"
    },
    {
        "page": 86,
        "text": "inefficiencies, CPIi should be measured and not just calculated from a table in the\nback of a reference manual.\nConsider our performance example on page 52, here modified to use measure-\nments of the frequency of the instructions and of the instruction CPI values, which,\nin practice, are obtained by simulation or by hardware instrumentation.\nExample\nSuppose we made the following measurements:\nFrequency of FP operations\u00bc25%\nAverage CPI of FP operations\u00bc4.0\nAverage CPI of other instructions\u00bc1.33\nFrequency of FSQRT\u00bc2%\nCPI of FSQRT\u00bc20\nAssume that the two design alternatives are to decrease the CPI of FSQRT to 2 or to\ndecrease the average CPI of all FP operations to 2.5. Compare these two design\nalternatives using the processor performance equation.\nAnswer\nFirst, observe that only the CPI changes; the clock rate and instruction\ncount remain identical. We start by finding the original CPI with neither\nenhancement:\nCPIoriginal \u00bc\nX\nn\ni\u00bc1\nCPIi \u0003\nICi\nInstruction count\n\u0003\n\u0004\n\u00bc 4\u000325%\n\u00f0\n\u00de + 1:33\u000375%\n\u00f0\n\u00de \u00bc 2:0\nWe can compute the CPI for the enhanced FSQRT by subtracting the cycles saved\nfrom the original CPI:\nCPIwith new FPSQR \u00bc CPIoriginal 2%\u0003 CPIold FPSQR CPIof new FPSQR only\n\u0005\n\u0006\n\u00bc 2:02%\u0003 202\n\u00f0\n\u00de \u00bc 1:64\nWe can compute the CPI for the enhancement of all FP instructions the same way\nor by summing the FP and non-FP CPIs. Using the latter gives us\nCPInew FP \u00bc 75%\u00031:33\n\u00f0\n\u00de + 25%\u00032:5\n\u00f0\n\u00de \u00bc 1:625\nSince the CPI of the overall FP enhancement is slightly lower, its performance will\nbe marginally better. Specifically, the speedup for the overall FP enhancement is\nSpeedupnew FP \u00bc CPU timeoriginal\nCPU timenew FP\n\u00bc IC\u0003Clock cycle\u0003CPIoriginal\nIC\u0003Clock cycle\u0003CPInew FP\n\u00bc CPIoriginal\nCPInew FP\n\u00bc 2:00\n1:625 \u00bc 1:23\nHappily, we obtained this same speedup using Amdahl\u2019s Law on page 51.\n54\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 87,
        "text": "It is often possible to measure the constituent parts of the processor performance\nequation. Such isolated measurements are a key advantage of using the processor\nperformance equation versus Amdahl\u2019s Law in the previous example. In particular,\nit may be difficult to measure things such as the fraction of execution time for which\na set of instructions is responsible. In practice, this would probably be computed\nby summing the product of the instruction count and the CPI for each of the instruc-\ntions in the set. Since the starting point is often individual instruction count and\nCPI measurements, the processor performance equation is incredibly useful.\nTouse theprocessor performance equationas adesign tool, we need tobeableto\nmeasure the various factors. For an existing processor, it is easy to obtain the exe-\ncution time by measurement, and we know the default clock speed. The challenge\nliesindiscoveringtheinstructioncountortheCPI.Mostprocessorsincludecounters\nfor both instructions executed and clock cycles. By periodically monitoring these\ncounters, it is also possible to attach execution time and instruction count to seg-\nments of the code, which can be helpful to programmers trying to understand and\ntune the performance of an application. Often designers or programmers will want\nto understand performance at a more fine-grained level than what is available from\nthe hardware counters. For example, they may want to know why the CPI is what it\nis. In such cases, the simulation techniques used are like those for processors that are\nbeing designed.\nTechniques that help with energy efficiency, such as dynamic voltage fre-\nquency scaling and overclocking (see Section 1.5), make this equation harder to\nuse, because the clock speed may vary while we measure the program. A simple\napproach is to turn off those features to make the results reproducible. Fortunately,\nas performance and energy efficiency are often highly correlated\u2014taking less time\nto run a program generally saves energy\u2014it\u2019s probably safe to consider perfor-\nmance without worrying about the impact of DVFS or overclocking on the results.\n1.10\nPutting It All Together: Performance, Price, and Power\nIn the \u201cPutting It All Together\u201d sections that appear near the end of every chapter,\nwe provide real examples that use the principles in that chapter. In this section, we\nlook at measures of performance and power-performance in small servers using the\nSPECpower benchmark.\nFigure 1.20 shows the three multiprocessor servers we are evaluating along\nwith their price. To keep the price comparison fair, all are Dell PowerEdge servers.\nThe first is the PowerEdge R710, which is based on the Intel Xeon \u000385670 micro-\nprocessor with a clock rate of 2.93 GHz. Unlike the Intel Core i7-6700 in Chapters\n2\u20135, which has 20 cores and a 40 MB L3 cache, this Intel chip has 22 cores and a\n55 MB L3 cache, although the cores themselves are identical. We selected a two-\nsocket system\u2014so 44 cores total\u2014with 128 GB of ECC-protected 2400 MHz\nDDR4 DRAM. The next server is the PowerEdge C630, with the same processor,\nnumber of sockets, and DRAM. The main difference is a smaller rack-mountable\npackage: \u201c2U\u201d high (3.5 inches) for the 730 versus \u201c1U\u201d (1.75 inches) for the 630.\n1.10\nPutting It All Together: Performance, Price, and Power\n\u25a0\n55"
    },
    {
        "page": 88,
        "text": "The third server is a cluster of 16 of the PowerEdge 630 s that is connected\ntogether with a 1 Gbit/s Ethernet switch. All are running the Oracle Java HotSpot\nversion 1.7 Java Virtual Machine (JVM) and the Microsoft Windows Server 2012\nR2 Datacenter version 6.3 operating system.\nNote that because of the forces of benchmarking (see Section 1.11), these are\nunusually configured servers. The systems in Figure 1.20 have little memory rel-\native to the amount of computation, and just a tiny 120 GB solid-state disk. It is\ninexpensive to add cores if you don\u2019t need to add commensurate increases in mem-\nory and storage!\nRather than run statically linked C programs of SPEC CPU, SPECpower uses a\nmore modern software stack written in Java. It is based on SPECjbb, and it repre-\nsents the server side of business applications, with performance measured as the\nnumber of transactions per second, called ssj_ops for server side Java operations\nper second. It exercises not only the processor of the server, as does SPEC CPU,\nbut also the caches, memory system, and even the multiprocessor interconnection\nsystem. In addition, it exercises the JVM, including the JIT runtime compiler and\ngarbage collector, as well as portions of the underlying operating system.\nAs the last two rows of Figure 1.20 show, the performance winner is the cluster\nof 16 R630s, which is hardly a surprise since it is by far the most expensive. The\nprice-performance winner is the PowerEdge R630, but it barely beats the cluster at\n213 versus 211 ssj-ops/$. Amazingly, the 16 node cluster is within 1% of the same\nprice-performances of a single node despite being 16 times as large.\nSystem 1\nSystem 2\nSystem 3\nComponent\nCost (% Cost)\nCost (% Cost)\nCost (% Cost)\nBase server\nPowerEdge R710\n$653 (7%)\nPowerEdge R815\n$1437 (15%)\nPowerEdge R815\n$1437 (11%)\nPower supply\n570 W\n1100 W\n1100 W\nProcessor\nXeon X5670\n$3738 (40%)\nOpteron 6174\n$2679 (29%)\nOpteron 6174\n$5358 (42%)\nClock rate\n2.93 GHz\n2.20 GHz\n2.20 GHz\nTotal cores\n12\n24\n48\nSockets\n2\n2\n4\nCores/socket\n6\n12\n12\nDRAM\n12 GB\n$484 (5%)\n16 GB\n$693 (7%)\n32 GB\n$1386 (11%)\nEthernet Inter.\nDual 1-Gbit\n$199 (2%)\nDual 1-Gbit\n$199 (2%)\nDual 1-Gbit\n$199 (2%)\nDisk\n50 GB SSD\n$1279 (14%)\n50 GB SSD\n$1279 (14%)\n50 GB SSD\n$1279 (10%)\nWindows OS\n$2999 (32%)\n$2999 (33%)\n$2999 (24%)\nTotal\n$9352 (100%)\n$9286 (100%)\n$12,658 (100%)\nMax ssj_ops\n910,978\n926,676\n1,840,450\nMax ssj_ops/$\n97\n100\n145\nFigure 1.20 Three Dell PowerEdge servers being measured and their prices as of July 2016. We calculated the cost\nof the processors by subtracting the cost of a second processor. Similarly, we calculated the overall cost of memory by\nseeing what the cost of extra memory was. Hence the base cost of the server is adjusted by removing the estimated\ncost of the default processor and memory. Chapter 5 describes how these multisocket systems are connected\ntogether, and Chapter 6 describes how clusters are connected together.\n56\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 89,
        "text": "While most benchmarks (and most computer architects) care only about per-\nformance of systems at peak load, computers rarely run at peak load. Indeed,\nFigure 6.2 in Chapter 6 shows the results of measuring the utilization of tens of\nthousands of servers over 6 months at Google, and less than 1% operate at an aver-\nage utilization of 100%. The majority have an average utilization of between 10%\nand 50%. Thus the SPECpower benchmark captures power as the target workload\nvaries from its peak in 10% intervals all the way to 0%, which is called Active Idle.\nFigure 1.21 plots the ssj_ops (SSJ operations/second) per watt and the average\npower as the target load varies from 100% to 0%. The Intel R730 always has the\nlowest power and the single node R630 has the best ssj_ops per watt across each\ntarget workload level. Since watts\u00bcjoules/second, this metric is proportional to\nSSJ operations per joule:\nssj_operations=second\nWatt\n\u00bc ssj_operations=second\nJoule=second\n\u00bc ssj_operations\nJoule\n0\n50\n100\n150\n200\n250\n300\n350\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n100%\n90%\n80%\n70%\n60%\n50%\n40%\n30%\n20%\n10%\nActive idle\nWatts\nTarget Workload\nssj_ops/watt\nDell 630 44 cores perf/watt\nDell 730 44 cores perf/watt\nDell 630 cluster 704 cores perf/watt\nDell 630 cluster 704 cores watts/node\nDell 630 44 cores watts\nDell 730 44 cores watts\nFigure 1.21 Power-performance of the three servers in Figure 1.20. Ssj_ops/watt values are on the left axis, with\nthe three columns associated with it, and watts are on the right axis, with the three lines associated with it. The hor-\nizontal axis shows the target workload, as it varies from 100% to Active Idle. The single node R630 has the best\nssj_ops/watt at each workload level, but R730 consumes the lowest power at each level.\n1.10\nPutting It All Together: Performance, Price, and Power\n\u25a0\n57"
    },
    {
        "page": 90,
        "text": "To calculate a single number to use to compare the power efficiency of sys-\ntems, SPECpower uses\nOverall ssj_ops=watt \u00bc\nX\nssj_ops\nX\npower\nThe overall ssj_ops/watt of the three servers is 10,802 for the R730, 11,157 for the\nR630, and 10,062 for the cluster of 16 R630s. Therefore the single node R630 has\nthe best power-performance. Dividing by the price of the servers, the ssj_ops/watt/\n$1,000 is 879 for the R730, 899 for the R630, and 789 (per node) for the 16-node\ncluster of R630s. Thus, after adding power, the single-node R630 is still in first\nplace in performance/price, but now the single-node R730 is significantly more\nefficient than the 16-node cluster.\n1.11\nFallacies and Pitfalls\nThe purpose of this section, which will be found in every chapter, is to explain\nsome commonly held misbeliefs or misconceptions that you should avoid. We call\nsuch misbeliefs fallacies. When discussing a fallacy, we try to give a counterex-\nample. We also discuss pitfalls\u2014easily made mistakes. Often pitfalls are general-\nizations of principles that are true in a limited context. The purpose of these\nsections is to help you avoid making these errors in computers that you design.\nPitfall\nAll exponential laws must come to an end.\nThe first to go was Dennard scaling. Dennard\u2019s 1974 observation was that power\ndensity was constant as transistors got smaller. If a transistor\u2019s linear region shrank\nby a factor 2, then both the current and voltage were also reduced by a factor of 2,\nand so the power it used fell by 4. Thus chips could be designed to operate faster and\nstill use less power. Dennard scaling ended 30 years after it was observed, not\nbecause transistors didn\u2019t continue to get smaller but because integrated circuit\ndependability limited how far current and voltage could drop. The threshold voltage\nwas driven so low that static power became a significant fraction of overall power.\nThe next deceleration was hard disk drives. Although there was no law for\ndisks, in the past 30 years the maximum areal density of hard drives\u2014which deter-\nmines disk capacity\u2014improved by 30%\u2013100% per year. In more recent years, it\nhas been less than 5% per year. Increasing density per drive has come primarily\nfrom adding more platters to a hard disk drive.\nNext up was the venerable Moore\u2019s Law. It\u2019s been a while since the number of\ntransistors per chip doubled every one to two years. For example, the DRAM chip\nintroduced in 2014 contained 8B transistors, and we won\u2019t have a 16B transistor\nDRAM chip in mass production until 2019, but Moore\u2019s Law predicts a 64B tran-\nsistor DRAM chip.\nMoreover, the actual end of scaling of the planar logic transistor was even pre-\ndicted to end by 2021. Figure 1.22 shows the predictions of the physical gate length\n58\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 91,
        "text": "of the logic transistor from two editions of the International Technology Roadmap\nfor Semiconductors (ITRS). Unlike the 2013 report that projected gate lengths\nto reach 5 nm by 2028, the 2015 report projects the length stopping at 10 nm\nby 2021. Density improvements thereafter would have to come from ways other\nthan shrinking the dimensions of transistors. It\u2019s not as dire as the ITRS suggests,\nas companies like Intel and TSMC have plans to shrink to 3 nm gate lengths, but\nthe rate of change is decreasing.\nFigure 1.23 shows the changes in increases in bandwidth over time for micro-\nprocessors and DRAM\u2014which are affected by the end of Dennard scaling\nand Moore\u2019s Law\u2014as well as for disks. The slowing of technology improvements\nis apparent in the dropping curves. The continued networking improvement is\ndue to advances in fiber optics and a planned change in pulse amplitude modu-\nlation (PAM-4) allowing two-bit encoding so as to transmit information at\n400 Gbit/s.\n0\n5\n10\n15\n20\n25\n2013\n2015\n2017\n2019\n2021\n2023\n2024\n2025\n2027\n2028\n2030\nYear\nPhysical gate length (nm)\n2013 report\n2015 report\nFigure 1.22 Predictions of logic transistor dimensions from two editions of the ITRS report. These reports started\nin 2001, but 2015 will be the last edition, as the group has disbanded because of waning interest. The only companies\nthat can produce state-of-the-art logic chips today are GlobalFoundaries, Intel, Samsung, and TSMC, whereas there\nwere 19 when the first ITRS report was released. With only four companies left, sharing of plans was too hard to\nsustain. From IEEE Spectrum, July 2016, \u201cTransistors will stop shrinking in 2021, Moore\u2019s Law Roadmap Predicts,\u201d\nby Rachel Courtland.\n1.11\nFallacies and Pitfalls\n\u25a0\n59"
    },
    {
        "page": 92,
        "text": "Fallacy\nMultiprocessors are a silver bullet.\nThe switch to multiple processors per chip around 2005 did not come from some\nbreakthrough that dramatically simplified parallel programming or made it easy to\nbuild multicore computers. The change occurred because there was no other option\ndue to the ILP walls and power walls. Multiple processors per chip do not guar-\nantee lower power; it\u2019s certainly feasible to design a multicore chip that uses more\npower. The potential is just that it\u2019s possible to continue to improve performance\nby replacing a high-clock-rate, inefficient core with several lower-clock-rate, effi-\ncient cores. As technology to shrink transistors improves, it can shrink both capac-\nitance and the supply voltage a bit so that we can get a modest increase in the\n1\n10\n100\n1000\n10,000\n100,000\n1975\n1980\n1985\n1990\n1995\n2000\n2005\n2010\n2015\n2020\nRelative Bandwidth Improvement\nYear\nMicroprocessor\nMemory\nNetwork\nDisk\nFigure 1.23 Relative bandwidth for microprocessors, networks, memory, and disks over time, based on data in\nFigure 1.10.\n60\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 93,
        "text": "number of cores per generation. For example, for the past few years, Intel has been\nadding two cores per generation in their higher-end chips.\nAs we will see in Chapters 4 and 5, performance is now a programmer\u2019s bur-\nden. The programmers\u2019 La-Z-Boy era of relying on a hardware designer to make\ntheir programs go faster without lifting a finger is officially over. If programmers\nwant their programs to go faster with each generation, they must make their pro-\ngrams more parallel.\nThe popular version of Moore\u2019s law\u2014increasing performance with each gen-\neration of technology\u2014is now up to programmers.\nPitfall\nFalling prey to Amdahl\u2019s heartbreaking law.\nVirtually every practicing computer architect knows Amdahl\u2019s Law. Despite this,\nwealmostalloccasionallyexpendtremendouseffortoptimizingsomefeaturebefore\nwe measure its usage. Only when the overall speedup is disappointing do we recall\nthat we should have measured first before we spent so much effort enhancing it!\nPitfall\nA single point of failure.\nThe calculations of reliability improvement using Amdahl\u2019s Law on page 53 show\nthat dependability is no stronger than the weakest link in a chain. No matter how\nmuch more dependable we make the power supplies, as we did in our example, the\nsingle fan will limit the reliability of the disk subsystem. This Amdahl\u2019s Law\nobservation led to a rule of thumb for fault-tolerant systems to make sure that every\ncomponent was redundant so that no single component failure could bring down\nthe whole system. Chapter 6 shows how a software layer avoids single points of\nfailure inside WSCs.\nFallacy\nHardware enhancements that increase performance also improve energy\nefficiency, or are at worst energy neutral.\nEsmaeilzadeh et al. (2011) measured SPEC2006 on just one core of a 2.67 GHz\nIntel Core i7 using Turbo mode (Section 1.5). Performance increased by a factor\nof 1.07 when the clock rate increased to 2.94 GHz (or a factor of 1.10), but the i7\nused a factor of 1.37 more joules and a factor of 1.47 more watt hours!\nFallacy\nBenchmarks remain valid indefinitely.\nSeveral factors influence the usefulness of a benchmark as a predictor of real per-\nformance, and some change over time. A big factor influencing the usefulness of a\nbenchmark is its ability to resist \u201cbenchmark engineering\u201d or \u201cbenchmarketing.\u201d\nOnce a benchmark becomes standardized and popular, there is tremendous pres-\nsure to improve performance by targeted optimizations or by aggressive interpre-\ntation of the rules for running the benchmark. Short kernels or programs that spend\ntheir time in a small amount of code are particularly vulnerable.\nFor example, despite the best intentions, the initial SPEC89 benchmark suite\nincluded a small kernel, called matrix300, which consisted of eight different\n300\u0003300 matrix multiplications. In this kernel, 99% of the execution time was\nin a single line (see SPEC, 1989). When an IBM compiler optimized this inner loop\n1.11\nFallacies and Pitfalls\n\u25a0\n61"
    },
    {
        "page": 94,
        "text": "(using a good idea called blocking, discussed in Chapters 2 and 4), performance\nimproved by a factor of 9 over a prior version of the compiler! This benchmark\ntested compiler tuning and was not, of course, a good indication of overall perfor-\nmance, nor of the typical value of this particular optimization.\nFigure 1.19 shows that if we ignore history, we may be forced to repeat it.\nSPEC Cint2006 had not been updated for a decade, giving compiler writers sub-\nstantial time to hone their optimizers to this suite. Note that the SPEC ratios of all\nbenchmarks but libquantum fall within the range of 16\u201352 for the AMD computer\nand from 22 to 78 for Intel. Libquantum runs about 250 times faster on AMD and\n7300 times faster on Intel! This \u201cmiracle\u201d is a result of optimizations by the Intel\ncompiler that automatically parallelizes the code across 22 cores and optimizes\nmemory by using bit packing, which packs together multiple narrow-range inte-\ngers to save memory space and thus memory bandwidth. If we drop this benchmark\nand recalculate the geometric means, AMD SPEC Cint2006 falls from 31.9 to 26.5\nand Intel from 63.7 to 41.4. The Intel computer is now about 1.5 times as fast as the\nAMD computer instead of 2.0 if we include libquantum, which is surely closer to\ntheir real relative performances. SPECCPU2017 dropped libquantum.\nTo illustrate the short lives of benchmarks, Figure 1.17 on page 43 lists the\nstatus of all 82 benchmarks from the various SPEC releases; Gcc is the lone sur-\nvivor from SPEC89. Amazingly, about 70% of all programs from SPEC2000 or\nearlier were dropped from the next release.\nFallacy\nThe rated mean time to failure of disks is 1,200,000 hours or almost 140 years,\nso disks practically never fail.\nThe current marketing practices of disk manufacturers can mislead users. How is\nsuch an MTTF calculated? Early in the process, manufacturers will put thousands\nof disks in a room, run them for a few months, and count the number that fail. They\ncompute MTTF as the total number of hours that the disks worked cumulatively\ndivided by the number that failed.\nOne problem is that this number far exceeds the lifetime of a disk, which is\ncommonly assumed to be five years or 43,800 hours. For this large MTTF to make\nsome sense, disk manufacturers argue that the model corresponds to a user who\nbuys a disk and then keeps replacing the disk every 5 years\u2014the planned lifetime\nof the disk. The claim is that if many customers (and their great-grandchildren) did\nthis for the next century, on average they would replace a disk 27 times before a\nfailure, or about 140 years.\nA more useful measure is the percentage of disks that fail, which is called\nthe annual failure rate. Assume 1000 disks with a 1,000,000-hour MTTF and\nthat the disks are used 24 hours a day. If you replaced failed disks with a new\none having the same reliability characteristics, the number that would fail in a year\n(8760 hours) is\nFailed disks \u00bc Number of disks\u0003Time period\nMTTF\n\u00bc 1000 disks\u00038760 hours=drive\n1,000,000 hours=failure\n\u00bc 9\nStated alternatively, 0.9% would fail per year, or 4.4% over a 5-year lifetime.\n62\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 95,
        "text": "Moreover, those high numbers are quoted assuming limited ranges of temper-\nature and vibration; if they are exceeded, then all bets are off. A survey of disk\ndrives in real environments (Gray and van Ingen, 2005) found that 3%\u20137% of\ndrives failed per year, for an MTTF of about 125,000\u2013300,000 hours. An even\nlarger study found annual disk failure rates of 2%\u201310% (Pinheiro et al., 2007).\nTherefore\nthe\nreal-world\nMTTF\nis\nabout\n2\u201310\ntimes\nworse\nthan\nthe\nmanufacturer\u2019s MTTF.\nFallacy\nPeak performance tracks observed performance.\nThe only universally true definition of peak performance is \u201cthe performance level\na computer is guaranteed not to exceed.\u201d Figure 1.24 shows the percentage of peak\nperformance for four programs on four multiprocessors. It varies from 5% to 58%.\nSince the gap is so large and can vary significantly by benchmark, peak perfor-\nmance is not generally useful in predicting observed performance.\nParatec\nplasma physics\n33%\n54%\n58%\n20%\n6%\n10%\n54%\nLBMHD\nmaterials science\nCactus\nastrophysics\nGTC\nmagnetic fusion\n0%\n30%\n20%\n10%\n40%\n50%\nPercentage of peak performance\n60%\n70%\nPower4\nItanium 2\nNEC earth simulator\nCray X1\n34%\n11%\n34%\n7%\n6%\n6% 5%\n16%\n11%\nFigure 1.24 Percentage of peak performance for four programs on four multiprocessors scaled to 64 processors.\nThe Earth Simulator and X1 are vector processors (see Chapter 4 and Appendix G). Not only did they deliver a higher\nfraction of peak performance, but they also had the highest peak performance and the lowest clock rates. Except for\nthe Paratec program, the Power 4 and Itanium 2 systems delivered between 5% and 10% of their peak. From Oliker,\nL., Canning, A., Carter, J., Shalf, J., Ethier, S., 2004. Scientific computations on modern parallel vector systems. In: Proc.\nACM/IEEE Conf. on Supercomputing, November 6\u201312, 2004, Pittsburgh, Penn., p. 10.\n1.11\nFallacies and Pitfalls\n\u25a0\n63"
    },
    {
        "page": 96,
        "text": "Pitfall\nFault detection can lower availability.\nThis apparently ironic pitfall is because computer hardware has a fair amount of\nstate that may not always be critical to proper operation. For example, it is not fatal\nif an error occurs in a branch predictor, because only performance may suffer.\nIn processors that try to exploit ILP aggressively, not all the operations are\nneeded for correct execution of the program. Mukherjee et al. (2003) found that\nless than 30% of the operations were potentially on the critical path for the\nSPEC2000 benchmarks.\nThe same observation is true about programs. If a register is \u201cdead\u201d in a pro-\ngram\u2014that is, the program will write the register before it is read again\u2014then\nerrors do not matter. If you were to crash the program upon detection of a transient\nfault in a dead register, it would lower availability unnecessarily.\nThe Sun Microsystems Division of Oracle lived this pitfall in 2000 with an L2\ncache that included parity, but not error correction, in its Sun E3000 to Sun E10000\nsystems. The SRAMs they used to build the caches had intermittent faults, which\nparity detected. If the data in the cache were not modified, the processor would\nsimply reread the data from the cache. Because the designers did not protect the\ncache with ECC (error-correcting code), the operating system had no choice but\nto report an error to dirty data and crash the program. Field engineers found no\nproblems on inspection in more than 90% of the cases.\nTo reduce the frequency of such errors, Sun modified the Solaris operating sys-\ntem to \u201cscrub\u201d the cache by having a process that proactively wrote dirty data to\nmemory. Because the processor chips did not have enough pins to add ECC, the\nonly hardware option for dirty data was to duplicate the external cache, using the\ncopy without the parity error to correct the error.\nThe pitfall is in detecting faults without providing a mechanism to correct\nthem. These engineers are unlikely to design another computer without ECC on\nexternal caches.\n1.12\nConcluding Remarks\nThis chapter has introduced a number of concepts and provided a quantitative\nframework that we will expand on throughout the book. Starting with the last edi-\ntion, energy efficiency is the constant companion to performance.\nIn Chapter 2, we start with the all-important area of memory system design. We\nwill examine a wide range of techniques that conspire to make memory look infi-\nnitely large while still being as fast as possible. (Appendix B provides introductory\nmaterial on caches for readers without much experience and background with\nthem.) As in later chapters, we will see that hardware-software cooperation has\nbecome a key to high-performance memory systems, just as it has to high-\nperformance pipelines. This chapter also covers virtual machines, an increasingly\nimportant technique for protection.\nIn Chapter 3, we look at ILP, of which pipelining is the simplest and most com-\nmon form. Exploiting ILP is one of the most important techniques for building\n64\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 97,
        "text": "high-speed uniprocessors. Chapter 3 begins with an extensive discussion of basic\nconcepts that will prepare you for the wide range of ideas examined in both chap-\nters. Chapter 3 uses examples that span about 40 years, drawing from one of the\nfirst supercomputers (IBM 360/91) to the fastest processors on the market in 2017.\nIt emphasizes what is called the dynamic or runtime approach to exploiting ILP. It\nalso talks about the limits to ILP ideas and introduces multithreading, which is fur-\nther developed in both Chapters 4 and 5. Appendix C provides introductory mate-\nrial on pipelining for readers without much experience and background in\npipelining. (We expect it to be a review for many readers, including those of\nour introductory text, Computer Organization and Design: The Hardware/Soft-\nware Interface.)\nChapter 4 explains three ways to exploit data-level parallelism. The classic and\noldest approach is vector architecture, and we start there to lay down the principles\nof SIMD design. (Appendix G goes into greater depth on vector architectures.) We\nnext explain the SIMD instruction set extensions found in most desktop micropro-\ncessors today. The third piece is an in-depth explanation of how modern graphics\nprocessing units (GPUs) work. Most GPU descriptions are written from the pro-\ngrammer\u2019s perspective, which usually hides how the computer really works. This\nsection explains GPUs from an insider\u2019s perspective, including a mapping between\nGPU jargon and more traditional architecture terms.\nChapter 5 focuses on the issue of achieving higher performance using multiple\nprocessors, or multiprocessors. Instead of using parallelism to overlap individual\ninstructions, multiprocessing uses parallelism to allow multiple instruction streams\nto be executed simultaneously on different processors. Our focus is on the domi-\nnant form of multiprocessors, shared-memory multiprocessors, though we intro-\nduce other types as well and discuss the broad issues that arise in any\nmultiprocessor. Here again we explore a variety of techniques, focusing on the\nimportant ideas first introduced in the 1980s and 1990s.\nChapter 6 introduces clusters and then goes into depth on WSCs, which com-\nputer architects help design. The designers of WSCs are the professional descen-\ndants of the pioneers of supercomputers, such as Seymour Cray, in that they are\ndesigning extreme computers. WSCs contain tens of thousands of servers, and\nthe equipment and the building that holds them cost nearly $200 million. The con-\ncerns of price-performance and energy efficiency of the earlier chapters apply to\nWSCs, as does the quantitative approach to making decisions.\nChapter 7 is new to this edition. It introduces domain-specific architectures as\nthe only path forward for improved performance and energy efficiency given the\nend of Moore\u2019s Law and Dennard scaling. It offers guidelines on how to build effec-\ntive domain-specific architectures, introduces the exciting domain of deep neural\nnetworks, describes four recent examples that take very different approaches to\naccelerating neural networks, and then compares their cost-performance.\nThis book comes with an abundance of material online (see Preface for more\ndetails), both to reduce cost and to introduce readers to a variety of advanced\ntopics. Figure 1.25 shows them all. Appendices A\u2013C, which appear in the book,\nwill be a review for many readers.\n1.12\nConcluding Remarks\n\u25a0\n65"
    },
    {
        "page": 98,
        "text": "In Appendix D, we move away from a processor-centric view and discuss\nissues in storage systems. We apply a similar quantitative approach, but one based\non observations of system behavior and using an end-to-end approach to perfor-\nmance analysis. This appendix addresses the important issue of how to store\nand retrieve data efficiently using primarily lower-cost magnetic storage technol-\nogies. Our focus is on examining the performance of disk storage systems for typ-\nical I/O-intensive workloads, such as the OLTP benchmarks mentioned in this\nchapter. We extensively explore advanced topics in RAID-based systems, which\nuse redundant disks to achieve both high performance and high availability.\nFinally, Appendix D introduces queuing theory, which gives a basis for trading\noff utilization and latency.\nAppendix E applies an embedded computing perspective to the ideas of each of\nthe chapters and early appendices.\nAppendix F explores the topic of system interconnect broadly, including wide\narea and system area networks that allow computers to communicate.\nAppendix H reviews VLIW hardware and software, which, in contrast, are less\npopular than when EPIC appeared on the scene just before the last edition.\nAppendix I describes large-scale multiprocessors for use in high-performance\ncomputing.\nAppendix J is the only appendix that remains from the first edition, and it\ncovers computer arithmetic.\nAppendix K provides a survey of instruction architectures, including the\n80x86, the IBM 360, the VAX, and many RISC architectures, including ARM,\nMIPS, Power, RISC-V, and SPARC.\nAppendix L is new and discusses advanced techniques for memory manage-\nment, focusing on support for virtual machines and design of address translation\nAppendix\nTitle\nA\nInstruction Set Principles\nB\nReview of Memory Hierarchies\nC\nPipelining: Basic and Intermediate Concepts\nD\nStorage Systems\nE\nEmbedded Systems\nF\nInterconnection Networks\nG\nVector Processors in More Depth\nH\nHardware and Software for VLIW and EPIC\nI\nLarge-Scale Multiprocessors and Scientific Applications\nJ\nComputer Arithmetic\nK\nSurvey of Instruction Set Architectures\nL\nAdvanced Concepts on Address Translation\nM\nHistorical Perspectives and References\nFigure 1.25 List of appendices.\n66\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 99,
        "text": "for very large address spaces. With the growth in cloud processors, these architec-\ntural enhancements are becoming more important.\nWe describe Appendix M next.\n1.13\nHistorical Perspectives and References\nAppendix M (available online) includes historical perspectives on the key ideas\npresented in each of the chapters in this text. These historical perspective sections\nallow us to trace the development of an idea through a series of machines or to\ndescribe significant projects. If you\u2019re interested in examining the initial develop-\nment of an idea or processor or want further reading, references are provided at the\nend of each history. For this chapter, see Section M.2, \u201cThe Early Development of\nComputers,\u201d for a discussion on the early development of digital computers and\nperformance measurement methodologies.\nAs you read the historical material, you\u2019ll soon come to realize that one of the\nimportant benefits of the youth of computing, compared to many other engineering\nfields, is that some of the pioneers are still alive\u2014we can learn the history by\nsimply asking them!\nCase Studies and Exercises by Diana Franklin\nCase Study 1: Chip Fabrication Cost\nConcepts illustrated by this case study\n\u25a0\nFabrication Cost\n\u25a0\nFabrication Yield\n\u25a0\nDefect Tolerance Through Redundancy\nMany factors are involved in the price of a computer chip. Intel is spending $7 billion\nto complete its Fab 42 fabrication facility for 7 nm technology. In this case study, we\nexplore a hypothetical company in the same situation and how different design deci-\nsions involving fabrication technology, area, and redundancy affect the cost of chips.\n1.1\n[10/10] <1.6> Figure 1.26 gives hypothetical relevant chip statistics that influence\nthe cost of several current chips. In the next few exercises, you will be exploring the\neffect of different possible design decisions for the Intel chips.\nChip\nDie Size\n(mm2)\nEstimated defect rate\n(per cm2)\nN\nManufacturing\nsize (nm)\nTransistors\n(billion)\nCores\nBlueDragon\n180\n0.03\n12\n10\n7.5\n4\nRedDragon\n120\n0.04\n14\n7\n7.5\n4\nPhoenix8\n200\n0.04\n14\n7\n12\n8\nFigure 1.26 Manufacturing cost factors for several hypothetical current and future processors.\nCase Studies and Exercises by Diana Franklin\n\u25a0\n67"
    },
    {
        "page": 100,
        "text": "a. [10] <1.6> What is the yield for the Phoenix chip?\nb. [10] <1.6> Why does Phoenix have a higher defect rate than BlueDragon?\n1.2\n[20/20/20/20] <1.6> They will sell a range of chips from that factory, and they\nneed to decide how much capacity to dedicate to each chip. Imagine that they will\nsell two chips. Phoenix is a completely new architecture designed with 7 nm tech-\nnology in mind, whereas RedDragon is the same architecture as their 10 nm Blue-\nDragon. Imagine that RedDragon will make a profit of $15 per defect-free\nchip. Phoenix will make a profit of $30 per defect-free chip. Each wafer has\na 450 mm diameter.\na. [20] <1.6> How much profit do you make on each wafer of Phoenix chips?\nb. [20] <1.6> How much profit do you make on each wafer of RedDragon\nchips?\nc. [20] <1.6> If your demand is 50,000 RedDragon chips per month and 25,000\nPhoenix chips per month, and your facility can fabricate 70 wafers a month, how\nmany wafers should you make of each chip?\n1.3\n[20/20] <1.6> Your colleague at AMD suggests that, since the yield is so poor,\nyou might make chips more cheaply if you released multiple versions of the same\nchip, just with different numbers of cores. For example, you could sell Phoenix8,\nPhoenix4, Phoenix2, and Phoenix1, which contain 8, 4, 2, and 1 cores on each chip,\nrespectively. If all eight cores are defect-free, then it is sold as Phoenix8. Chips with\nfour to seven defect-free cores are sold as Phoenix4, and those with two or three\ndefect-free cores are sold as Phoenix2. For simplification, calculate the yield for\na single core as the yield for a chip that is 1/8 the area of the original Phoenix chip.\nThen view that yield as an independent probability of a single core being defect\nfree. Calculate the yield for each configuration as the probability of at the corre-\nsponding number of cores being defect free.\na. [20] <1.6> What is the yield for a single core being defect free as well as the\nyield for Phoenix4, Phoenix2 and Phoenix1?\nb. [5] <1.6> Using your results from part a, determine which chips you think it\nwould be worthwhile to package and sell, and why.\nc. [10] <1.6> If it previously cost $20 dollars per chip to produce Phoenix8, what\nwill be the cost of the new Phoenix chips, assuming that there are no additional\ncosts associated with rescuing them from the trash?\nd. [20] <1.6> You currently make a profit of $30 for each defect-free Phoenix8, and\nyou will sell each Phoenix4 chip for $25. How much is your profit per Phoenix8\nchip if you consider (i) the purchase price of Phoenix4 chips to be entirely profit\nand (ii) apply the profit of Phoenix4 chips to each Phoenix8 chip in proportion\nto how many are produced? Use the yields calculated from part Problem 1.3a,\nnot from problem 1.1a.\n68\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 101,
        "text": "Case Study 2: Power Consumption in Computer Systems\nConcepts illustrated by this case study\n\u25a0\nAmdahl\u2019s Law\n\u25a0\nRedundancy\n\u25a0\nMTTF\n\u25a0\nPower Consumption\nPower consumption in modern systems is dependent on a variety of factors, includ-\ning the chip clock frequency, efficiency, and voltage. The following exercises\nexplore the impact on power and energy that different design decisions and use\nscenarios have.\n1.4\n[10/10/10/10] <1.5> A cell phone performs very different tasks, including stream-\ning music, streaming video, and reading email. These tasks perform very different\ncomputing tasks. Battery life and overheating are two common problems for cell\nphones, so reducing power and energy consumption are critical. In this problem,\nwe consider what to do when the user is not using the phone to its full computing\ncapacity. For these problems, we will evaluate an unrealistic scenario in which the\ncell phone has no specialized processing units. Instead, it has a quad-core, general-\npurpose processing unit. Each core uses 0.5 W at full use. For email-related tasks,\nthe quad-core is 8\u0003 as fast as necessary.\na. [10] <1.5> How much dynamic energy and power are required compared to\nrunning at full power? First, suppose that the quad-core operates for 1/8 of\nthe time and is idle for the rest of the time. That is, the clock is disabled for\n7/8 of the time, with no leakage occurring during that time. Compare total\ndynamic energy as well as dynamic power while the core is running.\nb. [10] <1.5> How much dynamic energy and power are required using fre-\nquency and voltage scaling? Assume frequency and voltage are both reduced\nto 1/8 the entire time.\nc. [10] <1.6, 1.9> Now assume the voltage may not decrease below 50% of the\noriginal voltage. This voltage is referred to as the voltage floor, and any voltage\nlower than that will lose the state. Therefore, while the frequency can keep\ndecreasing, the voltage cannot. What are the dynamic energy and power savings\nin this case?\nd. [10] <1.5> How much energy is used with a dark silicon approach? This\ninvolves creating specialized ASIC hardware for each major task and power\nCase Studies and Exercises by Diana Franklin\n\u25a0\n69"
    },
    {
        "page": 102,
        "text": "gating those elements when not in use. Only one general-purpose core would be\nprovided, and the rest of the chip would be filled with specialized units.\nFor email, the one core would operate for 25% the time and be turned\ncompletely off with power gating for the other 75% of the time. During the other\n75% of the time, a specialized ASIC unit that requires 20% of the energy of a\ncore would be running.\n1.5\n[10/10/10] <1.5> As mentioned in Exercise 1.4, cell phones run a wide variety of\napplications. We\u2019ll make the same assumptions for this exercise as the previous\none, that it is 0.5 W per core and that a quad core runs email 3\u0003 as fast.\na. [10] <1.5> Imagine that 80% of the code is parallelizable. By how much would\nthe frequency and voltage on a single core need to be increased in order to exe-\ncute at the same speed as the four-way parallelized code?\nb. [10] <1.5> What is the reduction in dynamic energy from using frequency and\nvoltage scaling in part a?\nc. [10] <1.5> How much energy is used with a dark silicon approach? In this\napproach, all hardware units are power gated, allowing them to turn off entirely\n(causing no leakage). Specialized ASICs are provided that perform the same\ncomputation for 20% of the power as the general-purpose processor. Imagine\nthat each core is power gated. The video game requires two ASICS and two\ncores. How much dynamic energy does it require compared to the baseline\nof parallelized on four cores?\n1.6\n[10/10/10/10/10/20] <1.5,1.9> General-purpose processes are optimized for\ngeneral-purpose computing. That is, they are optimized for behavior that is gener-\nally found across a large number of applications. However, once the domain is\nrestricted somewhat, the behavior that is found across a large number of the target\napplications may be different from general-purpose applications. One such appli-\ncation is deep learning or neural networks. Deep learning can be applied to many\ndifferent applications, but the fundamental building block of inference\u2014using the\nlearned information to make decisions\u2014is the same across them all. Inference\noperations are largely parallel, so they are currently performed on graphics proces-\nsing units, which are specialized more toward this type of computation, and not to\ninference in particular. In a quest for more performance per watt, Google has cre-\nated a custom chip using tensor processing units to accelerate inference operations\nin deep learning.1 This approach can be used for speech recognition and image\nrecognition, for example. This problem explores the trade-offs between this pro-\ncess, a general-purpose processor (Haswell E5-2699 v3) and a GPU (NVIDIA\nK80), in terms of performance and cooling. If heat is not removed from the com-\nputer efficiently, the fans will blow hot air back onto the computer, not cold air.\nNote: The differences are more than processor\u2014on-chip memory and DRAM also\ncome into play. Therefore statistics are at a system level, not a chip level.\n1Cite paper at this website: https://drive.google.com/file/d/0Bx4hafXDDq2EMzRNcy1vSUxtcEk/view.\n70\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 103,
        "text": "a. [10] <1.9> If Google\u2019s data center spends 70% of its time on workload A and\n30% of its time on workload B when running GPUs, what is the speedup of the\nTPU system over the GPU system?\nb. [10] <1.9> If Google\u2019s data center spends 70% of its time on workload A and\n30% of its time on workload B when running GPUs, what percentage of Max\nIPS does it achieve for each of the three systems?\nc. [15] <1.5, 1.9> Building on (b), assuming that the power scales linearly from\nidle to busy power as IPS grows from 0% to 100%, what is the performance per\nwatt of the TPU system over the GPU system?\nd. [10] <1.9> If another data center spends 40% of its time on workload A, 10%\nof its time on workload B, and 50% of its time on workload C, what are the\nspeedups of the GPU and TPU systems over the general-purpose system?\ne. [10] <1.5> A cooling door for a rack costs $4000 and dissipates 14 kW (into\nthe room; additional cost is required to get it out of the room). How many\nHaswell-, NVIDIA-, or Tensor-based servers can you cool with one cooling\ndoor, assuming TDP in Figures 1.27 and 1.28?\nf. [20] <1.5> Typical server farms can dissipate a maximum of 200 W per square\nfoot. Given that a server rack requires 11 square feet (including front and back\nclearance), how many servers from part (e) can be placed on a single rack, and\nhow many cooling doors are required?\nSystem\nChip\nThroughput\n% Max IPS\nA\nB\nC\nA\nB\nC\nGeneral-purpose\nHaswell E5-2699 v3\n5482\n13,194\n12,000\n42%\n100%\n90%\nGraphics processor\nNVIDIA K80\n13,461\n36,465\n15,000\n37%\n100%\n40%\nCustom ASIC\nTPU\n225,000\n280,000\n2000\n80%\n100%\n1%\nFigure 1.28 Performance characteristics for general-purpose processor, graphical processing unit-based or\ncustom ASIC-based system on two neural-net workloads (cite ISCA paper). Workloads A and B are from published\nresults. Workload C is a fictional, more general-purpose application.\nSystem\nChip\nTDP\nIdle power\nBusy power\nGeneral-purpose\nHaswell E5-2699 v3\n504 W\n159 W\n455 W\nGraphics processor\nNVIDIA K80\n1838 W\n357 W\n991 W\nCustom ASIC\nTPU\n861 W\n290 W\n384 W\nFigure 1.27 Hardware characteristics for general-purpose processor, graphical processing unit-based or custom\nASIC-based system, including measured power (cite ISCA paper).\nCase Studies and Exercises by Diana Franklin\n\u25a0\n71"
    },
    {
        "page": 104,
        "text": "Exercises\n1.7\n[10/15/15/10/10] <1.4, 1.5> One challenge for architects is that the design created\ntoday will require several years of implementation, verification, and testing before\nappearing on the market. This means that the architect must project what the tech-\nnology will be like several years in advance. Sometimes, this is difficult to do.\na. [10] <1.4> According to the trend in device scaling historically observed by\nMoore\u2019s Law, the number of transistors on a chip in 2025 should be how many\ntimes the number in 2015?\nb. [15] <1.5> The increase in performance once mirrored this trend. Had perfor-\nmance continued to climb at the same rate as in the 1990s, approximately what\nperformance would chips have over the VAX-11/780 in 2025?\nc. [15] <1.5> At the current rate of increase of the mid-2000s, what is a more\nupdated projection of performance in 2025?\nd. [10] <1.4> What has limited the rate of growth of the clock rate, and what are\narchitects doing with the extra transistors now to increase performance?\ne. [10] <1.4> The rate of growth for DRAM capacity has also slowed down. For\n20 years, DRAM capacity improved by 60% each year. If 8 Gbit DRAM was\nfirst available in 2015, and 16 Gbit is not available until 2019, what is the cur-\nrent DRAM growth rate?\n1.8\n[10/10] <1.5> You are designing a system for a real-time application in which\nspecific deadlines must be met. Finishing the computation faster gains nothing.\nYou find that your system can execute the necessary code, in the worst case, twice\nas fast as necessary.\na. [10] <1.5> How much energy do you save if you execute at the current speed\nand turn off the system when the computation is complete?\nb. [10] <1.5> How much energy do you save if you set the voltage and frequency\nto be half as much?\n1.9\n[10/10/20/20] <1.5> Server farms such as Google and Yahoo! provide enough\ncompute capacity for the highest request rate of the day. Imagine that most of\nthe time these servers operate at only 60% capacity. Assume further that the power\ndoes not scale linearly with the load; that is, when the servers are operating at 60%\ncapacity, they consume 90% of maximum power. The servers could be turned off,\nbut they would take too long to restart in response to more load. A new system has\nbeen proposed that allows for a quick restart but requires 20% of the maximum\npower while in this \u201cbarely alive\u201d state.\na. [10] <1.5> How much power savings would be achieved by turning off 60% of\nthe servers?\nb. [10] <1.5> How much power savings would be achieved by placing 60% of\nthe servers in the \u201cbarely alive\u201d state?\n72\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 105,
        "text": "c. [20] <1.5> How much power savings would be achieved by reducing the volt-\nage by 20% and frequency by 40%?\nd. [20] <1.5> How much power savings would be achieved by placing 30% of\nthe servers in the \u201cbarely alive\u201d state and 30% off?\n1.10\n[10/10/20] <1.7> Availability is the most important consideration for designing\nservers, followed closely by scalability and throughput.\na. [10] <1.7> We have a single processor with a failure in time (FIT) of 100. What\nis the mean time to failure (MTTF) for this system?\nb. [10] <1.7> If it takes one day to get the system running again, what is the avail-\nability of the system?\nc. [20] <1.7> Imagine that the government, to cut costs, is going to build a super-\ncomputer out of inexpensive computers rather than expensive, reliable com-\nputers. What is the MTTF for a system with 1000 processors? Assume that\nif one fails, they all fail.\n1.11\n[20/20/20] <1.1, 1.2, 1.7> In a server farm such as that used by Amazon or eBay, a\nsingle failure does not cause the entire system to crash. Instead, it will reduce the\nnumber of requests that can be satisfied at any one time.\na. [20] <1.7> If a company has 10,000 computers, each with an MTTF of 35 days,\nand it experiences catastrophic failure only if 1/3 of the computers fail, what is\nthe MTTF for the system?\nb. [20] <1.1, 1.7> If it costs an extra $1000, per computer, to double the MTTF,\nwould this be a good business decision? Show your work.\nc. [20] <1.2> Figure 1.3 shows, on average, the cost of downtimes, assuming that\nthe cost is equal at all times of the year. For retailers, however, the Christmas\nseason is the most profitable (and therefore the most costly time to lose sales). If\na catalog sales center has twice as much traffic in the fourth quarter as every\nother quarter, what is the average cost of downtime per hour during the fourth\nquarter and the rest of the year?\n1.12\n[20/10/10/10/15] <1.9> In this exercise, assume that we are considering enhanc-\ning a quad-core machine by adding encryption hardware to it. When computing\nencryption operations, it is 20 times faster than the normal mode of execution.\nWe will define percentage of encryption as the percentage of time in the original\nexecution that is spent performing encryption operations. The specialized hard-\nware increases power consumption by 2%.\na. [20] <1.9> Draw a graph that plots the speedup as a percentage of the compu-\ntation spent performing encryption. Label the y-axis \u201cNet speedup\u201d and label\nthe x-axis \u201cPercent encryption.\u201d\nb. [10] <1.9> With what percentage of encryption will adding encryption hard-\nware result in a speedup of 2?\nc. [10] <1.9> What percentage of time in the new execution will be spent on\nencryption operations if a speedup of 2 is achieved?\nCase Studies and Exercises by Diana Franklin\n\u25a0\n73"
    },
    {
        "page": 106,
        "text": "d. [15] <1.9> Suppose you have measured the percentage of encryption to be\n50%. The hardware design group estimates it can speed up the encryption hard-\nware even more with significant additional investment. You wonder whether\nadding a second unit in order to support parallel encryption operations would\nbe more useful. Imagine that in the original program, 90% of the encryption\noperations could be performed in parallel. What is the speedup of providing\ntwo or four encryption units, assuming that the parallelization allowed is limited\nto the number of encryption units?\n1.13\n[15/10] <1.9> Assume that we make an enhancement to a computer that improves\nsome mode of execution by a factor of 10. Enhanced mode is used 50% of the time,\nmeasured as a percentage of the execution time when the enhanced mode is in use.\nRecall that Amdahl\u2019s Law depends on the fraction of the original, unenhanced exe-\ncution time that could make use of enhanced mode. Thus we cannot directly use\nthis 50% measurement to compute speedup with Amdahl\u2019s Law.\na. [15] <1.9> What is the speedup we have obtained from fast mode?\nb. [10] <1.9> What percentage of the original execution time has been converted\nto fast mode?\n1.14\n[20/20/15] <1.9> When making changes to optimize part of a processor, it is often\nthe case that speeding up one type of instruction comes at the cost of slowing down\nsomething else. For example, if we put in a complicated fast floating-point unit,\nthat takes space, and something might have to be moved farther away from the\nmiddle to accommodate it, adding an extra cycle in delay to reach that unit. The\nbasic Amdahl\u2019s Law equation does not take into account this trade-off.\na. [20] <1.9> If the new fast floating-point unit speeds up floating-point opera-\ntions by, on average, 2x, and floating-point operations take 20% of the original\nprogram\u2019s execution time, what is the overall speedup (ignoring the penalty to\nany other instructions)?\nb. [20] <1.9> Now assume that speeding up the floating-point unit slowed down\ndata cache accesses, resulting in a 1.5x slowdown (or 2/3 speedup). Data cache\naccesses consume 10% of the execution time. What is the overall speedup now?\nc. [15] <1.9> After implementing the new floating-point operations, what per-\ncentage of execution time is spent on floating-point operations? What percent-\nage is spent on data cache accesses?\n1.15\n[10/10/20/20] <1.10> Your company has just bought a new 22-core processor,\nand you have been tasked with optimizing your software for this processor.\nYou will run four applications on this system, but the resource requirements are\nnot equal. Assume the system and application characteristics listed in Table 1.1.\nTable 1.1 Four applications\nApplication\nA\nB\nC\nD\n% resources needed\n41\n27\n18\n14\n% parallelizable\n50\n80\n60\n90\n74\n\u25a0\nChapter One Fundamentals of Quantitative Design and Analysis"
    },
    {
        "page": 107,
        "text": "The percentage of resources of assuming they are all run in serial. Assume\nthat when you parallelize a portion of the program by X, the speedup for that\nportion is X.\na. [10] <1.10> How much speedup would result from running application A on\nthe entire 22-core processor, as compared to running it serially?\nb. [10] <1.10> How much speedup would result from running application D on\nthe entire 22-core processor, as compared to running it serially?\nc. [20] <1.10> Given that application A requires 41% of the resources, if we stat-\nically assign it 41% of the cores, what is the overall speedup if A is run paral-\nlelized but everything else is run serially?\nd. [20] <1.10> What is the overall speedup if all four applications are statically\nassigned some of the cores, relative to their percentage of resource needs, and\nall run parallelized?\ne. [10] <1.10> Given acceleration through parallelization, what new percentage\nof the resources are the applications receiving, considering only active time on\ntheir statically-assigned cores?\n1.16\n[10/20/20/20/25] <1.10> When parallelizing an application, the ideal speedup is\nspeeding up by the number of processors. This is limited by two things: percentage\nof the application that can be parallelized and the cost of communication.\nAmdahl\u2019s Law takes into account the former but not the latter.\na. [10] <1.10> What is the speedup with N processors if 80% of the application is\nparallelizable, ignoring the cost of communication?\nb. [20] <1.10> What is the speedup with eight processors if, for every processor\nadded, the communication overhead is 0.5% of the original execution time.\nc. [20] <1.10> What is the speedup with eight processors if, for every time the\nnumber of processors is doubled, the communication overhead is increased\nby 0.5% of the original execution time?\nd. [20] <1.10> What is the speedup with N processors if, for every time the num-\nber of processors is doubled, the communication overhead is increased by 0.5%\nof the original execution time?\ne. [25] <1.10> Write the general equation that solves this question: What is the\nnumber of processors with the highest speedup in an application in which P% of\nthe original execution time is parallelizable, and, for every time the number of\nprocessors is doubled, the communication is increased by 0.5% of the original\nexecution time?\nCase Studies and Exercises by Diana Franklin\n\u25a0\n75"
    },
    {
        "page": 108,
        "text": "2.1\nIntroduction\n78\n2.2\nMemory Technology and Optimizations\n84\n2.3\nTen Advanced Optimizations of Cache Performance\n94\n2.4\nVirtual Memory and Virtual Machines\n118\n2.5\nCross-Cutting Issues: The Design of Memory Hierarchies\n126\n2.6\nPutting It All Together: Memory Hierarchies in the\nARM Cortex-A53 and Intel Core i7 6700\n129\n2.7\nFallacies and Pitfalls\n142\n2.8\nConcluding Remarks: Looking Ahead\n146\n2.9\nHistorical Perspectives and References\n148\nCase Studies and Exercises by Norman P. Jouppi,\nRajeev Balasubramonian, Naveen Muralimanohar,\nand Sheng Li\n148"
    },
    {
        "page": 109,
        "text": "2\nMemory Hierarchy\nDesign\nIdeally one would desire an indefinitely large memory capacity\nsuch that any particular\u2026 word would be immediately available\u2026\nWe are\u2026 forced to recognize the possibility of constructing a\nhierarchy of memories each of which has greater capacity than the\npreceding but which is less quickly accessible.\nA. W. Burks, H. H. Goldstine,\nand J. von Neumann,\nPreliminary Discussion of the\nLogical Design of an Electronic\nComputing Instrument (1946).\nComputer Architecture. https://doi.org/10.1016/B978-0-12-811905-1.00002-X\n\u00a9 2019 Elsevier Inc. All rights reserved."
    },
    {
        "page": 110,
        "text": "2.1\nIntroduction\nComputer pioneers correctly predicted that programmers would want unlimited\namounts of fast memory. An economical solution to that desire is a memory hierar-\nchy, which takes advantage of locality and trade-offs in the cost-performance of\nmemory technologies. The principle of locality, presented in the first chapter, says\nthat most programs do not access all code or data uniformly. Locality occurs in time\n(temporal locality) and in space (spatial locality). This principle plus the guideline\nthat for a given implementation technology and power budget, smaller hardware\ncan be made faster led to hierarchies based on memories of different speeds and\nsizes. Figure 2.1 shows several different multilevel memory hierarchies, including\ntypical sizes and speeds of access. As Flash and next generation memory technol-\nogies continue to close the gap with disks in cost per bit, such technologies are likely\nto increasingly replace magnetic disks for secondary storage. As Figure 2.1 shows,\nthese technologies are already used in many personal computers and increasingly in\nservers, where the advantages in performance, power, and density are significant.\nBecause fast memory is more expensive, a memory hierarchy is organized into\nseveral levels\u2014each smaller, faster, and more expensive per byte than the next\nlower level, which is farther from the processor. The goal is to provide a memory\nsystem with a cost per byte that is almost as low as the cheapest level of memory\nand a speed almost as fast as the fastest level. In most cases (but not all), the data\ncontained in a lower level are a superset of the next higher level. This property,\ncalled the inclusion property, is always required for the lowest level of the hierar-\nchy, which consists of main memory in the case of caches and secondary storage\n(disk or Flash) in the case of virtual memory.\nThe importance of the memory hierarchy has increased with advances in per-\nformance of processors. Figure 2.2 plots single processor performance projections\nagainst the historical performance improvement in time to access main memory.\nThe processor line shows the increase in memory requests per second on average\n(i.e., the inverse of the latency between memory references), while the memory line\nshows the increase in DRAM accesses per second (i.e., the inverse of the DRAM\naccess latency), assuming a single DRAM and a single memory bank. The reality is\nmore complex because the processor request rate is not uniform, and the memory\nsystem typically has multiple banks of DRAMs and channels. Although the gap in\naccess time increased significantly for many years, the lack of significant perfor-\nmance improvement in single processors has led to a slowdown in the growth of\nthe gap between processors and DRAM.\nBecause high-end processors have multiple cores, the bandwidth requirements\nare greater than for single cores. Although single-core bandwidth has grown more\nslowly in recent years, the gap between CPU memory demand and DRAM band-\nwidth continues to grow as the numbers of cores grow. A modern high-end desktop\nprocessor such as the Intel Core i7 6700 can generate two data memory references\nper core each clock cycle. With four cores and a 4.2 GHz clock rate, the i7 can\ngenerate a peak of 32.8 billion 64-bit data memory references per second, in addi-\ntion to a peak instruction demand of about 12.8 billion 128-bit instruction\n78\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 111,
        "text": "Size:\nSpeed:\n4\u2013 64 GB\n25 \u2013 50 us\n1\u2013 2 GB\n50 \u2013100 ns\n256 KB\n5-10 ns\n64 KB\n1 ns\n1000 bytes\n300 ps\nMemory hierarchy for a laptop or a desktop\nLaptop\nDesktop\nLevel 2 \nCache\nreference\nLevel 1 \nCache\nreference\nRegister\nreference\nMemory\nreference\nFlash\nmemory\nreference\nCPU\nRegisters\nMemory\nStorage\nMemory\nbus\nL1\nC\na\nc\nh\ne\nL2\nC\na\nc\nh\ne\nMemory hierarchy for a personal mobile device\n256 KB\n3\u201310 ns\n64 KB\n1 ns\nSize:\nSpeed:\n256 GB-1 TB\n50-100 uS\n4 \u201316 GB\n50 \u2013100 ns\n4-8 MB\n10 \u2013 20 ns\n1000 bytes\n300 ps\nLevel 1 \nCache\nreference\nRegister\nreference\nMemory\nreference\nFlash\nmemory\nreference\nLevel 2 \nCache\nreference\nLevel 3 \nCache\nreference\nCPU\nRegisters\nMemory\nStorage\nMemory\nbus\nL1\nC\na\nc\nh\ne\nL2\nC\na\nc\nh\ne\nL3\nC\na\nc\nh\ne\n256 KB\n3\u201310 ns\n64 KB\n1 ns\nSize:\nSpeed:\n256 GB-2 TB\n50-100 uS\n8\u201364 GB\n50 \u2013100 ns\n8-32  MB\n10 \u2013 20 ns\n2000 bytes\n300 ps\nMemory hierarchy for server\nSize:\nSpeed:\n16\u201364 TB\n5 \u201310 ms\n32\u2013256 GB\n50 \u2013100 ns\n16-64 MB\n10 \u2013 20 ns\n256 KB\n3\u201310 ns\n64 KB\n1 ns\n4000 bytes\n200 ps\nLevel 1 \nCache\nreference\nRegister\nreference\nMemory\nreference\nDisk\nmemory\nreference\nLevel 2 \nCache\nreference\nLevel 3 \nCache\nreference\nCPU\nRegisters\nMemory\nDisk storage\nI/O bus\nMemory\nbus\nL1\nC\na\nc\nh\ne\nL2\nC\na\nc\nh\ne\nL3\nC\na\nc\nh\ne\nFlash storage\n1-16 TB\n100-200 us\nFlash\nmemory\nreference\n(A)\n(B) \n(C) \nFigure 2.1 The levels in a typical memory hierarchy in a personal mobile device (PMD), such as a cell phone or\ntablet (A), in a laptop or desktop computer (B), and in a server (C). As we move farther away from the processor, the\nmemory in the level below becomes slower and larger. Note that the time units change by a factor of 109 from pico-\nseconds to milliseconds in the case of magnetic disks and that the size units change by a factor of 1010 from thou-\nsands of bytes to tens of terabytes. If we were to add warehouse-sized computers, as opposed to just servers, the\ncapacity scale would increase by three to six orders of magnitude. Solid-state drives (SSDs) composed of Flash are\nused exclusively in PMDs, and heavily in both laptops and desktops. In many desktops, the primary storage system is\nSSD, and expansion disks are primarily hard disk drives (HDDs). Likewise, many servers mix SSDs and HDDs.\n2.1\nIntroduction\n\u25a0\n79"
    },
    {
        "page": 112,
        "text": "references; this is a total peak demand bandwidth of 409.6 GiB/s! This incredible\nbandwidth is achieved by multiporting and pipelining the caches; by using three\nlevels of caches, with two private levels per core and a shared L3; and by using\na separate instruction and data cache at the first level. In contrast, the peak band-\nwidth for DRAM main memory, using two memory channels, is only 8% of the\ndemand bandwidth (34.1 GiB/s). Upcoming versions are expected to have an\nL4 DRAM cache using embedded or stacked DRAM (see Sections 2.2 and 2.3).\nTraditionally, designers of memory hierarchies focused on optimizing average\nmemory access time, which is determined by the cache access time, miss rate, and\nmiss penalty. More recently, however, power has become a major consideration. In\nhigh-end microprocessors, there may be 60 MiB or more of on-chip cache, and a\nlarge second- or third-level cache will consume significant power both as leakage\nwhen not operating (called static power) and as active power, as when performing a\nread or write (called dynamic power), as described in Section 2.3. The problem is\neven more acute in processors in PMDs where the CPU is less aggressive and the\npower budget may be 20 to 50 times smaller. In such cases, the caches can account\nfor 25% to 50% of the total power consumption. Thus more designs must consider\nboth performance and power trade-offs, and we will examine both in this chapter.\n1\n100\n10\n1000\nPerformance\n10,000\n100,000\n2010\n2005\n1980\n2000\n1995\nYear\nProcessor\nMemory\n1990\n1985\n2015\nFigure 2.2 Starting with 1980 performance as a baseline, the gap in performance,\nmeasured as the difference in the time between processor memory requests (for\na single processor or core) and the latency of a DRAM access, is plotted over time.\nIn mid-2017, AMD, Intel and Nvidia all announced chip sets using versions of HBM\ntechnology. Note that the vertical axis must be on a logarithmic scale to record the size\nof the processor-DRAM performance gap. The memory baseline is 64 KiB DRAM in 1980,\nwith a 1.07 per year performance improvement in latency (see Figure 2.4 on page 88).\nThe processor line assumes a 1.25 improvement per year until 1986, a 1.52 improve-\nment until 2000, a 1.20 improvement between 2000 and 2005, and only small improve-\nments in processor performance (on a per-core basis) between 2005 and 2015. As you\ncan see, until 2010 memory access times in DRAM improved slowly but consistently;\nsince 2010 the improvement in access time has reduced, as compared with the earlier\nperiods, although there have been continued improvements in bandwidth. See\nFigure 1.1 in Chapter 1 for more information.\n80\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 113,
        "text": "Basics of Memory Hierarchies: A Quick Review\nThe increasing size and thus importance of this gap led to the migration of the\nbasics of memory hierarchy into undergraduate courses in computer architecture,\nand even to courses in operating systems and compilers. Thus we\u2019ll start with\na quick review of caches and their operation. The bulk of the chapter, however,\ndescribes more advanced innovations that attack the processor\u2014memory\nperformance gap.\nWhen a word is not found in the cache, the word must be fetched from a lower\nlevel in the hierarchy (which may be another cache or the main memory) and\nplaced in the cache before continuing. Multiple words, called a block (or line),\nare moved for efficiency reasons, and because they are likely to be needed soon\ndue to spatial locality. Each cache block includes a tag to indicate which memory\naddress it corresponds to.\nA key design decision is where blocks (or lines) can be placed in a cache. The\nmost popular scheme is set associative, where a set is a group of blocks in the\ncache. A block is first mapped onto a set, and then the block can be placed any-\nwhere within that set. Finding a block consists of first mapping the block address to\nthe set and then searching the set\u2014usually in parallel\u2014to find the block. The set is\nchosen by the address of the data:\nBlock address\n\u00f0\n\u00de MOD Number of sets in cache\n\u00f0\n\u00de\nIf there are n blocks in a set, the cache placement is called n-way set associative.\nThe end points of set associativity have their own names. A direct-mapped cache\nhas just one block per set (so a block is always placed in the same location), and a\nfully associative cache has just one set (so a block can be placed anywhere).\nCaching data that is only read is easy because the copy in the cache and mem-\nory will be identical. Caching writes is more difficult; for example, how can the\ncopy in the cache and memory be kept consistent? There are two main strategies.\nA write-through cache updates the item in the cache and writes through to update\nmain memory. A write-back cache only updates the copy in the cache. When the\nblock is about to be replaced, it is copied back to memory. Both write strategies can\nuse a write buffer to allow the cache to proceed as soon as the data are placed in the\nbuffer rather than wait for full latency to write the data into memory.\nOne measure of the benefits of different cache organizations is miss rate. Miss\nrate is simply the fraction of cache accesses that result in a miss\u2014that is, the\nnumber of accesses that miss divided by the number of accesses.\nTo gain insights into the causes of high miss rates, which can inspire better\ncache designs, the three Cs model sorts all misses into three simple categories:\n\u25a0\nCompulsory\u2014The very first access to a block cannot be in the cache, so the\nblock must be brought into the cache. Compulsory misses are those that occur\neven if you were to have an infinite-sized cache.\n\u25a0\nCapacity\u2014If the cache cannot contain all the blocks needed during execution\nof a program, capacity misses (in addition to compulsory misses) will occur\nbecause of blocks being discarded and later retrieved.\n2.1\nIntroduction\n\u25a0\n81"
    },
    {
        "page": 114,
        "text": "\u25a0\nConflict\u2014If the block placement strategy is not fully associative, conflict mis-\nses (in addition to compulsory and capacity misses) will occur because a block\nmay be discarded and later retrieved if multiple blocks map to its set and\naccesses to the different blocks are intermingled.\nFigure B.8 on page 24 shows the relative frequency of cache misses broken down\nby the three Cs. As mentioned in Appendix B, the three C\u2019s model is conceptual,\nand although its insights usually hold, it is not a definitive model for explaining the\ncache behavior of individual references.\nAs we will see in Chapters 3 and 5, multithreading and multiple cores add com-\nplications for caches, both increasing the potential for capacity misses as well as\nadding a fourth C, for coherency misses due to cache flushes to keep multiple\ncaches coherent in a multiprocessor; we will consider these issues in Chapter 5.\nHowever, miss rate can be a misleading measure for several reasons. Therefore\nsome designers prefer measuring misses per instruction rather than misses per\nmemory reference (miss rate). These two are related:\nMisses\nInstruction \u00bc Miss rateMemory accesses\nInstruction count\n\u00bc Miss rateMemory accesses\nInstruction\n(This equation is often expressed in integers rather than fractions, as misses per\n1000 instructions.)\nThe problem with both measures is that they don\u2019t factor in the cost of a miss.\nA better measure is the average memory access time,\nAverage memory access time \u00bc Hit time + Miss rateMiss penalty\nwhere hit time is the time to hit in the cache and miss penalty is the time to replace\nthe block from memory (that is, the cost of a miss). Average memory access time is\nstill an indirect measure of performance; although it is a better measure than miss\nrate, it is not a substitute for execution time. In Chapter 3 we will see that specu-\nlative processors may execute other instructions during a miss, thereby reducing\nthe effective miss penalty. The use of multithreading (introduced in Chapter 3) also\nallows a processor to tolerate misses without being forced to idle. As we will exam-\nine shortly, to take advantage of such latency tolerating techniques, we need caches\nthat can service requests while handling an outstanding miss.\nIf this material is new to you, or if this quick review moves too quickly, see\nAppendix B. It covers the same introductory material in more depth and includes\nexamples of caches from real computers and quantitative evaluations of their\neffectiveness.\nSection B.3 in Appendix B presents six basic cache optimizations, which we\nquickly review here. The appendix also gives quantitative examples of the benefits\nof these optimizations. We also comment briefly on the power implications of\nthese trade-offs.\n1. Larger block size to reduce miss rate\u2014The simplest way to reduce the miss rate\nis to take advantage of spatial locality and increase the block size. Larger blocks\n82\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 115,
        "text": "reduce compulsory misses, but they also increase the miss penalty. Because\nlarger blocks lower the number of tags, they can slightly reduce static power.\nLarger block sizes can also increase capacity or conflict misses, especially in\nsmaller caches. Choosing the right block size is a complex trade-off that\ndepends on the size of cache and the miss penalty.\n2. Bigger caches to reduce miss rate\u2014The obvious way to reduce capacity misses\nis to increase cache capacity. Drawbacks include potentially longer hit time of\nthe larger cache memory and higher cost and power. Larger caches increase\nboth static and dynamic power.\n3. Higher associativity to reduce miss rate\u2014Obviously, increasing associativity\nreduces conflict misses. Greater associativity can come at the cost of increased\nhit time. As we will see shortly, associativity also increases power consumption.\n4. Multilevel caches to reduce miss penalty\u2014A difficult decision is whether to\nmake the cache hit time fast, to keep pace with the high clock rate of proces-\nsors, or to make the cache large to reduce the gap between the processor\naccesses and main memory accesses. Adding another level of cache between\nthe original cache and memory simplifies the decision. The first-level cache\ncan be small enough to match a fast clock cycle time, yet the second-level\n(or third-level) cache can be large enough to capture many accesses that would\ngo to main memory. The focus on misses in second-level caches leads to larger\nblocks, bigger capacity, and higher associativity. Multilevel caches are more\npower-efficient than a single aggregate cache. If L1 and L2 refer, respectively,\nto first- and second-level caches, we can redefine the average memory access\ntime:\nHit timeL1 + Miss rateL1  Hit timeL2 + Miss rateL2 Miss penaltyL2\n\u00f0\n\u00de\n5. Giving priority to read misses over writes to reduce miss penalty\u2014A write\nbuffer is a good place to implement this optimization. Write buffers create haz-\nards because they hold the updated value of a location needed on a read miss\u2014\nthat is, a read-after-write hazard through memory. One solution is to check the\ncontents of the write buffer on a read miss. If there are no conflicts, and if the\nmemory system is available, sending the read before the writes reduces the miss\npenalty. Most processors give reads priority over writes. This choice has little\neffect on power consumption.\n6. Avoiding address translation during indexing of the cache to reduce hit time\u2014\nCaches must cope with the translation of a virtual address from the processor to\na physical address to access memory. (Virtual memory is covered in\nSections 2.4 and B.4.) A common optimization is to use the page offset\u2014the\npart that is identical in both virtual and physical addresses\u2014to index the cache,\nas described in Appendix B, page B.38. This virtual index/physical tag method\nintroduces some system complications and/or limitations on the size and struc-\nture of the L1 cache, but the advantages of removing the translation lookaside\nbuffer (TLB) access from the critical path outweigh the disadvantages.\n2.1\nIntroduction\n\u25a0\n83"
    },
    {
        "page": 116,
        "text": "Note that each of the preceding six optimizations has a potential disadvantage\nthat can lead to increased, rather than decreased, average memory access time.\nThe rest of this chapter assumes familiarity with the preceding material and the\ndetails in Appendix B. In the \u201cPutting It All Together\u201d section, we examine the\nmemory hierarchy for a microprocessor designed for a high-end desktop or smaller\nserver, the Intel Core i7 6700, as well as one designed for use in a PMD, the Arm\nCortex-53, which is the basis for the processor used in several tablets and smart-\nphones. Within each of these classes, there is a significant diversity in approach\nbecause of the intended use of the computer.\nAlthough the i7 6700 has more cores and bigger caches than the Intel proces-\nsors designed for mobile uses, the processors have similar architectures. A proces-\nsor designed for small servers, such as the i7 6700, or larger servers, such as the\nIntel Xeon processors, typically is running a large number of concurrent processes,\noften for different users. Thus memory bandwidth becomes more important, and\nthese processors offer larger caches and more aggressive memory systems to boost\nthat bandwidth.\nIn contrast, PMDs not only serve one user but generally also have smaller oper-\nating systems, usually less multitasking (running of several applications simulta-\nneously), and simpler applications. PMDs must consider both performance and\nenergy consumption, which determines battery life. Before we dive into more\nadvanced cache organizations and optimizations, one needs to understand the\nvarious memory technologies and how they are evolving.\n2.2\nMemory Technology and Optimizations\n\u2026the one single development that put computers on their feet was the\ninvention of a reliable form of memory, namely, the core memory. \u2026Its cost\nwas reasonable, it was reliable and, because it was reliable, it could in due\ncourse be made large. (p. 209)\nMaurice Wilkes.\nMemoirs of a Computer Pioneer (1985)\nThis section describes the technologies used in a memory hierarchy, specifically in\nbuilding caches and main memory. These technologies are SRAM (static random-\naccess memory), DRAM (dynamic random-access memory), and Flash. The last of\nthese is used as an alternative to hard disks, but because its characteristics are based\non semiconductor technology, it is appropriate to include in this section.\nUsing SRAM addresses the need to minimize access time to caches. When a\ncache miss occurs, however, we need to move the data from the main memory as\nquickly as possible, which requires a high bandwidth memory. This high memory\nbandwidth can be achieved by organizing the many DRAM chips that make up the\nmain memory into multiple memory banks and by making the memory bus wider,\nor by doing both.\nTo allow memory systems to keep up with the bandwidth demands of modern\nprocessors, memory innovations started happening inside the DRAM chips\n84\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 117,
        "text": "themselves. This section describes the technology inside the memory chips and\nthose innovative, internal organizations. Before describing the technologies and\noptions, we need to introduce some terminology.\nWith the introduction of burst transfer memories, now widely used in both\nFlash and DRAM, memory latency is quoted using two measures\u2014access time\nand cycle time. Access time is the time between when a read is requested and when\nthe desired word arrives, and cycle time is the minimum time between unrelated\nrequests to memory.\nVirtually all computers since 1975 have used DRAMs for main memory and\nSRAMs for cache, with one to three levels integrated onto the processor chip with\nthe CPU. PMDs must balance power and performance, and because they have\nmore modest storage needs, PMDs use Flash rather than disk drives, a decision\nincreasingly being followed by desktop computers as well.\nSRAM Technology\nThe first letter of SRAM stands for static. The dynamic nature of the circuits in\nDRAM requires data to be written back after being read\u2014thus the difference\nbetween the access time and the cycle time as well as the need to refresh. SRAMs\ndon\u2019t need to refresh, so the access time is very close to the cycle time. SRAMs\ntypically use six transistors per bit to prevent the information from being disturbed\nwhen read. SRAM needs only minimal power to retain the charge in standby mode.\nIn earlier times, most desktop and server systems used SRAM chips for their\nprimary, secondary, or tertiary caches. Today, all three levels of caches are inte-\ngrated onto the processor chip. In high-end server chips, there may be as many\nas 24 cores and up to 60 MiB of cache; such systems are often configured with\n128\u2013256 GiB of DRAM per processor chip. The access times for large, third-level,\non-chip caches are typically two to eight times that of a second-level cache. Even\nso, the L3 access time is usually at least five times faster than a DRAM access.\nOn-chip, cache SRAMs are normally organized with a width that matches the\nblock size of the cache, with the tags stored in parallel to each block. This allows an\nentire block to be read out or written into a single cycle. This capability is partic-\nularly useful when writing data fetched after a miss into the cache or when writing\nback a block that must be evicted from the cache. The access time to the cache\n(ignoring the hit detection and selection in a set associative cache) is proportional\nto the number of blocks in the cache, whereas the energy consumption depends\nboth on the number of bits in the cache (static power) and on the number of blocks\n(dynamic power). Set associative caches reduce the initial access time to the mem-\nory because the size of the memory is smaller, but increase the time for hit detection\nand block selection, a topic we will cover in Section 2.3.\nDRAM Technology\nAs early DRAMs grew in capacity, the cost of a package with all the necessary\naddress lines was an issue. The solution was to multiplex the address lines, thereby\n2.2\nMemory Technology and Optimizations\n\u25a0\n85"
    },
    {
        "page": 118,
        "text": "cutting the number of address pins in half. Figure 2.3 shows the basic DRAM orga-\nnization. One-half of the address is sent first during the row access strobe (RAS).\nThe other half of the address, sent during the column access strobe (CAS), follows\nit. These names come from the internal chip organization, because the memory is\norganized as a rectangular matrix addressed by rows and columns.\nAn additional requirement of DRAM derives from the property signified by its\nfirst letter, D, for dynamic. To pack more bits per chip, DRAMs use only a single\ntransistor, which effectively acts as a capacitor, to store a bit. This has two implica-\ntions: first, the sensing wires that detect the charge must be precharged, which sets\nthem \u201chalfway\u201d between a logical 0 and 1, allowing the small charge stored in the cell\nto cause a 0 or 1 to be detected by the sense amplifiers. On reading, a row is placed\ninto a row buffer, where CAS signals can select a portion of the row to read out from\nthe DRAM. Because reading a row destroys the information, it must be written back\nwhenthe row isnolonger needed. Thiswrite backhappens in overlappedfashion,but\nin early DRAMs, it meant that the cycle time before a new row could be read was\nlarger than the time to read a row and access a portion of that row.\nIn addition, to prevent loss of information as the charge in a cell leaks away\n(assuming it is not read or written), each bit must be \u201crefreshed\u201d periodically. For-\ntunately, all the bits in a row can be refreshed simultaneously just by reading that\nrow and writing it back. Therefore every DRAM in the memory system must\naccess every row within a certain time window, such as 64 ms. DRAM controllers\ninclude hardware to refresh the DRAMs periodically.\nThis requirement means that the memory system is occasionally unavailable\nbecause it is sending a signal telling every chip to refresh. The time for a refresh\nis a row activation and a precharge that also writes the row back (which takes\nColumn\nRd/Wr\nPre\nAct\nRow\nBank\nFigure 2.3 Internal organization of a DRAM. Modern DRAMs are organized in banks,\nup to 16 for DDR4. Each bank consists of a series of rows. Sending an ACT (Activate)\ncommand opens a bank and a row and loads the row into a row buffer. When the\nrow is in the buffer, it can be transferred by successive column addresses at whatever\nthe width of the DRAM is (typically 4, 8, or 16 bits in DDR4) or by specifying a block trans-\nfer and the starting address. The Precharge commend (PRE) closes the bank and row\nand readies it for a new access. Each command, as well as block transfers, are synchro-\nnized with a clock. See the next section discussing SDRAM. The row and column signals\nare sometimes called RAS and CAS, based on the original names of the signals.\n86\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 119,
        "text": "roughly 2/3 of the time to get a datum because no column select is needed), and this\nis required for each row of the DRAM. Because the memory matrix in a DRAM is\nconceptually square, the number of steps in a refresh is usually the square root of\nthe DRAM capacity. DRAM designers try to keep time spent refreshing to less\nthan 5% of the total time. So far we have presented main memory as if it operated\nlike a Swiss train, consistently delivering the goods exactly according to schedule.\nIn fact, with SDRAMs, a DRAM controller (usually on the processor chip) tries to\noptimize accesses by avoiding opening new rows and using block transfer when\npossible. Refresh adds another unpredictable factor.\nAmdahl suggested as a rule of thumb that memory capacity should grow linearly\nwith processor speed to keep a balanced system. Thus a 1000 MIPS processor should\nhave 1000 MiB of memory. Processor designers rely on DRAMs to supply that\ndemand. In the past, they expected a fourfold improvement in capacity every three\nyears, or 55% per year. Unfortunately, the performance of DRAMs is growing at a\nmuch slower rate. The slower performance improvements arise primarily because of\nsmaller decreases in the row access time, which is determined by issues such as\npower limitations and the charge capacity (and thus the size) of an individual mem-\nory cell. Before we discuss these performance trends in more detail, we need to\ndescribe the major changes that occurred in DRAMs starting in the mid-1990s.\nImproving Memory Performance Inside\na DRAM Chip: SDRAMs\nAlthough very early DRAMs included a buffer allowing multiple column accesses\nto a single row, without requiring a new row access, they used an asynchronous\ninterface, which meant that every column access and transfer involved overhead\nto synchronize with the controller. In the mid-1990s, designers added a clock sig-\nnal to the DRAM interface so that the repeated transfers would not bear that over-\nhead, thereby creating synchronous DRAM (SDRAM). In addition to reducing\noverhead, SDRAMs allowed the addition of a burst transfer mode where multiple\ntransfers can occur without specifying a new column address. Typically, eight or\nmore 16-bit transfers can occur without sending any new addresses by placing the\nDRAM in burst mode. The inclusion of such burst mode transfers has meant that\nthere is a significant gap between the bandwidth for a stream of random accesses\nversus access to a block of data.\nTo overcome the problem of getting more bandwidth from the memory as\nDRAM density increased, DRAMS were made wider. Initially, they offered a\nfour-bit transfer mode; in 2017, DDR2, DDR3, and DDR DRAMS had up to 4, 8,\nor 16 bit buses.\nIn the early 2000s, a further innovation was introduced: double data rate\n(DDR), which allows a DRAM to transfer data both on the rising and the falling\nedge of the memory clock, thereby doubling the peak data rate.\nFinally, SDRAMs introduced banks to help with power management, improve\naccess time, and allow interleaved and overlapped accesses to different banks.\n2.2\nMemory Technology and Optimizations\n\u25a0\n87"
    },
    {
        "page": 120,
        "text": "Access to different banks can be overlapped with each other, and each bank has its\nown row buffer. Creating multiple banks inside a DRAM effectively adds another\nsegment to the address, which now consists of bank number, row address, and col-\numn address. When an address is sent that designates a new bank, that bank must\nbe opened, incurring an additional delay. The management of banks and row\nbuffers is completely handled by modern memory control interfaces, so that when\na subsequent access specifies the same row for an open bank, the access can happen\nquickly, sending only the column address.\nTo initiate a new access, the DRAM controller sends a bank and row number\n(called Activate in SDRAMs and formerly called RAS\u2014row select). That com-\nmand opens the row and reads the entire row into a buffer. A column address\ncan then be sent, and the SDRAM can transfer one or more data items, depending\non whether it is a single item request or a burst request. Before accessing a new\nrow, the bank must be precharged. If the row is in the same bank, then the pre-\ncharge delay is seen; however, if the row is in another bank, closing the row\nand precharging can overlap with accessing the new row. In synchronous DRAMs,\neach of these command cycles requires an integral number of clock cycles.\nFrom 1980 to 1995, DRAMs scaled with Moore\u2019s Law, doubling capacity\nevery 18 months (or a factor of 4 in 3 years). From the mid-1990s to 2010, capacity\nincreased more slowly with roughly 26 months between a doubling. From 2010 to\n2016, capacity only doubled! Figure 2.4 shows the capacity and access time for\nvarious generations of DDR SDRAMs. From DDR1 to DDR3, access times\nimproved by a factor of about 3, or about 7% per year. DDR4 improves power\nand bandwidth over DDR3, but has similar access latency.\nAs Figure 2.4 shows, DDR is a sequence of standards. DDR2 lowers power\nfrom DDR1 by dropping the voltage from 2.5 to 1.8 V and offers higher clock\nrates: 266, 333, and 400 MHz. DDR3 drops voltage to 1.5 V and has a maximum\nclock speed of 800 MHz. (As we discuss in the next section, GDDR5 is a graphics\nBest case access time (no precharge)\nPrecharge needed\nProduction year\nChip size\nDRAM type\nRAS time (ns)\nCAS time (ns)\nTotal (ns)\nTotal (ns)\n2000\n256M bit\nDDR1\n21\n21\n42\n63\n2002\n512M bit\nDDR1\n15\n15\n30\n45\n2004\n1G bit\nDDR2\n15\n15\n30\n45\n2006\n2G bit\nDDR2\n10\n10\n20\n30\n2010\n4G bit\nDDR3\n13\n13\n26\n39\n2016\n8G bit\nDDR4\n13\n13\n26\n39\nFigure 2.4 Capacity and access times for DDR SDRAMs by year of production. Access time is for a random memory\nword and assumes a new row must be opened. If the row is in a different bank, we assume the bank is precharged;\nif the row is not open, then a precharge is required, and the access time is longer. As the number of banks has\nincreased, the ability to hide the precharge time has also increased. DDR4 SDRAMs were initially expected in\n2014, but did not begin production until early 2016.\n88\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 121,
        "text": "RAM and is based on DDR3 DRAMs.) DDR4, which shipped in volume in early\n2016, but was expected in 2014, drops the voltage to 1\u20131.2 V and has a maximum\nexpected clock rate of 1600 MHz. DDR5 is unlikely to reach production quantities\nuntil 2020 or later.\nWith the introduction of DDR, memory designers increasing focused on band-\nwidth, because improvements in access time were difficult. Wider DRAMs, burst\ntransfers, and double data rate all contributed to rapid increases in memory band-\nwidth. DRAMs are commonly sold on small boards called dual inline memory\nmodules (DIMMs) that contain 4\u201316 DRAM chips and that are normally organized\nto be 8 bytes wide (+ ECC) for desktop and server systems. When DDR SDRAMs\nare packaged as DIMMs, they are confusingly labeled by the peak DIMM band-\nwidth. Therefore the DIMM name PC3200 comes from 200 MHz28 bytes,\nor 3200 MiB/s; it is populated with DDR SDRAM chips. Sustaining the confusion,\nthe chips themselves are labeled with the number of bits per second rather than\ntheir clock rate, so a 200 MHz DDR chip is called a DDR400. Figure 2.5 shows\nthe relationships\u2019 I/O clock rate, transfers per second per chip, chip bandwidth,\nchip name, DIMM bandwidth, and DIMM name.\nReducing Power Consumption in SDRAMs\nPower consumption in dynamic memory chips consists of both dynamic power\nused in a read or write and static or standby power; both depend on the operating\nvoltage. In the most advanced DDR4 SDRAMs, the operating voltage has dropped\nto 1.2 V, significantly reducing power versus DDR2 and DDR3 SDRAMs. The\naddition of banks also reduced power because only the row in a single bank is read.\nStandard\nI/O clock rate\nM transfers/s\nDRAM name\nMiB/s/DIMM\nDIMM name\nDDR1\n133\n266\nDDR266\n2128\nPC2100\nDDR1\n150\n300\nDDR300\n2400\nPC2400\nDDR1\n200\n400\nDDR400\n3200\nPC3200\nDDR2\n266\n533\nDDR2-533\n4264\nPC4300\nDDR2\n333\n667\nDDR2-667\n5336\nPC5300\nDDR2\n400\n800\nDDR2-800\n6400\nPC6400\nDDR3\n533\n1066\nDDR3-1066\n8528\nPC8500\nDDR3\n666\n1333\nDDR3-1333\n10,664\nPC10700\nDDR3\n800\n1600\nDDR3-1600\n12,800\nPC12800\nDDR4\n1333\n2666\nDDR4-2666\n21,300\nPC21300\nFigure 2.5 Clock rates, bandwidth, and names of DDR DRAMS and DIMMs in 2016. Note the numerical relationship\nbetween the columns. The third column is twice the second, and the fourth uses the number from the third column in\nthe name of the DRAM chip. The fifth column is eight times the third column, and a rounded version of this number is\nused in the name of the DIMM. DDR4 saw significant first use in 2016.\n2.2\nMemory Technology and Optimizations\n\u25a0\n89"
    },
    {
        "page": 122,
        "text": "In addition to these changes, all recent SDRAMs support a power-down mode,\nwhich is entered by telling the DRAM to ignore the clock. Power-down mode dis-\nables the SDRAM, except for internal automatic refresh (without which entering\npower-down mode for longer than the refresh time will cause the contents of mem-\nory to be lost). Figure 2.6 shows the power consumption for three situations in a\n2 GB DDR3 SDRAM. The exact delay required to return from low power mode\ndepends on the SDRAM, but a typical delay is 200 SDRAM clock cycles.\nGraphics Data RAMs\nGDRAMs or GSDRAMs (Graphics or Graphics Synchronous DRAMs) are a spe-\ncial class of DRAMs based on SDRAM designs but tailored for handling the higher\nbandwidth demands of graphics processing units. GDDR5 is based on DDR3 with\nearlier GDDRs based on DDR2. Because graphics processor units (GPUs; see\nChapter 4) require more bandwidth per DRAM chip than CPUs, GDDRs have\nseveral important differences:\n1. GDDRs have wider interfaces: 32-bits versus 4, 8, or 16 in current designs.\n2. GDDRs have a higher maximum clock rate on the data pins. To allow a higher\ntransfer rate without incurring signaling problems, GDRAMS normally connect\ndirectly to the GPU and are attached by soldering them to the board, unlike\nDRAMs, which are normally arranged in an expandable array of DIMMs.\nAltogether, these characteristics let GDDRs run at two to five times the bandwidth\nper DRAM versus DDR3 DRAMs.\n0\n100\n200\n300\n400\n500\n600\nLow\npower\nmode\nTypical\nusage\nFully\nactive\nPower in mW\nBackground power\nActivate power\nRead, write, terminate\npower\nFigure 2.6 Power consumption for a DDR3 SDRAM operating under three condi-\ntions: low-power (shutdown) mode, typical system mode (DRAM is active 30% of\nthe time for reads and 15% for writes), and fully active mode, where the DRAM is\ncontinuously reading or writing. Reads and writes assume bursts of eight transfers.\nThese data are based on a Micron 1.5V 2GB DDR3-1066, although similar savings occur\nin DDR4 SDRAMs.\n90\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 123,
        "text": "Packaging Innovation: Stacked or Embedded DRAMs\nThe newest innovation in 2017 in DRAMs is a packaging innovation, rather than a\ncircuit innovation. It places multiple DRAMs in a stacked or adjacent fashion\nembedded within the same package as the processor. (Embedded DRAM also is\nused to refer to designs that place DRAM on the processor chip.) Placing the\nDRAM and processor in the same package lowers access latency (by shortening\nthe delay between the DRAMs and the processor) and potentially increases band-\nwidth by allowing more and faster connections between the processor and DRAM;\nthus several producers have called it high bandwidth memory (HBM).\nOne version of this technology places the DRAM die directly on the CPU die\nusing solder bump technology to connect them. Assuming adequate heat manage-\nment, multiple DRAM dies can be stacked in this fashion. Another approach stacks\nonly DRAMs and abuts them with the CPU in a single package using a substrate\n(interposer) containing the connections. Figure 2.7 shows these two different inter-\nconnection schemes. Prototypes of HBM that allow stacking of up to eight chips\nhave been demonstrated. With special versions of SDRAMs, such a package could\ncontain 8 GiB of memory and have data transfer rates of 1 TB/s. The 2.5D tech-\nnique is currently available. Because the chips must be specifically manufactured\nto stack, it is quite likely that most early uses will be in high-end server chipsets.\nIn some applications, it may be possible to internally package enough DRAM\nto satisfy the needs of the application. For example, a version of an Nvidia GPU\nused as a node in a special-purpose cluster design is being developed using HBM,\nand it is likely that HBM will become a successor to GDDR5 for higher-end appli-\ncations. In some cases, it may be possible to use HBM as main memory, although\nthe cost limitations and heat removal issues currently rule out this technology for\nsome embedded applications. In the next section, we consider the possibility of\nusing HBM as an additional level of cache.\nDRAM\nDRAM\nVertical stacking (3D)\nInterposer stacking (2.5D)\nxPU\nxPU\nInterposer\nFigure 2.7 Two forms of die stacking. The 2.5D form is available now. 3D stacking is\nunder development and faces heat management challenges due to the CPU.\n2.2\nMemory Technology and Optimizations\n\u25a0\n91"
    },
    {
        "page": 124,
        "text": "Flash Memory\nFlash memory is a type of EEPROM (electronically erasable programmable read-\nonly memory), which is normally read-only but can be erased. The other key prop-\nerty of Flash memory is that it holds its contents without any power. We focus on\nNAND Flash, which has higher density than NOR Flash and is more suitable for\nlarge-scale nonvolatile memories; the drawback is that access is sequential and\nwriting is slower, as we explain below.\nFlash is used as the secondary storage in PMDs in the same manner that a disk\nfunctions in a laptop or server. In addition, because most PMDs have a limited\namount of DRAM, Flash may also act as a level of the memory hierarchy, to a\nmuch greater extent than it might have to do in a desktop or server with a main\nmemory that might be 10\u2013100 times larger.\nFlash uses a very different architecture and has different properties than stan-\ndard DRAM. The most important differences are\n1. Reads to Flash are sequential and read an entire page, which can be 512 bytes,\n2 KiB, or 4 KiB. Thus NAND Flash has a long delay to access the first byte\nfrom a random address (about 25 \u03bcS), but can supply the remainder of a page\nblock at about 40 MiB/s. By comparison, a DDR4 SDRAM takes about 40 ns to\nthe first byte and can transfer the rest of the row at 4.8 GiB/s. Comparing the\ntime to transfer 2 KiB, NAND Flash takes about 75 \u03bcS, while DDR SDRAM\ntakes less than 500 ns, making Flash about 150 times slower. Compared to mag-\nnetic disk, however, a 2 KiB read from Flash is 300 to 500 times faster. From\nthese numbers, we can see why Flash is not a candidate to replace DRAM for\nmain memory, but is a candidate to replace magnetic disk.\n2. Flash memory must be erased (thus the name flash for the \u201cflash\u201d erase process)\nbefore it is overwritten, and it is erased in blocks rather than individual bytes or\nwords. This requirement means that when data must be written to Flash, an\nentire block must be assembled, either as new data or by merging the data to\nbe written and the rest of the block\u2019s contents. For writing, Flash is about\n1500 times slower then SDRAM, and about 8\u201315 times as fast as magnetic disk.\n3. Flash memory is nonvolatile (i.e., it keeps its contents even when power is not\napplied) and draws significantly less power when not reading or writing (from\nless than half in standby mode to zero when completely inactive).\n4. Flash memory limits the number of times that any given block can be written,\ntypically at least 100,000. By ensuring uniform distribution of written blocks\nthroughout the memory, a system can maximize the lifetime of a Flash memory\nsystem. This technique, called write leveling, is handled by Flash memory\ncontrollers.\n5. High-density NAND Flash is cheaper than SDRAM but more expensive than\ndisks: roughly $2/GiB for Flash, $20 to $40/GiB for SDRAM, and $0.09/GiB\nfor magnetic disks. In the past five years, Flash has decreased in cost at a rate\nthat is almost twice as fast as that of magnetic disks.\n92\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 125,
        "text": "Like DRAM, Flash chips include redundant blocks to allow chips with small\nnumbers of defects to be used; the remapping of blocks is handled in the Flash chip.\nFlash controllers handle page transfers, provide caching of pages, and handle write\nleveling.\nThe rapid improvements in high-density Flash have been critical to the devel-\nopment of low-power PMDs and laptops, but they have also significantly changed\nboth desktops, which increasingly use solid state disks, and large servers, which\noften combine disk and Flash-based storage.\nPhase-Change Memory Technology\nPhase-change memory (PCM) has been an active research area for decades. The\ntechnology typically uses a small heating element to change the state of a bulk sub-\nstrate between its crystalline form and an amorphous form, which have different\nresistive properties. Each bit corresponds to a crosspoint in a two-dimensional net-\nwork that overlays the substrate. Reading is done by sensing the resistance between\nan x and y point (thus the alternative name memristor), and writing is accomplished\nby applying a current to change the phase of the material. The absence of an active\ndevice (such as a transistor) should lead to lower costs and greater density than that\nof NAND Flash.\nIn 2017 Micron and Intel began delivering Xpoint memory chips that are\nbelieved to be based on PCM. The technology is expected to have much better\nwrite durability than NAND Flash and, by eliminating the need to erase a page\nbefore writing, achieve an increase in write performance versus NAND of up to\na factor of ten. Read latency is also better than Flash by perhaps a factor of\n2\u20133. Initially, it is expected to be priced slightly higher than Flash, but the advan-\ntages in write performance and write durability may make it attractive, especially\nfor SSDs. Should this technology scale well and be able to achieve additional cost\nreductions, it may be the solid state technology that will depose magnetic disks,\nwhich have reigned as the primary bulk nonvolatile store for more than 50 years.\nEnhancing Dependability in Memory Systems\nLarge caches and main memories significantly increase the possibility of errors\noccurring both during the fabrication process and dynamically during operation.\nErrors that arise from a change in circuitry and are repeatable are called hard errors\nor permanent faults. Hard errors can occur during fabrication, as well as from a\ncircuit change during operation (e.g., failure of a Flash memory cell after many\nwrites). All DRAMs, Flash memory, and most SRAMs are manufactured with\nspare rows so that a small number of manufacturing defects can be accommodated\nby programming the replacement of a defective row by a spare row. Dynamic\nerrors, which are changes to a cell\u2019s contents, not a change in the circuitry, are\ncalled soft errors or transient faults.\nDynamic errors can be detected by parity bits and detected and fixed by the use\nof error correcting codes (ECCs). Because instruction caches are read-only, parity\n2.2\nMemory Technology and Optimizations\n\u25a0\n93"
    },
    {
        "page": 126,
        "text": "suffices. In larger data caches and in main memory, ECC is used to allow errors to\nbe both detected and corrected. Parity requires only one bit of overhead to detect a\nsingle error in a sequence of bits. Because a multibit error would be undetected\nwith parity, the number of bits protected by a parity bit must be limited. One parity\nbit per 8 data bits is a typical ratio. ECC can detect two errors and correct a single\nerror with a cost of 8 bits of overhead per 64 data bits.\nIn very large systems, the possibility of multiple errors as well as complete fail-\nure of a single memory chip becomes significant. Chipkill was introduced by IBM\nto solve this problem, and many very large systems, such as IBM and SUN servers\nand the Google Clusters, use this technology. (Intel calls their version SDDC.)\nSimilar in nature to the RAID approach used for disks, Chipkill distributes the data\nand ECC information so that the complete failure of a single memory chip can be\nhandled by supporting the reconstruction of the missing data from the remaining\nmemory chips. Using an analysis by IBM and assuming a 10,000 processor server\nwith 4 GiB per processor yields the following rates of unrecoverable errors in three\nyears of operation:\n\u25a0\nParity only: About 90,000, or one unrecoverable (or undetected) failure every\n17 minutes.\n\u25a0\nECC only: About 3500, or about one undetected or unrecoverable failure every\n7.5 hours.\n\u25a0\nChipkill: About one undetected or unrecoverable failure every 2 months.\nAnother way to look at this is to find the maximum number of servers (each\nwith 4 GiB) that can be protected while achieving the same error rate as demon-\nstrated for Chipkill. For parity, even a server with only one processor will have an\nunrecoverable error rate higher than a 10,000-server Chipkill protected system. For\nECC, a 17-server system would have about the same failure rate as a 10,000-server\nChipkill system. Therefore Chipkill is a requirement for the 50,000\u2013100,00 servers\nin warehouse-scale computers (see Section 6.8 of Chapter 6).\n2.3\nTen Advanced Optimizations of Cache Performance\nThe preceding average memory access time formula gives us three metrics for\ncache optimizations: hit time, miss rate, and miss penalty. Given the recent trends,\nwe add cache bandwidth and power consumption to this list. We can classify the 10\nadvanced cache optimizations we examine into five categories based on these\nmetrics:\n1. Reducing the hit time\u2014Small and simple first-level caches and way-prediction.\nBoth techniques also generally decrease power consumption.\n2. Increasing cache bandwidth\u2014Pipelined caches, multibanked caches, and non-\nblocking caches. These techniques have varying impacts on power consumption.\n94\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 127,
        "text": "3. Reducing the miss penalty\u2014Critical word first and merging write buffers.\nThese optimizations have little impact on power.\n4. Reducing the miss rate\u2014Compiler optimizations. Obviously any improvement\nat compile time improves power consumption.\n5. Reducing the miss penalty or miss rate via parallelism\u2014Hardware prefetching\nand compiler prefetching. These optimizations generally increase power con-\nsumption, primarily because of prefetched data that are unused.\nIn general, the hardware complexity increases as we go through these optimi-\nzations. In addition, several of the optimizations require sophisticated compiler\ntechnology, and the final one depends on HBM. We will conclude with a summary\nof the implementation complexity and the performance benefits of the 10 tech-\nniques presented in Figure 2.18 on page 113. Because some of these are straight-\nforward, we cover them briefly; others require more description.\nFirst Optimization: Small and Simple First-Level Caches\nto Reduce Hit Time and Power\nThe pressure of both a fast clock cycle and power limitations encourages limited\nsize for first-level caches. Similarly, use of lower levels of associativity can reduce\nboth hit time and power, although such trade-offs are more complex than those\ninvolving size.\nThe critical timing path in a cache hit is the three-step process of addressing the\ntag memory using the index portion of the address, comparing the read tag value to\nthe address, and setting the multiplexor to choose the correct data item if the cache is\nset associative. Direct-mapped caches can overlap the tag check with the transmis-\nsion of the data, effectively reducing hit time. Furthermore, lower levels of associa-\ntivity will usually reduce power because fewer cache lines must be accessed.\nAlthough the total amount of on-chip cache has increased dramatically with\nnew generations of microprocessors, because of the clock rate impact arising from\na larger L1 cache, the size of the L1 caches has recently increased either slightly or\nnot at all. In many recent processors, designers have opted for more associativity\nrather than larger caches. An additional consideration in choosing the associativity\nis the possibility of eliminating address aliases; we discuss this topic shortly.\nOne approach to determining the impact on hit time and power consumption in\nadvance of building a chip is to use CAD tools. CACTI is a program to estimate the\naccess time and energy consumption of alternative cache structures on CMOS\nmicroprocessors within 10% of more detailed CAD tools. For a given minimum\nfeature size, CACTI estimates the hit time of caches as a function of cache size,\nassociativity, number of read/write ports, and more complex parameters.\nFigure 2.8 shows the estimated impact on hit time as cache size and associativity\nare varied. Depending on cache size, for these parameters, the model suggests that\nthe hit time for direct mapped is slightly faster than two-way set associative and\nthat two-way set associative is 1.2 times as fast as four-way and four-way is 1.4\n2.3\nTen Advanced Optimizations of Cache Performance\n\u25a0\n95"
    },
    {
        "page": 128,
        "text": "times as fast as eight-way. Of course, these estimates depend on technology as well\nas the size of the cache, and CACTI must be carefully aligned with the technology;\nFigure 2.8 shows the relative tradeoffs for one technology.\nExample\nUsing the data in Figure B.8 in Appendix B and Figure 2.8, determine whether a\n32 KiB four-way set associative L1 cache has a faster memory access time than a\n32 KiB two-way set associative L1 cache. Assume the miss penalty to L2 is\n15 times the access time for the faster L1 cache. Ignore misses beyond L2. Which\nhas the faster average memory access time?\nAnswer\nLet the access time for the two-way set associative cache be 1. Then, for the two-\nway cache,\nAverage memory access time2-way \u00bc Hit time + Miss rateMiss penalty\n\u00bc 1 + 0:03815 \u00bc 1:38\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0\nCache size\n16 KB\n32 KB\n64 KB\n128 KB\n256 KB\n1-way\n8-way\n2-way\n4-way\nRelative access time in microseconds\nFigure 2.8 Relative access times generally increase as cache size and associativity\nare increased. These data come from the CACTI model 6.5 by Tarjan et al. (2005).\nThe data assume typical embedded SRAM technology, a single bank, and 64-byte\nblocks. The assumptions about cache layout and the complex trade-offs between inter-\nconnect delays (that depend on the size of a cache block being accessed) and the cost of\ntag checks and multiplexing lead to results that are occasionally surprising, such as the\nlower access time for a 64 KiB with two-way set associativity versus direct mapping. Sim-\nilarly, the results with eight-way set associativity generate unusual behavior as cache size\nis increased. Because such observations are highly dependent on technology and\ndetailed design assumptions, tools such as CACTI serve to reduce the search space.\nThese results are relative; nonetheless, they are likely to shift as we move to more recent\nand denser semiconductor technologies.\n96\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 129,
        "text": "For the four-way cache, the access time is 1.4 times longer. The elapsed time of the\nmiss penalty is 15/1.4\u00bc10.1. Assume 10 for simplicity:\nAverage memory access time4-way \u00bc Hit time2-way 1:4 + Miss rateMiss penalty\n\u00bc 1:4 + 0:03710 \u00bc 1:77\nClearly, the higher associativity looks like a bad trade-off; however, because cache\naccess in modern processors is often pipelined, the exact impact on the clock cycle\ntime is difficult to assess.\nEnergy consumption is also a consideration in choosing both the cache size and\nassociativity, as Figure 2.9 shows. The energy cost of higher associativity ranges\nfrom more than a factor of 2 to negligible in caches of 128 or 256 KiB when going\nfrom direct mapped to two-way set associative.\nAs energy consumption has become critical, designers have focused on ways\nto reduce the energy needed for cache access. In addition to associativity, the\nother key factor in determining the energy used in a cache access is the number\nof blocks in the cache because it determines the number of \u201crows\u201d that are\naccessed. A designer could reduce the number of rows by increasing the block size\n(holding total cache size constant), but this could increase the miss rate, especially\nin smaller L1 caches.\n7.0\n8.0\n9.0\n10.0\n6.0\n5.0\n4.0\n3.0\n1.0\n2.0\n0\n1-way\n8-way\n2-way\n4-way\nRelative energy per read in nano joules\nCache size\n16 KB\n32 KB\n64 KB\n128 KB\n256 KB\nFigure 2.9 Energy consumption per read increases as cache size and associativity\nare increased. As in the previous figure, CACTI is used for the modeling with the same\ntechnology parameters. The large penalty for eight-way set associative caches is due to\nthe cost of reading out eight tags and the corresponding data in parallel.\n2.3\nTen Advanced Optimizations of Cache Performance\n\u25a0\n97"
    },
    {
        "page": 130,
        "text": "An alternative is to organize the cache in banks so that an access activates\nonly a portion of the cache, namely the bank where the desired block resides.\nThe primary use of multibanked caches is to increase the bandwidth of the cache,\nan optimization we consider shortly. Multibanking also reduces energy because\nless of the cache is accessed. The L3 caches in many multicores are logically uni-\nfied, but physically distributed, and effectively act as a multibanked cache. Based\non the address of a request, only one of the physical L3 caches (a bank) is actually\naccessed. We discuss this organization further in Chapter 5.\nIn recent designs, there are three other factors that have led to the use of higher\nassociativity in first-level caches despite the energy and access time costs. First,\nmany processors take at least 2 clock cycles to access the cache and thus the impact\nof a longer hit time may not be critical. Second, to keep the TLB out of the critical\npath (a delay that would be larger than that associated with increased associativity),\nalmost all L1 caches should be virtually indexed. This limits the size of the cache to\nthe page size times the associativity because then only the bits within the page are\nused for the index. There are other solutions to the problem of indexing the cache\nbefore address translation is completed, but increasing the associativity, which also\nhas other benefits, is the most attractive. Third, with the introduction of multi-\nthreading (see Chapter 3), conflict misses can increase, making higher associativity\nmore attractive.\nSecond Optimization: Way Prediction to Reduce Hit Time\nAnother approach reduces conflict misses and yet maintains the hit speed of direct-\nmapped cache. In way prediction, extra bits are kept in the cache to predict the way\n(or block within the set) of the next cache access. This prediction means the mul-\ntiplexor is set early to select the desired block, and in that clock cycle, only a single\ntag comparison is performed in parallel with reading the cache data. A miss results\nin checking the other blocks for matches in the next clock cycle.\nAdded to each block of a cache are block predictor bits. The bits select which of\nthe blocks to try on the next cache access. If the predictor is correct, the cache\naccess latency is the fast hit time. If not, it tries the other block, changes the\nway predictor, and has a latency of one extra clock cycle. Simulations suggest that\nset prediction accuracy is in excess of 90% for a two-way set associative cache and\n80% for a four-way set associative cache, with better accuracy on I-caches than\nD-caches. Way prediction yields lower average memory access time for a two-\nway set associative cache if it is at least 10% faster, which is quite likely. Way\nprediction was first used in the MIPS R10000 in the mid-1990s. It is popular in\nprocessors that use two-way set associativity and was used in several ARM pro-\ncessors, which have four-way set associative caches. For very fast processors, it\nmay be challenging to implement the one-cycle stall that is critical to keeping\nthe way prediction penalty small.\nAn extended form of way prediction can also be used to reduce power con-\nsumption by using the way prediction bits to decide which cache block to actually\n98\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 131,
        "text": "access (the way prediction bits are essentially extra address bits); this approach,\nwhich might be called way selection, saves power when the way prediction is cor-\nrect but adds significant time on a way misprediction, because the access, not just\nthe tag match and selection, must be repeated. Such an optimization is likely to\nmake sense only in low-power processors. Inoue et al. (1999) estimated that using\nthe way selection approach with a four-way set associative cache increases the\naverage access time for the I-cache by 1.04 and for the D-cache by 1.13 on the\nSPEC95 benchmarks, but it yields an average cache power consumption relative\nto a normal four-way set associative cache that is 0.28 for the I-cache and 0.35 for\nthe D-cache. One significant drawback for way selection is that it makes it difficult\nto pipeline the cache access; however, as energy concerns have mounted, schemes\nthat do not require powering up the entire cache make increasing sense.\nExample\nAssume that there are half as many D-cache accesses as I-cache accesses and that\nthe I-cache and D-cache are responsible for 25% and 15% of the processor\u2019s power\nconsumption in a normal four-way set associative implementation. Determine if\nway selection improves performance per watt based on the estimates from the\npreceding study.\nAnswer\nFor the I-cache, the savings in power is 250.28\u00bc0.07 of the total power, while\nfor the D-cache it is 150.35\u00bc0.05 for a total savings of 0.12. The way prediction\nversion requires 0.88 of the power requirement of the standard four-way cache. The\nincrease in cache access time is the increase in I-cache average access time plus\none-half the increase in D-cache access time, or 1.04+0.50.13\u00bc1.11 times lon-\nger. This result means that way selection has 0.90 of the performance of a standard\nfour-way cache. Thus way selection improves performance per joule very slightly\nby a ratio of 0.90/0.88\u00bc1.02. This optimization is best used where power rather\nthan performance is the key objective.\nThird Optimization: Pipelined Access and Multibanked\nCaches to Increase Bandwidth\nThese optimizations increase cache bandwidth either by pipelining the cache access\nor by widening the cache with multiple banks to allow multiple accesses per clock;\nthese optimizations are the dual to the superpipelined and superscalar approaches to\nincreasing instruction throughput. These optimizations are primarily targeted at L1,\nwhere access bandwidth constrains instruction throughput. Multiple banks are also\nused in L2 and L3 caches, but primarily as a power-management technique.\nPipelining L1 allows a higher clock cycle, at the cost of increased latency. For\nexample, the pipeline for the instruction cache access for Intel Pentium processors\nin the mid-1990s took 1 clock cycle; for the Pentium Pro through Pentium III in the\nmid-1990s through 2000, it took 2 clock cycles; and for the Pentium 4, which\nbecame available in 2000, and the current Intel Core i7, it takes 4 clock cycles.\nPipelining the instruction cache effectively increases the number of pipeline stages,\n2.3\nTen Advanced Optimizations of Cache Performance\n\u25a0\n99"
    },
    {
        "page": 132,
        "text": "leading to a greater penalty on mispredicted branches. Correspondingly, pipelining\nthe data cache leads to more clock cycles between issuing the load and using the\ndata (see Chapter 3). Today, all processors use some pipelining of L1, if only for\nthe simple case of separating the access and hit detection, and many high-speed\nprocessors have three or more levels of cache pipelining.\nIt is easier to pipeline the instruction cache than the data cache because the pro-\ncessor can rely on high performance branch prediction to limit the latency effects.\nMany superscalar processors can issue and execute more than one memory refer-\nence per clock (allowing a load or store is common, and some processors allow\nmultiple loads). To handle multiple data cache accesses per clock, we can divide\nthe cache into independent banks, each supporting an independent access. Banks\nwere originally used to improve performance of main memory and are now used\ninside modern DRAM chips as well as with caches. The Intel Core i7 has four\nbanks in L1 (to support up to 2 memory accesses per clock).\nClearly, banking works best when the accesses naturally spread themselves\nacross the banks, so the mapping of addresses to banks affects the behavior of\nthe memory system. A simple mapping that works well is to spread the addresses\nof the block sequentially across the banks, which is called sequential interleaving.\nFor example, if there are four banks, bank 0 has all blocks whose address modulo 4\nis 0, bank 1 has all blocks whose address modulo 4 is 1, and so on. Figure 2.10\nshows this interleaving. Multiple banks also are a way to reduce power consump-\ntion in both caches and DRAM.\nMultiple banks are also useful in L2 or L3 caches, but for a different reason.\nWith multiple banks in L2, we can handle more than one outstanding L1 miss,\nif the banks do not conflict. This is a key capability to support nonblocking caches,\nour next optimization. The L2 in the Intel Core i7 has eight banks, while Arm\nCortex processors have used L2 caches with 1\u20134 banks. As mentioned earlier,\nmultibanking can also reduce energy consumption.\nFourth Optimization: Nonblocking Caches\nto Increase Cache Bandwidth\nFor pipelined computers that allow out-of-order execution (discussed in Chapter 3),\nthe processor need not stall on a data cache miss. For example, the processor could\n0\n4\n8\n12\nBank 0\nBlock\naddress\nBlock\naddress\n1\n5\n9\n13\nBank 1\nBlock\naddress\n2\n6\n10\n14\nBank 2\nBlock\naddress\n3\n7\n11\n15\nBank 3\nFigure 2.10 Four-way interleaved cache banks using block addressing. Assuming\n64 bytes per block, each of these addresses would be multiplied by 64 to get byte\naddressing.\n100\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 133,
        "text": "continue fetching instructions from the instruction cache while waiting for the data\ncache to return the missing data. A nonblocking cache or lockup-free cache esca-\nlates the potential benefits of such a scheme by allowing the data cache to continue\nto supply cache hits during a miss. This \u201chit under miss\u201d optimization reduces the\neffective miss penalty by being helpful during a miss instead of ignoring the\nrequests of the processor. A subtle and complex option is that the cache may further\nlower the effective miss penalty if it can overlap multiple misses: a \u201chit under\nmultiple miss\u201d or \u201cmiss under miss\u201d optimization. The second option is beneficial\nonly if the memory system can service multiple misses; most high-performance pro-\ncessors (such as the Intel Core processors) usually support both, whereas many\nlower-end processors provide only limited nonblocking support in L2.\nTo examine the effectiveness of nonblocking caches in reducing the cache miss\npenalty, Farkas and Jouppi (1994) did a study assuming 8 KiB caches with a\n14-cycle miss penalty (appropriate for the early 1990s). They observed a reduction\nin the effective miss penalty of 20% for the SPECINT92 benchmarks and 30% for\nthe SPECFP92 benchmarks when allowing one hit under miss.\nLi et al. (2011) updated this study to use a multilevel cache, more modern\nassumptions about miss penalties, and the larger and more demanding\nSPECCPU2006 benchmarks. The study was done assuming a model based on a\nsingle core of an Intel i7 (see Section 2.6) running the SPECCPU2006 benchmarks.\nFigure 2.11 shows the reduction in data cache access latency when allowing 1, 2,\nand 64 hits under a miss; the caption describes further details of the memory\nsystem. The larger caches and the addition of an L3 cache since the earlier\nstudy have reduced the benefits with the SPECINT2006 benchmarks showing\nan average reduction in cache latency of about 9% and the SPECFP2006 bench-\nmarks about 12.5%.\nExample\nWhich is more important for floating-point programs: two-way set associativity or\nhit under one miss for the primary data caches? What about integer programs?\nAssume the following average miss rates for 32 KiB data caches: 5.2% for\nfloating-point programs with a direct-mapped cache, 4.9% for the programs with\na two-way set associative cache, 3.5% for integer programs with a direct-mapped\ncache, and 3.2% for integer programs with a two-way set associative cache. Assume\nthe miss penalty to L2 is 10 cycles, and the L2 misses and penalties are the same.\nAnswer\nFor floating-point programs, the average memory stall times are\nMiss rateDM Miss penalty \u00bc 5:2%10 \u00bc 0:52\nMiss rate2-way Miss penalty \u00bc 4:9%10 \u00bc 0:49\nThe cache access latency (including stalls) for two-way associativity is 0.49/0.52\nor 94% of direct-mapped cache. Figure 2.11 caption indicates that a hit under\none miss reduces the average data cache access latency for floating-point programs\nto 87.5% of a blocking cache. Therefore, for floating-point programs, the\n2.3\nTen Advanced Optimizations of Cache Performance\n\u25a0\n101"
    },
    {
        "page": 134,
        "text": "direct-mapped data cache supporting one hit under one miss gives better perfor-\nmance than a two-way set-associative cache that blocks on a miss.\nFor integer programs, the calculation is\nMiss rateDM Miss penalty \u00bc 3:5%10 \u00bc 0:35\nMiss rate2-way Miss penalty \u00bc 3:2%10 \u00bc 0:32\nThe data cache access latency of a two-way set associative cache is thus 0.32/0.35\nor 91% of direct-mapped cache, while the reduction in access latency when allow-\ning a hit under one miss is 9%, making the two choices about equal.\nThe real difficulty with performance evaluation of nonblocking caches is that a\ncache miss does not necessarily stall the processor. In this case, it is difficult to\njudge the impact of any single miss and thus to calculate the average memory\naccess time. The effective miss penalty is not the sum of the misses but the\nnonoverlapped time that the processor is stalled. The benefit of nonblocking caches\nis complex, as it depends upon the miss penalty when there are multiple misses, the\nmemory reference pattern, and how many instructions the processor can execute\nwith a miss outstanding.\nIn general, out-of-order processors are capable of hiding much of the miss\npenalty of an L1 data cache miss that hits in the L2 cache but are not capable\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nbzip2\ngcc\nmcf\nhmmer\nsjeng\nlibquantum\nh264ref\nomnetpp\nastar\ngamess\nzeusmp\nmilc\ngromacs\ncactusADM\nnamd\nsoplex\npovray\ncalculix\nGemsFDTD\ntonto\nlbm\nwrf\nsphinx3\nSPECFP\nSPECINT\nCache access latency\nHit-under-2-misses\nHit-under-64-misses\nHit-under-1-miss\nFigure 2.11 The effectiveness of a nonblocking cache is evaluated by allowing 1, 2,\nor 64 hits under a cache miss with 9 SPECINT (on the left) and 9 SPECFP (on the right)\nbenchmarks. The data memory system modeled after the Intel i7 consists of a 32 KiB L1\ncache with a four-cycle access latency. The L2 cache (shared with instructions) is 256 KiB\nwith a 10-clock cycle access latency. The L3 is 2 MiB and a 36-cycle access latency. All the\ncaches are eight-way set associative and have a 64-byte block size. Allowing one hit\nunder miss reduces the miss penalty by 9% for the integer benchmarks and 12.5%\nfor the floating point. Allowing a second hit improves these results to 10% and 16%,\nand allowing 64 results in little additional improvement.\n102\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 135,
        "text": "of hiding a significant fraction of a lower-level cache miss. Deciding how many\noutstanding misses to support depends on a variety of factors:\n\u25a0\nThe temporal and spatial locality in the miss stream, which determines whether\na miss can initiate a new access to a lower-level cache or to memory.\n\u25a0\nThe bandwidth of the responding memory or cache.\n\u25a0\nTo allow more outstanding misses at the lowest level of the cache (where the\nmiss time is the longest) requires supporting at least that many misses at a\nhigher level, because the miss must initiate at the highest level cache.\n\u25a0\nThe latency of the memory system.\nThe following simplified example illustrates the key idea.\nExample\nAssume a main memory access time of 36 ns and a memory system capable of a\nsustained transfer rate of 16 GiB/s. If the block size is 64 bytes, what is the maximum\nnumber of outstanding misses we need to support assuming that we can maintain the\npeak bandwidth given the request stream and that accesses never conflict. If the prob-\nability of a reference colliding with one of the previous four is 50%, and we assume\nthat the access has to wait until the earlier access completes, estimate the number of\nmaximum outstanding references. For simplicity, ignore the time between misses.\nAnswer\nIn the first case, assuming that we can maintain the peak bandwidth, the memory\nsystem can support (1610)9/64\u00bc250 million references per second. Because\neach reference takes 36 ns, we can support 2501063610\u00039\u00bc9 references.\nIf the probability of a collision is greater than 0, then we need more outstanding ref-\nerences, because we cannot start work on those colliding references; the memory\nsystem needs more independent references, not fewer! To approximate, we can sim-\nply assume that half the memory references do not have to be issued to the memory.\nThis means that we must support twice as many outstanding references, or 18.\nIn Li, Chen, Brockman, and Jouppi\u2019s study, they found that the reduction in\nCPI for the integer programs was about 7% for one hit under miss and about\n12.7% for 64. For the floating-point programs, the reductions were 12.7% for\none hit under miss and 17.8% for 64. These reductions track fairly closely the\nreductions in the data cache access latency shown in Figure 2.11.\nImplementing a Nonblocking Cache\nAlthough nonblocking caches have the potential to improve performance, they are\nnontrivial to implement. Two initial types of challenges arise: arbitrating conten-\ntion between hits and misses, and tracking outstanding misses so that we know\nwhen loads or stores can proceed. Consider the first problem. In a blocking cache,\nmisses cause the processor to stall and no further accesses to the cache will occur\n2.3\nTen Advanced Optimizations of Cache Performance\n\u25a0\n103"
    },
    {
        "page": 136,
        "text": "until the miss is handled. In a nonblocking cache, however, hits can collide with\nmisses returning from the next level of the memory hierarchy. If we allow multiple\noutstanding misses, which almost all recent processors do, it is even possible for\nmisses to collide. These collisions must be resolved, usually by first giving priority\nto hits over misses, and second by ordering colliding misses (if they can occur).\nThe second problem arises because we need to track multiple outstanding mis-\nses. In a blocking cache, we always know which miss is returning, because only\none can be outstanding. In a nonblocking cache, this is rarely true. At first glance,\nyou might think that misses always return in order, so that a simple queue could be\nkept to match a returning miss with the longest outstanding request. Consider,\nhowever, a miss that occurs in L1. It may generate either a hit or miss in L2; if\nL2 is also nonblocking, then the order in which misses are returned to L1 will\nnot necessarily be the same as the order in which they originally occurred. Multi-\ncore and other multiprocessor systems that have nonuniform cache access times\nalso introduce this complication.\nWhen a miss returns, the processor must know which load or store caused the\nmiss, so that instruction can now go forward; and it must know where in the cache\nthe data should be placed (as well as the setting of tags for that block). In recent\nprocessors, this information is kept in a set of registers, typically called the Miss\nStatus Handling Registers (MSHRs). If we allow n outstanding misses, there will\nbe n MSHRs, each holding the information about where a miss goes in the cache\nand the value of any tag bits for that miss, as well as the information indicating\nwhich load or store caused the miss (in the next chapter, you will see how this\nis tracked). Thus, when a miss occurs, we allocate an MSHR for handling that miss,\nenter the appropriate information about the miss, and tag the memory request with\nthe index of the MSHR. The memory system uses that tag when it returns the data,\nallowing the cache system to transfer the data and tag information to the appropri-\nate cache block and \u201cnotify\u201d the load or store that generated the miss that the data is\nnow available and that it can resume operation. Nonblocking caches clearly require\nextra logic and thus have some cost in energy. It is difficult, however, to assess\ntheir energy costs exactly because they may reduce stall time, thereby decreasing\nexecution time and resulting energy consumption.\nIn addition to the preceding issues, multiprocessor memory systems, whether\nwithin a single chip or on multiple chips, must also deal with complex implemen-\ntation issues related to memory coherency and consistency. Also, because cache mis-\nses are no longer atomic (because the request and response are split and may be\ninterleaved among multiple requests), there are possibilities for deadlock. For the\ninterested reader, Section I.7 in online Appendix I deals with these issues in detail.\nFifth Optimization: Critical Word First and\nEarly Restart to Reduce Miss Penalty\nThis technique is based on the observation that the processor normally needs just\none word of the block at a time. This strategy is impatience: don\u2019t wait for the full\n104\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 137,
        "text": "block to be loaded before sending the requested word and restarting the processor.\nHere are two specific strategies:\n\u25a0\nCritical word first\u2014Request the missed word first from memory and send it to\nthe processor as soon as it arrives; let the processor continue execution while\nfilling the rest of the words in the block.\n\u25a0\nEarly restart\u2014Fetch the words in normal order, but as soon as the requested\nword of the block arrives, send it to the processor and let the processor continue\nexecution.\nGenerally, these techniques only benefit designs with large cache blocks\nbecause the benefit is low unless blocks are large. Note that caches normally\ncontinue to satisfy accesses to other blocks while the rest of the block is\nbeing filled.\nHowever, given spatial locality, there is a good chance that the next reference is\nto the rest of the block. Just as with nonblocking caches, the miss penalty is not\nsimple to calculate. When there is a second request in critical word first, the effec-\ntive miss penalty is the nonoverlapped time from the reference until the second\npiece arrives. The benefits of critical word first and early restart depend on the size\nof the block and the likelihood of another access to the portion of the block that has\nnot yet been fetched. For example, for SPECint2006 running on the i7 6700, which\nuses early restart and critical word first, there is more than one reference made to a\nblock with an outstanding miss (1.23 references on average with a range from 0.5\nto 3.0). We explore the performance of the i7 memory hierarchy in more detail in\nSection 2.6.\nSixth Optimization: Merging Write Buffer\nto Reduce Miss Penalty\nWrite-through caches rely on write buffers, as all stores must be sent to the next\nlower level of the hierarchy. Even write-back caches use a simple buffer when\na block is replaced. If the write buffer is empty, the data and the full address\nare written in the buffer, and the write is finished from the processor\u2019s perspective;\nthe processor continues working while the write buffer prepares to write the word\nto memory. If the buffer contains other modified blocks, the addresses can be\nchecked to see if the address of the new data matches the address of a valid write\nbuffer entry. If so, the new data are combined with that entry. Write merging is the\nname of this optimization. The Intel Core i7, among many others, uses write\nmerging.\nIf the buffer is full and there is no address match, the cache (and processor)\nmust wait until the buffer has an empty entry. This optimization uses the memory\nmore efficiently because multiword writes are usually faster than writes performed\none word at a time. Skadron and Clark (1997) found that even a merging four-entry\nwrite buffer generated stalls that led to a 5%\u201310% performance loss.\n2.3\nTen Advanced Optimizations of Cache Performance\n\u25a0\n105"
    },
    {
        "page": 138,
        "text": "The optimization also reduces stalls because of the write buffer being full.\nFigure 2.12 shows a write buffer with and without write merging. Assume we\nhad four entries in the write buffer, and each entry could hold four 64-bit words.\nWithout this optimization, four stores to sequential addresses would fill the buffer\nat one word per entry, even though these four words when merged fit exactly\nwithin a single entry of the write buffer.\nNote that input/output device registers are often mapped into the physical\naddress space. These I/O addresses cannot allow write merging because separate\nI/O registers may not act like an array of words in memory. For example, they may\nrequire one address and data word per I/O register rather than use multiword writes\nusing a single address. These side effects are typically implemented by marking the\npages as requiring nonmerging write through by the caches.\n100\n108\n116\n124\nWrite address\n1\n1\n1\n1\nV\n0\n0\n0\n0\nV\n0\n0\n0\n0\nV\n0\n0\n0\n0\nV\n100\nWrite address\n1\n0\n0\n0\nV\n1\n0\n0\n0\nV\n1\n0\n0\n0\nV\n1\n0\n0\n0\nV\nMem[100]\nMem[100]\nMem[108]\nMem[108]\nMem[116]\nMem[116]\nMem[124]\nMem[124]\nFigure 2.12 In this illustration of write merging, the write buffer on top does not use\nwrite merging while the write buffer on the bottom does. The four writes are merged\ninto a single buffer entry with write merging; without it, the buffer is full even though\nthree-fourths of each entry is wasted. The buffer has four entries, and each entry holds\nfour 64-bit words. The address for each entry is on the left, with a valid bit (V) indicating\nwhether the next sequential 8 bytes in this entry are occupied. (Without write merging,\nthe words to the right in the upper part of the figure would be used only for instructions\nthat wrote multiple words at the same time.)\n106\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 139,
        "text": "Seventh Optimization: Compiler Optimizations\nto Reduce Miss Rate\nThus far, our techniques have required changing the hardware. This next technique\nreduces miss rates without any hardware changes.\nThis magical reduction comes from optimized software\u2014the hardware\ndesigner\u2019s favorite solution! The increasing performance gap between processors\nand main memory has inspired compiler writers to scrutinize the memory hierarchy\nto see if compile time optimizations can improve performance. Once again, research\nis split between improvements in instruction misses and improvements in data mis-\nses. The optimizations presented next are found in many modern compilers.\nLoop Interchange\nSome programs have nested loops that access data in memory in nonsequential\norder. Simply exchanging the nesting of the loops can make the code access the\ndata in the order in which they are stored. Assuming the arrays do not fit in the\ncache, this technique reduces misses by improving spatial locality; reordering max-\nimizes use of data in a cache block before they are discarded. For example, if x is a\ntwo-dimensional array of size [5000,100] allocated so that x[i,j] and x[i,j\n+1] are adjacent (an order called row major because the array is laid out by rows),\nthen the two pieces of the following code show how the accesses can be optimized:\n/* Before */\nfor (j \u00bc 0; j < 100; j \u00bc j + 1)\nfor (i \u00bc 0; i < 5000; i \u00bc i + 1)\nx[i][j] \u00bc 2 * x[i][j];\n/* After */\nfor (i \u00bc 0; i < 5000; i \u00bc i + 1)\nfor (j \u00bc 0; j < 100; j \u00bc j + 1)\nx[i][j] \u00bc 2 * x[i][j];\nThe original code would skip through memory in strides of 100 words, while the\nrevised version accesses all the words in one cache block before going to the next\nblock. This optimization improves cache performance without affecting the num-\nber of instructions executed.\nBlocking\nThis optimization improves temporal locality to reduce misses. We are again deal-\ning with multiple arrays, with some arrays accessed by rows and some by columns.\nStoring the arrays row by row (row major order) or column by column (column\nmajor order) does not solve the problem because both rows and columns are used\nin every loop iteration. Such orthogonal accesses mean that transformations such\nas loop interchange still leave plenty of room for improvement.\n2.3\nTen Advanced Optimizations of Cache Performance\n\u25a0\n107"
    },
    {
        "page": 140,
        "text": "Instead of operating on entire rows or columns of an array, blocked algorithms\noperate on submatrices or blocks. The goal is to maximize accesses to the data\nloaded into the cache before the data are replaced. The following code example,\nwhich performs matrix multiplication, helps motivate the optimization:\n/* Before */\nfor (i \u00bc 0; i < N; i \u00bc i + 1)\nfor (j \u00bc 0; j < N; j \u00bc j + 1)\n{r \u00bc 0;\nfor (k \u00bc 0; k < N; k = k + 1)\nr \u00bc r + y[i][k]*z[k][j];\nx[i][j] \u00bc r;\n};\nThe two inner loops read all N-by-N elements of z, read the same N elements in a\nrow of y repeatedly, and write one row of N elements of x. Figure 2.13 gives a\nsnapshot of the accesses to the three arrays. A dark shade indicates a recent\naccess, a light shade indicates an older access, and white means not yet accessed.\nThe number of capacity misses clearly depends on N and the size of the cache.\nIf it can hold all three N-by-N matrices, then all is well, provided there are no cache\nconflicts. If the cache can hold one N-by-N matrix and one row of N, then at least\nthe ith row of y and the array z may stay in the cache. Less than that and misses\nmay occur for both x and z. In the worst case, there would be 2N3+N2 memory\nwords accessed for N3 operations.\nTo ensure that the elements being accessed can fit in the cache, the original\ncode is changed to compute on a submatrix of size B by B. Two inner loops\nnow compute in steps of size B rather than the full length of x and z. B is called\nthe blocking factor. (Assume x is initialized to zero.)\n0\n1\n2\n3\n4\n5\n1\n0\n2\n3\n4\n5\nx\nj\ni\n0\n1\n2\n3\n4\n5\n1\n0\n2\n3\n4\n5\ny\nk\ni\n0\n1\n2\n3\n4\n5\n1\n0\n2\n3\n4\n5\nz\nj\nk\nFigure 2.13 A snapshot of the three arrays x, y, and z when N56 and i51. The age of accesses to the array\nelements is indicated by shade: white means not yet touched, light means older accesses, and dark means newer\naccesses. The elements of y and z are read repeatedly to calculate new elements of x. The variables i, j, and k\nare shown along the rows or columns used to access the arrays.\n108\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 141,
        "text": "/* After */\nfor (jj \u00bc 0; jj < N; jj \u00bc jj + B)\nfor (kk \u00bc 0; kk < N; kk \u00bc kk + B)\nfor (i \u00bc 0; i < N; i \u00bc i + 1)\nfor (j \u00bc jj; j < min(jj + B,N); j \u00bc j + 1)\n{r \u00bc 0;\nfor (k \u00bc kk; k < min(kk + B,N); k \u00bc k + 1)\nr \u00bc r + y[i][k]*z[k][j];\nx[i][j] = x[i][j] + r;\n};\nFigure 2.14 illustrates the accesses to the three arrays using blocking. Looking\nonly at capacity misses, the total number of memory words accessed is 2N3/B+N2.\nThis total is an improvement by an approximate factor of B. Therefore blocking\nexploits a combination of spatial and temporal locality, because y benefits from\nspatial locality and z benefits from temporal locality. Although our example uses\na square block (BxB), we could also use a rectangular block, which would be nec-\nessary if the matrix were not square.\nAlthough we have aimed at reducing cache misses, blocking can also be used to\nhelp register allocation. By taking a small blocking size such that the block can be\nheld in registers, we can minimize the number of loads and stores in the program.\nAs we shall see in Section 4.8 of Chapter 4, cache blocking is absolutely nec-\nessary to get good performance from cache-based processors running applications\nusing matrices as the primary data structure.\nEighth Optimization: Hardware Prefetching of Instructions\nand Data to Reduce Miss Penalty or Miss Rate\nNonblocking caches effectively reduce the miss penalty by overlapping execution\nwith memory access. Another approach is to prefetch items before the processor\nrequests them. Both instructions and data can be prefetched, either directly into\n0\n1\n2\n3\n4\n5\n1\n0\n2\n3\n4\n5\nx\nj\ni\n0\n1\n2\n3\n4\n5\n1\n0\n2\n3\n4\n5\ny\nk\ni\n0\n1\n2\n3\n4\n5\n1\n0\n2\n3\n4\n5\nz\nj\nk\nFigure 2.14 The age of accesses to the arrays x, y, and z when B53. Note that, in contrast to Figure 2.13, a smaller\nnumber of elements is accessed.\n2.3\nTen Advanced Optimizations of Cache Performance\n\u25a0\n109"
    },
    {
        "page": 142,
        "text": "the caches or into an external buffer that can be more quickly accessed than main\nmemory.\nInstructionprefetchisfrequently done inhardware outsideofthecache.Typically,\nthe processor fetches two blocks on a miss: the requested block and the next consec-\nutive block. The requested block is placed in the instruction cache when it returns, and\nthe prefetched block is placed in the instruction stream buffer. If the requested block is\npresent in the instruction stream buffer, the original cache request is canceled, the\nblock is read from the stream buffer, and the next prefetch request is issued.\nA similar approach can be applied to data accesses (Jouppi, 1990). Palacharla\nand Kessler (1994) looked at a set of scientific programs and considered multiple\nstream buffers that could handle either instructions or data. They found that eight\nstream buffers could capture 50%\u201370% of all misses from a processor with two\n64 KiB four-way set associative caches, one for instructions and the other for data.\nThe Intel Core i7 supports hardware prefetching into both L1 and L2 with the\nmost common case of prefetching being accessing the next line. Some earlier Intel\nprocessors used more aggressive hardware prefetching, but that resulted in reduced\nperformance for some applications, causing some sophisticated users to turn off the\ncapability.\nFigure 2.15 shows the overall performance improvement for a subset of\nSPEC2000 programs when hardware prefetching is turned on. Note that this figure\n1.00\n1.20\n1.40\n1.60\n1.80\n2.00\n2.20\ngap\n1.16\nmcf\n1.45\nfam3d\n1.18\nwupwise\n1.20\ngalgel\n1.21\nfacerec\n1.26\nPerformance improvement\nswim\n1.29\napplu\n1.32\nSPECint2000\nSPECfp2000\nlucas\n1.40\nequake\n1.97\nmgrid\n1.49\nFigure 2.15 Speedup because of hardware prefetching on Intel Pentium 4 with hardware prefetching turned\non for 2 of 12 SPECint2000 benchmarks and 9 of 14 SPECfp2000 benchmarks. Only the programs that benefit\nthe most from prefetching are shown; prefetching speeds up the missing 15 SPECCPU benchmarks by less than\n15% (Boggs et al., 2004).\n110\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 143,
        "text": "includes only 2 of 12 integer programs, while it includes the majority of the\nSPECCPU floating-point programs. We will return to our evaluation of prefetch-\ning on the i7 in Section 2.6.\nPrefetching relies on utilizing memory bandwidth that otherwise would be\nunused, but if it interferes with demand misses, it can actually lower performance.\nHelp from compilers can reduce useless prefetching. When prefetching works\nwell, its impact on power is negligible. When prefetched data are not used\nor useful data are displaced, prefetching will have a very negative impact\non power.\nNinth Optimization: Compiler-Controlled Prefetching\nto Reduce Miss Penalty or Miss Rate\nAn alternative to hardware prefetching is for the compiler to insert prefetch instruc-\ntions to request data before the processor needs it. There are two flavors of\nprefetch:\n\u25a0\nRegister prefetch loads the value into a register.\n\u25a0\nCache prefetch loads data only into the cache and not the register.\nEither of these can be faulting or nonfaulting; that is, the address does or does\nnot cause an exception for virtual address faults and protection violations. Using\nthis terminology, a normal load instruction could be considered a \u201cfaulting register\nprefetch instruction.\u201d Nonfaulting prefetches simply turn into no-ops if they would\nnormally result in an exception, which is what we want.\nThe most effective prefetch is \u201csemantically invisible\u201d to a program: it doesn\u2019t\nchange the contents of registers and memory, and it cannot cause virtual memory\nfaults. Most processors today offer nonfaulting cache prefetches. This section\nassumes nonfaulting cache prefetch, also called nonbinding prefetch.\nPrefetching makes sense only if the processor can proceed while prefetching\nthe data; that is, the caches do not stall but continue to supply instructions and data\nwhile waiting for the prefetched data to return. As you would expect, the data cache\nfor such computers is normally nonblocking.\nLike hardware-controlled prefetching, the goal is to overlap execution with the\nprefetching of data. Loops are the important targets because they lend themselves\nto prefetch optimizations. If the miss penalty is small, the compiler just unrolls the\nloop once or twice, and it schedules the prefetches with the execution. If the miss\npenalty is large, it uses software pipelining (see Appendix H) or unrolls many times\nto prefetch data for a future iteration.\nIssuing prefetch instructions incurs an instruction overhead, however, so com-\npilers must take care to ensure that such overheads do not exceed the benefits.\nBy concentrating on references that are likely to be cache misses, programs can\navoid unnecessary prefetches while improving average memory access time\nsignificantly.\n2.3\nTen Advanced Optimizations of Cache Performance\n\u25a0\n111"
    },
    {
        "page": 144,
        "text": "Example\nFor the following code, determine which accesses are likely to cause data cache\nmisses. Next, insert prefetch instructions to reduce misses. Finally, calculate the\nnumber of prefetch instructions executed and the misses avoided by prefetching.\nLet\u2019s assume we have an 8 KiB direct-mapped data cache with 16-byte blocks, and\nit is a write-back cache that does write allocate. The elements of a and b are 8 bytes\nlong because they are double-precision floating-point arrays. There are 3 rows and\n100 columns for a and 101 rows and 3 columns for b. Let\u2019s also assume they are\nnot in the cache at the start of the program.\nfor (i \u00bc 0; i < 3; i \u00bc i + 1)\nfor (j \u00bc 0; j < 100; j \u00bc j + 1)\na[i][j] \u00bc b[j][0] * b[j + 1][0];\nAnswer\nThe compiler will first determine which accesses are likely to cause cache misses;\notherwise, we will waste time on issuing prefetch instructions for data that would\nbe hits. Elements of a are written in the order that they are stored in memory, so\na will benefit from spatial locality: The even values of j will miss and the odd\nvalues will hit. Because a has 3 rows and 100 columns, its accesses will lead to\n3(100/2), or 150 misses.\nThe array b does not benefit from spatial locality because the accesses are not in\nthe order it is stored. The array b does benefit twice from temporal locality: the\nsame elements are accessed for each iteration of i, and each iteration of j uses\nthe same value of b as the last iteration. Ignoring potential conflict misses, the\nmisses because of b will be for b[j+1][0] accesses when i\u00bc0, and also\nthe first access to b[j][0] when j\u00bc0. Because j goes from 0 to 99 when\ni\u00bc0, accesses to b lead to 100+1, or 101 misses.\nThus this loop will miss the data cache approximately 150 times for a plus 101\ntimes for b, or 251 misses.\nTo simplify our optimization, we will not worry about prefetching the first\naccesses of the loop. These may already be in the cache, or we will pay the miss\npenalty of the first few elements of a or b. Nor will we worry about suppressing the\nprefetches at the end of the loop that try to prefetch beyond the end of a (a[i]\n[100] \u2026 a[i][106]) and the end of b (b[101][0] \u2026 b[107][0]). If\nthese were faulting prefetches, we could not take this luxury. Let\u2019s assume that\nthe miss penalty is so large we need to start prefetching at least, say, seven itera-\ntions in advance. (Stated alternatively, we assume prefetching has no benefit until\nthe eighth iteration.) We underline the changes to the preceding code needed to add\nprefetching.\nfor (j \u00bc 0; j < 100; j \u00bc j + 1) {\nprefetch(b[j + 7][0]);\n/* b(j,0) for 7 iterations later */\nprefetch(a[0][j + 7]);\n/* a(0,j) for 7 iterations later */\na[0][j] \u00bc b[j][0] * b[j + 1][0];};\n112\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 145,
        "text": "for (i \u00bc 1; i < 3; i \u00bc i + 1)\nfor (j \u00bc 0; j < 100; j \u00bc j + 1) {\nprefetch(a[i][j + 7]);\n/* a(i,j) for + 7 iterations */\na[i][j] \u00bc b[j][0] * b[j + 1][0];}\nThis revised code prefetches a[i][7] through a[i][99] and b[7][0]\nthrough b[100][0], reducing the number of nonprefetched misses to\n\u25a0\n7 misses for elements b[0][0], b[1][0], \u2026 , b[6][0] in the first loop\n\u25a0\n4 misses ([7/2]) for elements a[0][0], a[0][1], \u2026 , a[0][6] in the first\nloop (spatial locality reduces misses to 1 per 16-byte cache block)\n\u25a0\n4 misses ([7/2]) for elements a[1][0], a[1][1], \u2026 , a[1][6] in the\nsecond loop\n\u25a0\n4 misses ([7/2]) for elements a[2][0], a[2][1], \u2026 , a[2][6] in the\nsecond loop\nor a total of 19 nonprefetched misses. The cost of avoiding 232 cache misses is\nexecuting 400 prefetch instructions, likely a good trade-off.\nExample\nCalculate the time saved in the preceding example. Ignore instruction cache misses\nand assume there are no conflict or capacity misses in the data cache. Assume that\nprefetches can overlap with each other and with cache misses, thereby transferring\nat the maximum memory bandwidth. Here are the key loop times ignoring cache\nmisses: the original loop takes 7 clock cycles per iteration, the first prefetch loop\ntakes 9 clock cycles per iteration, and the second prefetch loop takes 8 clock cycles\nper iteration (including the overhead of the outer for loop). A miss takes 100 clock\ncycles.\nAnswer\nThe original doubly nested loop executes the multiply 3100 or 300 times. Because\nthe loop takes 7 clock cycles per iteration, the total is 3007 or 2100 clock cycles\nplus cache misses. Cache misses add 251100 or 25,100 clock cycles, giving a total\nof 27,200 clock cycles. The first prefetch loop iterates 100 times; at 9 clock cycles\nper iteration the total is 900 clock cycles plus cache misses. Now add 11100 or\n1100 clock cycles for cache misses, giving a total of 2000. The second loop executes\n2100 or 200 times, and at 8 clock cycles per iteration, it takes 1600 clock cycles\nplus 8100 or 800 clock cycles for cache misses. This gives a total of 2400 clock\ncycles. From the prior example, we know that this code executes 400 prefetch\ninstructions during the 2000+2400 or 4400 clock cycles to execute these two loops.\nIf we assume that the prefetches are completely overlapped with the rest of the exe-\ncution, then the prefetch code is 27,200/4400, or 6.2 times faster.\n2.3\nTen Advanced Optimizations of Cache Performance\n\u25a0\n113"
    },
    {
        "page": 146,
        "text": "Although array optimizations are easy to understand, modern programs are\nmore likely to use pointers. Luk and Mowry (1999) have demonstrated that\ncompiler-based prefetching can sometimes be extended to pointers as well. Of\n10 programs with recursive data structures, prefetching all pointers when a node\nis visited improved performance by 4%\u201331% in half of the programs. On the other\nhand, the remaining programs were still within 2% of their original performance.\nThe issue is both whether prefetches are to data already in the cache and whether\nthey occur early enough for the data to arrive by the time it is needed.\nMany processors support instructions for cache prefetch, and high-end proces-\nsors (such as the Intel Core i7) often also do some type of automated prefetch in\nhardware.\nTenth Optimization: Using HBM to Extend\nthe Memory Hierarchy\nBecause most general-purpose processors in servers will likely want more memory\nthan can be packaged with HBM packaging, it has been proposed that the in-\npackage DRAMs be used to build massive L4 caches, with upcoming technologies\nranging from 128 MiB to 1 GiB and more, considerably more than current on-chip\nL3 caches. Using such large DRAM-based caches raises an issue: where do the\ntags reside? That depends on the number of tags. Suppose we were to use a\n64B block size; then a 1 GiB L4 cache requires 96 MiB of tags\u2014far more static\nmemory than exists in the caches on the CPU. Increasing the block size to\n4 KiB, yields a dramatically reduced tag store of 256 K entries or less than\n1 MiB total storage, which is probably acceptable, given L3 caches of\n4\u201316 MiB or more in next-generation, multicore processors. Such large block\nsizes, however, have two major problems.\nFirst, the cache may be used inefficiently when content of many blocks are not\nneeded; this is called the fragmentation problem, and it also occurs in virtual mem-\nory systems. Furthermore, transferring such large blocks is inefficient if much of\nthe data is unused. Second, because of the large block size, the number of distinct\nblocks held in the DRAM cache is much lower, which can result in more misses,\nespecially for conflict and consistency misses.\nOne partial solution to the first problem is to add sublocking. Subblocking\nallow parts of the block to be invalid, requiring that they be fetched on a miss. Sub-\nblocking, however, does nothing to address the second problem.\nThe tag storage is the major drawback for using a smaller block size. One pos-\nsible solution for that difficulty is to store the tags for L4 in the HBM. At first glance\nthis seems unworkable, because it requires two accesses to DRAM for each L4\naccess: one for the tags and one for the data itself. Because of the long access time\nfor random DRAM accesses, typically 100 or more processor clock cycles, such an\napproach had been discarded. Loh and Hill (2011) proposed a clever solution to this\nproblem: place the tags and the data in the same row in the HBM SDRAM.\nAlthough opening the row (and eventually closing it) takes a large amount of time,\nthe CAS latency to access a different part of the row is about one-third the new row\naccess time. Thus we can access the tag portion of the block first, and if it is a hit,\n114\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 147,
        "text": "then use a column access to choose the correct word. Loh and Hill (L-H) have pro-\nposed organizing the L4 HBM cache so that each SDRAM row consists of a set of\ntags (at the head of the block) and 29 data segments, making a 29-way set associa-\ntive cache. When L4 is accessed, the appropriate row is opened and the tags are\nread; a hit requires one more column access to get the matching data.\nQureshi and Loh (2012) proposed an improvement called an alloy cache that\nreduces the hit time. An alloy cache molds the tag and data together and uses a\ndirect mapped cache structure. This allows the L4 access time to be reduced to\na single HBM cycle by directly indexing the HBM cache and doing a burst transfer\nof both the tag and data. Figure 2.16 shows the hit latency for the alloy cache, the\nL-H scheme, and SRAM based tags. The alloy cache reduces hit time by more than\na factor of 2 versus the L-H scheme, in return for an increase in the miss rate by a\nfactor of 1.1\u20131.2. The choice of benchmarks is explained in the caption.\nUnfortunately, in both schemes, misses require two full DRAM accesses: one\nto get the initial tag and a follow-on access to the main memory (which is even\n0\n25\n50\n75\n100\n125\nmcf_r\nlbm_r\nsoplex_r\nmilc_r\nomnet_r bwaves_r\nBenchmarks\ngcc_r\nlibqntm_r\nsphinx_r\ngems_r\nLH-Cache\nSRAM-Tags\nAlloy cache\nAverage hit latency\nFigure 2.16 Average hit time latency in clock cycles for the L-H scheme, a currently-impractical scheme using\nSRAM for the tags, and the alloy cache organization. In the SRAM case, we assume the SRAM is accessible in\nthe same time as L3 and that it is checked before L4 is accessed. The average hit latencies are 43 (alloy cache),\n67 (SRAM tags), and 107 (L-H). The 10 SPECCPU2006 benchmarks used here are the most memory-intensive ones;\neach of them would run twice as fast if L3 were perfect.\n2.3\nTen Advanced Optimizations of Cache Performance\n\u25a0\n115"
    },
    {
        "page": 148,
        "text": "slower). If we could speed up the miss detection, we could reduce the miss time.\nTwo different solutions have been proposed to solve this problem: one uses a map\nthat keeps track of the blocks in the cache (not the location of the block, just\nwhether it is present); the other uses a memory access predictor that predicts likely\nmisses using history prediction techniques, similar to those used for global branch\nprediction (see the next chapter). It appears that a small predictor can predict likely\nmisses with high accuracy, leading to an overall lower miss penalty.\nFigure 2.17 shows the speedup obtained on SPECrate for the memory-\nintensive benchmarks used in Figure 2.16. The alloy cache approach outperforms\nthe LH scheme and even the impractical SRAM tags, because the combination of a\nfast access time for the miss predictor and good prediction results lead to a shorter\ntime to predict a miss, and thus a lower miss penalty. The alloy cache performs\nclose to the Ideal case, an L4 with perfect miss prediction and minimal hit time.\n1\n1.1\n1.2\n1.3\n1.4\n1.5\n64 MB\n128 MB\n256 MB\n512 MB\n1 GB\nL4 cache size\nLH-Cache\nSRAM-Tags\nAlloy cache\nSpedup on SPECRate\nIdeal\nFigure 2.17 Performance speedup running the SPECrate benchmark for the LH scheme, an SRAM tag scheme,\nand an ideal L4 (Ideal); a speedup of 1 indicates no improvement with the L4 cache, and a speedup of 2 would\nbe achievable if L4 were perfect and took no access time. The 10 memory-intensive benchmarks are used with\neach benchmark run eight times. The accompanying miss prediction scheme is used. The Ideal case assumes that\nonly the 64-byte block requested in L4 needs to be accessed and transferred and that prediction accuracy for L4\nis perfect (i.e., all misses are known at zero cost).\n116\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 149,
        "text": "HBM is likely to have widespread use in a variety of different configurations,\nfrom containing the entire memory system for some high-performance, special-\npurpose systems to use as an L4 cache for larger server configurations.\nCache Optimization Summary\nThe techniques to improve hit time, bandwidth, miss penalty, and miss rate gen-\nerally affect the other components of the average memory access equation as well\nas the complexity of the memory hierarchy. Figure 2.18 summarizes these tech-\nniques and estimates the impact on complexity, with + meaning that the technique\nTechnique\nHit\ntime\nBand-\nwidth\nMiss\npenalty\nMiss\nrate\nPower\nconsumption\nHardware cost/\ncomplexity\nComment\nSmall and simple\ncaches\n+\n\u2013\n+\n0\nTrivial; widely used\nWay-predicting caches\n+\n+\n1\nUsed in Pentium 4\nPipelined & banked\ncaches\n\u2013\n+\n1\nWidely used\nNonblocking caches\n+\n+\n3\nWidely used\nCritical word first and\nearly restart\n+\n2\nWidely used\nMerging write buffer\n+\n1\nWidely used with write\nthrough\nCompiler techniques to\nreduce cache misses\n+\n0\nSoftware is a challenge, but\nmany compilers handle\ncommon linear algebra\ncalculations\nHardware prefetching\nof instructions and data\n+\n+\n\u2013\n2 instr.,\n3 data\nMost provide prefetch\ninstructions; modern high-\nend processors also\nautomatically prefetch in\nhardware\nCompiler-controlled\nprefetching\n+\n+\n3\nNeeds nonblocking cache;\npossible instruction\noverhead; in many CPUs\nHBM as additional\nlevel of cache\n+/\u2013\n\u2013\n+\n+\n3\nDepends on new packaging\ntechnology. Effects depend\nheavily on hit rate\nimprovements\nFigure 2.18 Summary of 10 advanced cache optimizations showing impact on cache performance, power con-\nsumption, and complexity. Although generally a technique helps only one factor, prefetching can reduce misses if\ndone sufficiently early; if not, it can reduce miss penalty. + means that the technique improves the factor, \u0003 means it\nhurts that factor, and blank means it has no impact. The complexity measure is subjective, with 0 being the easiest\nand 3 being a challenge.\n2.3\nTen Advanced Optimizations of Cache Performance\n\u25a0\n117"
    },
    {
        "page": 150,
        "text": "improves the factor, \u0003 meaning it hurts that factor, and blank meaning it has no\nimpact. Generally, no technique helps more than one category.\n2.4\nVirtual Memory and Virtual Machines\nA virtual machine is taken to be an efficient, isolated duplicate of the real\nmachine. We explain these notions through the idea of a virtual machine\nmonitor (VMM)\u2026 a VMM has three essential characteristics. First, the VMM\nprovides an environment for programs which is essentially identical with\nthe original machine; second, programs run in this environment show at worst\nonly minor decreases in speed; and last, the VMM is in complete control of\nsystem resources.\nGerald Popek and Robert Goldberg,\n\u201cFormal requirements for virtualizable third generation architectures,\u201d\nCommunications of the ACM (July 1974).\nSection B.4 in Appendix B describes the key concepts in virtual memory. Recall\nthat virtual memory allows the physical memory to be treated as a cache of sec-\nondary storage (which may be either disk or solid state). Virtual memory moves\npages between the two levels of the memory hierarchy, just as caches move blocks\nbetween levels. Likewise, TLBs act as caches on the page table, eliminating the\nneed to do a memory access every time an address is translated. Virtual memory\nalso provides separation between processes that share one physical memory but\nhave separate virtual address spaces. Readers should ensure that they understand\nboth functions of virtual memory before continuing.\nIn this section, we focus on additional issues in protection and privacy between\nprocesses sharing the same processor. Security and privacy are two of the most\nvexing challenges for information technology in 2017. Electronic burglaries, often\ninvolving lists of credit card numbers, are announced regularly, and it\u2019s widely\nbelieved that many more go unreported. Of course, such problems arise from pro-\ngramming errors that allow a cyberattack to access data it should be unable to\naccess. Programming errors are a fact of life, and with modern complex software\nsystems, they occur with significant regularity. Therefore both researchers and\npractitioners are looking for improved ways to make computing systems more\nsecure. Although protecting information is not limited to hardware, in our view\nreal security and privacy will likely involve innovation in computer architecture\nas well as in systems software.\nThis section starts with a review of the architecture support for protecting pro-\ncesses from each other via virtual memory. It then describes the added protection\nprovided by virtual machines, the architecture requirements of virtual machines,\nand the performance of a virtual machine. As we will see in Chapter 6, virtual\nmachines are a foundational technology for cloud computing.\n118\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 151,
        "text": "Protection via Virtual Memory\nPage-based virtual memory, including a TLB that caches page table entries, is the\nprimary mechanism that protects processes from each other. Sections B.4 and B.5\nin Appendix B review virtual memory, including a detailed description of protec-\ntion via segmentation and paging in the 80x86. This section acts as a quick review;\nif it\u2019s too quick, please refer to the denoted Appendix B sections.\nMultiprogramming, where several programs running concurrently share a\ncomputer, has led to demands for protection and sharing among programs and\nto the concept of a process. Metaphorically, a process is a program\u2019s breathing\nair and living space\u2014that is, a running program plus any state needed to continue\nrunning it. At any instant, it must be possible to switch from one process to another.\nThis exchange is called a process switch or context switch.\nThe operating system and architecture join forces to allow processes to share the\nhardware yet not interfere with each other. To do this, the architecture must limit\nwhat a process can access when running a user process yet allow an operating sys-\ntem process to access more. At a minimum, the architecture must do the following:\n1. Provide at least two modes, indicating whether the running process is a user\nprocess or an operating system process. This latter process is sometimes called\na kernel process or a supervisor process.\n2. Provide a portion of the processor state that a user process can use but not write.\nThisstateincludesauser/supervisormodebit,anexceptionenable/disablebit,and\nmemory protection information. Users are prevented from writing this state\nbecausetheoperatingsystemcannotcontroluserprocessesifuserscangivethem-\nselves supervisor privileges, disable exceptions, or change memory protection.\n3. Provide mechanisms whereby the processor can go from user mode to super-\nvisor mode and vice versa. The first direction is typically accomplished by a\nsystem call, implemented as a special instruction that transfers control to a ded-\nicated location in supervisor code space. The PC is saved from the point of the\nsystem call, and the processor is placed in supervisor mode. The return to user\nmode is like a subroutine return that restores the previous user/supervisor mode.\n4. Provide mechanisms to limit memory accesses to protect the memory state of a\nprocess without having to swap the process to disk on a context switch.\nAppendix A describes several memory protection schemes, but by far the most\npopular is adding protection restrictions to each page of virtual memory. Fixed-\nsized pages, typically 4 KiB, 16 KiB, or larger, are mapped from the virtual address\nspace into physical address space via a page table. The protection restrictions are\nincluded in each page table entry. The protection restrictions might determine\nwhether a user process can read this page, whether a user process can write to this\npage, and whether code can be executed from this page. In addition, a process can\n2.4\nVirtual Memory and Virtual Machines\n\u25a0\n119"
    },
    {
        "page": 152,
        "text": "neither read nor write a page if it is not in the page table. Because only the OS can\nupdate the page table, the paging mechanism provides total access protection.\nPaged virtual memory means that every memory access logically takes at least\ntwice as long, with one memory access to obtain the physical address and a second\naccess to get the data. This cost would be far too dear. The solution is to rely on the\nprinciple of locality; if the accesses have locality, then the address translations for\nthe accesses must also have locality. By keeping these address translations in a spe-\ncial cache, a memory access rarely requires a second access to translate the address.\nThis special address translation cache is referred to as a TLB.\nA TLB entry is like a cache entry where the tag holds portions of the virtual\naddress and the data portion holds a physical page address, protection field, valid\nbit, and usually a use bit and a dirty bit. The operating system changes these bits by\nchanging the value in the page table and then invalidating the corresponding TLB\nentry. When the entry is reloaded from the page table, the TLB gets an accurate\ncopy of the bits.\nAssuming the computer faithfully obeys the restrictions on pages and maps vir-\ntual addresses to physical addresses, it would seem that we are done. Newspaper\nheadlines suggest otherwise.\nThe reason we\u2019re not done is that we depend on the accuracy of the operating\nsystem as well as the hardware. Today\u2019s operating systems consist of tens of mil-\nlions of lines of code. Because bugs are measured in number per thousand lines of\ncode, there are thousands of bugs in production operating systems. Flaws in the OS\nhave led to vulnerabilities that are routinely exploited.\nThis problem and the possibility that not enforcing protection could be much\nmore costly than in the past have led some to look for a protection model with a\nmuch smaller code base than the full OS, such as virtual machines.\nProtection via Virtual Machines\nAn idea related to virtual memory that is almost as old are virtual machines (VMs).\nThey were first developed in the late 1960s, and they have remained an important\npart of mainframe computing over the years. Although largely ignored in the\ndomain of single-user computers in the 1980s and 1990s, they have recently gained\npopularity because of\n\u25a0\nthe increasing importance of isolation and security in modern systems;\n\u25a0\nthe failures in security and reliability of standard operating systems;\n\u25a0\nthe sharing of a single computer among many unrelated users, such as in a data\ncenter or cloud; and\n\u25a0\nthe dramatic increases in the raw speed of processors, which make the overhead\nof VMs more acceptable.\nThe broadest definition of VMs includes basically all emulation methods that\nprovide a standard software interface, such as the Java VM. We are interested in\n120\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 153,
        "text": "VMs that provide a complete system-level environment at the binary instruction set\narchitecture (ISA) level. Most often, the VM supports the same ISA as the under-\nlying hardware; however, it is also possible to support a different ISA, and such\napproaches are often employed when migrating between ISAs in order to allow\nsoftware from the departing ISA to be used until it can be ported to the new\nISA. Our focus here will be on VMs where the ISA presented by the VM and\nthe underlying hardware match. Such VMs are called (operating) system virtual\nmachines. IBM VM/370, VMware ESX Server, and Xen are examples. They pre-\nsent the illusion that the users of a VM have an entire computer to themselves,\nincluding a copy of the operating system. A single computer runs multiple VMs\nand can support a number of different operating systems (OSes). On a conventional\nplatform, a single OS \u201cowns\u201d all the hardware resources, but with a VM, multiple\nOSes all share the hardware resources.\nThe software that supports VMs is called a virtual machine monitor (VMM) or\nhypervisor; the VMM is the heart of virtual machine technology. The underlying\nhardware platform is called the host, and its resources are shared among the guest\nVMs. The VMM determines how to map virtual resources to physical resources: A\nphysical resource may be time-shared, partitioned, or even emulated in software.\nThe VMM is much smaller than a traditional OS; the isolation portion of a VMM is\nperhaps only 10,000 lines of code.\nIn general, the cost of processor virtualization depends on the workload. User-\nlevel processor-bound programs, such as SPECCPU2006, have zero virtualization\noverhead because the OS is rarely invoked, so everything runs at native speeds.\nConversely, I/O-intensive workloads generally are also OS-intensive and execute\nmany system calls (which doing I/O requires) and privileged instructions that can\nresult in high virtualization overhead. The overhead is determined by the number\nof instructions that must be emulated by the VMM and how slowly they are emu-\nlated. Therefore, when the guest VMs run the same ISA as the host, as we assume\nhere, the goal of the architecture and the VMM is to run almost all instructions\ndirectly on the native hardware. On the other hand, if the I/O-intensive workload\nis also I/O-bound, the cost of processor virtualization can be completely hidden by\nlow processor utilization because it is often waiting for I/O.\nAlthough our interest here is in VMs for improving protection, VMs provide\ntwo other benefits that are commercially significant:\n1. Managing software\u2014VMs provide an abstraction that can run the complete\nsoftware stack, even including old operating systems such as DOS. A typical\ndeployment might be some VMs running legacy OSes, many running the cur-\nrent stable OS release, and a few testing the next OS release.\n2. Managing hardware\u2014One reason for multiple servers is to have each applica-\ntion running with its own compatible version of the operating system on sep-\narate computers, as this separation can improve dependability. VMs allow\nthese separate software stacks to run independently yet share hardware, thereby\nconsolidating the number of servers. Another example is that most newer\nVMMs support migration of a running VM to a different computer, either to\n2.4\nVirtual Memory and Virtual Machines\n\u25a0\n121"
    },
    {
        "page": 154,
        "text": "balance load or to evacuate from failing hardware. The rise of cloud computing\nhas made the ability to swap out an entire VM to another physical processor\nincreasingly useful.\nThese two reasons are why cloud-based servers, such as Amazon\u2019s, rely on virtual\nmachines.\nRequirements of a Virtual Machine Monitor\nWhat must a VM monitor do? It presents a software interface to guest software, it\nmust isolate the state of guests from each other, and it must protect itself from guest\nsoftware (including guest OSes). The qualitative requirements are\n\u25a0\nGuest software should behave on a VM exactly as if it were running on the\nnative hardware, except for performance-related behavior or limitations of\nfixed resources shared by multiple VMs.\n\u25a0\nGuest software should not be able to directly change allocation of real system\nresources.\nTo \u201cvirtualize\u201d the processor, the VMM must control just about everything\u2014\naccess to privileged state, address translation, I/O, exceptions and interrupts\u2014even\nthough the guest VM and OS currently running are temporarily using them.\nFor example, in the case of a timer interrupt, the VMM would suspend the cur-\nrently running guest VM, save its state, handle the interrupt, determine which guest\nVM to run next, and then load its state. Guest VMs that rely on a timer interrupt are\nprovided with a virtual timer and an emulated timer interrupt by the VMM.\nTo be in charge, the VMM must be at a higher privilege level than the guest\nVM, which generally runs in user mode; this also ensures that the execution of any\nprivileged instruction will be handled by the VMM. The basic requirements of sys-\ntem virtual machines are almost identical to those for the previously mentioned\npaged virtual memory:\n\u25a0\nAt least two processor modes, system and user.\n\u25a0\nA privileged subset of instructions that is available only in system mode, result-\ning in a trap if executed in user mode. All system resources must be controllable\nonly via these instructions.\nInstruction Set Architecture Support for Virtual Machines\nIf VMs are planned for during the design of the ISA, it\u2019s relatively easy to reduce\nboth the number of instructions that must be executed by a VMM and how long it\ntakes to emulate them. An architecture that allows the VM to execute directly on\nthe hardware earns the title virtualizable, and the IBM 370 architecture proudly\nbears that label.\n122\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 155,
        "text": "However, because VMs have been considered for desktop and PC-based server\napplications only fairly recently, most instruction sets were created without virtua-\nlization in mind. These culprits include 80x86 and most of the original RISC archi-\ntectures, although the latter had fewer issues than the 80x86 architecture. Recent\nadditions to the x86 architecture have attempted to remedy the earlier shortcom-\nings, and RISC V explicitly includes support for virtualization.\nBecause the VMM must ensure that the guest system interacts only with virtual\nresources, a conventional guest OS runs as a user mode program on top of the\nVMM. Then, if a guest OS attempts to access or modify information related to\nhardware resources via a privileged instruction\u2014for example, reading or writing\nthe page table pointer\u2014it will trap to the VMM. The VMM can then effect the\nappropriate changes to corresponding real resources.\nTherefore, if any instruction that tries to read or write such sensitive informa-\ntion traps when executed in user mode, the VMM can intercept it and support a\nvirtual version of the sensitive information as the guest OS expects.\nIn the absence of such support, other measures must be taken. A VMM must\ntake special precautions to locate all problematic instructions and ensure that they\nbehave correctly when executed by a guest OS, thereby increasing the complexity\nof the VMM and reducing the performance of running the VM. Sections 2.5 and\n2.7 give concrete examples of problematic instructions in the 80x86 architecture.\nOne attractive extension allows the VM and the OS to operate at different privilege\nlevels, each of which is distinct from the user level. By introducing an additional\nprivilege level, some OS operations\u2014e.g., those that exceed the permissions\ngranted to a user program but do not require intervention by the VMM (because\nthey cannot affect any other VM)\u2014can execute directly without the overhead of\ntrapping and invoking the VMM. The Xen design, which we examine shortly,\nmakes use of three privilege levels.\nImpact of Virtual Machines on Virtual Memory and I/O\nAnother challenge is virtualization of virtual memory, as each guest OS in every\nVM manages its own set of page tables. To make this work, the VMM separates the\nnotions of real and physical memory (which are often treated synonymously) and\nmakes real memory a separate, intermediate level between virtual memory and\nphysical memory. (Some use the terms virtual memory, physical memory, and\nmachine memory to name the same three levels.) The guest OS maps virtual mem-\nory to real memory via its page tables, and the VMM page tables map the guests\u2019\nreal memory to physical memory. The virtual memory architecture is specified\neither via page tables, as in IBM VM/370 and the 80x86, or via the TLB structure,\nas in many RISC architectures.\nRather than pay an extra level of indirection on every memory access, the\nVMM maintains a shadow page table that maps directly from the guest virtual\naddress space to the physical address space of the hardware. By detecting all mod-\nifications to the guest\u2019s page table, the VMM can ensure that the shadow page table\n2.4\nVirtual Memory and Virtual Machines\n\u25a0\n123"
    },
    {
        "page": 156,
        "text": "entries being used by the hardware for translations correspond to those of the guest\nOS environment, with the exception of the correct physical pages substituted for\nthe real pages in the guest tables. Therefore the VMM must trap any attempt by the\nguest OS to change its page table or to access the page table pointer. This is com-\nmonly done by write protecting the guest page tables and trapping any access to the\npage table pointer by a guest OS. As previously noted, the latter happens naturally\nif accessing the page table pointer is a privileged operation.\nThe IBM 370 architecture solved the page table problem in the 1970s with an\nadditional level of indirection that is managed by the VMM. The guest OS keeps its\npage tables as before, so the shadow pages are unnecessary. AMD has implemen-\nted a similar scheme for its 80x86.\nTo virtualize the TLB in many RISC computers, the VMM manages the real\nTLB and has a copy of the contents of the TLB of each guest VM. To pull this off,\nany instructions that access the TLB must trap. TLBs with Process ID tags can sup-\nport a mix of entries from different VMs and the VMM, thereby avoiding flushing\nof the TLB on a VM switch. Meanwhile, in the background, the VMM supports a\nmapping between the VMs\u2019 virtual Process IDs and the real Process IDs. Section\nL.7 of online Appendix L describes additional details.\nThe final portion of the architecture to virtualize is I/O. This is by far the most\ndifficult part of system virtualization because of the increasing number of I/O\ndevices attached to the computer and the increasing diversity of I/O device types.\nAnother difficulty is the sharing of a real device among multiple VMs, and yet\nanother comes from supporting the myriad of device drivers that are required, espe-\ncially if different guest OSes are supported on the same VM system. The VM illu-\nsion can be maintained by giving each VM generic versions of each type of I/O\ndevice driver, and then leaving it to the VMM to handle real I/O.\nThe method for mapping a virtual-to-physical I/O device depends on the type\nof device. For example, physical disks are normally partitioned by the VMM to\ncreate virtual disks for guest VMs, and the VMM maintains the mapping of virtual\ntracks and sectors to the physical ones. Network interfaces are often shared\nbetween VMs in very short time slices, and the job of the VMM is to keep track\nof messages for the virtual network addresses to ensure that guest VMs receive\nonly messages intended for them.\nExtending the Instruction Set for Efficient Virtualization\nand Better Security\nIn the past 5\u201310 years, processor designers, including those at AMD and Intel (and\nto a lesser extent ARM), have introduced instruction set extensions to more effi-\nciently support virtualization. Two primary areas of performance improvement\nhave been in handling page tables and TLBs (the cornerstone of virtual memory)\nand in I/O, specifically handling interrupts and DMA. Virtual memory perfor-\nmance is enhanced by avoiding unnecessary TLB flushes and by using the nested\npage table mechanism, employed by IBM decades earlier, rather than a complete\n124\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 157,
        "text": "set of shadow page tables (see Section L.7 in Appendix L). To improve I/O per-\nformance, architectural extensions are added that allow a device to directly use\nDMA to move data (eliminating a potential copy by the VMM) and allow device\ninterrupts and commands to be handled by the guest OS directly. These extensions\nshow significant performance gains in applications that are intensive either in their\nmemory-management aspects or in the use of I/O.\nWith the broad adoption of public cloud systems for running critical applica-\ntions, concerns have risen about security of data in such applications. Any mali-\ncious code that is able to access a higher privilege level than data that must be\nkept secure compromises the system. For example, if you are running a credit card\nprocessing application, you must be absolutely certain that malicious users cannot\nget access to the credit card numbers, even when they are using the same hardware\nand intentionally attack the OS or even the VMM. Through the use of virtualiza-\ntion, we can prevent accesses by an outside user to the data in a different VM, and\nthis provides significant protection compared to a multiprogrammed environment.\nThat might not be enough, however, if the attacker compromises the VMM or can\nfind out information by observations in another VMM. For example, suppose the\nattacker penetrates the VMM; the attacker can then remap memory so as to access\nany portion of the data.\nAlternatively, an attack might rely on a Trojan horse (see Appendix B) intro-\nduced into the code that can access the credit cards. Because the Trojan horse is\nrunning in the same VM as the credit card processing application, the Trojan horse\nonly needs to exploit an OS flaw to gain access to the critical data. Most cyberat-\ntacks have used some form of Trojan horse, typically exploiting an OS flaw, that\neither has the effect of returning access to the attacker while leaving the CPU still in\nprivilege mode or allows the attacker to upload and execute code as if it were part\nof the OS. In either case, the attacker obtains control of the CPU and, using the\nhigher privilege mode, can proceed to access anything within the VM. Note that\nencryption alone does not prevent this attacker. If the data in memory is unen-\ncrypted, which is typical, then the attacker has access to all such data. Furthermore,\nif the attacker knows where the encryption key is stored, the attacker can freely\naccess the key and then access any encrypted data.\nMore recently, Intel introduced a set of instruction set extensions, called the\nsoftware guard extensions (SGX), to allow user programs to create enclaves, por-\ntions of code and data that are always encrypted and decrypted only on use and\nonly with the key provided by the user code. Because the enclave is always\nencrypted, standard OS operations for virtual memory or I/O can access the\nenclave (e.g., to move a page) but cannot extract any information. For an enclave\nto work, all the code and all the data required must be part of the enclave. Although\nthe topic of finer-grained protection has been around for decades, it has gotten little\ntraction before because of the high overhead and because other solutions that are\nmore efficient and less intrusive have been acceptable. The rise of cyberattacks and\nthe amount of confidential information online have led to a reexamination of tech-\nniques for improving such fine-grained security. Like Intel\u2019s SGX, IBM and\nAMD\u2019s recent processors support on-the-fly encryption of memory.\n2.4\nVirtual Memory and Virtual Machines\n\u25a0\n125"
    },
    {
        "page": 158,
        "text": "An Example VMM: The Xen Virtual Machine\nEarly in the development of VMs, a number of inefficiencies became apparent. For\nexample, a guest OS manages its virtual-to-real page mapping, but this mapping is\nignored by the VMM, which performs the actual mapping to physical pages. In\nother words, a significant amount of wasted effort is expended just to keep the\nguest OS happy. To reduce such inefficiencies, VMM developers decided that\nit may be worthwhile to allow the guest OS to be aware that it is running on a\nVM. For example, a guest OS could assume a real memory as large as its virtual\nmemory so that no memory management is required by the guest OS.\nAllowing small modifications to the guest OS to simplify virtualization is\nreferred to as paravirtualization, and the open source Xen VMM is a good exam-\nple. The Xen VMM, which is used in Amazon\u2019s web services data centers, pro-\nvides a guest OS with a virtual machine abstraction that is similar to the\nphysical hardware, but drops many of the troublesome pieces. For example, to\navoid flushing the TLB, Xen maps itself into the upper 64 MiB of the address space\nof each VM. Xen allows the guest OS to allocate pages, checking only to be sure\nthe guest OS does not violate protection restrictions. To protect the guest OS from\nthe user programs in the VM, Xen takes advantage of the four protection levels\navailable in the 80x86. The Xen VMM runs at the highest privilege level (0),\nthe guest OS runs at the next level (1), and the applications run at the lowest priv-\nilege level (3). Most OSes for the 80x86 keep everything at privilege levels 0 or 3.\nFor subsetting to work properly, Xen modifies the guest OS to not use prob-\nlematic portions of the architecture. For example, the port of Linux to Xen changes\nabout 3000 lines, or about 1% of the 80x86-specific code. These changes, how-\never, do not affect the application binary interfaces of the guest OS.\nTo simplify the I/O challenge of VMs, Xen assigned privileged virtual\nmachines to each hardware I/O device. These special VMs are called driver\ndomains. (Xen calls VMs \u201cdomains.\u201d) Driver domains run the physical device\ndrivers, although interrupts are still handled by the VMM before being sent to\nthe appropriate driver domain. Regular VMs, called guest domains, run simple vir-\ntual device drivers that must communicate with the physical device drivers in the\ndriver domains over a channel to access the physical I/O hardware. Data are sent\nbetween guest and driver domains by page remapping.\n2.5\nCross-Cutting Issues: The Design of Memory Hierarchies\nThis section describes four topics discussed in other chapters that are fundamental\nto memory hierarchies.\nProtection, Virtualization, and Instruction Set Architecture\nProtection is a joint effort of architecture and operating systems, but architects had\nto modify some awkward details of existing instruction set architectures when vir-\ntual memory became popular. For example, to support virtual memory in the IBM\n126\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 159,
        "text": "370, architects had to change the successful IBM 360 instruction set architecture\nthat had been announced just 6 years before. Similar adjustments are being made\ntoday to accommodate virtual machines.\nFor example, the 80x86 instruction POPF loads the flag registers from the top\nof the stack in memory. One of the flags is the Interrupt Enable (IE) flag. Until\nrecent changes to support virtualization, running the POPF instruction in user\nmode, rather than trapping it, simply changed all the flags except IE. In system\nmode, it does change the IE flag. Because a guest OS runs in user mode inside\na VM, this was a problem, as the OS would expect to see a changed IE. Extensions\nof the 80x86 architecture to support virtualization eliminated this problem.\nHistorically, IBM mainframe hardware and VMM took three steps to improve\nperformance of virtual machines:\n1. Reduce the cost of processor virtualization.\n2. Reduce interrupt overhead cost due to the virtualization.\n3. Reduce interrupt cost by steering interrupts to the proper VM without\ninvoking VMM.\nIBM is still the gold standard of virtual machine technology. For example, an IBM\nmainframe ran thousands of Linux VMs in 2000, while Xen ran 25 VMs in 2004\n(Clark et al., 2004). Recent versions of Intel and AMD chipsets have added special\ninstructions to support devices in a VM to mask interrupts at lower levels from each\nVM and to steer interrupts to the appropriate VM.\nAutonomous Instruction Fetch Units\nMany processors with out-of-order execution and even some with simply deep\npipelines decouple the instruction fetch (and sometimes initial decode), using a\nseparate instruction fetch unit (see Chapter 3). Typically, the instruction fetch unit\naccesses the instruction cache to fetch an entire block before decoding it into indi-\nvidual instructions; such a technique is particularly useful when the instruction\nlength varies. Because the instruction cache is accessed in blocks, it no longer\nmakes sense to compare miss rates to processors that access the instruction cache\nonce per instruction. In addition, the instruction fetch unit may prefetch blocks into\nthe L1 cache; these prefetches may generate additional misses, but may actually\nreduce the total miss penalty incurred. Many processors also include data prefetch-\ning, which may increase the data cache miss rate, even while decreasing the total\ndata cache miss penalty.\nSpeculation and Memory Access\nOne of the major techniques used in advanced pipelines is speculation, whereby an\ninstruction is tentatively executed before the processor knows whether it is really\nneeded. Such techniques rely on branch prediction, which if incorrect requires that\n2.5\nCross-Cutting Issues: The Design of Memory Hierarchies\n\u25a0\n127"
    },
    {
        "page": 160,
        "text": "the speculated instructions are flushed from the pipeline. There are two separate\nissues in a memory system supporting speculation: protection and performance.\nWith speculation, the processor may generate memory references, which will\nnever be used because the instructions were the result of incorrect speculation.\nThose references, if executed, could generate protection exceptions. Obviously,\nsuch faults should occur only if the instruction is actually executed. In the next\nchapter, we will see how such \u201cspeculative exceptions\u201d are resolved. Because a\nspeculative processor may generate accesses to both the instruction and data\ncaches, and subsequently not use the results of those accesses, speculation may\nincrease the cache miss rates. As with prefetching, however, such speculation\nmay actually lower the total cache miss penalty. The use of speculation, like the\nuse of prefetching, makes it misleading to compare miss rates to those seen in pro-\ncessors without speculation, even when the ISA and cache structures are otherwise\nidentical.\nSpecial Instruction Caches\nOne of the biggest challenges in superscalar processors is to supply the instruc-\ntion bandwidth. For designs that translate the instructions into micro-operations,\nsuch as most recent Arm and i7 processors, instruction bandwidth demands and\nbranch misprediction penalties can be reduced by keeping a small cache of\nrecently translated instructions. We explore this technique in greater depth in\nthe next chapter.\nCoherency of Cached Data\nData can be found in memory and in the cache. As long as the processor is the sole\ncomponent changing or reading the data and the cache stands between the proces-\nsor and memory, there is little danger in the processor seeing the old or stale copy.\nAs we will see, multiple processors and I/O devices raise the opportunity for copies\nto be inconsistent and to read the wrong copy.\nThe frequency of the cache coherency problem is different for multiprocessors\nthan for I/O. Multiple data copies are a rare event for I/O\u2014one to be avoided when-\never possible\u2014but a program running on multiple processors will want to have\ncopies of the same data in several caches. Performance of a multiprocessor pro-\ngram depends on the performance of the system when sharing data.\nThe I/O cache coherency question is this: where does the I/O occur in the com-\nputer\u2014between the I/O device and the cache or between the I/O device and main\nmemory? If input puts data into the cache and output reads data from the cache,\nboth I/O and the processor see the same data. The difficulty in this approach is that\nit interferes with the processor and can cause the processor to stall for I/O. Input\nmay also interfere with the cache by displacing some information with new data\nthat are unlikely to be accessed soon.\n128\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 161,
        "text": "The goal for the I/O system in a computer with a cache is to prevent the stale\ndata problem while interfering as little as possible. Many systems therefore prefer\nthat I/O occur directly to main memory, with main memory acting as an I/O buffer.\nIf a write-through cache were used, then memory would have an up-to-date copy of\nthe information, and there would be no stale data issue for output. (This benefit is a\nreason processors used write through.) However, today write through is usually\nfound only in first-level data caches backed by an L2 cache that uses write back.\nInput requires some extra work. The software solution is to guarantee that no\nblocks of the input buffer are in the cache. A page containing the buffer can be\nmarked as noncachable, and the operating system can always input to such a page.\nAlternatively, the operating system can flush the buffer addresses from the cache\nbefore the input occurs. A hardware solution is to check the I/O addresses on input\nto see if they are in the cache. If there is a match of I/O addresses in the cache, the\ncache entries are invalidated to avoid stale data. All of these approaches can also be\nused for output with write-back caches.\nProcessor cache coherency is a critical subject in the age of multicore proces-\nsors, and we will examine it in detail in Chapter 5.\n2.6\nPutting It All Together: Memory Hierarchies in the\nARM Cortex-A53 and Intel Core i7 6700\nThis section reveals the ARM Cortex-A53 (hereafter called the A53) and Intel Core\ni76700 (hereafter called i7) memory hierarchies and shows the performance of\ntheir components on a set of single-threaded benchmarks. We examine the\nCortex-A53 first because it has a simpler memory system; we go into more detail\nfor the i7, tracing out a memory reference in detail. This section presumes that\nreaders are familiar with the organization of a two-level cache hierarchy using vir-\ntually indexed caches. The basics of such a memory system are explained in detail\nin Appendix B, and readers who are uncertain of the organization of such a system\nare strongly advised to review the Opteron example in Appendix B. Once they\nunderstand the organization of the Opteron, the brief explanation of the A53 sys-\ntem, which is similar, will be easy to follow.\nThe ARM Cortex-A53\nThe Cortex-A53 is a configurable core that supports the ARMv8A instruction set\narchitecture, which includes both 32-bit and 64-bit modes. The Cortex-A53 is\ndelivered as an IP (intellectual property) core. IP cores are the dominant form\nof technology delivery in the embedded, PMD, and related markets; billions of\nARM and MIPS processors have been created from these IP cores. Note that IP\ncores are different from the cores in the Intel i7 or AMD Athlon multicores. An\nIP core (which may itself be a multicore) is designed to be incorporated with\nother logic (thus it is the core of a chip), including application-specific processors\n2.6\nPutting It All Together: Memory Hierarchies in the ARM Cortex-A53 and Intel Core i7 6700\n\u25a0\n129"
    },
    {
        "page": 162,
        "text": "(such as an encoder or decoder for video), I/O interfaces, and memory interfaces,\nand then fabricated to yield a processor optimized for a particular application. For\nexample, the Cortex-A53 IP core is used in a variety of tablets and smartphones; it\nis designed to be highly energy-efficient, a key criteria in battery-based PMDs. The\nA53 core is capable of being configured with multiple cores per chip for use in\nhigh-end PMDs; our discussion here focuses on a single core.\nGenerally, IP cores come in two flavors. Hard cores are optimized for a par-\nticular semiconductor vendor and are black boxes with external (but still on-chip)\ninterfaces. Hard cores typically allow parametrization only of logic outside the\ncore, such as L2 cache sizes, and the IP core cannot be modified. Soft cores are\nusually delivered in a form that uses a standard library of logic elements. A soft\ncore can be compiled for different semiconductor vendors and can also be modi-\nfied, although extensive modifications are very difficult because of the complexity\nof modern-day IP cores. In general, hard cores provide higher performance and\nsmaller die area, while soft cores allow retargeting to other vendors and can be\nmore easily modified.\nThe Cortex-A53 can issue two instructions per clock at clock rates up to\n1.3 GHz. It supports both a two-level TLB and a two-level cache; Figure 2.19 sum-\nmarizes the organization of the memory hierarchy. The critical term is returned\nfirst, and the processor can continue while the miss completes; a memory system\nwith up to four banks can be supported. For a D-cache of 32 KiB and a page size of\n4 KiB, each physical page could map to two different cache addresses; such aliases\nare avoided by hardware detection on a miss as in Section B.3 of Appendix B.\nFigure 2.20 shows how the 32-bit virtual address is used to index the TLB and\nthe caches, assuming 32 KiB primary caches and a 1 MiB secondary cache with\n16 KiB page size.\nStructure\nSize\nOrganization\nTypical miss penalty\n(clock cycles)\nInstruction MicroTLB\n10 entries\nFully associative\n2\nData MicroTLB\n10 entries\nFully associative\n2\nL2 Unified TLB\n512 entries\n4-way set associative\n20\nL1 Instruction cache\n8\u201364 KiB\n2-way set associative; 64-byte block\n13\nL1 Data cache\n8\u201364 KiB\n2-way set associative; 64-byte block\n13\nL2 Unified cache\n128 KiB to 2 MiB\n16-way set associative; LRU\n124\nFigure 2.19 The memory hierarchy of the Cortex A53 includes multilevel TLBs and caches. A page map cache\nkeeps track of the location of a physical page for a set of virtual pages; it reduces the L2 TLB miss penalty. The\nL1 caches are virtually indexed and physically tagged; both the L1 D cache and L2 use a write-back policy defaulting\nto allocate on write. Replacement policy is LRU approximation in all the caches. Miss penalties to L2 are higher if both\na MicroTLB and L1 miss occur. The L2 to main memory bus is 64\u2013128 bits wide, and the miss penalty is larger for the\nnarrow bus.\n130\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 163,
        "text": "Virtual address <32>\nPhysical address <32>\nL2 tag compare address <16>\nL2 cache index <10>\nBlock offset <6>\nReal page number <16>\nL2 cache tag <16>\nL2 data <64 bytes>\n=?\n=?\nTo CPU\nTo CPU\nTo CPU\nTo L1 cache or CPU\nL1 cache tag <19>\nL1 data <64 bytes>\nTLB tag  <16>\nInstruction TLB\nVirtual address <32>\nPhysical address <32>\nPage offset <16>\nVirtual page number <16>\nPage offset <16>\nVirtual page number <16>\nReal page number <16>\n=?\nTo CPU\nTo CPU\nL1 cache index <10> Block offset <6>\nL1 cache index <10> Block offset <6>\nL1 cache tag <18> L1 data <64 bytes>\nTLB tag  <16>\nTo L2 (see part b below)\nThe instruction access path\n(A)\n(B)\nThe data access path\nInstruction cache\n8\n2\nData TLB\nData cache\n7\n3\n=?\nL2 TLB\n7\n9\nReal page number <16>\nTLB tag  <9>\nFigure 2.20 The virtual address, physical and data blocks for the ARM Cortex-A53 caches and TLBs, assuming 32-\nbit addresses. The top half (A) shows the instruction access; the bottom half (B) shows the data access, including L2.\nThe TLB (instruction or data) is fully associative each with 10 entries, using a 64 KiB page in this example. The L1 I-\ncache is two-way set associative, with 64-byte blocks and 32 KiB capacity; the L1 D-cache is 32 KiB, four-way set asso-\nciative, and 64-byte blocks. The L2 TLB is 512 entries and four-way set associative. The L2 cache is 16-way set asso-\nciative with 64-byte blocks and 128 cKiB to 2 MiB capacity; a 1 MiB L2 is shown. This figure doesn\u2019t show the valid bits\nand protection bits for the caches and TLB."
    },
    {
        "page": 164,
        "text": "Performance of the Cortex-A53 Memory Hierarchy\nThe memory hierarchy of the Cortex-A8 was measured with 32 KiB primary\ncaches and a 1 MiB L2 cache running the SPECInt2006 benchmarks. The instruc-\ntion cache miss rates for these SPECInt2006 are very small even for just the L1:\nclose to zero for most and under 1% for all of them. This low rate probably results\nfrom the computationally intensive nature of the SPECCPU programs and the two-\nway set associative cache that eliminates most conflict misses.\nFigure 2.21 shows the data cache results, which have significant L1 and\nL2 miss rates. The L1 rate varies by a factor of 75, from 0.5% to 37.3% with a\nmedian miss rate of 2.4%. The global L2 miss rate varies by a factor of 180, from\n0.05% to 9.0% with a median of 0.3%. MCF, which is known as a cache buster,\nsets the upper bound and significantly affects the mean. Remember that the L2\nglobal miss rate is significantly lower than the L2 local miss rate; for example,\nthe median L2 stand-alone miss rate is 15.1% versus the global miss rate of 0.3%.\nUsing these miss penalties in Figure 2.19, Figure 2.22 shows the average pen-\nalty per data access. Although the L1 miss rates are about seven times higher than\nthe L2 miss rate, the L2 penalty is 9.5 times as high, leading to L2 misses slightly\ndominating for the benchmarks that stress the memory system. In the next chapter,\nwe will examine the impact of the cache misses on overall CPI.\nhmmer\nh264ref\nlibquantum\nbzip2\ngobmk\nxalancbmk\ngcc\nastar\nomnetpp\nmcf\nsjeng\nperlbench\n0.0%\n5.0%\n10.0%\n15.0%\n20.0%\n25.0%\n30.0%\n35.0%\n40.0%\nL1 data miss rate\nL2 data miss rate\nFigure 2.21 The data miss rate for ARM with a 32 KiB L1 and the global data miss rate for a 1 MiB L2 using the\nSPECInt2006 benchmarks are significantly affected by the applications. Applications with larger memory footprints\ntend to have higher miss rates in both L1 and L2. Note that the L2 rate is the global miss rate that is counting all\nreferences, including those that hit in L1. MCF is known as a cache buster.\n132\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 165,
        "text": "The Intel Core i7 6700\nThe i7 supports the x86-64 instruction set architecture, a 64-bit extension of the\n80x86 architecture. The i7 is an out-of-order execution processor that includes four\ncores. In this chapter, we focus on the memory system design and performance\nfrom the viewpoint of a single core. The system performance of multiprocessor\ndesigns, including the i7 multicore, is examined in detail in Chapter 5.\nEach core in an i7 can execute up to four 80x86 instructions per clock cycle,\nusing a multiple issue, dynamically scheduled, 16-stage pipeline, which we\ndescribe in detail in Chapter 3. The i7 can also support up to two simultaneous\nthreads per processor, using a technique called simultaneous multithreading,\ndescribed in Chapter 4. In 2017 the fastest i7 had a clock rate of 4.0 GHz (in Turbo\nBoost mode), which yielded a peak instruction execution rate of 16 billion instruc-\ntions per second, or 64 billion instructions per second for the four-core design. Of\ncourse, there is a big gap between peak and sustained performance, as we will see\nover the next few chapters.\nThe i7 can support up to three memory channels, each consisting of a separate\nset of DIMMs, and each of which can transfer in parallel. Using DDR3-1066\n(DIMM PC8500), the i7 has a peak memory bandwidth of just over 25 GB/s.\nhmmer\nh264ref\nlibquantum\nbzip2\ngobmk\nxalancbmk\ngcc\nastar\nomnetpp\nmcf\nsjeng\nperlbench\n0\n2\n4\n6\n8\nMiss penalty per data reference \n10\n12\n14\n16\nL2 data average memory penalty\nL1 data average memory penalty\nFigure 2.22 The average memory access penalty per data memory reference coming from L1 and L2 is shown for\nthe A53 processor when running SPECInt2006. Although the miss rates for L1 are significantly higher, the L2 miss\npenalty, which is more than five times higher, means that the L2 misses can contribute significantly.\n2.6\nPutting It All Together: Memory Hierarchies in the ARM Cortex-A53 and Intel Core i7 6700\n\u25a0\n133"
    },
    {
        "page": 166,
        "text": "i7 uses 48-bit virtual addresses and 36-bit physical addresses, yielding a\nmaximum physical memory of 36 GiB. Memory management is handled with a\ntwo-level TLB (see Appendix B, Section B.4), summarized in Figure 2.23.\nFigure 2.24 summarizes the i7\u2019s three-level cache hierarchy. The first-level\ncaches\nare\nvirtually\nindexed\nand\nphysically\ntagged\n(see\nAppendix\nB,\nSection B.3), while the L2 and L3 caches are physically indexed. Some versions\nof the i7 6700 will support a fourth-level cache using HBM packaging.\nFigure 2.25 is labeled with the steps of an access to the memory hierarchy.\nFirst, the PC is sent to the instruction cache. The instruction cache index is\n2Index \u00bc\nCache size\nBlock sizeSet associativity \u00bc 32K\n648 \u00bc 64 \u00bc 26\nCharacteristic\nInstruction TLB\nData DLB\nSecond-level TLB\nEntries\n128\n64\n1536\nAssociativity\n8-way\n4-way\n12-way\nReplacement\nPseudo-LRU\nPseudo-LRU\nPseudo-LRU\nAccess latency\n1 cycle\n1 cycle\n8 cycles\nMiss\n9 cycles\n9 cycles\nHundreds of cycles to access\npage table\nFigure 2.23 Characteristics of the i7\u2019s TLB structure, which has separate first-level\ninstruction and data TLBs, both backed by a joint second-level TLB. The first-level TLBs\nsupport the standard 4 KiB page size, as well as having a limited number of entries of\nlarge 2\u20134 MiB pages; only 4 KiB pages are supported in the second-level TLB. The i7 has\nthe ability to handle two L2 TLB misses in parallel. See Section L.3 of online Appendix L\nfor more discussion of multilevel TLBs and support for multiple page sizes.\nCharacteristic\nL1\nL2\nL3\nSize\n32 KiB I/32 KiB D\n256 KiB\n2 MiB per core\nAssociativity\nboth 8-way\n4-way\n16-way\nAccess latency\n4 cycles, pipelined\n12 cycles\n44 cycles\nReplacement scheme\nPseudo-LRU\nPseudo-LRU\nPseudo-LRU but with an\nordered selection algorithm\nFigure 2.24 Characteristics of the three-level cache hierarchy in the i7. All three\ncaches use write back and a block size of 64 bytes. The L1 and L2 caches are separate\nfor each core, whereas the L3 cache is shared among the cores on a chip and is a total of\n2 MiB per core. All three caches are nonblocking and allow multiple outstanding writes.\nA merging write buffer is used for the L1 cache, which holds data in the event that the\nline is not present in L1 when it is written. (That is, an L1 write miss does not cause the\nline to be allocated.) L3 is inclusive of L1 and L2; we explore this property in further detail\nwhen we explain multiprocessor caches. Replacement is by a variant on pseudo-LRU; in\nthe case of L3, the block replaced is always the lowest numbered way whose access bit is\noff. This is not quite random but is easy to compute.\n134\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 167,
        "text": "Data\n<128x4>\nData\n<512>\nVirtual page\nnumber <36> \nData in <64>\nInstruction\n<128>\n<128>\n<7>\n<64>\n<30>\nPage\noffset <12>\nPC\nCPU\n2:1 mux\n<20>\nTag\n<10>\nL2\nC\nA\nC\nH\nE\nC\nA\nC\nH\nE\nData virtual page\nnumber <36> \nPage\noffset <12>\n<6>\nIndex\nBlock offset\nI\nC\nA\nC\nH\nE\nI\nT\nL\nB\nL2\nT\nL\nB\n<24>\n=?\n8:1 mux\n8:1 mux\n12:1 mux\n2\n1\n3\n5\n5\n6\n8\n9\n7\n16\n10\nV\n<1>\nD\n<1>\nV\n<1>\nD\n<1>\nTag\n<21>\nV\n<1>\nD\n<1>\nTag\n<17>\n4:1 mux\n=?\n=?\n<7>\n<6>\nD\nC\nA\nC\nH\nE\nD\nT\nL\nB\n8:1 mux\n<4>\nProt\n<1>\nV\n4:1 mux\n(64 PTEs in 4 banks)\n(128 PTEs in 8 banks)\n<31>\nTag\n<24>\nPhysical address\n<4>\nProt\n<1>\nV\n<32>\nTag\n<24>\nPhysical address\n<4>\nProt\n<1>\nV\n<29>\nTag\n<24>\nPhysical address\n=?\n(1536 PTEs\n in 12 banks)\n(512 blocks in 8 banks)\nData\n<128\u00d74>\n(512 blocks in 8 banks)\nData\n<64>\n(4K blocks in 4 banks)\nData\n<512>\n<17>\n<13>\nL3\n11\n12\n13\n16:1 mux\n=?\n(128K blocks in 16 banks)\n<64>\n<64>\nDIMM\nDIMM\nM\nA\nI\nN\nM\nE\nM\nO\nR\nY\n15\nMemory Interface\n<64>\nDIMM\n14\n16\n4\nIndex\nTag\nIndex\nV\n<1>\nD\n<1>\nTag\n<24>\nIndex\nBlock offset\n<6>\n<6>\n<24>\n<28>\nFigure 2.25 The Intel i7 memory hierarchy and the steps in both instruction and data access. We show only reads.\nWrites are similar, except that misses are handled by simply placing the data in a write buffer, because the L1 cache is\nnot write-allocated.\n2.6\nPutting It All Together: Memory Hierarchies in the ARM Cortex-A53 and Intel Core i7 6700\n\u25a0\n135"
    },
    {
        "page": 168,
        "text": "or 6 bits. The page frame of the instruction\u2019s address (36\u00bc48\u000312 bits) is sent to\nthe instruction TLB (step 1). At the same time, the 12-bit page offset from the vir-\ntual address is sent to the instruction cache (step 2). Notice that for the eight-way\nassociative instruction cache, 12 bits are needed for the cache address: 6 bits to\nindex the cache plus 6 bits of block offset for the 64-byte block, so no aliases\nare possible. The previous versions of the i7 used a four-way set associative\nI-cache, meaning that a block corresponding to a virtual address could actually\nbe in two different places in the cache, because the corresponding physical address\ncould have either a 0 or 1 in this location. For instructions this did not pose a prob-\nlem because even if an instruction appeared in the cache in two different locations,\nthe two versions must be the same. If such duplication, or aliasing, of data is\nallowed, the cache must be checked when the page map is changed, which is an\ninfrequent event. Note that a very simple use of page coloring (see Appendix B,\nSection B.3) can eliminate the possibility of these aliases. If even-address virtual\npages are mapped to even-address physical pages (and the same for odd pages),\nthen these aliases can never occur because the low-order bit in the virtual and phys-\nical page number will be identical.\nThe instruction TLB is accessed to find a match between the address and a valid\npage table entry (PTE) (steps 3 and 4). In addition to translating the address, the\nTLB checks to see if the PTE demands that this access result in an exception\nbecause of an access violation.\nAn instruction TLB miss first goes to the L2 TLB, which contains 1536 PTEs\nof 4 KiB page sizes and is 12-way set associative. It takes 8 clock cycles to\nload the L1 TLB from the L2 TLB, which leads to the 9-cycle miss penalty\nincluding the initial clock cycle to access the L1 TLB. If the L2 TLB misses,\na hardware algorithm is used to walk the page table and update the TLB entry.\nSections L.5 and L.6 of online Appendix L describe page table walkers and page\nstructure caches. In the worst case, the page is not in memory, and the operating\nsystem gets the page from secondary storage. Because millions of instructions\ncould execute during a page fault, the operating system will swap in another pro-\ncess if one is waiting to run. Otherwise, if there is no TLB exception, the instruc-\ntion cache access continues.\nThe index field of the address is sent to all eight banks of the instruction cache\n(step 5). The instruction cache tag is 36 bits\u00036 bits (index)\u00036 bits (block offset),\nor 24 bits. The four tags and valid bits are compared to the physical page frame\nfrom the instruction TLB (step 6). Because the i7 expects 16 bytes each instruction\nfetch, an additional 2 bits are used from the 6-bit block offset to select the appro-\npriate 16 bytes. Therefore 6+2 or 8 bits are used to send 16 bytes of instructions to\nthe processor. The L1 cache is pipelined, and the latency of a hit is 4 clock cycles\n(step 7). A miss goes to the second-level cache.\nAs mentioned earlier, the instruction cache is virtually addressed and physi-\ncally tagged. Because the second-level caches are physically addressed, the phys-\nical page address from the TLB is composed with the page offset to make an\naddress to access the L2 cache. The L2 index is\n2Index \u00bc\nCache size\nBlock sizeSet associativity \u00bc 256K\n644 \u00bc 1024 \u00bc 210\n136\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 169,
        "text": "so the 30-bit block address (36-bit physical address\u00036-bit block offset) is divided\ninto a 20-bit tag and a 10-bit index (step 8). Once again, the index and tag are sent\nto the four banks of the unified L2 cache (step 9), which are compared in parallel. If\none matches and is valid (step 10), it returns the block in sequential order after the\ninitial 12-cycle latency at a rate of 8 bytes per clock cycle.\nIf the L2 cache misses, the L3 cache is accessed. For a four-core i7, which has\nan 8 MiB L3, the index size is\n2Index \u00bc\nCache size\nBlock sizeSet associativity \u00bc\n8M\n6416 \u00bc 8192 \u00bc 213\nThe 13-bit index (step 11) is sent to all 16 banks of the L3 (step 12). The L3 tag,\nwhich is 36\u0003(13+6)\u00bc17 bits, is compared against the physical address from the\nTLB (step 13). If a hit occurs, the block is returned after an initial latency of 42\nclock cycles, at a rate of 16 bytes per clock and placed into both L1 and L3.\nIf L3 misses, a memory access is initiated.\nIf the instruction is not found in the L3 cache, the on-chip memory controller\nmust get the block from main memory. The i7 has three 64-bit memory channels\nthat can act as one 192-bit channel, because there is only one memory controller\nand the same address is sent on both channels (step 14). Wide transfers happen\nwhen both channels have identical DIMMs. Each channel supports up to four\nDDR DIMMs (step 15). When the data return they are placed into L3 and L1 (step\n16) because L3 is inclusive.\nThe total latency of the instruction miss that is serviced by main memory is\napproximately 42 processor cycles to determine that an L3 miss has occurred, plus\nthe DRAM latency for the critical instructions. For a single-bank DDR4-2400\nSDRAM and 4.0 GHz CPU, the DRAM latency is about 40 ns or 160 clock cycles\nto the first 16 bytes, leading to a total miss penalty of about 200 clock cycles. The\nmemory controller fills the remainder of the 64-byte cache block at a rate of 16\nbytes per I/O bus clock cycle, which takes another 5 ns or 20 clock cycles.\nBecause the second-level cache is a write-back cache, any miss can lead to an\nold block being written back to memory. The i7 has a 10-entry merging write\nbuffer that writes back dirty cache lines when the next level in the cache is unused\nfor a read. The write buffer is checked on a miss to see if the cache line exists in the\nbuffer; if so, the miss is filled from the buffer. A similar buffer is used between\nthe L1 and L2 caches. If this initial instruction is a load, the data address is sent\nto the data cache and data TLBs, acting very much like an instruction cache access.\nSuppose the instruction is a store instead of a load. When the store issues, it\ndoes a data cache lookup just like a load. A miss causes the block to be placed\nin a write buffer because the L1 cache does not allocate the block on a write miss.\nOn a hit, the store does not update the L1 (or L2) cache until later, after it is known\nto be nonspeculative. During this time, the store resides in a load-store queue, part\nof the out-of-order control mechanism of the processor.\nThe I7 also supports prefetching for L1 and L2 from the next level in the\nhierarchy. In most cases, the prefetched line is simply the next block in the cache.\nBy prefetching only for L1 and L2, high-cost unnecessary fetches to memory are\navoided.\n2.6\nPutting It All Together: Memory Hierarchies in the ARM Cortex-A53 and Intel Core i7 6700\n\u25a0\n137"
    },
    {
        "page": 170,
        "text": "Performance of the i7 memory system\nWe evaluate the performance of the i7 cache structure using the SPECint2006\nbenchmarks. The data in this section were collected by Professor Lu Peng and\nPhD student Qun Liu, both of Louisiana State University. Their analysis is based\non earlier work (see Prakash and Peng, 2008).\nThe complexity of the i7 pipeline, with its use of an autonomous instruction\nfetch unit, speculation, and both instruction and data prefetch, makes it hard to\ncompare cache performance against simpler processors. As mentioned on page\n110, processors that use prefetch can generate cache accesses independent of\nthe memory accesses performed by the program. A cache access that is generated\nbecause of an actual instruction access or data access is sometimes called a\ndemand access to distinguish it from a prefetch access. Demand accesses can\ncome from both speculative instruction fetches and speculative data accesses,\nsome of which are subsequently canceled (see Chapter 3 for a detailed description\nof speculation and instruction graduation). A speculative processor generates at\nleast as many misses as an in-order nonspeculative processor, and typically more.\nIn addition to demand misses, there are prefetch misses for both instructions\nand data.\nThe i7\u2019s instruction fetch unit attempts to fetch 16 bytes every cycle, which com-\nplicates comparing instruction cache miss rates because multiple instructions are\nfetched every cycle (roughly 4.5 on average). In fact, the entire 64-byte cache line\nisreadandsubsequent16-bytefetchesdonotrequireadditionalaccesses.Thusmisses\nare tracked only on the basis of 64-byte blocks. The 32 KiB, eight-way set associative\ninstruction cache leads to a very low instruction miss rate for the SPECint2006\nprograms. If, for simplicity, we measure the miss rate of SPECint2006 as the number\nof misses for a 64-byte block divided by the number of instructions that complete, the\nmiss rates are all under 1% except for one benchmark (XALANCBMK), which has a\n2.9% miss rate. Because a 64-byte block typically contains 16\u201320 instructions, the\neffective miss rate per instruction is much lower, depending on the degree of spatial\nlocality in the instruction stream.\nThe frequency at which the instruction fetch unit is stalled waiting for the\nI-cache misses is similarly small (as a percentage of total cycles) increasing to\n2% for two benchmarks and 12% for XALANCBMK, which has the highest\nI-cache miss rate. In the next chapter, we will see how stalls in the IFU contribute\nto overall reductions in pipeline throughput in the i7.\nThe L1 data cache is more interesting and even trickier to evaluate because in\naddition to the effects of prefetching and speculation, the L1 data cache is not\nwrite-allocated, and writes to cache blocks that are not present are not treated as\nmisses. For this reason, we focus only on memory reads. The performance monitor\nmeasurements in the i7 separate out prefetch accesses from demand accesses, but\nonly keep demand accesses for those instructions that graduate. The effect of spec-\nulative instructions that do not graduate is not negligible, although pipeline effects\nprobably dominate secondary cache effects caused by speculation; we will return\nto the issue in the next chapter.\n138\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 171,
        "text": "To address these issues, while keeping the amount of data reasonable,\nFigure 2.26 shows the L1 data cache misses in two ways:\n1. The L1 miss rate relative to demand references given by the L1 miss rate includ-\ning prefetches and speculative loads/L1 demand read references for those\ninstructions that graduate.\nASTAR\nBZIP2\nGCC\nHMMER\nLIBQUANTUM\nMCF\nOMNETPP\nPERLBENCH\nSJENG\nXALANCBMK\nH264REF\nGOBMK\n0%\nL1 miss rate prefetches and demand reads\nL1 miss rate demand reads only\n5%\n10%\n15%\nMiss rate\n20%\n25%\n30%\n35%\n40%\n45%\n11%\n3%\n5%\n2%\n18%\n4% 3%\n1%\n5%\n1%\n5%\n1%\n35%\n11%\n41%\n22%\n15%\n7%\n2%1% 1%1%\n6%\n3%\nFigure 2.26 The L1 data cache miss rate for the SPECint2006 benchmarks is shown in two ways relative to the\ndemand L1 reads: one including both demand and prefetch accesses and one including only demand accesses.\nThe i7 separates out L1 misses for a block not present in the cache and L1 misses for a block already outstanding that\nis being prefetched from L2; we treat the latter group as hits because they would hit in a blocking cache. These data,\nlike the rest in this section, were collected by Professor Lu Peng and PhD student Qun Liu, both of Louisiana State\nUniversity, based on earlier studies of the Intel Core Duo and other processors (see Peng et al., 2008).\n2.6\nPutting It All Together: Memory Hierarchies in the ARM Cortex-A53 and Intel Core i7 6700\n\u25a0\n139"
    },
    {
        "page": 172,
        "text": "2. The demand miss rate given by L1 demand misses/L1 demand read references,\nboth measurements only for instructions that graduate.\nOn average, the miss rate including prefetches is 2.8 times as high as the demand-\nonly miss rate. Comparing this data to that from the earlier i7 920, which had the\nsame size L1, we see that the miss rate including prefetches is higher on the newer\ni7, but the number of demand misses, which are more likely to cause a stall, are\nusually fewer.\nTo understand the effectiveness of the aggressive prefetch mechanisms in the\ni7, let\u2019s look at some measurements of prefetching. Figure 2.27 shows both the\nfraction of L2 requests that are prefetches versus demand requests and the prefetch\nmiss rate. The data are probably astonishing at first glance: there are roughly\n1.5 times as many prefetches as there are L2 demand requests, which come directly\nfrom L1 misses. Furthermore, the prefetch miss rate is amazingly high, with an\naverage miss rate of 58%. Although the prefetch ratio varies considerably, the pre-\nfetch miss rate is always significant. At first glance, you might conclude that the\ndesigners made a mistake: they are prefetching too much, and the miss rate is too\nhigh. Notice, however, that the benchmarks with the higher prefetch ratios\n(ASTAR, BZIP2, HMMER, LIBQUANTUM, and OMNETPP) also show the\ngreatest gap between the prefetch miss rate and the demand miss rate, more than\na factor of 2 in each case. The aggressive prefetching is trading prefetch misses,\nwhich occur earlier, for demand misses, which occur later; and as a result, a pipe-\nline stall is less likely to occur due to the prefetching.\nSimilarly, consider the high prefetch miss rate. Suppose that the majority of the\nprefetches are actually useful (this is hard to measure because it involves tracking\nindividual cache blocks), then a prefetch miss indicates a likely L2 cache miss in\nthe future. Uncovering and handling the miss earlier via the prefetch is likely to\nreduce the stall cycles. Performance analysis of speculative superscalars, like\nthe i7, has shown that cache misses tend to be the primary cause of pipeline stalls,\nbecause it is hard to keep the processor going, especially for longer running L2 and\nL3 misses. The Intel designers could not easily increase the size of the caches with-\nout incurring both energy and cycle time impacts; thus the use of aggressive pre-\nfetching to try to lower effective cache miss penalties is an interesting alternative\napproach.\nWith the combination of the L1 demand misses and prefetches going to L2,\nroughly 17% of the loads generate an L2 request. Analyzing L2 performance\nrequires including the effects of writes (because L2 is write-allocated), as well\nas the prefetch hit rate and the demand hit rate. Figure 2.28 shows the miss rates\nof the L2 caches for demand and prefetch accesses, both versus the number of L1\nreferences (reads and writes). As with L1, prefetches are a significant contributor,\ngenerating 75% of the L2 misses. Comparing the L2 demand miss rate with that of\nearlier i7 implementations (again with the same L2 size) shows that the i7 6700 has\na lower L2 demand miss rate by an approximate factor of 2, which may well justify\nthe higher prefetch miss rate.\n140\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 173,
        "text": "Because the cost for a miss to memory is over 100 cycles and the average data\nmiss rate in L2 combining both prefetch and demand misses is over 7%, L3 is obvi-\nously critical. Without L3 and assuming that about one-third of the instructions are\nloads or stores, L2 cache misses could add over two cycles per instruction to the\nCPI! Obviously, prefetching past L2 would make no sense without an L3.\nIn comparison, the average L3 data miss rate of 0.5% is still significant but less\nthan one-third of the L2 demand miss rate and 10 times less than the L1 demand\nmiss rate. Only in two benchmarks (OMNETPP and MCF) is the L3 miss rate\nASTAR\nBZIP2\nGCC\nHMMER\nLIBQUANTUM\nMCF\nOMNETPP\nPERLBENCH\nSJENG\nXALANCBMK\nH264REF\nGOBMK\n0\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n0.5\n1.0\n1.5\nPrefetches to LA/All L2 demand references\nPrefetch miss rate\n2.0\n2.5\n3.0\n3.5\n4.0\n5.0\n4.5\nPrefetches/demand accesses\nPrefetches miss ratio\nFigure 2.27 The fraction of L2 requests that are prefetches is shown via the columns and the left axis. The right\naxis and the line shows the prefetch hit rate. These data, like the rest in this section, were collected by Professor Lu\nPeng and PhD student Qun Liu, both of Louisiana State University, based on earlier studies of the Intel Core Duo and\nother processors (see Peng et al., 2008).\n2.6\nPutting It All Together: Memory Hierarchies in the ARM Cortex-A53 and Intel Core i7 6700\n\u25a0\n141"
    },
    {
        "page": 174,
        "text": "above 0.5%; in those two cases, the miss rate of about 2.3% likely dominates all\nother performance losses. In the next chapter, we will examine the relationship\nbetween the i7 CPI and cache misses, as well as other pipeline effects.\n2.7\nFallacies and Pitfalls\nAs the most naturally quantitative of the computer architecture disciplines, mem-\nory hierarchy would seem to be less vulnerable to fallacies and pitfalls. Yet we\nwere limited here not by lack of warnings, but by lack of space!\nFallacy\nPredicting cache performance of one program from another.\nFigure 2.29 shows the instruction miss rates and data miss rates for three programs\nfrom the SPEC2000 benchmark suite as cache size varies. Depending on the\nL2 miss rate\nastar\nbzip2\ngcc\nhmmer\nlibquantum\nmcf\nomnetpp\nperlbench\nsjeng\nxalancbmk\nh264ref\ngobmk\n0%\n2%\n4%\n6%\n8%\n10%\n12%\n14%\n16%\n18%\n20%\n22%\n2%\n7%\n1%\n3%\n1%\n10%\n0%\n1%\n0%\n1%\n0%\n3%\n0%\n4%\n12%\n22%\n4%\n11%\n0%\n1%\n0%0%\n1%\n3%\nL2 demand miss rate\nL2 prefetch miss rate\nFigure 2.28 The L2 demand miss rate and prefetch miss rate, both shown relative to\nall the references to L1, which also includes prefetches, speculative loads that do not\ncomplete, and program-generated loads and stores (demand references). These data,\nlike the rest in this section, were collected by Professor Lu Peng and PhD student Qun\nLiu, both of Louisiana State University.\n142\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 175,
        "text": "program, the data misses per thousand instructions for a 4096 KiB cache are 9, 2, or\n90, and the instruction misses per thousand instructions for a 4 KiB cache are 55,\n19, or 0.0004. Commercial programs such as databases will have significant miss\nrates even in large second-level caches, which is generally not the case for the\nSPECCPU programs. Clearly, generalizing cache performance from one program\nto another is unwise. As Figure 2.24 reminds us, there is a great deal of variation,\nand even predictions about the relative miss rates of integer and floating-point-\nintensive programs can be wrong, as mcf and sphnix3 remind us!\nPitfall\nSimulating\nenough\ninstructions\nto\nget\naccurate\nperformance\nmeasures\nof the memory hierarchy.\nThere are really three pitfalls here. One is trying to predict performance of a large\ncache using a small trace. Another is that a program\u2019s locality behavior is not con-\nstant over the run of the entire program. The third is that a program\u2019s locality\nbehavior may vary depending on the input.\nFigure 2.30 shows the cumulative average instruction misses per thousand\ninstructions for five inputs to a single SPEC2000 program. For these inputs, the\naverage memory rate for the first 1.9 billion instructions is very different from\nthe average miss rate for the rest of the execution.\nPitfall\nNot delivering high memory bandwidth in a cache-based system.\nCaches help with average cache memory latency but may not deliver high memory\nbandwidth to an application that must go to main memory. The architect must\ndesign a high bandwidth memory behind the cache for such applications. We will\nrevisit this pitfall in Chapters 4 and 5.\n0\n20\n40\n60\n80\n100\n120\n140\n160\nMisses per 1000 instructions\n4\n16\n64\n256\n1024\n4096\nCache size (KB)\nD: lucas\nD: gcc\nD: gap\nI: gap\nI: gcc\nI: lucas\nFigure 2.29 Instruction and data misses per 1000 instructions as cache size varies\nfrom 4 KiB to 4096 KiB. Instruction misses for gcc are 30,000\u201340,000 times larger than\nfor lucas, and, conversely, data misses for lucas are 2\u201360 times larger than for gcc. The\nprograms gap, gcc, and lucas are from the SPEC2000 benchmark suite.\n2.7\nFallacies and Pitfalls\n\u25a0\n143"
    },
    {
        "page": 176,
        "text": "0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nInstruction misses per 1000 references\nInstruction misses per 1000 references\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\n1\n1.1 1.2 1.3 1.4\n1\n2, 3, 4, 5\n1.5 1.6 1.7 1.8 1.9\nInstructions (billions)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\n2\n4\n6\n8\n10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42\nInstructions (billions)\n5\n2\n3\n4\n1\nFigure 2.30 Instruction misses per 1000 references for five inputs to the perl bench-\nmark in SPEC2000. There is little variation in misses and little difference between the\nfive inputs for the first 1.9 billion instructions. Running to completion shows how misses\nvary over the life of the program and how they depend on the input. The top graph\nshows the running average misses for the first 1.9 billion instructions, which starts at\nabout 2.5 and ends at about 4.7 misses per 1000 references for all five inputs. The bot-\ntom graph shows the running average misses to run to completion, which takes 16\u201341\nbillion instructions depending on the input. After the first 1.9 billion instructions, the\nmisses per 1000 references vary from 2.4 to 7.9 depending on the input. The simulations\nwere for the Alpha processor using separate L1 caches for instructions and data, each\nbeing two-way 64 KiB with LRU, and a unified 1 MiB direct-mapped L2 cache.\n144\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 177,
        "text": "Pitfall\nImplementing a virtual machine monitor on an instruction set architecture that\nwasn\u2019t designed to be virtualizable.\nMany architects in the 1970s and 1980s weren\u2019t careful to make sure that all\ninstructions reading or writing information related to hardware resource informa-\ntion were privileged. This laissez faire attitude causes problems for VMMs for all\nof these architectures, including the 80x86, which we use here as an example.\nFigure 2.31 describes the 18 instructions that cause problems for paravirtuali-\nzation (Robin and Irvine, 2000). The two broad classes are instructions that\n\u25a0\nread control registers in user mode that reveal that the guest operating system is\nrunning in a virtual machine (such as POPF mentioned earlier) and\n\u25a0\ncheck protection as required by the segmented architecture but assume that the\noperating system is running at the highest privilege level.\nVirtual memory is also challenging. Because the 80x86 TLBs do not support\nprocess ID tags, as do most RISC architectures, it is more expensive for the VMM\nand guest OSes to share the TLB; each address space change typically requires a\nTLB flush.\nProblem category\nProblem 80x86 instructions\nAccess sensitive registers without\ntrapping when running in user mode\nStore global descriptor table register (SGDT)\nStore local descriptor table register (SLDT)\nStore interrupt descriptor table register (SIDT)\nStore machine status word (SMSW)\nPush flags (PUSHF, PUSHFD)\nPop flags (POPF, POPFD)\nWhen accessing virtual memory\nmechanisms in user mode,\ninstructions fail the\n80x86 protection checks\nLoad access rights from segment descriptor (LAR)\nLoad segment limit from segment descriptor (LSL)\nVerify if segment descriptor is readable (VERR)\nVerify if segment descriptor is writable (VERW)\nPop to segment register (POP CS, POP SS, \u2026)\nPush segment register (PUSH CS, PUSH SS, \u2026)\nFar call to different privilege level (CALL)\nFar return to different privilege level (RET)\nFar jump to different privilege level (JMP)\nSoftware interrupt (INT)\nStore segment selector register (STR)\nMove to/from segment registers (MOVE)\nFigure 2.31 Summary of 18 80x86 instructions that cause problems for virtualization\n(Robin and Irvine, 2000). The first five instructions of the top group allow a program in\nuser mode to read a control register, such as a descriptor table register without causing\na trap. The pop flags instruction modifies a control register with sensitive information\nbut fails silently when in user mode. The protection checking of the segmented archi-\ntecture of the 80x86 is the downfall of the bottom group because each of these instruc-\ntions checks the privilege level implicitly as part of instruction execution when reading a\ncontrol register. The checking assumes that the OS must be at the highest privilege\nlevel, which is not the case for guest VMs. Only the MOVE to segment register tries\nto modify control state, and protection checking foils it as well.\n2.7\nFallacies and Pitfalls\n\u25a0\n145"
    },
    {
        "page": 178,
        "text": "Virtualizing I/O is also a challenge for the 80x86, in part because it supports\nmemory-mapped I/O and has separate I/O instructions, but more importantly\nbecause there are a very large number and variety of types of devices and device\ndrivers of PCs for the VMM to handle. Third-party vendors supply their own\ndrivers, and they may not properly virtualize. One solution for conventional\nVM implementations is to load real device drivers directly into the VMM.\nTo simplify implementations of VMMs on the 80x86, both AMD and Intel\nhave proposed extensions to the architecture. Intel\u2019s VT-x provides a new execu-\ntion mode for running VMs, a architected definition of the VM state, instructions to\nswap VMs rapidly, and a large set of parameters to select the circumstances where\na VMM must be invoked. Altogether, VT-x adds 11 new instructions for the\n80x86. AMD\u2019s Secure Virtual Machine (SVM) provides similar functionality.\nAfter turning on the mode that enables VT-x support (via the new VMXON instruc-\ntion), VT-x offers four privilege levels for the guest OS that are lower in priority than\nthe original four (and fix issues like the problem with the POPF instruction mentioned\nearlier).VT-xcapturesallthestatesofavirtualmachineintheVirtualMachineControl\nState (VMCS) and then provides atomic instructions to save and restore a VMCS.\nIn addition to critical state, the VMCS includes configuration information to deter-\nmine when to invoke the VMM and then specifically what caused the VMM to be\ninvoked. To reduce the number of times the VMM must be invoked, this mode adds\nshadowversionsofsomesensitiveregistersandaddsmasksthatchecktoseewhether\ncritical bits of a sensitive register will be changed before trapping. To reduce the cost\nof virtualizing virtual memory, AMD\u2019s SVM adds an additional level of indirection,\ncallednestedpagetables,whichmakesshadowpagetablesunnecessary(seeSection\nL.7 of Appendix L).\n2.8\nConcluding Remarks: Looking Ahead\nOver the past thirty years there have been several predictions of the eminent [sic]\ncessation of the rate of improvement in computer performance. Every such pre-\ndiction was wrong. They were wrong because they hinged on unstated assump-\ntions that were overturned by subsequent events. So, for example, the failure to\nforesee the move from discrete components tointegrated circuits led toa predic-\ntion thatthe speedoflightwouldlimit computer speedstoseveralordersof mag-\nnitude slower than they are now. Our prediction of the memory wall is probably\nwrong too but it suggests that we have to start thinking \u201cout of the box.\u201d\nWm. A. Wulf and Sally A. McKee,\nHitting the Memory Wall: Implications of the Obvious,\nDepartment of Computer Science, University of Virginia (December 1994).\nThis paper introduced the term memory wall.\nThe possibility of using a memory hierarchy dates back to the earliest days of\ngeneral-purpose digital computers in the late 1940s and early 1950s. Virtual mem-\nory was introduced in research computers in the early 1960s and into IBM main-\nframes in the 1970s. Caches appeared around the same time. The basic concepts\n146\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 179,
        "text": "have been expanded and enhanced over time to help close the access time gap\nbetween main memory and processors, but the basic concepts remain.\nOne trend that is causing a significant change in the design of memory hierar-\nchies is a continued slowdown in both density and access time of DRAMs. In the\npast 15 years, both these trends have been observed and have been even more obvi-\nous over the past 5 years. While some increases in DRAM bandwidth have been\nachieved, decreases in access time have come much more slowly and almost van-\nished between DDR4 and DDR3. The end of Dennard scaling as well as a slow-\ndown in Moore\u2019s Law both contributed to this situation. The trenched capacitor\ndesign used in DRAMs is also limiting its ability to scale. It may well be the case\nthat packaging technologies such as stacked memory will be the dominant source\nof improvements in DRAM access bandwidth and latency.\nIndependently of improvements in DRAM, Flash memory has been playing a\nmuch larger role. In PMDs, Flash has dominated for 15 years and became the stan-\ndard for laptops almost 10 years ago. In the past few years, many desktops have\nshipped with Flash as the primary secondary storage. Flash\u2019s potential advantage\nover DRAMs, specifically the absence of a per-bit transistor to control writing, is\nalso its Achilles heel. Flash must use bulk erase-rewrite cycles that are consider-\nably slower. As a result, although Flash has become the fastest growing form of\nsecondary storage, SDRAMs still dominate for main memory.\nAlthough phase-change materials as a basis for memory have been around for a\nwhile,theyhaveneverbeenseriouscompetitorseitherformagneticdisksorforFlash.\nThe recent announcement by Intel and Micron of the cross-point technology may\nchangethis.ThetechnologyappearstohaveseveraladvantagesoverFlash,including\nthe elimination of the slow erase-to-write cycle and greater longevity in terms. It\ncould be that this technology will finally be the technology that replaces the electro-\nmechanical disks that have dominated bulk storage for more than 50 years!\nFor some years, a variety of predictions have been made about the coming\nmemory wall (see previously cited quote and paper), which would lead to serious\nlimits on processor performance. Fortunately, the extension of caches to multiple\nlevels (from 2 to 4), more sophisticated refill and prefetch schemes, greater com-\npiler and programmer awareness of the importance of locality, and tremendous\nimprovements in DRAM bandwidth (a factor of over 150 times since the mid-\n1990s) have helped keep the memory wall at bay. In recent years, the combination\nof access time constraints on the size of L1 (which is limited by the clock cycle) and\nenergy-related limitations on the size of L2 and L3 have raised new challenges. The\nevolution of the i7 processor class over 6\u20137 years illustrates this: the caches are the\nsame size in the i7 6700 as they were in the first generation i7 processors! The more\naggressive use of prefetching is an attempt to overcome the inability to increase L2\nand L3. Off-chip L4 caches are likely to become more important because they are\nless energy-constrained than on-chip caches.\nIn addition to schemes relying on multilevel caches, the introduction of out-of-\norder pipelines with multiple outstanding misses has allowed available instruction-\nlevel parallelism to hide the memory latency remaining in a cache-based system.\nThe introduction of multithreading and more thread-level parallelism takes this a\nstep further by providing more parallelism and thus more latency-hiding\n2.8\nConcluding Remarks: Looking Ahead\n\u25a0\n147"
    },
    {
        "page": 180,
        "text": "opportunities. It is likely that the use of instruction- and thread-level parallelism\nwill be a more important tool in hiding whatever memory delays are encountered\nin modern multilevel cache systems.\nOne idea that periodically arises is the use of programmer-controlled scratch-\npad or other high-speed visible memories, which we will see are used in GPUs.\nSuch ideas have never made the mainstream in general-purpose processors for sev-\neral reasons: First, they break the memory model by introducing address spaces\nwith different behavior. Second, unlike compiler-based or programmer-based\ncache optimizations (such as prefetching), memory transformations with scratch-\npads must completely handle the remapping from main memory address space to\nthe scratchpad address space. This makes such transformations more difficult and\nlimited in applicability. In GPUs (see Chapter 4), where local scratchpad memories\nare heavily used, the burden for managing them currently falls on the programmer.\nFor domain-specific software systems that can use such memories, the perfor-\nmance gains are very significant. It is likely that HBM technologies will thus be\nused for caching in large, general-purpose computers and quite possibility as\nthe main working memories in graphics and similar systems. As domain-specific\narchitectures become more important in overcoming the limitations arising from\nthe end of Dennard\u2019s Law and the slowdown in Moore\u2019s Law (see Chapter 7),\nscratchpad memories and vector-like register sets are likely to see more use.\nThe implications of the end of Dennard\u2019s Law affect both DRAM and proces-\nsor technology. Thus, rather than a widening gulf between processors and main\nmemory, we are likely to see a slowdown in both technologies, leading to slower\noverall growth rates in performance. New innovations in computer architecture and\nin related software that together increase performance and efficiency will be key to\ncontinuing the performance improvements seen over the past 50 years.\n2.9\nHistorical Perspectives and References\nIn Section M.3 (available online) we examine the history of caches, virtual mem-\nory, and virtual machines. IBM plays a prominent role in the history of all three.\nReferences for further reading are included.\nCase Studies and Exercises by Norman P. Jouppi, Rajeev\nBalasubramonian, Naveen Muralimanohar, and Sheng Li\nCase Study 1: Optimizing Cache Performance via\nAdvanced Techniques\nConcepts illustrated by this case study\n\u25a0\nNonblocking Caches\n\u25a0\nCompiler Optimizations for Caches\n\u25a0\nSoftware and Hardware Prefetching\n\u25a0\nCalculating Impact of Cache Performance on More Complex Processors\n148\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 181,
        "text": "The transpose of a matrix interchanges its rows and columns; this concept is\nillustrated here:\nA11\nA11\nA21\nA31\nA41\nA12\nA22\nA32\nA42\nA13\nA23\nA33\nA43\nA14\nA24\nA34\nA44\nA12\nA22\nA23\nA24\nA13\nA14\nA21\nA31\nA32\nA33\nA34\nA41\nA42\nA43\nA44\n\u21d2\nHere is a simple C loop to show the transpose:\nfor (i = 0; i < 3; i++) {\nfor (j = 0; j < 3; j++) {\noutput[j][i] = input[i][j];\n}\n}\nAssume that both the input and output matrices are stored in the row major order\n(row major order means that the row index changes fastest). Assume that you are\nexecuting a 256\u0004256 double-precision transpose on a processor with a 16 KB fully\nassociative (don\u2019t worry about cache conflicts) least recently used (LRU) replace-\nment L1 data cache with 64-byte blocks. Assume that the L1 cache misses or pre-\nfetches require 16 cycles and always hit in the L2 cache, and that the L2 cache can\nprocess a request every 2 processor cycles. Assume that each iteration of the pre-\nceding inner loop requires 4 cycles if the data are present in the L1 cache. Assume\nthat the cache has a write-allocate fetch-on-write policy for write misses. Unreal-\nistically, assume that writing back dirty cache blocks requires 0 cycles.\n2.1\n[10/15/15/12/20] <2.3> For the preceding simple implementation, this execution\norder would be nonideal for the input matrix; however, applying a loop interchange\noptimization would create a nonideal order for the output matrix. Because loop\ninterchange is not sufficient to improve its performance, it must be blocked instead.\na. [10] <2.3> What should be the minimum size of the cache to take advantage of\nblocked execution?\nb. [15] <2.3> How do the relative number of misses in the blocked and\nunblocked versions compare in the preceding minimum-sized cache?\nc. [15] <2.3> Write code to perform a transpose with a block size parameter B\nthat uses B\u0004B blocks.\nd. [12] <2.3> What is the minimum associativity required of the L1 cache for\nconsistent performance independent of both arrays\u2019 position in memory?\ne. [20] <2.3> Try out blocked and nonblocked 256\u0004256 matrix transpositions on\na computer. How closely do the results match your expectations based on what\nyou know about the computer\u2019s memory system? Explain any discrepancies if\npossible.\nCase Studies and Exercises\n\u25a0\n149"
    },
    {
        "page": 182,
        "text": "2.2\n[10] <2.3> Assume you are designing a hardware prefetcher for the preceding\nunblocked matrix transposition code. The simplest type of hardware prefetcher only\nprefetches sequential cache blocks after a miss. More complicated \u201cnonunit stride\u201d\nhardware prefetchers can analyze a miss reference stream and detect and prefetch\nnonunit strides.Incontrast, softwareprefetching can determine nonunit strides aseas-\nily asitcan determine unit strides.Assumeprefetcheswritedirectlyintothecacheand\nthat there is no \u201cpollution\u201d (overwriting data that must be used before the data that are\nprefetched).For bestperformance given a nonunit strideprefetcher, inthe steadystate\nof the inner loop, how many prefetches must be outstanding at a given time?\n2.3\n[15/20] <2.3> With software prefetching, it is important to be careful to have the\nprefetches occur in time for use but also to minimize the number of outstanding\nprefetches to live within the capabilities of the microarchitecture and minimize\ncache pollution. This is complicated by the fact that different processors have dif-\nferent capabilities and limitations.\na. [15] <2.3> Create a blocked version of the matrix transpose with software\nprefetching.\nb. [20] <2.3> Estimate and compare the performance of the blocked and\nunblocked transpose codes both with and without software prefetching.\nCase Study 2: Putting It All Together: Highly Parallel\nMemory Systems\nConcept illustrated by this case study\n\u25a0\nCross-Cutting Issues: The Design of Memory Hierarchies\nThe program in Figure 2.32 can be used to evaluate the behavior of a memory sys-\ntem. The key is having accurate timing and then having the program stride through\nmemory to invoke different levels of the hierarchy. Figure 2.32 shows the code in\nC. The first part is a procedure that uses a standard utility to get an accurate measure\nof the user CPU time; this procedure may have to be changed to work on some\nsystems. The second part is a nested loop to read and write memory at different\nstrides and cache sizes. To get accurate cache timing, this code is repeated many\ntimes. The third part times the nested loop overhead only so that it can be\nsubtracted from overall measured times to see how long the accesses were. The\nresults are output in .csv file format to facilitate importing into spreadsheets.\nYou may need to change CACHE_MAX depending on the question you are answer-\ning and the size of memory on the system you are measuring. Running the program\nin single-user mode or at least without other active applications will give more con-\nsistent results. The code in Figure 2.32 was derived from a program written by\nAndrea Dusseau at the University of California-Berkeley and was based on a\ndetailed description found in Saavedra-Barrera (1992). It has been modified to\nfix a number of issues with more modern machines and to run under Microsoft\n150\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 183,
        "text": "#include \"stdafx.h\"\n#include <stdio.h>\n#include <time.h>\n#define ARRAY_MIN (1024) /* 1/4 smallest cache */\n#define ARRAY_MAX (4096*4096) /* 1/4 largest cache */\nint x[ARRAY_MAX]; /* array going to stride through */\ndouble get_seconds() { /* routine to read time in seconds */\n__time64_t ltime;\n_time64( &ltime );\nreturn (double) ltime;\n}\nint label(int i) {/* generate text labels */\nif (i<1e3) printf(\"%1dB,\",i);\nelse if (i<1e6) printf(\"%1dK,\",i/1024);\nelse if (i<1e9) printf(\"%1dM,\",i/1048576);\nelse printf(\"%1dG,\",i/1073741824);\nreturn 0;\n}\nint _tmain(int argc, _TCHAR* argv[]) {\nint register nextstep, i, index, stride;\nint csize;\ndouble steps, tsteps;\ndouble loadtime, lastsec, sec0, sec1, sec; /* timing variables */\n/* Initialize output */\nprintf(\" ,\");\nfor (stride=1; stride <= ARRAY_MAX/2; stride=stride*2)\nlabel(stride*sizeof(int));\nprintf(\"\\n\");\n/* Main loop for each configuration */\nfor (csize=ARRAY_MIN; csize <= ARRAY_MAX; csize=csize*2) {\nlabel(csize*sizeof(int)); /* print cache size this loop */\nfor (stride=1; stride <= csize/2; stride=stride*2) {\n/* Lay out path of memory references in array */\nfor (index=0; index < csize; index=index+stride)\nx[index] = index + stride; /* pointer to next */\nx[index-stride] = 0; /* loop back to beginning */\n/* Wait for timer to roll over */\nlastsec = get_seconds();\n sec0 = get_seconds(); while (sec0 == lastsec);\n/* Walk through path in array for twenty seconds */\n/* This gives 5% accuracy with second resolution */\nsteps = 0.0; /* number of steps taken */\nnextstep = 0; /* start at beginning of path */\nsec0 = get_seconds(); /* start timer */\n{ /* repeat until collect 20 seconds */\n(i=stride;i!=0;i=i-1) { /* keep samples same */\nnextstep = 0;\ndo nextstep = x[nextstep]; /* dependency */\nwhile (nextstep != 0);\n}\nsteps = steps + 1.0; /* count loop iterations */\nsec1 = get_seconds(); /* end timer */\n} while ((sec1 - sec0) < 20.0); /* collect 20 seconds */\nsec = sec1 - sec0;\n/* Repeat empty loop to loop subtract overhead */\ntsteps = 0.0; /* used to match no. while iterations */\nsec0 = get_seconds(); /* start timer */\n{ /* repeat until same no. iterations as above */\n(i=stride;i!=0;i=i-1) { /* keep samples same */\nindex = 0;\ndo index = index + stride;\nwhile (index < csize);\n}\ntsteps = tsteps + 1.0;\nsec1 = get_seconds(); /* - overhead */\n} while (tsteps<steps); /* until = no. iterations */\nsec = sec - (sec1 - sec0);\nloadtime = (sec*1e9)/(steps*csize);\n/* write out results in .csv format for Excel */\nprintf(\"%4.1f,\", (loadtime<0.1) ? 0.1 : loadtime);\n  }; /* end of inner for loop */\n  printf(\"\\n\");\n}; /* end of outer for loop */\nreturn 0;\n}\nFigure 2.32 C program for evaluating memory system.\nCase Studies and Exercises\n\u25a0\n151"
    },
    {
        "page": 184,
        "text": "Visual C++. It can be downloaded from http://www.hpl.hp.com/research/cacti/\naca_ch2_cs2.c.\nThe preceding program assumes that program addresses track physical\naddresses, which is true on the few machines that use virtually addressed caches,\nsuch as the Alpha 21264. In general, virtual addresses tend to follow physical\naddresses shortly after rebooting, so you may need to reboot the machine in order\nto get smooth lines in your results. To answer the following questions, assume that\nthe sizes of all components of the memory hierarchy are powers of 2. Assume that\nthe size of the page is much larger than the size of a block in a second-level cache (if\nthere is one) and that the size of a second-level cache block is greater than or equal\nto the size of a block in a first-level cache. An example of the output of the program\nis plotted in Figure 2.33; the key lists the size of the array that is exercised.\n2.4\n[12/12/12/10/12] <2.6> Using the sample program results in Figure 2.33:\na. [12] <2.6> What are the overall size and block size of the second-level cache?\nb. [12] <2.6> What is the miss penalty of the second-level cache?\nc. [12] <2.6> What is the associativity of the second-level cache?\nd. [10] <2.6> What is the size of the main memory?\ne. [12] <2.6> What is the paging time if the page size is 4 KB?\nRead (ns)\n1000\n100\n10\n1\n4B\n16B\n64B\n256B\n4K\n1K\n16K\n64K\n256K\n4M\n1M\n16M\n64M\n256M\nStride\n8K\n16K\n32K\n64K\n128K\n256K\n512K\n1M\n2M\n4M\n8M\n16M\n32M\n64M\n128M\n256M\n512M\nFigure 2.33 Sample results from program in Figure 2.32.\n152\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 185,
        "text": "2.5\n[12/15/15/20] <2.6> If necessary, modify the code in Figure 2.32 to measure the\nfollowing system characteristics. Plot the experimental results with elapsed time on\nthe y-axis and the memory stride on the x-axis. Use logarithmic scales for both\naxes, and draw a line for each cache size.\na. [12] <2.6> What is the system page size?\nb. [15] <2.6> How many entries are there in the TLB?\nc. [15] <2.6> What is the miss penalty for the TLB?\nd. [20] <2.6> What is the associativity of the TLB?\n2.6\n[20/20] <2.6> In multiprocessor memory systems, lower levels of the memory\nhierarchy may not be able to be saturated by a single processor but should be able\nto be saturated by multiple processors working together. Modify the code in\nFigure 2.32, and run multiple copies at the same time. Can you determine:\na. [20] <2.6> How many actual processors are in your computer system and how\nmany system processors are just additional multithreaded contexts?\nb. [20] <2.6> How many memory controllers does your system have?\n2.7\n[20] <2.6> Can you think of a way to test some of the characteristics of an instruc-\ntion cache using a program? Hint: The compiler may generate a large number of\nnonobvious instructions from a piece of code. Try to use simple arithmetic instruc-\ntions of known length in your instruction set architecture (ISA).\nCase Study 3: Studying the Impact of Various\nMemory System Organizations\nConcepts illustrated by this case study\n\u25a0\nDDR3 memory systems\n\u25a0\nImpact of ranks, banks, row buffers on performance and power\n\u25a0\nDRAM timing parameters\nA processor chip typically supports a few DDR3 or DDR4 memory channels. We\nwill focus on a single memory channel in this case study and explore how its per-\nformance and power are impacted by varying several parameters. Recall that the\nchannel is populated with one or more DIMMs. Each DIMM supports one or more\nranks\u2014a rank is a collection of DRAM chips that work in unison to service a single\ncommand issued by the memory controller. For example, a rank may be composed\nof 16 DRAM chips, where each chip deals with a 4-bit input or output on every\nchannel clock edge. Each such chip is referred to as a 4 (by four) chip. In other\nexamples, a rank may be composed of 88 chips or 416 chips\u2014note that in\neach case, a rank can handle data that are being placed on a 64-bit memory channel.\nA rank is itself partitioned into 8 (DDR3) or 16 (DDR4) banks. Each bank has a\nrow buffer that essentially remembers the last row read out of a bank. Here\u2019s an\nexample of a typical sequence of memory commands when performing a read from\na bank:\nCase Studies and Exercises\n\u25a0\n153"
    },
    {
        "page": 186,
        "text": "(i) The memory controller issues a Precharge command to get the bank ready to\naccess a new row. The precharge is completed after time tRP.\n(ii) The memory controller then issues an Activate command to read the appro-\npriate row out of the bank. The activation is completed after time tRCD and the\nrow is deemed to be part of the row buffer.\n(iii) The memory controller can then issue a column-read or CAS command that\nplaces a specific subset of the row buffer on the memory channel. After time\nCL, the first 64 bits of the data burst are placed on the memory channel.\nA burst typically includes eight 64-bit transfers on the memory channel, per-\nformed on the rising and falling edges of 4 memory clock cycles (referred to as\ntransfer time).\n(iv) If the memory controller wants to then access data in a different row of the bank,\nreferred to as a row buffer miss, it repeats steps (i)\u2013(iii). For now, we will\nassume that after CL has elapsed, the Precharge in step (i) can be issued; in some\ncases, an additional delay must be added, but we will ignore that delay here. If\nthe memory controller wants to access another block of data in the same row,\nreferred to as a row buffer hit, it simply issues another CAS command. Two\nback-to-back CAS commands have to be separated by at least 4 cycles so that\nthe first data transfer is complete before the second data transfer can begin.\nNote that a memory controller can issue commands to different banks in successive\ncycles so that it can perform many memory reads/writes in parallel and it is not\nsitting idle waiting for tRP, tRCD, and CL to elapse in a single bank. For the sub-\nsequent questions, assume that tRP\u00bctRCD\u00bcCL\u00bc13 ns, and that the memory\nchannel frequency is 1 GHz, that is, a transfer time of 4 ns.\n2.8\n[10] <2.2> What is the read latency experienced by a memory controller on a row\nbuffer miss?\n2.9\n[10] <2.2> What is the latency experienced by a memory controller on a row\nbuffer hit?\n2.10\n[10] <2.2> If the memory channel supports only one bank and the memory access\npattern is dominated by row buffer misses, what is the utilization of the memory\nchannel?\n2.11\n[15] <2.2> Assuming a 100% row buffer miss rate, what is the minimum number\nof banks that the memory channel should support in order to achieve a 100% mem-\nory channel utilization?\n2.12\n[10] <2.2> Assuming a 50% row buffer miss rate, what is the minimum number of\nbanks that the memory channel should support in order to achieve a 100% memory\nchannel utilization?\n2.13\n[15] <2.2> Assume that we are executing an application with four threads and the\nthreads exhibit zero spatial locality, that is, a 100% row buffer miss rate. Every\n200 ns, each of the four threads simultaneously inserts a read operation into the\n154\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 187,
        "text": "memory controller queue. What is the average memory latency experienced if the\nmemory channel supports only one bank? What if the memory channel supported\nfour banks?\n2.14\n[10] <2.2> From these questions, what have you learned about the benefits and\ndownsides of growing the number of banks?\n2.15\n[20] <2.2> Now let\u2019s turn our attention to memory power. Download a copy of the\nMicron power calculator from this link: https://www.micron.com/\u0005/media/\ndocuments/products/power-calculator/ddr3_power_calc.xlsm. This spreadsheet\nis preconfigured to estimate the power dissipation in a single 2 Gb 8 DDR3\nSDRAM memory chip manufactured by Micron. Click on the \u201cSummary\u201d tab\nto see the power breakdown in a single DRAM chip under default usage conditions\n(reads occupy the channel for 45% of all cycles, writes occupy the channel for 25%\nof all cycles, and the row buffer hit rate is 50%). This chip consumes 535 mW, and\nthe breakdown shows that about half of that power is expended in Activate oper-\nations, about 38% in CAS operations, and 12% in background power. Next, click\non the \u201cSystem Config\u201d tab. Modify the read/write traffic and the row buffer hit\nrate and observe how that changes the power profile. For example, what is the\ndecrease in power when channel utilization is 35% (25% reads and 10% writes),\nor when row buffer hit rate is increased to 80%?\n2.16\n[20] <2.2> In the default configuration, a rank consists of eight 8 2 Gb DRAM\nchips. A rank can also comprise164 chips or 416 chips. You can also vary the\ncapacity of each DRAM chip\u20141 Gb, 2 Gb, and 4 Gb. These selections can be\nmade in the \u201cDDR3 Config\u201d tab of the Micron power calculator. Tabulate the total\npower consumed for each rank organization. What is the most power-efficient\napproach to constructing a rank of a given capacity?\nExercises\n2.17\n[12/12/15] <2.3> The following questions investigate the impact of small and\nsimple caches using CACTI and assume a 65 nm (0.065 m) technology. (CACTI\nis available in an online form at http://quid.hpl.hp.com:9081/cacti/.)\na. [12] <2.3> Compare the access times of 64 KB caches with 64-byte blocks and\na single bank. What are the relative access times of two-way and four-way set\nassociative caches compared to a direct mapped organization?\nb. [12] <2.3> Compare the access times of four-way set associative caches with\n64-byte blocks and a single bank. What are the relative access times of 32 and\n64 KB caches compared to a 16 KB cache?\nc. [15] <2.3> For a 64 KB cache, find the cache associativity between 1 and\n8 with the lowest average memory access time given that misses per instruction\nfor a certain workload suite is 0.00664 for direct-mapped, 0.00366 for two-way\nset associative, 0.000987 for four-way set associative, and 0.000266 for eight-\nway set associative cache. Overall, there are 0.3 data references per instruction.\nAssume cache misses take 10 ns in all models. To calculate the hit time in\nCase Studies and Exercises\n\u25a0\n155"
    },
    {
        "page": 188,
        "text": "cycles, assume the cycle time output using CACTI, which corresponds to the\nmaximum frequency a cache can operate without any bubbles in the pipeline.\n2.18\n[12/15/15/10] <2.3> You are investigating the possible benefits of a way-\npredicting L1 cache. Assume that a 64 KB four-way set associative single-banked\nL1 data cache is the cycle time limiter in a system. For an alternative cache orga-\nnization, you are considering a way-predicted cache modeled as a 64 KB direct-\nmapped cache with 80% prediction accuracy. Unless stated otherwise, assume that\na mispredicted way access that hits in the cache takes one more cycle. Assume the\nmiss rates and the miss penalties in question 2.8 part (c).\na. [12] <2.3> What is the average memory access time of the current cache (in\ncycles) versus the way-predicted cache?\nb. [15] <2.3> If all other components could operate with the faster way-predicted\ncache cycle time (including the main memory), what would be the impact on\nperformance from using the way-predicted cache?\nc. [15] <2.3> Way-predicted caches have usually been used only for instruction\ncaches that feed an instruction queue or buffer. Imagine that you want to try out\nway prediction on a data cache. Assume that you have 80% prediction accuracy\nand that subsequent operations (e.g., data cache access of other instructions,\ndependent operations) are issued assuming a correct way prediction. Thus a\nway misprediction necessitates a pipe flush and replay trap, which requires\n15 cycles. Is the change in average memory access time per load instruction\nwith data cache way prediction positive or negative, and how much is it?\nd. [10] <2.3> As an alternative to way prediction, many large associative L2\ncaches serialize tag and data access so that only the required dataset array\nneeds to be activated. This saves power but increases the access time. Use\nCACTI\u2019s detailed web interface for a 0.065 m process 1 MB four-way set\nassociative cache with 64-byte blocks, 144 bits read out, 1 bank, only 1\nread/write port, 30 bit tags, and ITRS-HP technology with global wires. What\nis the ratio of the access times for serializing tag and data access compared to\nparallel access?\n2.19\n[10/12] <2.3> You have been asked to investigate the relative performance of a\nbanked versus pipelined L1 data cache for a new microprocessor. Assume a 64 KB\ntwo-way set associative cache with 64-byte blocks. The pipelined cache would\nconsist of three pipe stages, similar in capacity to the Alpha 21264 data cache.\nA banked implementation would consist of two 32 KB two-way set associative\nbanks. Use CACTI and assume a 65 nm (0.065 m) technology to answer the fol-\nlowing questions. The cycle time output in the web version shows at what\nfrequency a cache can operate without any bubbles in the pipeline.\na. [10] <2.3> What is the cycle time of the cache in comparison to its access time,\nand how many pipe stages will the cache take up (to two decimal places)?\nb. [12] <2.3> Compare the area and total dynamic read energy per access of the\npipelined design versus the banked design. State which takes up less area and\nwhich requires more power, and explain why that might be.\n156\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 189,
        "text": "2.20\n[12/15] <2.3> Consider the usage of critical word first and early restart on\nL2 cache misses. Assume a 1 MB L2 cache with 64-byte blocks and a refill path\nthat is 16 bytes wide. Assume that the L2 can be written with 16 bytes every 4\nprocessor cycles, the time to receive the first 16 byte block from the memory con-\ntroller is 120 cycles, each additional 16 byte block from main memory requires 16\ncycles, and data can be bypassed directly into the read port of the L2 cache. Ignore\nany cycles to transfer the miss request to the L2 cache and the requested data to the\nL1 cache.\na. [12] <2.3> How many cycles would it take to service an L2 cache miss with\nand without critical word first and early restart?\nb. [15] <2.3> Do you think critical word first and early restart would be more\nimportant for L1 caches or L2 caches, and what factors would contribute to their\nrelative importance?\n2.21\n[12/12] <2.3> You are designing a write buffer between a write-through L1 cache\nand a write-back L2 cache. The L2 cache write data bus is 16 B wide and can per-\nform a write to an independent cache address every four processor cycles.\na. [12] <2.3> How many bytes wide should each write buffer entry be?\nb. [15] <2.3> What speedup could be expected in the steady state by using a\nmerging write buffer instead of a nonmerging buffer when zeroing memory\nby the execution of 64-bit stores if all other instructions could be issued in\nparallel with the stores and the blocks are present in the L2 cache?\nc. [15] <2.3> What would the effect of possible L1 misses be on the number of\nrequired write buffer entries for systems with blocking and nonblocking\ncaches?\n2.22\n[20] <2.1, 2.2, 2.3> A cache acts as a filter. For example, for every 1000 instruc-\ntions of a program, an average of 20 memory accesses may exhibit low enough\nlocality that they cannot be serviced by a 2 MB cache. The 2 MB cache is said\nto have an MPKI (misses per thousand instructions) of 20, and this will be largely\ntrue regardless of the smaller caches that precede the 2 MB cache. Assume the fol-\nlowing cache/latency/MPKI values: 32 KB/1/100, 128 KB/2/80, 512 KB/4/50,\n2 MB/8/40, 8 MB/16/10. Assume that accessing the off-chip memory system\nrequires 200 cycles on average. For the following cache configurations, calculate\nthe average time spent accessing the cache hierarchy. What do you observe about\nthe downsides of a cache hierarchy that is too shallow or too deep?\na. 32 KB L1; 8 MB L2; off-chip memory\nb. 32 KB L1; 512 KB L2; 8 MB L3; off-chip memory\nc. 32 KB L1; 128 KB L2; 2 MB L3; 8 MB L4; off-chip memory\n2.23\n[15] <2.1, 2.2, 2.3> Consider a 16 MB 16-way L3 cache that is shared by two\nprograms A and B. There is a mechanism in the cache that monitors cache miss\nrates for each program and allocates 1\u201315 ways to each program such that the over-\nall number of cache misses is reduced. Assume that program A has an MPKI of 100\nwhen it is assigned 1 MB of the cache. Each additional 1 MB assigned to program\nCase Studies and Exercises\n\u25a0\n157"
    },
    {
        "page": 190,
        "text": "A reduces the MPKI by 1. Program B has an MPKI of 50 when it is assigned 1 MB\nof cache; each additional 1 MB assigned to program B reduces its MPKI by 2.\nWhat is the best allocation of ways to programs A and B?\n2.24\n[20] <2.1, 2.6> You are designing a PMD and optimizing it for low energy. The\ncore, including an 8 KB L1 data cache, consumes 1 W whenever it is not in hiber-\nnation. If the core has a perfect L1 cache hit rate, it achieves an average CPI of 1 for\na given task, that is, 1000 cycles to execute 1000 instructions. Each additional\ncycle accessing the L2 and beyond adds a stall cycle for the core. Based on the\nfollowing specifications, what is the size of L2 cache that achieves the lowest\nenergy for the PMD (core, L1, L2, memory) for that given task?\na. The core frequency is 1 GHz, and the L1 has an MPKI of 100.\nb. A 256 KB L2 has a latency of 10 cycles, an MPKI of 20, a background power of\n0.2 W, and each L2 access consumes 0.5 nJ.\nc. A 1 MB L2 has a latency of 20 cycles, an MPKI of 10, a background power of\n0.8 W, and each L2 access consumes 0.7 nJ.\nd. The memory system has an average latency of 100 cycles, a background power\nof 0.5 W, and each memory access consumes 35 nJ.\n2.25\n[15] <2.1, 2.6> You are designing a PMD that is optimized for low power. Qual-\nitatively explain the impact on cache hierarchy (L2 and memory) power and overall\napplication energy if you design an L2 cache with:\na. Small block size\nb. Small cache size\nc. High associativity\n2.30\n[10/10] <2.1, 2.2, 2.3> The ways of a set can be viewed as a priority list, ordered\nfrom high priority to low priority. Every time the set is touched, the list can be\nreorganized to change block priorities. With this view, cache management policies\ncan be decomposed into three sub-policies: Insertion, Promotion, and Victim\nSelection. Insertion defines where newly fetched blocks are placed in the priority\nlist. Promotion defines how a block\u2019s position in the list is changed every time it is\ntouched (a cache hit). Victim Selection defines which entry of the list is evicted to\nmake room for a new block when there is a cache miss.\na. Can you frame the LRU cache policy in terms of the Insertion, Promotion, and\nVictim Selection sub-policies?\nb. Can you define other Insertion and Promotion policies that may be competitive\nand worth exploring further?\n2.31\n[15] <2.1, 2.3> In a processor that is running multiple programs, the last-level\ncache is typically shared by all the programs. This leads to interference, where\none program\u2019s behavior and cache footprint can impact the cache available to other\nprograms. First, this is a problem from a quality-of-service (QoS) perspective,\nwhere the interference leads to a program receiving fewer resources and lower\n158\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 191,
        "text": "performance than promised, say by the operator of a cloud service. Second, this is a\nproblem in terms of privacy. Based on the interference it sees, a program can infer\nthe memory access patterns of other programs. This is referred to as a timing chan-\nnel, a form of information leakage from one program to others that can be exploited\nto compromise data privacy or to reverse-engineer a competitor\u2019s algorithm. What\npolicies can you add to your last-level cache so that the behavior of one program is\nimmune to the behavior of other programs sharing the cache?\n2.32\n[15] <2.3> A large multimegabyte L3 cache can take tens of cycles to access\nbecause of the long wires that have to be traversed. For example, it may take\n20 cycles to access a 16 MB L3 cache. Instead of organizing the 16 MB cache such\nthat every access takes 20 cycles, we can organize the cache so that it is an array of\nsmaller cache banks. Some of these banks may be closer to the processor core,\nwhile others may be further. This leads to nonuniform cache access (NUCA),\nwhere 2 MB of the cache may be accessible in 8 cycles, the next 2 MB in 10 cycles,\nand so on until the last 2 MB is accessed in 22 cycles. What new policies can you\nintroduce to maximize performance in a NUCA cache?\n2.33\n[10/10/10] <2.2> Consider a desktop system with a processor connected to a\n2 GB DRAM with error-correcting code (ECC). Assume that there is only one\nmemory channel of width 72 bits (64 bits for data and 8 bits for ECC).\na. [10] <2.2> How many DRAM chips are on the DIMM if 1 Gb DRAM chips\nare used, and how many data I/Os must each DRAM have if only one DRAM\nconnects to each DIMM data pin?\nb. [10] <2.2> What burst length is required to support 32 B L2 cache blocks?\nc. [10] <2.2> Calculate the peak bandwidth for DDR2-667 and DDR2-533\nDIMMs for reads from an active page excluding the ECC overhead.\n2.34\n[10/10] <2.2> A sample DDR2 SDRAM timing diagram is shown in Figure 2.34.\ntRCD is the time required to activate a row in a bank, and column address\nstrobe (CAS) latency (CL) is the number of cycles required to read out a column\nin a row. Assume that the RAM is on a standard DDR2 DIMM with ECC, having\n72 data lines. Also assume burst lengths of 8 that read out 8 bits, or a total of 64 B\nfrom the DIMM. Assume tRCD = CAS (or CL) clock_frequency, and\nclock_frequency = transfers_per_second/2. The on-chip latency\nACT\nB0, Rx\nRD\nB0, Cx\nData out\nCAS latency\ntRCD\nData out\nCMD/\nADD\nClock\nData\nFigure 2.34 DDR2 SDRAM timing diagram.\nCase Studies and Exercises\n\u25a0\n159"
    },
    {
        "page": 192,
        "text": "on a cache miss through levels 1 and 2 and back, not including the DRAM access,\nis 20 ns.\na. [10] <2.2> How much time is required from presentation of the activate\ncommand until the last requested bit of data from the DRAM transitions\nfrom valid to invalid for the DDR2-667 1 Gb CL\u00bc5 DIMM? Assume that\nfor every request, we automatically prefetch another adjacent cache line in\nthe same page.\nb. [10] <2.2> What is the relative latency when using the DDR2-667 DIMM of a\nread requiring a bank activate versus one to an already open page, including the\ntime required to process the miss inside the processor?\n2.35\n[15] <2.2> Assume that a DDR2-667 2 GB DIMM with CL\u00bc5 is available for 130\nand a DDR2-533 2 GB DIMM with CL\u00bc4 is available for 100. Assume that two\nDIMMs are used in a system, and the rest of the system costs 800. Consider the\nperformance of the system using the DDR2-667 and DDR2-533 DIMMs on a\nworkload with 3.33 L2 misses per 1K instructions, and assume that 80% of all\nDRAM reads require an activate. What is the cost-performance of the entire system\nwhen using the different DIMMs, assuming only one L2 miss is outstanding at a\ntime and an in-order core with a CPI of 1.5 not including L2 cache miss memory\naccess time?\n2.36\n[12] <2.2> You are provisioning a server with eight-core 3 GHz CMP that can\nexecute a workload with an overall CPI of 2.0 (assuming that L2 cache miss refills\nare not delayed). The L2 cache line size is 32 bytes. Assuming the system uses\nDDR2-667 DIMMs, how many independent memory channels should be provided\nso the system is not limited by memory bandwidth if the bandwidth required is\nsometimes twice the average? The workloads incur, on average, 6.67 L2 misses\nper 1 K instructions.\n2.37\n[15] <2.2> Consider a processor that has four memory channels. Should consec-\nutive memory blocks be placed in the same bank, or should they be placed in dif-\nferent banks on different channels?\n2.38\n[12/12] <2.2> A large amount (more than a third) of DRAM power can be due to\npage activation (see http://download.micron.com/pdf/technotes/ddr2/TN4704.pdf\nand http://www.micron.com/systemcalc). Assume you are building a system with\n2 GB of memory using either 8-bank 2 Gb 8 DDR2 DRAMs or 8-bank 1 Gb\n8 DRAMs, both with the same speed grade. Both use a page size of 1 KB,\nand the last-level cache line size is 64 bytes. Assume that DRAMs that are not\nactive are in precharged standby and dissipate negligible power. Assume that\nthe time to transition from standby to active is not significant.\na. [12] <2.2> Which type of DRAM would be expected to provide the higher\nsystem performance? Explain why.\nb. [12] <2.2> How does a 2 GB DIMM made of 1 Gb 8 DDR2 DRAMs com-\npare with a DIMM with similar capacity made of 1 Gb 4 DDR2 DRAMs in\nterms of power?\n160\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 193,
        "text": "2.39\n[20/15/12] <2.2> To access data from a typical DRAM, we first have to activate\nthe appropriate row. Assume that this brings an entire page of size 8 KB to the row\nbuffer. Then we select a particular column from the row buffer. If subsequent\naccesses to DRAM are to the same page, then we can skip the activation step; oth-\nerwise, we have to close the current page and precharge the bitlines for the next\nactivation. Another popular DRAM policy is to proactively close a page and\nprecharge bitlines as soon as an access is over. Assume that every read or write\nto DRAM is of size 64 bytes and DDR bus latency (data from Figure 2.33) for\nsending 512 bits is Tddr.\na. [20] <2.2> Assuming DDR2-667, if it takes five cycles to precharge, five\ncycles to activate, and four cycles to read a column, for what value of the row\nbuffer hit rate (r) will you choose one policy over another to get the best access\ntime? Assume that every access to DRAM is separated by enough time to finish\na random new access.\nb. [15] <2.2> If 10% of the total accesses to DRAM happen back to back or\ncontiguously without any time gap, how will your decision change?\nc. [12] <2.2> Calculate the difference in average DRAM energy per access\nbetween the two policies using the previously calculated row buffer hit rate.\nAssume that precharging requires 2 nJ and activation requires 4 nJ and that\n100 pJ/bit are required to read or write from the row buffer.\n2.40\n[15] <2.2> Whenever a computer is idle, we can either put it in standby (where\nDRAM is still active) or we can let it hibernate. Assume that, to hibernate, we have\nto copy just the contents of DRAM to a nonvolatile medium such as Flash. If read-\ning or writing a cache line of size 64 bytes to Flash requires 2.56 J and DRAM\nrequires 0.5 nJ, and if idle power consumption for DRAM is 1.6 W (for 8 GB),\nhow long should a system be idle to benefit from hibernating? Assume a main\nmemory of size 8 GB.\n2.41\n[10/10/10/10/10] <2.4> Virtual machines (VMs) have the potential for adding\nmany beneficial capabilities to computer systems, such as improved total cost\nof ownership (TCO) or availability. Could VMs be used to provide the following\ncapabilities? If so, how could they facilitate this?\na. [10] <2.4> Test applications in production environments using development\nmachines?\nb. [10] <2.4> Quick redeployment of applications in case of disaster or failure?\nc. [10] <2.4> Higher performance in I/O-intensive applications?\nd. [10] <2.4> Fault isolation between different applications, resulting in higher\navailability for services?\ne. [10] <2.4> Performing software maintenance on systems while applications\nare running without significant interruption?\n2.42\n[10/10/12/12]<2.4>Virtualmachinescanloseperformancefromanumberofevents,\nsuch as the execution of privileged instructions, TLB misses, traps, and I/O.\nCase Studies and Exercises\n\u25a0\n161"
    },
    {
        "page": 194,
        "text": "These events are usually handled in system code. Thus one way of estimating the\nslowdown when running under a VM is the percentage of application execution\ntime in system versus user mode. For example, an application spending 10% of its\nexecution in system mode might slow down by 60% when running on a VM.\nFigure 2.35 lists the early performance of various system calls under native execu-\ntion, pure virtualization, and paravirtualization for LMbench using Xen on\nan Itanium system with times measured in microseconds (courtesy of Matthew\nChapman of the University of New South Wales).\na. [10] <2.4> What types of programs would be expected to have smaller\nslowdowns when running under VMs?\nb. [10] <2.4> If slowdowns were linear as a function of system time, given the\npreceding slowdown, how much slower would a program spending 20% of its\nexecution in system time be expected to run?\nc. [12] <2.4> What is the median slowdown of the system calls in the table above\nunder pure virtualization and paravirtualization?\nd. [12] <2.4> Which functions in the table above have the largest slowdowns?\nWhat do you think the cause of this could be?\n2.43\n[12] <2.4> Popek and Goldberg\u2019s definition of a virtual machine said that it would\nbe indistinguishable from a real machine except for its performance. In this ques-\ntion, we will use that definition to find out if we have access to native execution on\na processor or are running on a virtual machine. The Intel VT-x technology effec-\ntively provides a second set of privilege levels for the use of the virtual machine.\nWhat would a virtual machine running on top of another virtual machine have to\ndo, assuming VT-x technology?\n2.44\n[20/25] <2.4> With the adoption of virtualization support on the x86 architecture,\nvirtual machines are actively evolving and becoming mainstream. Compare and\ncontrast the Intel VT-x and AMD\u2019s AMD-V virtualization technologies.\nBenchmark\nNative\nPure\nPara\nNull call\n0.04\n0.96\n0.50\nNull I/O\n0.27\n6.32\n2.91\nStat\n1.10\n10.69\n4.14\nOpen/close\n1.99\n20.43\n7.71\nInstall signal handler\n0.33\n7.34\n2.89\nHandle signal\n1.69\n19.26\n2.36\nFork\n56.00\n513.00\n164.00\nExec\n316.00\n2084.00\n578.00\nFork+exec sh\n1451.00\n7790.00\n2360.00\nFigure 2.35 Early performance of various system calls under native execution, pure\nvirtualization, and paravirtualization.\n162\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 195,
        "text": "(Information on AMD-V can be found at http://sites.amd.com/us/business/it-\nsolutions/virtualization/Pages/resources.aspx.)\na. [20] <2.4> Which one could provide higher performance for memory-\nintensive applications with large memory footprints?\nb. [25] <2.4> Information on AMD\u2019s IOMMU support for virtualized I/O can be\nfound at http://developer.amd.com/documentation/articles/pages/892006101.\naspx. What do Virtualization Technology and an input/output memory manage-\nment unit (IOMMU) do to improve virtualized I/O performance?\n2.45\n[30] <2.2, 2.3> Since instruction-level parallelism can also be effectively\nexploited on in-order superscalar processors and very long instruction word\n(VLIW) processors with speculation, one important reason for building an out-\nof-order (OOO) superscalar processor is the ability to tolerate unpredictable mem-\nory latency caused by cache misses. Thus you can think about hardware supporting\nOOO issue as being part of the memory system. Look at the floorplan of the Alpha\n21264 in Figure 2.36 to find the relative area of the integer and floating-point issue\nqueues and mappers versus the caches. The queues schedule instructions for issue,\nFloat\nmap\nand\nqueue\nMemory\ncontroller\nBus\ninterface\nunit\nData and control buses\nMemory controller\nData\ncache\nInstruction\ncache\nInteger\nmapper\nInteger\nqueue\nInteger unit\n(cluster 0)\nInteger unit\n(cluster 1)\nFloating-point units\nInstruction\nfetch\nBIU\nFigure 2.36 Floorplan of the Alpha 21264 [Kessler 1999].\nCase Studies and Exercises\n\u25a0\n163"
    },
    {
        "page": 196,
        "text": "and the mappers rename register specifiers. Therefore these are necessary additions\nto support OOO issue. The 21264 only has L1 data and instruction caches on chip,\nand they are both 64 KB two-way set associative. Use an OOO superscalar sim-\nulator such as SimpleScalar (http://www.cs.wisc.edu/\u0005mscalar/simplescalar.\nhtml) on memory-intensive benchmarks to find out how much performance is lost\nif the area of the issue queues and mappers is used for additional L1 data cache area\nin an in-order superscalar processor, instead of OOO issue in a model of the 21264.\nMake sure the other aspects of the machine are as similar as possible to make the\ncomparison fair. Ignore any increase in access or cycle time from larger caches and\neffects of the larger data cache on the floorplan of the chip. (Note that this com-\nparison will not be totally fair, as the code will not have been scheduled for the\nin-order processor by the compiler.)\n2.46\n[15] <2.2, 2.7> As discussed in Section 2.7, the Intel i7 processor has an aggres-\nsive prefetcher. What are potential disadvantages in designing a prefetcher that is\nextremely aggressive?\n2.47\n[20/20/20] <2.6> The Intel performance analyzer VTune can be used to make\nmany measurements of cache behavior. A free evaluation version of VTune on\nboth Windows and Linux can be downloaded from http://software.intel.com/en-\nus/articles/intel-vtune-amplifier-xe/. The program (aca_ch2_cs2.c) used in\nCase Study 2 has been modified so that it can work with VTune out of the box\non Microsoft Visual C++. The program can be downloaded from http://www.\nhpl.hp.com/research/cacti/aca_ch2_cs2_vtune.c. Special VTune functions have\nbeen inserted to exclude initialization and loop overhead during the performance\nanalysis process. Detailed VTune setup directions are given in the README sec-\ntion in the program. The program keeps looping for 20 seconds for every config-\nuration. In the following experiment, you can find the effects of data size on cache\nand overall processor performance. Run the program in VTune on an Intel proces-\nsor with the input dataset sizes of 8 KB, 128 KB, 4 MB, and 32 MB, and keep a\nstride of 64 bytes (stride one cache line on Intel i7 processors). Collect statistics on\noverall performance and L1 data cache, L2, and L3 cache performance.\na. [20] <2.6> List the number of misses per 1K instruction of L1 data cache, L2,\nand L3 for each dataset size and your processor model and speed. Based on the\nresults, what can you say about the L1 data cache, L2, and L3 cache sizes on\nyour processor? Explain your observations.\nb. [20] <2.6> List the instructions per clock (IPC) for each dataset size and your\nprocessor model and speed. Based on the results, what can you say about the\nL1, L2, and L3 miss penalties on your processor? Explain your observations.\nc. [20] <2.6> Run the program in VTune with input dataset size of 8 KB and\n128 KB on an Intel OOO processor. List the number of L1 data cache and\nL2 cache misses per 1K instructions and the CPI for both configurations. What\ncan you say about the effectiveness of memory latency hiding techniques in\nhigh-performance OOO processors? Hint: You need to find the L1 data cache\nmiss latency for your processor. For recent Intel i7 processors, it is approxi-\nmately 11 cycles.\n164\n\u25a0\nChapter Two Memory Hierarchy Design"
    },
    {
        "page": 197,
        "text": "This page intentionally left blank"
    },
    {
        "page": 198,
        "text": "3.1\nInstruction-Level Parallelism: Concepts and Challenges\n168\n3.2\nBasic Compiler Techniques for Exposing ILP\n176\n3.3\nReducing Branch Costs With Advanced Branch Prediction\n182\n3.4\nOvercoming Data Hazards With Dynamic Scheduling\n191\n3.5\nDynamic Scheduling: Examples and the Algorithm\n201\n3.6\nHardware-Based Speculation\n208\n3.7\nExploiting ILP Using Multiple Issue and Static Scheduling\n218\n3.8\nExploiting ILP Using Dynamic Scheduling, Multiple Issue,\nand Speculation\n222\n3.9\nAdvanced Techniques for Instruction Delivery and Speculation\n228\n3.10\nCross-Cutting Issues\n240\n3.11\nMultithreading: Exploiting Thread-Level Parallelism to Improve\nUniprocessor Throughput\n242\n3.12\nPutting It All Together: The Intel Core i7 6700 and ARM Cortex-A53\n247\n3.13\nFallacies and Pitfalls\n258\n3.14\nConcluding Remarks: What\u2019s Ahead?\n264\n3.15\nHistorical Perspective and References\n266\nCase Studies and Exercises by Jason D. Bakos and Robert P. Colwell\n266"
    },
    {
        "page": 199,
        "text": "3\nInstruction-Level Parallelism\nand Its Exploitation\n\u201cWho\u2019s first?\u201d\n\u201cAmerica.\u201d\n\u201cWho\u2019s second?\u201d\n\u201cSir, there is no second.\u201d\nDialog between two observers of the\nsailing race in 1851, later named \u201cThe America\u2019s Cup,\u201d\nwhich was the inspiration for John Cocke\u2019s\nnaming of an IBM research processor as \u201cAmerica,\u201d the first\nsuperscalar processor, and a precursor to the PowerPC.\nThus, the IA-64 gambles that, in the future, power will not be the critical\nlimitation, and massive resources\u2026will not penalize clock speed, path length,\nor CPI factors. My view is clearly skeptical\u2026\nMarty Hopkins (2000), IBM Fellow and Early RISC pioneer\ncommenting in 2000 on the new Intel Itanium, a joint development\nof Intel and HP. The Itanium used a static ILP approach (see\nAppendix H) and was a massive investment for Intel. It never\naccounted for more than 0.5% of Intel\u2019s microprocessor sales.\nComputer Architecture. https://doi.org/10.1016/B978-0-12-811905-1.00003-1\n\u00a9 2019 Elsevier Inc. All rights reserved."
    },
    {
        "page": 200,
        "text": "3.1\nInstruction-Level Parallelism: Concepts and Challenges\nAll processors since about 1985 have used pipelining to overlap the execution of\ninstructions and improve performance. This potential overlap among instructions\nis called instruction-level parallelism (ILP), because the instructions can be eval-\nuated in parallel. In this chapter and Appendix H, we look at a wide range of tech-\nniques for extending the basic pipelining concepts by increasing the amount of\nparallelism exploited among instructions.\nThis chapter is at a considerably more advanced level than the material on basic\npipelining in Appendix C. If you are not thoroughly familiar with the ideas in\nAppendix C, you should review that appendix before venturing into this chapter.\nWe start this chapter by looking at the limitation imposed by data and control\nhazards and then turn to the topic of increasing the ability of the compiler and the\nprocessortoexploit parallelism.Thesesectionsintroducealargenumberofconcepts,\nwhichwebuildonthroughoutthischapterandthenext.Whilesomeofthemorebasic\nmaterial in this chapter could be understood without all of the ideas in the first two\nsections, this basic material is important to later sections of this chapter.\nThere are two largely separable approaches to exploiting ILP: (1) an approach\nthat relies on hardware to help discover and exploit the parallelism dynamically,\nand (2) an approach that relies on software technology to find parallelism statically\nat compile time. Processors using the dynamic, hardware-based approach, includ-\ning all recent Intel and many ARM processors, dominate in the desktop and server\nmarkets. In the personal mobile device market, the same approaches are used in\nprocessors found in tablets and high-end cell phones. In the IOT space, where\npower and cost constraints dominate performance goals, designers exploit lower\nlevels of instruction-level parallelism. Aggressive compiler-based approaches\nhave been attempted numerous times beginning in the 1980s and most recently\nin the Intel Itanium series, introduced in 1999. Despite enormous efforts, such\napproaches have been successful only in domain-specific environments or in\nwell-structured scientific applications with significant data-level parallelism.\nIn the past few years, many of the techniques developed for one approach have\nbeen exploited within a design relying primarily on the other. This chapter intro-\nduces the basic concepts and both approaches. A discussion of the limitations on\nILP approaches is included in this chapter, and it was such limitations that directly\nled to the movement toward multicore. Understanding the limitations remains\nimportant in balancing the use of ILP and thread-level parallelism.\nIn this section, we discuss features of both programs and processors that limit\nthe amount of parallelism that can be exploited among instructions, as well as the\ncritical mapping between program structure and hardware structure, which is key\nto understanding whether a program property will actually limit performance and\nunder what circumstances.\nThe value of the CPI (cycles per instruction) for a pipelined processor is the\nsum of the base CPI and all contributions from stalls:\nPipeline CPI \u00bc Ideal pipelineCPI + Structural stalls + Data hazardstalls + Control stalls\n168\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 201,
        "text": "The ideal pipeline CPI is a measure of the maximum performance attainable by the\nimplementation. By reducing each of the terms of the right-hand side, we decrease\nthe overall pipeline CPI or, alternatively, increase the IPC (instructions per clock).\nThe preceding equation allows us to characterize various techniques by what com-\nponent of the overall CPI a technique reduces. Figure 3.1 shows the techniques we\nexamine in this chapter and in Appendix H, as well as the topics covered in the\nintroductory material in Appendix C. In this chapter, we will see that the tech-\nniques we introduce to decrease the ideal pipeline CPI can increase the importance\nof dealing with hazards.\nWhat Is Instruction-Level Parallelism?\nAll the techniques in this chapter exploit parallelism among instructions. The\namount of parallelism available within a basic block\u2014a straight-line code sequence\nwith no branches in except to the entry and no branches out except at the exit\u2014is\nquite small. For typical RISC programs, the average dynamic branch frequency is\noften between 15% and 25%, meaning that between three and six instructions exe-\ncute between a pair of branches. Because these instructions are likely to depend\nupon one another, the amount of overlap we can exploit within a basic block is\nlikely to be less than the average basic block size. To obtain substantial performance\nenhancements, we must exploit ILP across multiple basic blocks.\nThe simplest and most common way to increase the ILP is to exploit parallel-\nism among iterations of a loop. This type of parallelism is often called loop-level\nTechnique\nReduces\nSection\nForwarding and bypassing\nPotential data hazard stalls\nC.2\nSimple branch scheduling and prediction\nControl hazard stalls\nC.2\nBasic compiler pipeline scheduling\nData hazard stalls\nC.2, 3.2\nBasic dynamic scheduling (scoreboarding)\nData hazard stalls from true dependences\nC.7\nLoop unrolling\nControl hazard stalls\n3.2\nAdvanced branch prediction\nControl stalls\n3.3\nDynamic scheduling with renaming\nStalls from data hazards, output dependences, and\nantidependences\n3.4\nHardware speculation\nData hazard and control hazard stalls\n3.6\nDynamic memory disambiguation\nData hazard stalls with memory\n3.6\nIssuing multiple instructions per cycle\nIdeal CPI\n3.7, 3.8\nCompiler dependence analysis, software pipelining,\ntrace scheduling\nIdeal CPI, data hazard stalls\nH.2, H.3\nHardware support for compiler speculation\nIdeal CPI, data hazard stalls, branch hazard stalls\nH.4, H.5\nFigure 3.1 The major techniques examined in Appendix C, Chapter 3, and Appendix H are shown together with\nthe component of the CPI equation that the technique affects.\n3.1\nInstruction-Level Parallelism: Concepts and Challenges\n\u25a0\n169"
    },
    {
        "page": 202,
        "text": "parallelism. Here is a simple example of a loop that adds two 1000-element arrays\nand is completely parallel:\nfor (i=0; i<=999; i=i+1)\nx[i] = x[i] + y[i];\nEvery iteration of the loop can overlap with any other iteration, although within\neach loop iteration, there is little or no opportunity for overlap.\nWe will examine a number of techniques for converting such loop-level\nparallelism into instruction-level parallelism. Basically, such techniques work\nby unrolling the loop either statically by the compiler (as in the next section) or\ndynamically by the hardware (as in Sections 3.5 and 3.6).\nAn important alternative method for exploiting loop-level parallelism is the use\nof SIMD in both vector processors and graphics processing units (GPUs), both of\nwhich are covered in Chapter 4. A SIMD instruction exploits data-level parallelism\nby operating on a small to moderate number of data items in parallel (typically\ntwo to eight). A vector instruction exploits data-level parallelism by operating\non many data items in parallel using both parallel execution units and a deep pipe-\nline. For example, the preceding code sequence, which in simple form requires\nseven instructions per iteration (two loads, an add, a store, two address updates,\nand a branch) for a total of 7000 instructions, might execute in one-quarter as many\ninstructions in some SIMD architecture where four data items are processed per\ninstruction. On some vector processors, this sequence might take only four instruc-\ntions: two instructions to load the vectors x and y from memory, one instruction to\nadd the two vectors, and an instruction to store back the result vector. Of course,\nthese instructions would be pipelined and have relatively long latencies, but these\nlatencies may be overlapped.\nData Dependences and Hazards\nDetermining how one instruction depends on another is critical to determining how\nmuch parallelism exists in a program and how that parallelism can be exploited.\nIn particular, to exploit instruction-level parallelism, we must determine which\ninstructions can be executed in parallel. If two instructions are parallel, they\ncan execute simultaneously in a pipeline of arbitrary depth without causing any\nstalls, assuming the pipeline has sufficient resources (and thus no structural hazards\nexist). If two instructions are dependent, they are not parallel and must be executed\nin order, although they may often be partially overlapped. The key in both cases is\nto determine whether an instruction is dependent on another instruction.\nData Dependences\nThere are three different types of dependences: data dependences (also called true\ndata dependences), name dependences, and control dependences. An instruction j\nis data-dependent on instruction i if either of the following holds:\n170\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 203,
        "text": "\u25a0\nInstruction i produces a result that may be used by instruction j.\n\u25a0\nInstruction j is data-dependent on instruction k, and instruction k is data-\ndependent on instruction i.\nThe second condition simply states that one instruction is dependent on another if\nthere exists a chain of dependences of the first type between the two instructions.\nThis dependence chain can be as long as the entire program. Note that a depen-\ndence within a single instruction (such as add x1,x1,x1) is not considered a\ndependence.\nFor example, consider the following RISC-V code sequence that increments a\nvector of values in memory (starting at 0(x1) ending with the last element at\n0(x2)) by a scalar in register f2.\nLoop:\nfld\nf0,0(x1)\n//f0=array element\nfadd.d\nf4,f0,f2\n//add scalar in f2\nfsd\nf4,0(x1)\n//store result\naddi\nx1,x1,8\n//decrement pointer 8 bytes\nbne\nx1,x2,Loop\n//branch x16\u00bcx2\nThe data dependences in this code sequence involve both floating-point data:\nLoop: fld\nf0,0(x1)\n//f0=array element\nfadd.d\nf4,f0,f2\n//add scalar in f2\nfsd\nf4,0(x1)\n//store result\nand integer data:\naddi\n x1,x1,-8\n//decrement pointer \n//8 bytes (per DW)\nbne\n x1,x2,Loop//branch x1ax2\nIn both of the preceding dependent sequences, as shown by the arrows, each\ninstruction depends on the previous one. The arrows here and in following exam-\nples show the order that must be preserved for correct execution. The arrow points\nfrom an instruction that must precede the instruction that the arrowhead points to.\nIf two instructions are data-dependent, they must execute in order and cannot\nexecute simultaneously or be completely overlapped. The dependence implies that\nthere would be a chain of one or more data hazards between the two instructions.\n(See Appendix C for a brief description of data hazards, which we will define\nprecisely in a few pages.) Executing the instructions simultaneously will cause\na processor with pipeline interlocks (and a pipeline depth longer than the distance\nbetween the instructions in cycles) to detect a hazard and stall, thereby reducing or\neliminating the overlap. In a processor without interlocks that relies on compiler\nscheduling, the compiler cannot schedule dependent instructions in such a way that\n3.1\nInstruction-Level Parallelism: Concepts and Challenges\n\u25a0\n171"
    },
    {
        "page": 204,
        "text": "they completely overlap because the program will not execute correctly. The pres-\nence of a data dependence in an instruction sequence reflects a data dependence\nin the source code from which the instruction sequence was generated. The effect\nof the original data dependence must be preserved.\nDependences are a property of programs. Whether a given dependence results\nin an actual hazard being detected and whether that hazard actually causes a stall\nare properties of the pipeline organization. This difference is critical to understand-\ning how instruction-level parallelism can be exploited.\nA data dependence conveys three things: (1) the possibility of a hazard, (2) the\norder in which results must be calculated, and (3) an upper bound on how\nmuch parallelism can possibly be exploited. Such limits are explored in a pitfall\non page 262 and in Appendix H in more detail.\nBecause a data dependence can limit the amount of instruction-level parallel-\nism we can exploit, a major focus of this chapter is overcoming these limitations. A\ndependence can be overcome in two different ways: (1) maintaining the depen-\ndence but avoiding a hazard, and (2) eliminating a dependence by transforming\nthe code. Scheduling the code is the primary method used to avoid a hazard without\naltering a dependence, and such scheduling can be done both by the compiler and\nby the hardware.\nA data value may flow between instructions either through registers or through\nmemory locations. When the data flow occurs through a register, detecting the\ndependence is straightforward because the register names are fixed in the instruc-\ntions, although it gets more complicated when branches intervene and correctness\nconcerns force a compiler or hardware to be conservative.\nDependences that flow through memory locations are more difficult to detect\nbecause two addresses may refer to the same location but look different: For exam-\nple, 100(x4) and 20(x6) may be identical memory addresses. In addition, the\neffective address of a load or store may change from one execution of the instruc-\ntion to another (so that 20(x4) and 20(x4) may be different), further compli-\ncating the detection of a dependence.\nIn this chapter, we examine hardware for detecting data dependences that\ninvolve memory locations, but we will see that these techniques also have limita-\ntions. The compiler techniques for detecting such dependences are critical in unco-\nvering loop-level parallelism.\nName Dependences\nThe second type of dependence is a name dependence. A name dependence occurs\nwhen two instructions use the same register or memory location, called a name, but\nthere is no flow of data between the instructions associated with that name. There\nare two types of name dependences between an instruction i that precedes instruc-\ntion j in program order:\n1. An antidependence between instruction i and instruction j occurs when instruc-\ntion j writes a register or memory location that instruction i reads. The original\n172\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 205,
        "text": "ordering must be preserved to ensure that i reads the correct value. In the example\non page 171, there is an antidependence between fsd and addi on register x1.\n2. An output dependence occurs when instruction i and instruction j write the same\nregister or memory location. The ordering between the instructions must be\npreserved to ensure that the value finally written corresponds to instruction j.\nBoth antidependences and output dependences are name dependences, as opposed\nto true data dependences, because there is no value being transmitted between the\ninstructions. Because a name dependence is not a true dependence, instructions\ninvolved in a name dependence can execute simultaneously or be reordered, if\nthe name (register number or memory location) used in the instructions is changed\nso the instructions do not conflict.\nThis renaming can be more easily done for register operands, where it is called\nregister renaming. Register renaming can be done either statically by a compiler or\ndynamically bythehardware.Beforedescribingdependencesarising from branches,\nlet\u2019s examine the relationship between dependences and pipeline data hazards.\nData Hazards\nA hazard exists whenever there is a name or data dependence between instructions,\nand they are close enough that the overlap during execution would change the\norder of access to the operand involved in the dependence. Because of the depen-\ndence, we must preserve what is called program order\u2014that is, the order that the\ninstructions would execute in if executed sequentially one at a time as determined\nby the original source program. The goal of both our software and hardware tech-\nniques is to exploit parallelism by preserving program order only where it affects\nthe outcome of the program. Detecting and avoiding hazards ensures that neces-\nsary program order is preserved.\nData hazards, which are informally described in Appendix C, may be classified\nas one of three types, depending on the order of read and write accesses in the\ninstructions. By convention, the hazards are named by the ordering in the program\nthat must be preserved by the pipeline. Consider two instructions i and j, with i\npreceding j in program order. The possible data hazards are\n\u25a0\nRAW (read after write)\u2014j tries to read a source before i writes it, so j incor-\nrectly gets the old value. This hazard is the most common type and corresponds\nto a true data dependence. Program order must be preserved to ensure that j\nreceives the value from i.\n\u25a0\nWAW (write after write)\u2014j tries to write an operand before it is written by i.\nThe writes end up being performed in the wrong order, leaving the value writ-\nten by i rather than the value written by j in the destination. This hazard cor-\nresponds to an output dependence. WAW hazards are present only in pipelines\nthat write in more than one pipe stage or allow an instruction to proceed even\nwhen a previous instruction is stalled.\n3.1\nInstruction-Level Parallelism: Concepts and Challenges\n\u25a0\n173"
    },
    {
        "page": 206,
        "text": "\u25a0\nWAR (write after read)\u2014j tries to write a destination before it is read by i, so i\nincorrectly gets the new value. This hazard arises from an antidependence (or\nname dependence). WAR hazards cannot occur in most static issue pipelines\u2014\neven deeper pipelines or floating-point pipelines\u2014because all reads are early\n(in ID in the pipeline in Appendix C) and all writes are late (in WB in the pipe-\nline in Appendix C). A WAR hazard occurs either when there are some instruc-\ntions that write results early in the instruction pipeline and other instructions\nthat read a source late in the pipeline, or when instructions are reordered, as\nwe will see in this chapter.\nNote that the RAR (read after read) case is not a hazard.\nControl Dependences\nThe last type of dependence is a control dependence. A control dependence deter-\nmines the ordering of an instruction, i, with respect to a branch instruction so that\ninstruction i is executed in correct program order and only when it should be. Every\ninstruction, except for those in the first basic block of the program, is control-\ndependent on some set of branches, and in general, these control dependences must\nbe preserved to preserve program order. One of the simplest examples of a control\ndependence is the dependence of the statements in the \u201cthen\u201d part of an if statement\non the branch. For example, in the code segment\nif p1 {\nS1;\n};\nif p2 {\nS2;\n}\nS1 is control-dependent on p1, and S2 is control-dependent on p2 but not\non p1.\nIn general, two constraints are imposed by control dependences:\n1. An instruction that is control-dependent on a branch cannot be moved before the\nbranch so that its execution is no longer controlled by the branch. For example,\nwe cannot take an instruction from the then portion of an if statement and move\nit before the if statement.\n2. An instruction that is not control-dependent on a branch cannot be moved after\nthe branch so that its execution is controlled by the branch. For example, we\ncannot take a statement before the if statement and move it into the then portion.\nWhen processors preserve strict program order, they ensure that control depen-\ndences are also preserved. We may be willing to execute instructions that should\nnot have been executed, however, thereby violating the control dependences, if we\n174\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 207,
        "text": "can do so without affecting the correctness of the program. Thus control depen-\ndence is not the critical property that must be preserved. Instead, the two properties\ncritical to program correctness\u2014and normally preserved by maintaining both data\nand control dependences\u2014are the exception behavior and the data flow.\nPreserving the exception behavior means that any changes in the ordering of\ninstruction execution must not change how exceptions are raised in the program.\nOften this is relaxed to mean that the reordering of instruction execution must not\ncause any new exceptions in the program. A simple example shows how maintain-\ning the control and data dependences can prevent such situations. Consider this\ncode sequence:\nadd\nx2,x3,x4\nbeq\nx2,x0,L1\nld\nx1,0(x2)\nL1:\nIn this case, it is easy to see that if we do not maintain the data dependence involv-\ning x2, we can change the result of the program. Less obvious is the fact that if we\nignore the control dependence and move the load instruction before the branch, the\nload instruction may cause a memory protection exception. Notice that no data\ndependence prevents us from interchanging the beqz and the ld; it is only the\ncontrol dependence. To allow us to reorder these instructions (and still preserve\nthe data dependence), we want to just ignore the exception when the branch is\ntaken. In Section 3.6, we will look at a hardware technique, speculation, which\nallows us to overcome this exception problem. Appendix H looks at software tech-\nniques for supporting speculation.\nThe second property preserved by maintenance of data dependences and con-\ntrol dependences is the data flow. The data flow is the actual flow of data values\namong instructions that produce results and those that consume them. Branches\nmake the data flow dynamic because they allow the source of data for a given\ninstruction to come from many points. Put another way, it is insufficient to just\nmaintain data dependences because an instruction may be data-dependent on more\nthan one predecessor. Program order is what determines which predecessor will\nactually deliver a data value to an instruction. Program order is ensured by main-\ntaining the control dependences.\nFor example, consider the following code fragment:\nadd\nx1,x2,x3\nbeq\nx4,x0,L\nsub\nx1,x5,x6\nL:\n...\nor\nx7,x1,x8\nIn this example, the value of x1 used by the or instruction depends on whether the\nbranch is taken or not. Data dependence alone is not sufficient to preserve correct-\nness. The or instruction is data-dependent on both the add and sub instructions,\nbut preserving that order alone is insufficient for correct execution.\n3.1\nInstruction-Level Parallelism: Concepts and Challenges\n\u25a0\n175"
    },
    {
        "page": 208,
        "text": "Instead, when the instructions execute, the data flow must be preserved: If the\nbranch is not taken, then the value of x1 computed by the sub should be used by\nthe or, and if the branch is taken, the value of x1 computed by the add should\nbe used by the or. By preserving the control dependence of the or on the branch,\nwe prevent an illegal change to the data flow. For similar reasons, the sub instruc-\ntion cannot be moved above the branch. Speculation, which helps with the excep-\ntion problem, will also allow us to lessen the impact of the control dependence\nwhile still maintaining the data flow, as we will see in Section 3.6.\nSometimes we can determine that violating the control dependence cannot\naffect either the exception behavior or the data flow. Consider the following code\nsequence:\nadd\nx1,x2,x3\nbeq\nx12,x0,skip\nsub\nx4,x5,x6\nadd\nx5,x4,x9\nskip:\nor\nx7,x8,x9\nSuppose weknewthatthe registerdestination ofthe sub instruction (x4) was unused\naftertheinstructionlabeledskip.(Thepropertyofwhetheravaluewillbeusedbyan\nupcoming instruction is called liveness.) If x4 were unused, then changing the value\nof x4 just before the branch would not affect the data flow because x4 would be dead\n(rather thanlive) inthecoderegionafterskip.Thus,ifx4weredeadandtheexisting\nsub instruction could not generate an exception (other than those from which the\nprocessor resumes the same process), we could move the sub instruction before\nthe branch because the data flow could not be affected by this change.\nIf the branch is taken, the sub instruction will execute and will be useless, but\nit will not affect the program results. This type of code scheduling is also a form of\nspeculation, often called software speculation, because the compiler is betting on\nthe branch outcome; in this case, the bet is that the branch is usually not taken.\nMore ambitious compiler speculation mechanisms are discussed in Appendix H.\nNormally, it will be clear when we say speculation or speculative whether the\nmechanism is a hardware or software mechanism; when it is not clear, it is best\nto say \u201chardware speculation\u201d or \u201csoftware speculation.\u201d\nControl dependence is preserved by implementing control hazard detection\nthat causes control stalls. Control stalls can be eliminated or reduced by a variety\nof hardware and software techniques, which we examine in Section 3.3.\n3.2\nBasic Compiler Techniques for Exposing ILP\nThis section examines the use of simple compiler technology to enhance a proces-\nsor\u2019s ability to exploit ILP. These techniques are crucial for processors that use static\nissue or static scheduling. Armed with this compiler technology, we will shortly\nexamine the design and performance of processors using static issuing. Appendix\nH will investigate more sophisticated compiler and associated hardware schemes\ndesigned to enable a processor to exploit more instruction-level parallelism.\n176\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 209,
        "text": "Basic Pipeline Scheduling and Loop Unrolling\nTo keep a pipeline full, parallelism among instructions must be exploited by find-\ning sequences of unrelated instructions that can be overlapped in the pipeline. To\navoid a pipeline stall, the execution of a dependent instruction must be separated\nfrom the source instruction by a distance in clock cycles equal to the pipeline\nlatency of that source instruction. A compiler\u2019s ability to perform this scheduling\ndepends both on the amount of ILP available in the program and on the latencies of\nthe functional units in the pipeline. Figure 3.2 shows the FP unit latencies we\nassume in this chapter, unless different latencies are explicitly stated. We assume\nthe standard five-stage integer pipeline so that branches have a delay of one clock\ncycle. We assume that the functional units are fully pipelined or replicated (as\nmany times as the pipeline depth) so that an operation of any type can be issued\non every clock cycle and there are no structural hazards.\nIn this section, we look at how the compiler can increase the amount of avail-\nable ILP by transforming loops. This example serves both to illustrate an important\ntechnique as well as to motivate the more powerful program transformations\ndescribed in Appendix H. We will rely on the following code segment, which adds\na scalar to a vector:\nfor (i=999; i>=0; i=i1)\nx[i] = x[i] + s;\nWe can see that this loop is parallel by noticing that the body of each iteration is\nindependent. We formalize this notion in Appendix H and describe how we can test\nwhether loop iterations are independent at compile time. First, let\u2019s look at the per-\nformance of this loop, which shows how we can use the parallelism to improve its\nperformance for a RISC-V pipeline with the preceding latencies.\nThefirst stepistotranslate the precedingsegment toRISC-Vassembly language.\nIn the following code segment, x1 is initially the address of the element in the array\nwith the highest address, and f2 contains the scalar value s. Register x2 is precom-\nputed so that Regs[x2]+8 is the address of the last element to operate on.\nInstruction producing result\nInstruction using result\nLatency in clock cycles\nFP ALU op\nAnother FP ALU op\n3\nFP ALU op\nStore double\n2\nLoad double\nFP ALU op\n1\nLoad double\nStore double\n0\nFigure 3.2 Latencies of FP operations used in this chapter. The last column is the\nnumber of intervening clock cycles needed to avoid a stall. These numbers are similar\nto the average latencies we would see on an FP unit. The latency of a floating-point load\nto a store is 0 because the result of the load can be bypassed without stalling the store.\nWe will continue to assume an integer load latency of 1 and an integer ALU operation\nlatency of 0 (which includes ALU operation to branch).\n3.2\nBasic Compiler Techniques for Exposing ILP\n\u25a0\n177"
    },
    {
        "page": 210,
        "text": "The straightforward RISC-V code, not scheduled for the pipeline, looks like\nthis:\nLoop:\nfld\nf0,0(x1)\n//f0=array element\nfadd.d\nf4,f0,f2\n//add scalar in f2\nfsd\nf4,0(x1)\n//store result\naddi\nx1,x1,8\n//decrement pointer\n//8 bytes (per DW)\nbne\nx1,x2,Loop\n//branch x16\u00bcx2\nLet\u2019s start by seeing how well this loop will run when it is scheduled on a sim-\nple pipeline for RISC-V with the latencies in Figure 3.2.\nExample\nShow how the loop would look on RISC-V, both scheduled and unscheduled,\nincluding any stalls or idle clock cycles. Schedule for delays from floating-point\noperations.\nAnswer\nWithout any scheduling, the loop will execute as follows, taking nine cycles:\nClock cycle issued\nLoop:\nfld\nf0,0(x1)\n1\nstall\n2\nfadd.d\nf4,f0,f2\n3\nstall\n4\nstall\n5\nfsd\nf4,0(x1)\n6\naddi\nx1,x1,8\n7\nbne\nx1,x2,Loop\n8\nWe can schedule the loop to obtain only two stalls and reduce the time to seven\ncycles:\nLoop:\nfld\nf0,0(x1)\naddi\nx1,x1,8\nfadd.d\nf4,f0,f2\nstall\nstall\nfsd\nf4,8(x1)\nbne\nx1,x2,Loop\nThe stalls after fadd.d are for use by the fsd, and repositioning the addi pre-\nvents the stall after the fld.\nIn the previous example, we complete one loop iteration and store back one array\nelement every seven clock cycles, but the actual work of operating on the array\nelement takes just three (the load, add, and store) of those seven clock cycles.\n178\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 211,
        "text": "The remaining four clock cycles consist of loop overhead\u2014the addi and bne\u2014\nand two stalls. To eliminate these four clock cycles, we need to get more operations\nrelative to the number of overhead instructions.\nA simple scheme for increasing the number of instructions relative to the\nbranch and overhead instructions is loop unrolling. Unrolling simply replicates\nthe loop body multiple times, adjusting the loop termination code.\nLoop unrolling can also be used to improve scheduling. Because it eliminates\nthe branch, it allows instructions from different iterations to be scheduled together.\nIn this case, we can eliminate the data use stalls by creating additional independent\ninstructions within the loop body. If we simply replicated the instructions when we\nunrolled the loop, the resulting use of the same registers could prevent us from\neffectively scheduling the loop. Thus we will want to use different registers for\neach iteration, increasing the required number of registers.\nExample\nShow our loop unrolled so that there are four copies of the loop body, assuming\nx1  x2 (that is, the size of the array) is initially a multiple of 32, which means that\nthe number of loop iterations is a multiple of 4. Eliminate any obviously redundant\ncomputations and do not reuse any of the registers.\nAnswer\nHere is the result after merging the addi instructions and dropping the unnec-\nessary bne operations that are duplicated during unrolling. Note that x2 must\nnow be set so that Regs[x2]+32 is the starting address of the last four\nelements.\nLoop:\nfld\nf0,0(x1)\nfadd.d\nf4,f0,f2\nfsd\nf4,0(x1)\n//drop addi & bne\nfld\nf6,8(x1)\nfadd.d\nf8,f6,f2\nfsd\nf8,8(x1)\n//drop addi & bne\nfld\nf0,16(x1)\nfadd.d\nf12,f0,f2\nfsd\nf12,16(x1)\n//drop addi & bne\nfld\nf14,24(x1)\nfadd.d\nf16,f14,f2\nfsd\nf16,24(x1)\naddi\nx1,x1,32\nbne\nx1,x2,Loop\nWe have eliminated three branches and three decrements of x1. The addresses on\nthe loads and stores have been compensated to allow the addi instructions on x1\nto be merged. This optimization may seem trivial, but it is not; it requires symbolic\nsubstitution and simplification. Symbolic substitution and simplification will rear-\nrange expressions so as to allow constants to be collapsed, allowing an expression\nsuch as ((i+1)+1) to be rewritten as (i+(1+1)) and then simplified to (i+2).\n3.2\nBasic Compiler Techniques for Exposing ILP\n\u25a0\n179"
    },
    {
        "page": 212,
        "text": "We will see more general forms of these optimizations that eliminate dependent\ncomputations in Appendix H.\nWithout scheduling, every FP load or operation in the unrolled loop is followed\nby a dependent operation and thus will cause a stall. This unrolled loop will run in\n26 clock cycles\u2014each fld has 1 stall, each fadd.d has 2, plus 14 instruction\nissue cycles\u2014or 6.5 clock cycles for each of the four elements, but it can be sched-\nuled to improve performance significantly. Loop unrolling is normally done early\nin the compilation process so that redundant computations can be exposed and\neliminated by the optimizer.\nIn real programs, we do not usually know the upper bound on the loop. Sup-\npose it is n, and we want to unroll the loop to make k copies of the body. Instead of a\nsingle unrolled loop, we generate a pair of consecutive loops. The first executes (n\nmod k) times and has a body that is the original loop. The second is the unrolled\nbody surrounded by an outer loop that iterates (n/k) times. (As we will see in\nChapter 4, this technique is similar to a technique called strip mining, used in com-\npilers for vector processors.) For large values of n, most of the execution time will\nbe spent in the unrolled loop body.\nIn the previous example, unrolling improves the performance of this loop by\neliminating overhead instructions, although it increases code size substantially.\nHow will the unrolled loop perform when it is scheduled for the pipeline described\nearlier?\nExample\nShow the unrolled loop in the previous example after it has been scheduled for the\npipeline with the latencies in Figure 3.2.\nAnswer\nLoop:\nfld\nf0,0(x1)\nfld\nf6,8(x1)\nfld\nf0,16(x1)\nfld\nf14,24(x1)\nfadd.d\nf4,f0,f2\nfadd.d\nf8,f6,f2\nfadd.d\nf12,f0,f2\nfadd.d\nf16,f14,f2\nfsd\nf4,0(x1)\nfsd\nf8,8(x1)\nfsd\nf12,16(x1)\nfsd\nf16,8(x1)\naddi\nx1,x1,32\nbne\nx1,x2,Loop\nThe execution time of the unrolled loop has dropped to a total of 14 clock\ncycles, or 3.5 clock cycles per element, compared with 8 cycles per element before\nany unrolling or scheduling and 6.5 cycles when unrolled but not scheduled.\n180\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 213,
        "text": "The gain from scheduling on the unrolled loop is even larger than on the original\nloop. This increase arises because unrolling the loop exposes more computation\nthat can be scheduled to minimize the stalls; the preceding code has no stalls.\nScheduling the loop in this fashion necessitates realizing that the loads and stores\nare independent and can be interchanged.\nSummary of the Loop Unrolling and Scheduling\nThroughout this chapter and Appendix H, we will look at a variety of hardware and\nsoftware techniques that allow us to take advantage of instruction-level parallelism\nto fully utilize the potential of the functional units in a processor. The key to most\nof these techniques is to know when and how the ordering among instructions may\nbe changed. In our example, we made many such changes, which to us, as human\nbeings, were obviously allowable. In practice, this process must be performed in a\nmethodical fashion either by a compiler or by hardware. To obtain the final\nunrolled code, we had to make the following decisions and transformations:\n\u25a0\nDetermine that unrolling the loop would be useful by finding that the loop iter-\nations were independent, except for the loop maintenance code.\n\u25a0\nUse different registers to avoid unnecessary constraints that would be forced by\nusing the same registers for different computations (e.g., name dependences).\n\u25a0\nEliminate the extra test and branch instructions and adjust the loop termination\nand iteration code.\n\u25a0\nDetermine that the loads and stores in the unrolled loop can be interchanged by\nobserving that the loads and stores from different iterations are independent.\nThis transformation requires analyzing the memory addresses and finding that\nthey do not refer to the same address.\n\u25a0\nSchedule the code, preserving any dependences needed to yield the same result\nas the original code.\nThe key requirement underlying all of these transformations is an understanding of\nhow one instruction depends on another and how the instructions can be changed\nor reordered given the dependences.\nThree different effects limit the gains from loop unrolling: (1) a decrease in the\namount of overhead amortized with each unroll, (2) code size limitations, and\n(3) compiler limitations. Let\u2019s consider the question of loop overhead first. When\nwe unrolled the loop four times, it generated sufficient parallelism among the\ninstructions that the loop could be scheduled with no stall cycles. In fact, in 14\nclock cycles, only 2 cycles were loop overhead: the addi, which maintains the\nindex value, and the bne, which terminates the loop. If the loop is unrolled eight\ntimes, the overhead is reduced from 1/2 cycle per element to 1/4.\nA second limit to unrolling is the resulting growth in code size. For larger\nloops, the code size growth may be a concern, particularly if it causes an increase\nin the instruction cache miss rate.\n3.2\nBasic Compiler Techniques for Exposing ILP\n\u25a0\n181"
    },
    {
        "page": 214,
        "text": "Another factor often more important than code size is the potential shortfall in\nregisters that is created by aggressive unrolling and scheduling. This secondary\neffect that results from instruction scheduling in large code segments is called reg-\nister pressure. It arises because scheduling code to increase ILP causes the number\nof live values to increase. After aggressive instruction scheduling, it may not be\npossible to allocate all the live values to registers. The transformed code, while the-\noretically faster, may lose some or all of its advantage because it leads to a shortage\nof registers. Without unrolling, aggressive scheduling is sufficiently limited by\nbranches so that register pressure is rarely a problem. The combination of unrolling\nand aggressive scheduling can, however, cause this problem. The problem becomes\nespecially challenging in multiple-issue processors that require the exposure of\nmore independent instruction sequences whose execution can be overlapped.\nIn general, the use of sophisticated high-level transformations, whose potential\nimprovements are difficult to measure before detailed code generation, has led to\nsignificant increases in the complexity of modern compilers.\nLoop unrolling is a simple but useful method for increasing the size of straight-\nline code fragments that can be scheduled effectively. This transformation is useful\nin a variety of processors, from simple pipelines like those we have examined so far\nto the multiple-issue superscalars and VLIWs explored later in this chapter.\n3.3\nReducing Branch Costs With Advanced Branch\nPrediction\nBecause of the need to enforce control dependences through branch hazards and\nstalls, branches will hurt pipeline performance. Loop unrolling is one way to\nreduce the number of branch hazards; we can also reduce the performance losses\nof branches by predicting how they will behave. In Appendix C, we examine sim-\nple branch predictors that rely either on compile-time information or on the\nobserved dynamic behavior of a single branch in isolation. As the number of\ninstructions in flight has increased with deeper pipelines and more issues per clock,\nthe importance of more accurate branch prediction has grown. In this section, we\nexamine techniques for improving dynamic prediction accuracy. This section\nmakes extensive use of the simple 2-bit predictor covered in Section C.2, and\nit is critical that the reader understand the operation of that predictor before\nproceeding.\nCorrelating Branch Predictors\nThe 2-bit predictor schemes in Appendix C use only the recent behavior of a single\nbranch to predict the future behavior of that branch. It may be possible to improve\nthe prediction accuracy if we also look at the recent behavior of other branches\nrather than just the branch we are trying to predict. Consider a small code fragment\nfrom the eqntott benchmark, a member of early SPEC benchmark suites that dis-\nplayed particularly bad branch prediction behavior:\n182\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 215,
        "text": "if (aa==2)\naa=0;\nif (bb==2)\nbb=0;\nif (aa!=bb) {\nHere is the RISC-V code that we would typically generate for this code frag-\nment assuming that aa and bb are assigned to registers x1 and x2:\naddi\nx3,x1,\u20132\nbnez\nx3,L1\n//branch b1\n(aa!=2)\nadd\nx1,x0,x0\n//aa=0\nL1:\naddi\nx3,x2,\u20132\nbnez\nx3,L2\n//branch b2\n(bb!=2)\nadd\nx2,x0,x0\n//bb=0\nL2:\nsub\nx3,x1,x2\n//x3=aa-bb\nbeqz\nx3,L3\n//branch b3\n(aa==bb)\nLet\u2019s label these branches b1, b2, and b3. The key observation is that the behavior\nof branch b3 is correlated with the behavior of branches b1 and b2. Clearly, if nei-\nther branches b1 nor b2 are taken (i.e., if the conditions both evaluate to true and aa\nand bb are both assigned 0), then b3 will be taken, because aa and bb are clearly\nequal. A predictor that uses the behavior of only a single branch to predict the out-\ncome of that branch can never capture this behavior.\nBranch predictors that use the behavior of other branches to make a prediction\nare called correlating predictors or two-level predictors. Existing correlating pre-\ndictors add information about the behavior of the most recent branches to decide\nhow to predict a given branch. For example, a (1,2) predictor uses the behavior of\nthe last branch to choose from among a pair of 2-bit branch predictors in predicting\na particular branch. In the general case, an (m,n) predictor uses the behavior of the\nlast m branches to choose from 2m branch predictors, each of which is an n-bit pre-\ndictor for a single branch. The attraction of this type of correlating branch predictor\nis that it can yield higher prediction rates than the 2-bit scheme and requires only a\ntrivial amount of additional hardware.\nThe simplicity of the hardware comes from a simple observation: the global\nhistory of the most recent m branches can be recorded in an m-bit shift register,\nwhere each bit records whether the branch was taken or not taken. The branch-\nprediction buffer can then be indexed using a concatenation of the low-order bits\nfrom the branch address with the m-bit global history. For example, in a (2,2) buffer\nwith 64 total entries, the 4 low-order address bits of the branch (word address)\nand the 2 global bits representing the behavior of the two most recently executed\nbranches form a 6-bit index that can be used to index the 64 counters. By combin-\ning the local and global information by concatenation (or a simple hash function),\nwe can index the predictor table with the result and get a prediction as fast as we\ncould for the standard 2-bit predictor, as we will do very shortly.\n3.3\nReducing Branch Costs With Advanced Branch Prediction\n\u25a0\n183"
    },
    {
        "page": 216,
        "text": "How much better do the correlating branch predictors work when compared\nwith the standard 2-bit scheme? To compare them fairly, we must compare\npredictors that use the same number of state bits. The number of bits in an\n(m,n) predictor is\n2m \u0003 n\u0003 Number of prediction entriesselected by the branchaddress\nA 2-bit predictor with no global history is simply a (0,2) predictor.\nExample\nHow many bits are in the (0,2) branch predictor with 4K entries? How many entries\nare in a (2,2) predictor with the same number of bits?\nAnswer\nThe predictor with 4K entries has\n20 \u00032\u00034K \u00bc 8Kbits\nHow many branch-selected entries are in a (2,2) predictor that has a total of 8K bits\nin the prediction buffer? We know that\n22 \u00032\u0003Number of prediction entries selected by the branch \u00bc 8K\nTherefore the number of prediction entries selected by the branch\u00bc1K.\nFigure 3.3 compares the misprediction rates of the earlier (0,2) predictor with 4K\nentries and a (2,2) predictor with 1K entries. As you can see, this correlating pre-\ndictor not only outperforms a simple 2-bit predictor with the same total number of\nstate bits, but it also often outperforms a 2-bit predictor with an unlimited number\nof entries.\nPerhaps the best-known example of a correlating predictor is McFarling\u2019s\ngshare predictor. In gshare the index is formed by combining the address of the\nbranch and the most recent conditional branch outcomes using an exclusive-\nOR, which essentially acts as a hash of the branch address and the branch history.\nThe hashed result is used to index a prediction array of 2-bit counters, as shown in\nFigure 3.4. The gshare predictor works remarkably well for a simple predictor, and\nis often used as the baseline for comparison with more sophisticated predictors.\nPredictors that combine local branch information and global branch history are also\ncalled alloyed predictors or hybrid predictors.\nTournament Predictors: Adaptively Combining Local and\nGlobal Predictors\nThe primary motivation for correlating branch predictors came from the observa-\ntion that the standard 2-bit predictor, using only local information, failed on some\nimportant branches. Adding global history could help remedy this situation.\nTournament predictors take this insight to the next level, by using multiple predic-\ntors, usually a global predictor and a local predictor, and choosing between them\n184\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 217,
        "text": "with a selector, as shown in Figure 3.5. A global predictor uses the most recent\nbranch history to index the predictor, while a local predictor uses the address\nof the branch as the index. Tournament predictors are another form of hybrid or\nalloyed predictors.\nTournament predictors can achieve better accuracy at medium sizes (8K\u201332K\nbits) and also effectively use very large numbers of prediction bits. Existing tour-\nnament predictors use a 2-bit saturating counter per branch to choose among two\ndifferent predictors based on which predictor (local, global, or even some time-\nvarying mix) was most effective in recent predictions. As in a simple 2-bit predic-\ntor, the saturating counter requires two mispredictions before changing the identity\nof the preferred predictor.\nThe advantage of a tournament predictor is its ability to select the right predic-\ntor for a particular branch, which is particularly crucial for the integer benchmarks.\nnASA7\nmatrix300\ntomcatv\ndoduc\nSPEC89 benchmarks\nspice\nfpppp\ngcc\nespresso\neqntott\nli\n0%\n2%\n4%\n6%\n8% 10% 12% 14% 16% 18%\nFrequency of mispredictions\n1%\n0%\n1%\n0%\n0%\n0%\n1%\n0%\n1%\n5%\n5%\n5%\n9%\n9%\n5%\n9%\n9%\n5%\n12%\n11%\n11%\n5%\n5%\n4%\n18%\n18%\n6%\n10%\n10%\n5%\n1024 entries:\n(2,2)\nUnlimited entries:\n2 bits per entry\n4096 entries:\n2 bits per entry\nFigure 3.3 Comparison of 2-bit predictors. A noncorrelating predictor for 4096 bits is first, followed by a noncor-\nrelating 2-bit predictor with unlimited entries and a 2-bit predictor with 2 bits of global history and a total of 1024\nentries. Although these data are for an older version of SPEC, data for more recent SPEC benchmarks would show\nsimilar differences in accuracy.\n3.3\nReducing Branch Costs With Advanced Branch Prediction\n\u25a0\n185"
    },
    {
        "page": 218,
        "text": "Branch history\nBranch address\nPrediction\n10-bit shift register\nMost recent branch\nresult (not taken/taken)\nExclusive\nOR\n1024 2-bit predictors\n10\n10\n10\nFigure 3.4 A gshare predictor with 1024 entries, each being a standard 2-bit predictor.\nBranch history\nPrediction\nm\nu\nx\nGlobal predictors\nBranch address\nSelector\nLocal predictors\nFigure 3.5 A tournament predictor using the branch address to index a set of 2-bit selection counters, which\nchoose between a local and a global predictor. In this case, the index to the selector table is the current branch\naddress. The two tables are also 2-bit predictors that are indexed by the global history and branch address, respec-\ntively. The selector acts like a 2-bit predictor, changing the preferred predictor for a branch address when two mis-\npredicts occur in a row. The number of bits of the branch address used to index the selector table and the local\npredictor table is equal to the length of the global branch history used to index the global prediction table. Note that\nmisprediction is a bit tricky because we need to change both the selector table and either the global or local predictor.\n186\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 219,
        "text": "A typical tournament predictor will select the global predictor almost 40% of the\ntime for the SPEC integer benchmarks and less than 15% of the time for the SPEC\nFP benchmarks. In addition to the Alpha processors that pioneered tournament pre-\ndictors, several AMD processors have used tournament-style predictors.\nFigure 3.6 looks at the performance of three different predictors (a local 2-bit\npredictor, a correlating predictor, and a tournament predictor) for different num-\nbers of bits using SPEC89 as the benchmark. The local predictor reaches its limit\nfirst. The correlating predictor shows a significant improvement, and the tourna-\nment predictor generates a slightly better performance. For more recent versions\nof the SPEC, the results would be similar, but the asymptotic behavior would\nnot be reached until slightly larger predictor sizes.\nThe local predictor consists of a two-level predictor. The top level is a local\nhistory table consisting of 1024 10-bit entries; each 10-bit entry corresponds to\nthe most recent 10 branch outcomes for the entry. That is, if the branch is taken\n10 or more times in a row, the entry in the local history table will be all 1s. If\nthe branch is alternately taken and untaken, the history entry consists of alternating\n0s and 1s. This 10-bit history allows patterns of up to 10 branches to be discovered\nand predicted. The selected entry from the local history table is used to index a table\nof 1K entries consisting of 3-bit saturating counters, which provide the local pre-\ndiction. This combination, which uses a total of 29K bits, leads to high accuracy in\n6%\n7%\n8%\n5%\n4%\n3%\n2%\n1%\n0%\nConditional branch misprediction rate\nTotal predictor size\nLocal 2-bit predictors\nCorrelating predictors\nTournament predictors\n512\n480\n448\n416\n384\n352\n320\n288\n256\n224\n192\n160\n128\n96\n64\n32\n0\nFigure 3.6 The misprediction rate for three different predictors on SPEC89 versus the size of the predictor in\nkilobits. The predictors are a local 2-bit predictor, a correlating predictor that is optimally structured in its use of\nglobal and local information at each point in the graph, and a tournament predictor. Although these data are\nfor an older version of SPEC, data for more recent SPEC benchmarks show similar behavior, perhaps converging\nto the asymptotic limit at slightly larger predictor sizes.\n3.3\nReducing Branch Costs With Advanced Branch Prediction\n\u25a0\n187"
    },
    {
        "page": 220,
        "text": "branch prediction while requiring fewer bits than a single level table with the same\nprediction accuracy.\nTagged Hybrid Predictors\nThe best performing branch prediction schemes as of 2017 involve combining\nmultiple predictors that track whether a prediction is likely to be associated with\nthe current branch. One important class of predictors is loosely based on an algo-\nrithm for statistical compression called PPM (Prediction by Partial Matching).\nPPM (see Jim\u0001enez and Lin, 2001), like a branch prediction algorithm, attempts\nto predict future behavior based on history. This class of branch predictors, which\nwe call tagged hybrid predictors (see Seznec and Michaud, 2006), employs a\nseries of global predictors indexed with different length histories.\nFor example, as shown in Figure 3.7, a five-component tagged hybrid predictor\nhas five prediction tables: P(0), P(1), . . . P(4), where P(i) is accessed using a hash of\nBase predictor\npred\nP(0)\nP(1)\ntag\n=?\nhash\npc\npc\nh[0:L(1)]\nhash\npred\nP(2)\ntag\n=?\nhash\npc\nh[0:L(2)]\nhash\npred\nP(3)\ntag\n=?\nhash\npc\nh[0:L(3)]\nhash\npred\nPrediction\nP(4)\ntag\n=?\nhash\npc\nh[0:L(4)]\nhash\nFigure 3.7 A five-component tagged hybrid predictor has five separate prediction tables, indexed by a hash of\nthe branch address and a segment of recent branch history of length 0\u20134 labeled \u201ch\u201d in this figure. The hash can\nbe as simple as an exclusive-OR, as in gshare. Each predictor is a 2-bit (or possibly 3-bit) predictor. The tags are\ntypically 4\u20138 bits. The chosen prediction is the one with the longest history where the tags also match.\n188\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 221,
        "text": "the PC and the history of the most recent i branches (kept in a shift register, h, just\nas in gshare). The use of multiple history lengths to index separate predictors is the\nfirst critical difference. The second critical difference is the use of tags in tables\nP(1) through P(4). The tags can be short because 100% matches are not required:\na small tag of 4\u20138 bits appears to gain most of the advantage. A prediction from\nP(1), . . . P(4) is used only if the tags match the hash of the branch address and\nglobal branch history. Each of the predictors in P(0\u2026n) can be a standard 2-bit\npredictor. In practice a 3-bit counter, which requires three mispredictions to change\na prediction, gives slightly better results than a 2-bit counter.\nThe prediction for a given branch is the predictor with the longest branch his-\ntory that also has matching tags. P(0) always matches because it uses no tags and\nbecomes the default prediction if none of P(1) through P(n) match. The tagged\nhybrid version of this predictor also includes a 2-bit use field in each of the\nhistory-indexed predictors. The use field indicates whether a prediction was\nrecently used and therefore likely to be more accurate; the use field can be period-\nically reset in all entries so that old predictions are cleared. Many more details are\ninvolved in implementing this style of predictor, especially how to handle mispre-\ndictions. The search space for the optimal predictor is also very large because the\nnumber of predictors, the exact history used for indexing, and the size of each pre-\ndictor are all variable.\nTagged hybrid predictors (sometimes called TAGE\u2014TAgged GEometic\u2014\npredictors) and the earlier PPM-based predictors have been the winners in recent\nannual international branch-prediction competitions. Such predictors outperform\ngshare and the tournament predictors with modest amounts of memory (32\u2013\n64 KiB), and in addition, this class of predictors seems able to effectively use larger\nprediction caches to deliver improved prediction accuracy.\nAnother issue for larger predictors is how to initialize the predictor. It could be\ninitialized randomly, in which case, it will take a fair amount of execution time to\nfill the predictor with useful predictions. Some predictors (including many recent\npredictors) include a valid bit, indicating whether an entry in the predictor has been\nset or is in the \u201cunused state.\u201d In the latter case, rather than use a random prediction,\nwe could use some method to initialize that prediction entry. For example, some\ninstruction sets contain a bit that indicates whether an associated branch is expected\nto be taken or not. In the days before dynamic branch prediction, such hint bits\nwere the prediction; in recent processors, that hint bit can be used to set the initial\nprediction. We could also set the initial prediction on the basis of the branch direc-\ntion: forward going branches are initialized as not taken, while backward going\nbranches, which are likely to be loop branches, are initialized as taken. For pro-\ngrams with shorter running times and processors with larger predictors, this initial\nsetting can have a measurable impact on prediction performance.\nFigure 3.8 shows that a hybrid tagged predictor significantly outperforms\ngshare, especially for the less predictable programs like SPECint and server appli-\ncations. In this figure, performance is measured as mispredicts per thousand\ninstructions; assuming a branch frequency of 20%\u201325%, gshare has a mispredict\nrate (per branch) of 2.7%\u20133.4% for the multimedia benchmarks, while the tagged\n3.3\nReducing Branch Costs With Advanced Branch Prediction\n\u25a0\n189"
    },
    {
        "page": 222,
        "text": "hybrid predictor has a misprediction rate of 1.8%\u20132.2%, or roughly one-third fewer\nmispredicts. Compared to gshare, tagged hybrid predictors are more complex to\nimplement and are probably slightly slower because of the need to check multiple\ntags and choose a prediction result. Nonetheless, for deeply pipelined processors\nwith large penalties for branch misprediction, the increased accuracy outweighs\nthose disadvantages. Thus many designers of higher-end processors have opted\nto include tagged hybrid predictors in their newest implementations.\nThe Evolution of the Intel Core i7 Branch Predictor\nAs mentioned in the previous chapter, there were six generations of Intel Core i7\nprocessors between 2008 (Core i7 920 using the Nehalem microarchitecture) and\n2016 (Core i7 6700 using the Skylake microarchitecture). Because of the combi-\nnation of deep pipelining and multiple issues per clock, the i7 has many instruc-\ntions in-flight at once (up to 256, and typically at least 30). This makes branch\nprediction critical, and it has been an area where Intel has been making constant\nimprovements. Perhaps because of the performance-critical nature of the branch\npredictor, Intel has tended to keep the details of its branch predictors highly secret.\n0\nSPECfp\n1.301\n1\n2\n3\n4\nMisses per one thousand instructions\n5\n6\n7\n8\nSPECint\n3.299\n6.607\nMultiMedia\n4.45\n6.778\nServer\n2.939\n6.52\nTAGE\ngshare\n0.59\nFigure 3.8 A comparison of the misprediction rate (measured as mispredicts per 1000 instructions executed) for\ntagged hybrid versus gshare. Both predictors use the same total number of bits, although tagged hybrid uses some\nof that storage for tags, while gshare contains no tags. The benchmarks consist of traces from SPECfp and SPECint, a\nseries of multimedia and server benchmarks. The latter two behave more like SPECint.\n190\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 223,
        "text": "Even for older processors such as the Core i7 920 introduced in 2008, they have\nreleased only limited amounts of information. In this section, we briefly describe\nwhat is known and compare the performance of predictors of the Core i7 920 with\nthose in the latest Core i7 6700.\nThe Core i7 920 used a two-level predictor that has a smaller first-level predictor,\ndesigned to meet the cycle constraints of predicting a branch every clock cycle, and a\nlarger second-level predictor as a backup. Each predictor combines three different\npredictors: (1) the simple 2-bit predictor, which is introduced in Appendix C (and\nused in the preceding tournament predictor); (2) a global history predictor, like those\nwe just saw; and (3) a loop exit predictor. The loop exit predictor uses a counter\nto predict the exact number of taken branches (which is the number of loop itera-\ntions) for a branch that is detected as a loop branch. For each branch, the best pre-\ndiction is chosen from among the three predictors by tracking the accuracy of each\nprediction, like a tournament predictor. In addition to this multilevel main predictor,\na separate unit predicts target addresses for indirect branches, and a stack to predict\nreturn addresses is also used.\nAlthough even less is known about the predictors in the newest i7 processors,\nthere is good reason to believe that Intel is employing a tagged hybrid predictor.\nOne advantage of such a predictor is that it combines the functions of all three\nsecond-level predictors in the earlier i7. The tagged hybrid predictor with different\nhistory lengths subsumes the loop exit predictor as well as the local and global his-\ntory predictor. A separate return address predictor is still employed.\nAs in other cases, speculation causes some challenges in evaluating the predic-\ntor because a mispredicted branch can easily lead to another branch being fetched\nand mispredicted. To keep things simple, we look at the number of mispredictions\nas a percentage of the number of successfully completed branches (those that\nwere not the result of misspeculation). Figure 3.9 shows these data for SPEC-\nPUint2006 benchmarks. These benchmarks are considerably larger than SPEC89\nor SPEC2000, with the result being that the misprediction rates are higher than\nthose in Figure 3.6 even with a more powerful combination of predictors. Because\nbranch misprediction leads to ineffective speculation, it contributes to the wasted\nwork, as we will see later in this chapter.\n3.4\nOvercoming Data Hazards With Dynamic Scheduling\nA simple statically scheduled pipeline fetches an instruction and issues it, unless\nthere is a data dependence between an instruction already in the pipeline and the\nfetched instruction that cannot be hidden with bypassing or forwarding. (Forward-\ning logic reduces the effective pipeline latency so that the certain dependences do\nnot result in hazards.) If there is a data dependence that cannot be hidden, then the\nhazard detection hardware stalls the pipeline starting with the instruction that uses\nthe result. No new instructions are fetched or issued until the dependence is cleared.\nIn this section, we explore dynamic scheduling, a technique by which the hard-\nware reorders the instruction execution to reduce the stalls while maintaining data\n3.4\nOvercoming Data Hazards With Dynamic Scheduling\n\u25a0\n191"
    },
    {
        "page": 224,
        "text": "flow and exception behavior. Dynamic scheduling offers several advantages. First,\nit allows code that was compiled with one pipeline in mind to run efficiently on a\ndifferent pipeline, eliminating the need to have multiple binaries and recompile for a\ndifferent microarchitecture. In today\u2019s computing environment, where much of the\nsoftware is from third parties and distributed in binary form, this advantage is sig-\nnificant. Second, it enables handling some cases when dependences are unknown at\ncompile time; for example, they may involve a memory reference or a data-\ndependent branch, or they may result from a modern programming environment\nthat uses dynamic linking or dispatching. Third, and perhaps most importantly,\nit allows the processor to tolerate unpredictable delays, such as cache misses, by\nexecuting other code while waiting for the miss to resolve. In Section 3.6, we\nexplore hardware speculation, a technique with additional performance advantages,\nwhich builds on dynamic scheduling. As we will see, the advantages of dynamic\nscheduling are gained at the cost of a significant increase in hardware complexity.\nAlthough a dynamically scheduled processor cannot change the data flow, it\ntries to avoid stalling when dependences are present. In contrast, static pipeline\nscheduling by the compiler (covered in Section 3.2) tries to minimize stalls by sep-\narating dependent instructions so that they will not lead to hazards. Of course,\n0.0%\nastar\nbzip2\ngcc\ngobmk\nh264ref\nhmmer\nlibquantum\nmcf\nomnetpp\nperlbench\nsjeng\nxalancbmk\n1.0%\n2.0%\n3.0%\n4.0%\nBranch misprediction rate\n5.0%\n6.0%\n7.0%\ni7 6700\ni7 920\n8.0%\n9.0%\nFigure 3.9 The misprediction rate for the integer SPECCPU2006 benchmarks on the Intel Core i7 920 and 6700.\nThe misprediction rate is computed as the ratio of completed branches that are mispredicted versus all completed\nbranches. This could understate the misprediction rate somewhat because if a branch is mispredicted and led to\nanother mispredicted branch (which should not have been executed), it will be counted as only one misprediction.\nOn average, the i7 920 mispredicts branches 1.3 times as often as the i7 6700.\n192\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 225,
        "text": "compiler pipeline scheduling can also be used in code destined to run on a proces-\nsor with a dynamically scheduled pipeline.\nDynamic Scheduling: The Idea\nA major limitation of simple pipelining techniques is that they use in-order instruc-\ntion issue and execution: instructions are issued in program order, and if an instruc-\ntion is stalled in the pipeline, no later instructions can proceed. Thus, if there is a\ndependence between two closely spaced instructions in the pipeline, it will lead to\na hazard, and a stall will result. If there are multiple functional units, these units\ncould lie idle. If instruction j depends on a long-running instruction i, currently\nin execution in the pipeline, then all instructions after j must be stalled until i is\nfinished and j can execute. For example, consider this code:\nfdiv.d\nf0,f2,f4\nfadd.d\nf10,f0,f8\nfsub.d\nf12,f8,f14\nThe fsub.d instruction cannot execute because the dependence of fadd.d on\nfdiv.d causes the pipeline to stall; yet, fsub.d is not data-dependent on any-\nthing in the pipeline. This hazard creates a performance limitation that can be elim-\ninated by not requiring instructions to execute in program order.\nIn the classic five-stage pipeline, both structural and data hazards could be\nchecked during instruction decode (ID): when an instruction could execute without\nhazards, it was issued from ID, with the recognition that all data hazards had been\nresolved.\nTo allow us to begin executing the fsub.d in the preceding example, we must\nseparate the issue process into two parts: checking for any structural hazards and\nwaiting for the absence of a data hazard. Thus we still use in-order instruction issue\n(i.e., instructions issued in program order), but we want an instruction to begin exe-\ncution as soon as its data operands are available. Such a pipeline does out-of-order\nexecution, which implies out-of-order completion.\nOut-of-order execution introduces the possibility of WAR and WAW hazards,\nwhich do not exist in the five-stage integer pipeline and its logical extension to an\nin-order floating-point pipeline. Consider the following RISC-V floating-point\ncode sequence:\nfdiv.d\nf0,f2,f4\nfmul.d\nf6,f0,f8\nfadd.d\nf0,f10,f14\nThere is an antidependence between the fmul.d and the fadd.d (for the register\nf0), and if the pipeline executes the fadd.d before the fmul.d (which is wait-\ning for the fdiv.d), it will violate the antidependence, yielding a WAR hazard.\nLikewise, to avoid violating output dependences, such as the write of f0 by\nfadd.d before fdiv.d completes, WAW hazards must be handled. As we will\nsee, both these hazards are avoided by the use of register renaming.\n3.4\nOvercoming Data Hazards With Dynamic Scheduling\n\u25a0\n193"
    },
    {
        "page": 226,
        "text": "Out-of-order completion also creates major complications in handling excep-\ntions. Dynamic scheduling with out-of-order completion must preserve exception\nbehavior in the sense that exactly those exceptions that would arise if the program\nwere executed in strict program order actually do arise. Dynamically scheduled\nprocessors preserve exception behavior by delaying the notification of an associ-\nated exception until the processor knows that the instruction should be the next one\ncompleted.\nAlthough exception behavior must be preserved, dynamically scheduled pro-\ncessors could generate imprecise exceptions. An exception is imprecise if the\nprocessor state when an exception is raised does not look exactly as if the instruc-\ntions were executed sequentially in strict program order. Imprecise exceptions can\noccur because of two possibilities:\n1. The pipeline may have already completed instructions that are later in program\norder than the instruction causing the exception.\n2. The pipeline may have not yet completed some instructions that are earlier in\nprogram order than the instruction causing the exception.\nImprecise exceptions make it difficult to restart execution after an exception.\nRather than address these problems in this section, we will discuss a solution that\nprovides precise exceptions in the context of a processor with speculation in\nSection 3.6. For floating-point exceptions, other solutions have been used, as dis-\ncussed in Appendix J.\nTo allow out-of-order execution, we essentially split the ID pipe stage of our\nsimple five-stage pipeline into two stages:\n1. Issue\u2014Decode instructions, check for structural hazards.\n2. Read operands\u2014Wait until no data hazards, then read operands.\nAn instruction fetch stage precedes the issue stage and may fetch either to an\ninstruction register or into a queue of pending instructions; instructions are then\nissued from the register or queue. The execution stage follows the read operands\nstage, just as in the five-stage pipeline. Execution may take multiple cycles, depend-\ning on the operation.\nWe distinguish when an instruction begins execution and when it completes\nexecution; between the two times, the instruction is in execution. Our pipeline\nallows multiple instructions to be in execution at the same time; without this capa-\nbility, a major advantage of dynamic scheduling is lost. Having multiple instruc-\ntions in execution at once requires multiple functional units, pipelined functional\nunits, or both. Because these two capabilities\u2014pipelined functional units and\nmultiple functional units\u2014are essentially equivalent for the purposes of pipeline\ncontrol, we will assume the processor has multiple functional units.\nIn a dynamically scheduled pipeline, all instructions pass through the issue\nstage in order (in-order issue); however, they can be stalled or can bypass each\n194\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 227,
        "text": "other in the second stage (read operands) and thus enter execution out of order.\nScoreboarding is a technique for allowing instructions to execute out of order when\nthere are sufficient resources and no data dependences; it is named after the CDC\n6600 scoreboard, which developed this capability. Here we focus on a more sophis-\nticated technique, called Tomasulo\u2019s algorithm. The primary difference is that\nTomasulo\u2019s algorithm handles antidependences and output dependences by effec-\ntively renaming the registers dynamically. Additionally, Tomasulo\u2019s algorithm\ncan be extended to handle speculation, a technique to reduce the effect of control\ndependences by predicting the outcome of a branch, executing instructions at the\npredicted destination address, and taking corrective actions when the prediction\nwas wrong. While the use of scoreboarding is probably sufficient to support sim-\npler processors, more sophisticated, higher performance processors make use of\nspeculation.\nDynamic Scheduling Using Tomasulo\u2019s Approach\nThe IBM 360/91 floating-point unit used a sophisticated scheme to allow out-of-\norder execution. This scheme, invented by Robert Tomasulo, tracks when oper-\nands for instructions are available to minimize RAW hazards and introduces reg-\nister renaming in hardware to minimize WAW and WAR hazards. Although there\nare many variations of this scheme in recent processors, they all rely on two key\nprinciples: dynamically determining when an instruction is ready to execute and\nrenaming registers to avoid unnecessary hazards.\nIBM\u2019s goal was to achieve high floating-point performance from an instruction\nset and from compilers designed for the entire 360 computer family, rather than\nfrom specialized compilers for the high-end processors. The 360 architecture\nhad only four double-precision floating-point registers, which limited the effective-\nness of compiler scheduling; this fact was another motivation for the Tomasulo\napproach. In addition, the IBM 360/91 had long memory accesses and long\nfloating-point delays, which Tomasulo\u2019s algorithm was designed to overcome.\nAt the end of the section, we will see that Tomasulo\u2019s algorithm can also support\nthe overlapped execution of multiple iterations of a loop.\nWe explain the algorithm, which focuses on the floating-point unit and load-\nstore unit, in the context of the RISC-V instruction set. The primary difference\nbetween RISC-V and the 360 is the presence of register-memory instructions in\nthe latter architecture. Because Tomasulo\u2019s algorithm uses a load functional unit,\nno significant changes are needed to add register-memory addressing modes. The\nIBM 360/91 also had pipelined functional units, rather than multiple functional\nunits, but we describe the algorithm as if there were multiple functional units. It\nis a simple conceptual extension to also pipeline those functional units.\nRAW hazards are avoided by executing an instruction only when its operands\nare available, which is exactly what the simpler scoreboarding approach provides.\nWAR and WAW hazards, which arise from name dependences, are eliminated by\nregister renaming. Register renaming eliminates these hazards by renaming all\n3.4\nOvercoming Data Hazards With Dynamic Scheduling\n\u25a0\n195"
    },
    {
        "page": 228,
        "text": "destination registers, including those with a pending read or write for an earlier\ninstruction, so that the out-of-order write does not affect any instructions that\ndepend on an earlier value of an operand. The compiler could typically implement\nsuch renaming, if there were enough registers available in the ISA. The original\n360/91 had only four floating-point registers, and Tomasulo\u2019s algorithm was cre-\nated to overcome this shortage. Whereas modern processors have 32\u201364 floating-\npoint and integer registers, the number of renaming registers available in recent\nimplementations is in the hundreds.\nTo better understand how register renaming eliminates WAR and WAW haz-\nards, consider the following example code sequence that includes potential WAR\nand WAW hazards:\nfdiv.d\nf0,f2,f4\nfadd.d\nf6,f0,f8\nfsd\nf6,0(x1)\nfsub.d\nf8,f10,f14\nfmul.d\nf6,f10,f8\nThere are two antidependences: between the fadd.d and the fsub.d and\nbetween the fsd and the fmul.d. There is also an output dependence between\nthe fadd.d and the fmul.d, leading to three possible hazards: WAR hazards on\nthe use of f8 by fadd.d and its use by the fsub.d, as well as a WAW hazard\nbecause the fadd.d may finish later than the fmul.d. There are also three true\ndata dependences: between the fdiv.d and the fadd.d, between the fsub.d\nand the fmul.d, and between the fadd.d and the fsd.\nThese three name dependences can all be eliminated by register renaming. For\nsimplicity, assume the existence of two temporary registers, S and T. Using S and\nT, the sequence can be rewritten without any dependences as\nfdiv.d\nf0,f2,f4\nfadd.d\nS,f0,f8\nfsd\nS,0(x1)\nfsub.d\nT,f10,f14\nfmul.d\nf6,f10,T\nIn addition, any subsequent uses of f8 must be replaced by the register T. In this\nexample, the renaming process can be done statically by the compiler. Finding any\nuses of f8 that are later in the code requires either sophisticated compiler analysis\nor hardware support because there may be intervening branches between the pre-\nceding code segment and a later use of f8. As we will see, Tomasulo\u2019s algorithm\ncan handle renaming across branches.\nIn Tomasulo\u2019s scheme, register renaming is provided by reservation stations,\nwhich buffer the operands of instructions waiting to issue and are associated with\nthe functional units. The basic idea is that a reservation station fetches and buffers\nan operand as soon as it is available, eliminating the need to get the operand from\na register. In addition, pending instructions designate the reservation station that\nwill provide their input. Finally, when successive writes to a register overlap in\n196\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 229,
        "text": "execution, only the last one is actually used to update the register. As instructions\nare issued, the register specifiers for pending operands are renamed to the names of\nthe reservation station, which provides register renaming.\nBecause there can be more reservation stations than real registers, the technique\ncan even eliminate hazards arising from name dependences that could not be elim-\ninated by a compiler. As we explore the components of Tomasulo\u2019s scheme, we\nwill return to the topic of register renaming and see exactly how the renaming\noccurs and how it eliminates WAR and WAW hazards.\nThe use of reservation stations, rather than a centralized register file, leads to\ntwo other important properties. First, hazard detection and execution control are\ndistributed: the information held in the reservation stations at each functional unit\ndetermines when an instruction can begin execution at that unit. Second, results are\npassed directly to functional units from the reservation stations where they are\nbuffered, rather than going through the registers. This bypassing is done with a\ncommon result bus that allows all units waiting for an operand to be loaded simul-\ntaneously (on the 360/91, this is called the common data bus, or CDB). In pipelines\nthat issue multiple instructions per clock and also have multiple execution units,\nmore than one result bus will be needed.\nFigure 3.10 shows the basic structure of a Tomasulo-based processor, includ-\ning both the floating-point unit and the load/store unit; none of the execution con-\ntrol tables is shown. Each reservation station holds an instruction that has been\nissued and is awaiting execution at a functional unit. If the operand values for that\ninstruction have been computed, they are also stored in that entry; otherwise, the\nreservation station entry keeps the names of the reservation stations that will pro-\nvide the operand values.\nThe load buffers and store buffers hold data or addresses coming from and\ngoing to memory and behave almost exactly like reservation stations, so we dis-\ntinguish them only when necessary. The floating-point registers are connected\nby a pair of buses to the functional units and by a single bus to the store buffers.\nAll results from the functional units and from memory are sent on the common data\nbus, which goes everywhere except to the load buffer. All reservation stations have\ntag fields, employed by the pipeline control.\nBefore we describe the details of the reservation stations and the algorithm,\nlet\u2019s look at the steps an instruction goes through. There are only three steps,\nalthough each one can now take an arbitrary number of clock cycles:\n1. Issue\u2014Get the next instruction from the head of the instruction queue, which is\nmaintained in FIFO order to ensure the maintenance of correct data flow. If there\nis a matching reservation station that is empty, issue the instruction to the station\nwith the operand values, if they are currently in the registers. If there is not an\nempty reservation station, then there is a structural hazard, and the instruction\nissue stalls until a station or buffer is freed. If the operands are not in the reg-\nisters, keep track of the functional units that will produce the operands. This step\nrenames registers, eliminating WAR and WAW hazards. (This stage is some-\ntimes called dispatch in a dynamically scheduled processor.)\n3.4\nOvercoming Data Hazards With Dynamic Scheduling\n\u25a0\n197"
    },
    {
        "page": 230,
        "text": "2. Execute\u2014If one or more of the operands is not yet available, monitor the com-\nmon data bus while waiting for it to be computed. When an operand becomes\navailable, it is placed into any reservation station awaiting it. When all the oper-\nands are available, the operation can be executed at the corresponding functional\nunit. By delaying instruction execution until the operands are available, RAW\nhazards are avoided. (Some dynamically scheduled processors call this step\n\u201cissue,\u201d but we use the name \u201cexecute,\u201d which was used in the first dynamically\nscheduled processor, the CDC 6600.)\nFrom instruction unit\nFloating-point\noperations\nFP registers\nReservation\nstations\nFP adders\nFP multipliers\n3\n2\n1\n2\n1\nCommon data bus (CDB)\nOperation bus\nOperand\nbuses\nLoad/store\noperations\nAddress unit\nLoad buffers\nMemory unit\nAddress\nData\nInstruction\nqueue\nStore buffers\nFigure 3.10 The basic structure of a RISC-V floating-point unit using Tomasulo\u2019s algorithm. Instructions are sent\nfrom the instruction unit into the instruction queue from which they are issued in first-in, first-out (FIFO) order. The\nreservation stations include the operation and the actual operands, as well as information used for detecting and\nresolving hazards. Load buffers have three functions: (1) hold the components of the effective address until it is com-\nputed, (2) track outstanding loads that are waiting on the memory, and (3) hold the results of completed loads that\nare waiting for the CDB. Similarly, store buffers have three functions: (1) hold the components of the effective address\nuntil it is computed, (2) hold the destination memory addresses of outstanding stores that are waiting for the data\nvalue to store, and (3) hold the address and value to store until the memory unit is available. All results from either the\nFP units or the load unit are put on the CDB, which goes to the FP register file as well as to the reservation stations and\nstore buffers. The FP adders implement addition and subtraction, and the FP multipliers do multiplication and\ndivision.\n198\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 231,
        "text": "Notice that several instructions could become ready in the same clock cycle\nfor the same functional unit. Although independent functional units could\nbegin execution in the same clock cycle for different instructions, if more than\none instruction is ready for a single functional unit, the unit will have to choose\namong them. For the floating-point reservation stations, this choice may be made\narbitrarily; loads and stores, however, present an additional complication.\nLoads and stores require a two-step execution process. The first step com-\nputes the effective address when the base register is available, and the effective\naddress is then placed in the load or store buffer. Loads in the load buffer exe-\ncute as soon as the memory unit is available. Stores in the store buffer wait for\nthe value to be stored before being sent to the memory unit. Loads and stores are\nmaintained in program order through the effective address calculation, which\nwill help to prevent hazards through memory.\nTo preserve exception behavior, no instruction is allowed to initiate execu-\ntion until a branch that precedes the instruction in program order has completed.\nThis restriction guarantees that an instruction that causes an exception during\nexecution really would have been executed. In a processor using branch predic-\ntion (as all dynamically scheduled processors do), this means that the processor\nmust know that the branch prediction was correct before allowing an instruction\nafter the branch to begin execution. If the processor records the occurrence of\nthe exception, but does not actually raise it, an instruction can start execution but\nnot stall until it enters Write Result.\nSpeculation provides a more flexible and more complete method to handle\nexceptions, so we will delay making this enhancement and show how specula-\ntion handles this problem later.\n3. Write result\u2014When the result is available, write it on the CDB and from there\ninto the registers and into any reservation stations (including store buffers) wait-\ning for this result. Stores are buffered in the store buffer until both the value to be\nstored and the store address are available; then the result is written as soon as the\nmemory unit is free.\nThe data structures that detect and eliminate hazards are attached to the reserva-\ntion stations, to the register file, and to the load and store buffers with slightly dif-\nferent information attached to different objects. These tags are essentially names for\nan extended set of virtual registers used for renaming. In our example, the tag field is\na 4-bit quantity that denotes one of the five reservation stations or one of the five load\nbuffers. This combination produces the equivalent of 10 registers (5 reservation sta-\ntions+5 load buffers) that can be designated as result registers (as opposed to the\nfour double-precision registers that the 360 architecture contains). In a processor\nwith more real registers, we want renaming to provide an even larger set of virtual\nregisters, often numbering in the hundreds. The tag field describes which reservation\nstation contains the instruction that will produce a result needed as a source operand.\nOnce an instruction has issued and is waiting for a source operand, it refers to\nthe operand by the reservation station number where the instruction that will write\n3.4\nOvercoming Data Hazards With Dynamic Scheduling\n\u25a0\n199"
    },
    {
        "page": 232,
        "text": "the register has been assigned. Unused values, such as zero, indicate that the oper-\nand is already available in the registers. Because there are more reservation stations\nthan actual register numbers, WAW and WAR hazards are eliminated by renaming\nresults using reservation station numbers. Although in Tomasulo\u2019s scheme the res-\nervation stations are used as the extended virtual registers, other approaches could\nuse a register set with additional registers or a structure like the reorder buffer,\nwhich we will see in Section 3.6.\nIn Tomasulo\u2019s scheme, as well as the subsequent methods we look at for sup-\nporting speculation, results are broadcast on a bus (the CDB), which is monitored\nby the reservation stations. The combination of the common result bus and the\nretrieval of results from the bus by the reservation stations implements the forward-\ning and bypassing mechanisms used in a statically scheduled pipeline. In doing so,\nhowever, a dynamically scheduled scheme, such as Tomasulo\u2019s algorithm, intro-\nduces one cycle of latency between source and result because the matching of a\nresult and its use cannot be done until the end of the Write Result stage, as opposed\nto the end of the Execute stage for a simpler pipeline. Thus, in a dynamically sched-\nuled pipeline, the effective latency between a producing instruction and a consum-\ning instruction is at least one cycle longer than the latency of the functional unit\nproducing the result.\nIt is important to remember that the tags in the Tomasulo scheme refer to the\nbuffer or unit that will produce a result; the register names are discarded when an\ninstruction issues to a reservation station. (This is a key difference between Toma-\nsulo\u2019s scheme and scoreboarding: in scoreboarding, operands stay in the registers\nand are read only after the producing instruction completes and the consuming\ninstruction is ready to execute.)\nEach reservation station has seven fields:\n\u25a0\nOp\u2014The operation to perform on source operands S1 and S2.\n\u25a0\nQj, Qk\u2014The reservation stations that will produce the corresponding source\noperand; a value of zero indicates that the source operand is already available\nin Vj or Vk, or is unnecessary.\n\u25a0\nVj, Vk\u2014The value of the source operands. Note that only one of the V fields or\nthe Q field is valid for each operand. For loads, the Vk field is used to hold the\noffset field.\n\u25a0\nA\u2014Used to hold information for the memory address calculation for a load or\nstore. Initially, the immediate field of the instruction is stored here; after the\naddress calculation, the effective address is stored here.\n\u25a0\nBusy\u2014Indicates that this reservation station and its accompanying functional\nunit are occupied.\nThe register file has a field, Qi:\n\u25a0\nQi\u2014The number of the reservation station that contains the operation whose\nresult should be stored into this register. If the value of Qi is blank (or 0), no\n200\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 233,
        "text": "currently active instruction is computing a result destined for this register,\nmeaning that the value is simply the register contents.\nThe load and store buffers each have a field, A, which holds the result of the effec-\ntive address once the first step of execution has been completed.\nIn the next section, we will first consider some examples that show how these\nmechanisms work and then examine the detailed algorithm.\n3.5\nDynamic Scheduling: Examples and the Algorithm\nBefore we examine Tomasulo\u2019s algorithm in detail, let\u2019s consider a few examples\nthat will help illustrate how the algorithm works.\nExample\nShow what the information tables look like for the following code sequence when\nonly the first load has completed and written its result:\n1.\nfld\nf6,32(x2)\n2.\nfld\nf2,44(x3)\n3.\nfmul.d\nf0,f2,f4\n4.\nfsub.d\nf8,f2,f6\n5.\nfdiv.d\nf0,f0,f6\n6.\nfadd.d\nf6,f8,f2\nAnswer\nFigure 3.11 shows the result in three tables. The numbers appended to the names\nAdd, Mult, and Load stand for the tag for that reservation station\u2014Add1 is the tag\nfor the result from the first add unit. In addition, we have included an instruction\nstatus table. This table is included only to help you understand the algorithm; it is\nnot actually a part of the hardware. Instead, the reservation station keeps the state of\neach operation that has issued.\nTomasulo\u2019s scheme offers two major advantages over earlier and simpler\nschemes: (1) the distribution of the hazard detection logic, and (2) the elimination\nof stalls for WAW and WAR hazards.\nThe first advantage arises from the distributed reservation stations and the use\nof the CDB. If multiple instructions are waiting on a single result, and each instruc-\ntion already has its other operand, then the instructions can be released simulta-\nneously by the broadcast of the result on the CDB. If a centralized register file\nwere used, the units would have to read their results from the registers when reg-\nister buses were available.\nThe second advantage, the elimination of WAW and WAR hazards, is accom-\nplished by renaming registers using the reservation stations and by the process of\nstoring operands into the reservation station as soon as they are available.\nFor example, the code sequence in Figure 3.11 issues both the fdiv.d and the\nfadd.d, even though there is a WAR hazard involving f6. The hazard is\n3.5\nDynamic Scheduling: Examples and the Algorithm\n\u25a0\n201"
    },
    {
        "page": 234,
        "text": "eliminated in one of two ways. First, if the instruction providing the value for the\nfdiv.d has completed, then Vk will store the result, allowing fdiv.d to execute\nindependent of the fadd.d (this is the case shown). On the other hand, if the fld\nhasn\u2019t completed, then Qk will point to the Load1 reservation station, and the\nfdiv.d instruction will be independent of the fadd.d. Thus, in either case,\nthe fadd.d can issue and begin executing. Any uses of the result of the\nfdiv.d will point to the reservation station, allowing the fadd.d to complete\nand store its value into the registers without affecting the fdiv.d.\nInstruction status\nInstruction\nIssue\nExecute\nWrite result\nfld\nf6,32(x2)\n\u221a\n\u221a\n\u221a\nfld\nf2,44(x3)\n\u221a\n\u221a\nfmul.d\nf0,f2,f4\n\u221a\nfsub.d\nf8,f2,f6\n\u221a\nfdiv.d\nf0,f0,f6\n\u221a\nfadd.d\nf6,f8,f2\n\u221a\nReservation stations\nName\nBusy\nOp\nVj\nVk\nQj\nQk\nA\nLoad1\nNo\nLoad2\nYes\nLoad\n44 + Regs[x3]\nAdd1\nYes\nSUB\nMem[32 + Regs[x2]]\nLoad2\nAdd2\nYes\nADD\nAdd1\nLoad2\nAdd3\nNo\nMult1\nYes\nMUL\nRegs[f4]\nLoad2\nMult2\nYes\nDIV\nMem[32 + Regs[x2]]\nMult1\nRegister status\nField\nf0\nf2\nf4\nf6\nf8\nf10\nf12\n\u2026\nf30\nQi\nMult1\nLoad2\nAdd2\nAdd1\nMult2\nFigure 3.11 Reservation stations and register tags shown when all of the instructions have issued but only the\nfirst load instruction has completed and written its result to the CDB. The second load has completed effective\naddress calculation but is waiting on the memory unit. We use the array Regs[ ] to refer to the register file and\nthe array Mem[ ] to refer to the memory. Remember that an operand is specified by either a Q field or a V field\nat any time. Notice that the fadd.d instruction, which has a WAR hazard at the WB stage, has issued and could\ncomplete before the fdiv.d initiates.\n202\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 235,
        "text": "We\u2019ll see an example of the elimination of a WAW hazard shortly. But let\u2019s\nfirst look at how our earlier example continues execution. In this example, and\nthe ones that follow in this chapter, assume the following latencies: load is 1 clock\ncycle, add is 2 clock cycles, multiply is 6 clock cycles, and divide is 12 clock\ncycles.\nExample\nUsing the same code segment as in the previous example (page 201), show what\nthe status tables look like when the fmul.d is ready to write its result.\nAnswer\nThe result is shown in the three tables in Figure 3.12. Notice that fadd.d has com-\npleted because the operands of fdiv.d were copied, thereby overcoming the\nWAR hazard. Notice that even if the load of f6 was fdiv.d, the add into f6\ncould be executed without triggering a WAW hazard.\nInstruction status\nInstruction\nIssue\nExecute\nWrite result\nfld\nf6,32(x2)\n\u221a\n\u221a\n\u221a\nfld\nf2,44(x3)\n\u221a\n\u221a\n\u221a\nfmul.d\nf0,f2,f4\n\u221a\n\u221a\nfsub.d\nf8,f2,f6\n\u221a\n\u221a\n\u221a\nfdiv.d\nf0,f0,f6\n\u221a\nfadd.d\nf6,f8,f2\n\u221a\n\u221a\n\u221a\nReservation stations\nName\nBusy\nOp\nVj\nVk\nQj\nQk\nA\nLoad1\nNo\nLoad2\nNo\nAdd1\nNo\nAdd2\nNo\nAdd3\nNo\nMult1\nYes\nMUL\nMem[44 + Regs[x3]]\nRegs[f4]\nMult2\nYes\nDIV\nMem[32 + Regs[x2]]\nMult1\nRegister status\nField\nf0\nf2\nf4\nf6\nf8\nf10\nf12\n\u2026\nf30\nQi\nMult1\nMult2\nFigure 3.12 Multiply and divide are the only instructions not finished.\n3.5\nDynamic Scheduling: Examples and the Algorithm\n\u25a0\n203"
    },
    {
        "page": 236,
        "text": "Tomasulo\u2019s Algorithm: The Details\nFigure 3.13 specifies the checks and steps that each instruction must go through.\nAs mentioned earlier, loads and stores go through a functional unit for effective\naddress computation before proceeding to independent load or store buffers.\nLoads take a second execution step to access memory and then go to Write\nResult to send the value from memory to the register file and/or any waiting\nreservation stations. Stores complete their execution in the Write Result stage,\nwhich writes the result to memory. Notice that all writes occur in Write Result,\nwhether the destination is a register or memory. This restriction simplifies\nTomasulo\u2019s algorithm and is critical to its extension with speculation in\nSection 3.6.\nTomasulo\u2019s Algorithm: A Loop-Based Example\nTo understand the full power of eliminating WAW and WAR hazards through\ndynamic renaming of registers, we must look at a loop. Consider the following sim-\nple sequence for multiplying the elements of an array by a scalar in f2:\nLoop:\nfld\nf0,0(x1)\nfmul.d\nf4,f0,f2\nfsd\nf4,0(x1)\naddi\nx1,x1,8\nbne\nx1,x2,Loop\n// branches if x16\u00bcx2\nIf we predict that branches are taken, using reservation stations will allow\nmultiple executions of this loop to proceed at once. This advantage is gained\nwithout changing the code\u2014in effect, the loop is unrolled dynamically by the hard-\nware using the reservation stations obtained by renaming to act as additional\nregisters.\nLet\u2019s assume we have issued all the instructions in two successive iterations\nof the loop, but none of the floating-point load/stores or operations have com-\npleted. Figure 3.14 shows reservation stations, register status tables, and load\nand store buffers at this point. (The integer ALU operation is ignored, and it\nis assumed the branch was predicted as taken.) Once the system reaches this\nstate, two copies of the loop could be sustained with a CPI close to 1.0, provided\nthe multiplies could complete in four clock cycles. With a latency of six cycles,\nadditional iterations will need to be processed before the steady state can be\nreached. This requires more reservation stations to hold instructions that are\nin execution. As we will see later in this chapter, when extended with multiple\nissue instructions, Tomasulo\u2019s approach can sustain more than one instruction\nper clock.\nA load and a store can be done safely out of order, provided they access dif-\nferent addresses. If a load and a store access the same address, one of two things\nhappens:\n204\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 237,
        "text": "Instruction state\nWait until\nAction or bookkeeping\nIssue\nFP operation\nStation r empty\nif (RegisterStat[rs].Qi6\u00bc0)\n{RS[r].Qj  RegisterStat[rs].Qi}\nelse {RS[r].Vj  Regs[rs]; RS[r].Qj  0};\nif (RegisterStat[rt].Qi6\u00bc0)\n{RS[r].Qk  RegisterStat[rt].Qi\nelse {RS[r].Vk  Regs[rt]; RS[r].Qk  0};\nRS[r].Busy  yes; RegisterStat[rd].Q  r;\nLoad or store\nBuffer r empty\nif (RegisterStat[rs].Qi6\u00bc0)\n{RS[r].Qj  RegisterStat[rs].Qi}\nelse {RS[r].Vj  Regs[rs]; RS[r].Qj  0};\nRS[r].A  imm; RS[r].Busy  yes;\nLoad only\nRegisterStat[rt].Qi  r;\nStore only\nif (RegisterStat[rt].Qi6\u00bc0)\n{RS[r].Qk  RegisterStat[rs].Qi}\nelse {RS[r].Vk  Regs[rt]; RS[r].Qk  0};\nExecute\nFP operation\n(RS[r].Qj = 0) and\n(RS[r].Qk = 0)\nCompute result: operands are in Vj and Vk\nLoad/storestep 1\nRS[r].Qj \u00bc 0 & r is head of\nload-store queue\nRS[r].A  RS[r].Vj + RS[r].A;\nLoad step 2\nLoad step 1 complete\nRead from Mem[RS[r].A]\nWrite result\nFP operation\nor load\nExecution complete at r &\nCDB available\n8x(if (RegisterStat[x].Qi=r) {Regs[x]  result;\nRegisterStat[x].Qi  0});\n8x(if (RS[x].Qj=r)\n{RS[x].Vj  \nresult;RS[x].Qj  0});\n8x(if (RS[x].Qk=r)\n{RS[x].Vk  \nresult;RS[x].Qk  0});\nRS[r].Busy  no;\nStore\nExecution complete at r &\nRS[r].Qk = 0\nMem[RS[r].A]  RS[r].Vk;\nRS[r].Busy  no;\nFigure 3.13 Steps in the algorithm and what is required for each step. For the issuing instruction, rd is the des-\ntination, rs and rt are the source register numbers, imm is the sign-extended immediate field, and r is the reser-\nvation station or buffer that the instruction is assigned to. RS is the reservation station data structure. The value\nreturned by an FP unit or by the load unit is called result. RegisterStat is the register status data structure\n(not the register file, which is Regs[]). When an instruction is issued, the destination register has its Qi field set\nto the number of the buffer or reservation station to which the instruction is issued. If the operands are available\nin the registers, they are stored in the V fields. Otherwise, the Q fields are set to indicate the reservation station that\nwill produce the values needed as source operands. The instruction waits at the reservation station until both its\noperands are available, indicated by zero in the Q fields. The Q fields are set to zero either when this instruction\nis issued or when an instruction on which this instruction depends completes and does its write back. When an\ninstruction has finished execution and the CDB is available, it can do its write back. All the buffers, registers, and\nreservation stations whose values of Qj or Qk are the same as the completing reservation station update their values\nfrom the CDB and mark the Q fields to indicate that values have been received. Thus the CDB can broadcast its result\nto many destinations in a single clock cycle, and if the waiting instructions have their operands, they can all begin\nexecution on the next clock cycle. Loads go through two steps in execute, and stores perform slightly differently\nduring Write Result, where they may have to wait for the value to store. Remember that, to preserve exception behav-\nior, instructions should not be allowed to execute if a branch that is earlier in program order has not yet completed.\nBecause no concept of program order is maintained after the issue stage, this restriction is usually implemented by\npreventing any instruction from leaving the issue step if there is a pending branch already in the pipeline. In\nSection 3.6, we will see how speculation support removes this restriction."
    },
    {
        "page": 238,
        "text": "\u25a0\nThe load is before the store in program order and interchanging them results in\na WAR hazard.\n\u25a0\nThe store is before the load in program order and interchanging them results in\na RAW hazard.\nSimilarly, interchanging two stores to the same address results in a WAW hazard.\nTherefore, to determine if a load can be executed at a given time, the processor\ncan check whether any uncompleted store that precedes the load in program order\nInstruction status\nInstruction\nFrom iteration\nIssue\nExecute\nWrite result\nfld\nf0,0(x1)\n1\n\u221a\n\u221a\nfmul.d\nf4,f0,f2\n1\n\u221a\nfsd\nf4,0(x1)\n1\n\u221a\nfld\nf0,0(x1)\n2\n\u221a\n\u221a\nfmul.d\nf4,f0,f2\n2\n\u221a\nfsd\nf4,0(x1)\n2\n\u221a\nReservation stations\nName\nBusy\nOp\nVj\nVk\nQj\nQk\nA\nLoad1\nYes\nLoad\nRegs[x1] + 0\nLoad2\nYes\nLoad\nRegs[x1]  8\nAdd1\nNo\nAdd2\nNo\nAdd3\nNo\nMult1\nYes\nMUL\nRegs[f2]\nLoad1\nMult2\nYes\nMUL\nRegs[f2]\nLoad2\nStore1\nYes\nStore\nRegs[x1]\nMult1\nStore2\nYes\nStore\nRegs[x1]  8\nMult2\nRegister status\nField\nf0\nf2\nf4\nf6\nf8\nf10\nf12\n\u2026\nf30\nQi\nLoad2\nMult2\nFigure 3.14 Two active iterations of the loop with no instruction yet completed. Entries in the multiplier reser-\nvation stations indicate that the outstanding loads are the sources. The store reservation stations indicate that\nthe multiply destination is the source of the value to store.\n206\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 239,
        "text": "shares the same data memory address as the load. Similarly, a store must wait until\nthere are no unexecuted loads or stores that are earlier in program order and share\nthe same data memory address. We consider a method to eliminate this restriction\nin Section 3.9.\nTo detect such hazards, the processor must have computed the data memory\naddress associated with any earlier memory operation. A simple, but not necessar-\nily optimal, way to guarantee that the processor has all such addresses is to perform\nthe effective address calculations in program order. (We really only need to keep\nthe relative order between stores and other memory references; that is, loads can be\nreordered freely.)\nLet\u2019s consider the situation of a load first. If we perform effective address\ncalculation in program order, then when a load has completed effective address\ncalculation, we can check whether there is an address conflict by examining the A\nfield of all active store buffers. If the load address matches the address of any\nactive entries in the store buffer, that load instruction is not sent to the load buffer\nuntil the conflicting store completes. (Some implementations bypass the value\ndirectly to the load from a pending store, reducing the delay for this RAW\nhazard.)\nStores operate similarly, except that the processor must check for conflicts in\nboth the load buffers and the store buffers because conflicting stores cannot be\nreordered with respect to either a load or a store.\nA dynamically scheduled pipeline can yield very high performance, pro-\nvided branches are predicted accurately\u2014an issue we addressed in the previous\nsection. The major drawback of this approach is the complexity of the Toma-\nsulo scheme, which requires a large amount of hardware. In particular, each\nreservation station must contain an associative buffer, which must run at high\nspeed, as well as complex control logic. The performance can also be limited\nby the single CDB. Although additional CDBs can be added, each CDB must\ninteract with each reservation station, and the associative tag-matching hard-\nware would have to be duplicated at each station for each CDB. In the\n1990s, only high-end processors could take advantage of dynamic scheduling\n(and its extension to speculation); however, recently even processors designed\nfor PMDs are using these techniques, and processors for high-end desktops and\nsmall servers have hundreds of buffers to support dynamic scheduling.\nIn Tomasulo\u2019s scheme, two different techniques are combined: the renaming of\nthe architectural registers to a larger set of registers and the buffering of source\noperands from the register file. Source operand buffering resolves WAR hazards\nthat arise when the operand is available in the registers. As we will see later, it is\nalso possible to eliminate WAR hazards by the renaming of a register together\nwith the buffering of a result until no outstanding references to the earlier version\nof the register remain. This approach will be used when we discuss hardware\nspeculation.\nTomasulo\u2019s scheme was unused for many years after the 360/91, but was\nwidely adopted in multiple-issue processors starting in the 1990s for several\nreasons:\n3.5\nDynamic Scheduling: Examples and the Algorithm\n\u25a0\n207"
    },
    {
        "page": 240,
        "text": "1. Although Tomasulo\u2019s algorithm was designed before caches, the presence of\ncaches, with the inherently unpredictable delays, has become one of the major\nmotivations for dynamic scheduling. Out-of-order execution allows the proces-\nsors to continue executing instructions while awaiting the completion of a cache\nmiss, thus hiding all or part of the cache miss penalty.\n2. As processors became more aggressive in their issue capability and designers\nwere concerned with the performance of difficult-to-schedule code (such as\nmost nonnumeric code), techniques such as register renaming, dynamic sched-\nuling, and speculation became more important.\n3. It can achieve high performance without requiring the compiler to target code to\na specific pipeline structure, a valuable property in the era of shrink-wrapped\nmass market software.\n3.6\nHardware-Based Speculation\nAs we try to exploit more instruction-level parallelism, maintaining control depen-\ndences becomes an increasing burden. Branch prediction reduces the direct stalls\nattributable to branches, but for a processor executing multiple instructions per\nclock, just predicting branches accurately may not be sufficient to generate the\ndesired amount of instruction-level parallelism. A wide-issue processor may need\nto execute a branch every clock cycle to maintain maximum performance. Thus\nexploiting more parallelism requires that we overcome the limitation of control\ndependence.\nOvercoming control dependence is done by speculating on the outcome of\nbranches and executing the program as if our guesses are correct. This mech-\nanism represents a subtle, but important, extension over branch prediction with\ndynamic scheduling. In particular, with speculation, we fetch, issue, and exe-\ncute instructions, as if our branch predictions are always correct; dynamic\nscheduling only fetches and issues such instructions. Of course, we need mech-\nanisms to handle the situation where the speculation is incorrect. Appendix H\ndiscusses a variety of mechanisms for supporting speculation by the compiler.\nIn this section, we explore hardware speculation, which extends the ideas of\ndynamic scheduling.\nHardware-based speculation combines three key ideas: (1) dynamic branch\nprediction to choose which instructions to execute, (2) speculation to allow\nthe execution of instructions before the control dependences are resolved\n(with the ability to undo the effects of an incorrectly speculated sequence),\nand (3) dynamic scheduling to deal with the scheduling of different combina-\ntions of basic blocks. (In comparison, dynamic scheduling without speculation\nonly partially overlaps basic blocks because it requires that a branch be\nresolved before actually executing any instructions in the successor basic\nblock.)\n208\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 241,
        "text": "Hardware-based speculation follows the predicted flow of data values to\nchoose when to execute instructions. This method of executing programs is essen-\ntially a data flow execution: Operations execute as soon as their operands are\navailable.\nTo extend Tomasulo\u2019s algorithm to support speculation, we must separate the\nbypassing of results among instructions, which is needed to execute an instruction\nspeculatively, from the actual completion of an instruction. By making this sepa-\nration, we can allow an instruction to execute and to bypass its results to other\ninstructions, without allowing the instruction to perform any updates that cannot\nbe undone, until we know that the instruction is no longer speculative.\nUsing the bypassed value is like performing a speculative register read because\nwe do not know whether the instruction providing the source register value is pro-\nviding the correct result until the instruction is no longer speculative. When an\ninstruction is no longer speculative, we allow it to update the register file or mem-\nory; we call this additional step in the instruction execution sequence instruction\ncommit.\nThe key idea behind implementing speculation is to allow instructions to exe-\ncute out of order but to force them to commit in order and to prevent any irrevo-\ncable action (such as updating state or taking an exception) until an instruction\ncommits. Therefore, when we add speculation, we need to separate the process\nof completing execution from instruction commit, because instructions may finish\nexecution considerably before they are ready to commit. Adding this commit phase\nto the instruction execution sequence requires an additional set of hardware buffers\nthat hold the results of instructions that have finished execution but have not com-\nmitted. This hardware buffer, which we call the reorder buffer, is also used to pass\nresults among instructions that may be speculated.\nThe reorder buffer (ROB) provides additional registers in the same way as the\nreservation stations in Tomasulo\u2019s algorithm extend the register set. The ROB\nholds the result of an instruction between the time the operation associated with\nthe instruction completes and the time the instruction commits. The ROB therefore\nis a source of operands for instructions, just as the reservation stations provide\noperands in Tomasulo\u2019s algorithm. The key difference is that in Tomasulo\u2019s algo-\nrithm, once an instruction writes its result, all subsequently issued instructions will\nfind the result in the register file. With speculation, the register file is not updated\nuntil the instruction commits (and we know definitively that the instruction should\nexecute); thus, the ROB supplies operands in the interval between completion of\ninstruction execution and instruction commit. The ROB is similar to the store\nbuffer in Tomasulo\u2019s algorithm, and we integrate the function of the store buffer\ninto the ROB for simplicity.\nFigure 3.15 shows the hardware structure of the processor including the ROB.\nEach entry in the ROB contains four fields: the instruction type, the destination\nfield, the value field, and the ready field. The instruction type field indicates whether\nthe instruction is a branch (and has no destination result), a store (which has a mem-\nory address destination), or a register operation (ALU operation or load, which has\nregister destinations). The destination field supplies the register number (for loads\n3.6\nHardware-Based Speculation\n\u25a0\n209"
    },
    {
        "page": 242,
        "text": "and ALU operations) or the memory address (for stores) where the instruction result\nshould be written. The value field is used to hold the value of the instruction result\nuntil the instruction commits. We will see an example of ROB entries shortly.\nFinally, the ready field indicates that the instruction has completed execution,\nand the value is ready.\nThe ROB subsumes the store buffers. Stores still execute in two steps, but the\nsecond step is performed by instruction commit. Although the renaming function\nof the reservation stations is replaced by the ROB, we still need a place to buffer oper-\nations (and operands) between the time they issue and the time they begin execution.\nFrom instruction unit\nFP registers\nReservation\nstations\nFP adders\nFP multipliers\n3\n2\n1\n2\n1\nCommon data bus (CDB)\nOperation bus\nOperand\nbuses\nAddress unit\nLoad buffers\nMemory unit\nReorder buffer\nData\nReg #\nStore\ndata\nAddress\nLoad\ndata\nStore\naddress\nFloating-point\noperations\nLoad/store\noperations\nInstruction\nqueue\nFigure 3.15 The basic structure of a FP unit using Tomasulo\u2019s algorithm and extended to handle speculation.\nComparing this to Figure 3.10 on page 198, which implemented Tomasulo\u2019s algorithm, we can see that the major\nchange is the addition of the ROB and the elimination of the store buffer, whose function is integrated into the\nROB. This mechanism can be extended to allow multiple issues per clock by making the CDB wider to allow for mul-\ntiple completions per clock.\n210\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 243,
        "text": "This function is still provided by the reservation stations. Because every instruction\nhasapositionintheROBuntilitcommits,wetagaresultusingtheROBentrynumber\nrather than using the reservation station number. This tagging requires that the ROB\nassigned for an instruction must be tracked in the reservation station. Later in this sec-\ntion,wewillexploreanalternativeimplementationthatusesextraregisters for renam-\ning and a queue that replaces the ROB to decide when instructions can commit.\nHere are the four steps involved in instruction execution:\n1. Issue\u2014Get an instruction from the instruction queue. Issue the instruction if\nthere is an empty reservation station and an empty slot in the ROB; send the\noperands to the reservation station if they are available in either the registers\nor the ROB. Update the control entries to indicate the buffers are in use. The\nnumber of the ROB entry allocated for the result is also sent to the reservation\nstation so that the number can be used to tag the result when it is placed on the\nCDB. If either all reservations are full or the ROB is full, then the instruction\nissue is stalled until both have available entries.\n2. Execute\u2014If one or more of the operands is not yet available, monitor the CDB\nwhile waiting for the register to be computed. This step checks for RAW haz-\nards. When both operands are available at a reservation station, execute the\noperation. Instructions may take multiple clock cycles in this stage, and loads\nstill require two steps in this stage. Stores only need the base register at this\nstep, because execution for a store at this point is only effective address\ncalculation.\n3. Write result\u2014When the result is available, write it on the CDB (with the ROB\ntag sent when the instruction issued) and from the CDB into the ROB, as well as\nto any reservation stations waiting for this result. Mark the reservation station as\navailable. Special actions are required for store instructions. If the value to be\nstored is available, it is written into the Value field of the ROB entry for the\nstore. If the value to be stored is not available yet, the CDB must be monitored\nuntil that value is broadcast, at which time the Value field of the ROB entry of\nthe store is updated. For simplicity we assume that this occurs during the Write\nResult stage of a store; we discuss relaxing this requirement later.\n4. Commit\u2014This is the final stage of completing an instruction, after which only\nits result remains. (Some processors call this commit phase \u201ccompletion\u201d or\n\u201cgraduation.\u201d) There are three different sequences of actions at commit depend-\ning on whether the committing instruction is a branch with an incorrect predic-\ntion, a store, or any other instruction (normal commit). The normal commit case\noccurs when an instruction reaches the head of the ROB and its result is present\nin the buffer; at this point, the processor updates the register with the result and\nremoves the instruction from the ROB. Committing a store is similar except that\nmemory is updated rather than a result register. When a branch with incorrect\nprediction reaches the head of the ROB, it indicates that the speculation was\nwrong. The ROB is flushed and execution is restarted at the correct successor\nof the branch. If the branch was correctly predicted, the branch is finished.\n3.6\nHardware-Based Speculation\n\u25a0\n211"
    },
    {
        "page": 244,
        "text": "Once an instruction commits, its entry in the ROB is reclaimed, and the register or\nmemory destination is updated, eliminating the need for the ROB entry. If the ROB\nfills, we simply stop issuing instructions until an entry is made free. Now let\u2019s\nexamine how this scheme would work with the same example we used for Toma-\nsulo\u2019s algorithm.\nExample\nAssume the same latencies for the floating-point functional units as in earlier exam-\nples: add is 2 clock cycles, multiply is 6 clock cycles, and divide is 12 clock cycles.\nUsing the following code segment, the same one we used to generate Figure 3.12,\nshow what the status tables look like when the fmul.d is ready to go to commit.\nfld\nf6,32(x2)\nfld\nf2,44(x3)\nfmul.d\nf0,f2,f4\nfsub.d\nf8,f2,f6\nfdiv.d\nf0,f0,f6\nfadd.d\nf6,f8,f2\nAnswer\nFigure 3.16 shows the result in the three tables. Notice that although the fsub.d\ninstruction has completed execution, it does not commit until the fmul.d com-\nmits. The reservation stations and register status field contain the same basic infor-\nmation that they did for Tomasulo\u2019s algorithm (see page 200 for a description of\nthose fields). The differences are that reservation station numbers are replaced with\nROB entry numbers in the Qj and Qk fields, as well as in the register status fields,\nand we added the Dest field to the reservation stations. The Dest field designates\nthe ROB entry that is the destination for the result produced by this reservation\nstation entry.\nThe preceding example illustrates the key important difference between a pro-\ncessor with speculation and a processor with dynamic scheduling. Compare the\ncontent of Figure 3.16 with that of Figure 3.12 on page 184, which shows the same\ncode sequence in operation on a processor with Tomasulo\u2019s algorithm. The key\ndifference is that, in the preceding example, no instruction after the earliest uncom-\npleted instruction (fmul.d in preceding example) is allowed to complete. In con-\ntrast, in Figure 3.12 the fsub.d and fadd.d instructions have also completed.\nOne implication of this difference is that the processor with the ROB can\ndynamically execute code while maintaining a precise interrupt model. For exam-\nple, if the fmul.d instruction caused an interrupt, we could simply wait until it\nreached the head of the ROB and take the interrupt, flushing any other pending\ninstructions from the ROB. Because instruction commit happens in order, this\nyields a precise exception.\nBy contrast, in the example using Tomasulo\u2019s algorithm, the fsub.d and\nfadd.d instructions could both complete before the fmul.d raised the excep-\ntion. The result is that the registers f8 and f6 (destinations of the fsub.d and\n212\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 245,
        "text": "fadd.d instructions) could be overwritten, in which case the interrupt would be\nimprecise.\nSome users and architects have decided that imprecise floating-point excep-\ntions are acceptable in high-performance processors because the program will\nlikely terminate; see Appendix J for further discussion of this topic. Other types\nReorder buffer\nEntry\nBusy\nInstruction\nState\nDestination\nValue\n1\nNo\nfld\nf6,32(x2)\nCommit\nf6\nMem[32 + Regs[x2]]\n2\nNo\nfld\nf2,44(x3)\nCommit\nf2\nMem[44 + Regs[x3]]\n3\nYes\nfmul.d\nf0,f2,f4\nWrite result\nf0\n#2 \u0003 Regs[f4]\n4\nYes\nfsub.d\nf8,f2,f6\nWrite result\nf8\n#2#1\n5\nYes\nfdiv.d\nf0,f0,f6\nExecute\nf0\n6\nYes\nfadd.d\nf6,f8,f2\nWrite result\nf6\n#4 + #2\nReservation stations\nName\nBusy\nOp\nVj\nVk\nQj\nQk\nDest\nA\nLoad1\nNo\nLoad2\nNo\nAdd1\nNo\nAdd2\nNo\nAdd3\nNo\nMult1\nNo\nfmul.d\nMem[44 + Regs[x3]]\nRegs[f4]\n#3\nMult2\nYes\nfdiv.d\nMem[32 + Regs[x2]]\n#3\n#5\nFP register status\nField\nf0\nf1\nf2\nf3\nf4\nf5\nf6\nf7\nf8\nf10\nReorder #\n3\n6\n4\n5\nBusy\nYes\nNo\nNo\nNo\nNo\nNo\nYes\n\u2026\nYes\nYes\nFigure 3.16 At the time the fmul.d is ready to commit, only the two fld instructions have committed, although\nseveral others have completed execution. The fmul.d is at the head of the ROB, and the two fld instructions are\nthere only to ease understanding. The fsub.d and fadd.d instructions will not commit until the fmul.d instruc-\ntion commits, although the results of the instructions are available and can be used as sources for other instructions.\nThe fdiv.d is in execution, but has not completed solely because of its longer latency than that of fmul.d. The\nValue column indicates the value being held; the format #X is used to refer to a value field of ROB entry X. Reorder\nbuffers 1 and 2 are actually completed but are shown for informational purposes. We do not show the entries for the\nload/store queue, but these entries are kept in order.\n3.6\nHardware-Based Speculation\n\u25a0\n213"
    },
    {
        "page": 246,
        "text": "of exceptions, such as page faults, are much more difficult to accommodate if they\nare imprecise because the program must transparently resume execution after han-\ndling such an exception.\nThe use of a ROB with in-order instruction commit provides precise exceptions,\nin addition to supporting speculative execution, as the next example shows.\nExample\nConsider the code example used earlier for Tomasulo\u2019s algorithm and shown in\nFigure 3.14 in execution:\nLoop:\nfld\nf0,0(x1)\nfmul.d\nf4,f0,f2\nfsd\nf4,0(x1)\naddi\nx1,x1,8\nbne\nx1,x2,Loop\n//branches if x16\u00bcx2\nAssume that we have issued all the instructions in the loop twice. Let\u2019s also assume\nthat the fld and fmul.d from the first iteration have committed and all other\ninstructions have completed execution. Normally, the store would wait in the\nROB for both the effective address operand (x1 in this example) and the value\n(f4 in this example). Because we are only considering the floating-point pipeline,\nassume the effective address for the store is computed by the time the instruction\nis issued.\nAnswer\nFigure 3.17 shows the result in two tables.\nBecause neither the register values nor any memory values are actually written\nuntil an instruction commits, the processor can easily undo its speculative actions\nwhen a branch is found to be mispredicted. Suppose that the branch bne is not\ntaken the first time in Figure 3.17. The instructions prior to the branch will simply\ncommit when each reaches the head of the ROB; when the branch reaches the head\nof that buffer, the buffer is simply cleared and the processor begins fetching\ninstructions from the other path.\nIn practice, processors that speculate try to recover as early as possible after a\nbranch is mispredicted. This recovery can be done by clearing the ROB for all\nentries that appear after the mispredicted branch, allowing those that are before\nthe branch in the ROB to continue, and restarting the fetch at the correct branch\nsuccessor. In speculative processors, performance is more sensitive to the branch\nprediction because the impact of a misprediction will be higher. Thus all the\naspects of handling branches\u2014prediction accuracy, latency of misprediction\ndetection, and misprediction recovery time\u2014increase in importance.\nExceptions are handled by not recognizing the exception until it is ready to\ncommit. If a speculated instruction raises an exception, the exception is recorded\nin the ROB. If a branch misprediction arises and the instruction should not have\nbeen executed, the exception is flushed along with the instruction when the\n214\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 247,
        "text": "ROB is cleared. If the instruction reaches the head of the ROB, then we know it is\nno longer speculative and the exception should really be taken. We can also try to\nhandle exceptions as soon as they arise and all earlier branches are resolved, but\nthis is more challenging in the case of exceptions than for branch mispredict and,\nbecause it occurs less frequently, not as critical.\nFigure 3.18 shows the steps of execution for an instruction, as well as the\nconditions that must be satisfied to proceed to the step and the actions taken.\nWe show the case where mispredicted branches are not resolved until commit.\nAlthough speculation seems like a simple addition to dynamic scheduling, a\ncomparison of Figure 3.18 with the comparable figure for Tomasulo\u2019s algo-\nrithm in Figure 3.13 shows that speculation adds significant complications\nto the control. In addition, remember that branch mispredictions are somewhat\nmore complex.\nThere is an important difference in how stores are handled in a speculative\nprocessor versus in Tomasulo\u2019s algorithm. In Tomasulo\u2019s algorithm, a store\ncan update memory when it reaches Write Result (which ensures that the effec-\ntive address has been calculated) and the data value to store is available. In a\nspeculative processor, a store updates memory only when it reaches the head of\nReorder buffer\nEntry\nBusy\nInstruction\nState\nDestination\nValue\n1\nNo\nfld\nf0,0(x1)\nCommit\nf0\nMem[0 + Regs[x1]]\n2\nNo\nfmul.d\nf4,f0,f2\nCommit\nf4\n#1 \u0003 Regs[f2]\n3\nYes\nfsd\nf4,0(x1)\nWrite result\n0 + Regs[x1]\n#2\n4\nYes\naddi\nx1,x1,8\nWrite result\nx1\nRegs[x1]  8\n5\nYes\nbne\nx1,x2,Loop\nWrite result\n6\nYes\nfld\nf0,0(x1)\nWrite result\nf0\nMem[#4]\n7\nYes\nfmul.d\nf4,f0,f2\nWrite result\nf4\n#6 \u0003 Regs[f2]\n8\nYes\nfsd\nf4,0(x1)\nWrite result\n0 + #4\n#7\n9\nYes\naddi\nx1,x1,8\nWrite result\nx1\n#48\n10\nYes\nbne\nx1,x2,Loop\nWrite result\nFP register status\nField\nf0\nf1\nf2\nf3\nf4\nF5\nf6\nF7\nf8\nReorder #\n6\nBusy\nYes\nNo\nNo\nNo\nYes\nNo\nNo\n\u2026\nNo\nFigure 3.17 Only the fld and fmul.d instructions have committed, although all the others have completed\nexecution. Thus no reservation stations are busy and none are shown. The remaining instructions will be committed\nas quickly as possible. The first two reorder buffers are empty, but are shown for completeness.\n3.6\nHardware-Based Speculation\n\u25a0\n215"
    },
    {
        "page": 248,
        "text": "Status\nWait until\nAction or bookkeeping\nIssue all\ninstructions\nFP\noperations\nand stores\nFP\noperations\nLoads\nStores\nReservation\nstation (r)\nand\nROB (b)\nboth available\nif (RegisterStat[rs].Busy)/*in-flight instr. writes rs*/\n{h  RegisterStat[rs].Reorder;\nif (ROB[h].Ready)/* Instr completed already */\n{RS[r].Vj  ROB[h].Value; RS[r].Qj  0;}\nelse {RS[r].Qj  h;} /* wait for instruction */\n} else {RS[r].Vj  Regs[rs]; RS[r].Qj  0;};\nRS[r].Busy  yes; RS[r].Dest  b;\nROB[b].Instruction  opcode; ROB[b].Dest  rd;ROB[b].Ready  no;\nif (RegisterStat[rt].Busy) /*in-flight instr writes rt*/\n{h  RegisterStat[rt].Reorder;\nif (ROB[h].Ready)/* Instr completed already */\n{RS[r].Vk  ROB[h].Value; RS[r].Qk  0;}\nelse {RS[r].Qk  h;} /* wait for instruction */\n} else {RS[r].Vk  Regs[rt]; RS[r].Qk  0;};\nRegisterStat[rd].Reorder  b; RegisterStat[rd].Busy  yes;\nROB[b].Dest  rd;\nRS[r].A  imm; RegisterStat[rt].Reorder  b;\nRegisterStat[rt].Busy  yes; ROB[b].Dest  rt;\nRS[r].A  imm;\nExecute FP\nop\n(RS[r].Qj == 0) and\n(RS[r].Qk == 0)\nCompute results\u2014operands are in Vj and Vk\nLoad step 1\n(RS[r].Qj == 0) and\nthere are no stores\nearlier in the queue\nRS[r].A  RS[r].Vj + RS[r].A;\nLoad step 2\nLoad step 1 done and\nall stores earlier in\nROB have different address\nRead from Mem[RS[r].A]\nStore\n(RS[r].Qj == 0) and\nstore at queue head\nROB[h].Address  RS[r].Vj + RS[r].A;\nWrite result\nall but store\nExecution done at r and\nCDB available\nb  RS[r].Dest; RS[r].Busy  no;\n8x(if (RS[x].Qj==b) {RS[x].Vj  result; RS[x].Qj  0});\n8x(if (RS[x].Qk==b) {RS[x].Vk  result; RS[x].Qk  0});\nROB[b].Value  result; ROB[b].Ready  yes;\nStore\nExecution done at r and\n(RS[r].Qk == 0)\nROB[h].Value  RS[r].Vk;\nCommit\nInstruction is at the\nhead of the ROB (entry h)\nand ROB[h].ready ==\nyes\nd  ROB[h].Dest; /* register dest, if exists */\nif (ROB[h].Instruction==Branch)\n{if (branch is mispredicted)\n{clear ROB[h], RegisterStat; fetch branch dest;};}\nelse if (ROB[h].Instruction==Store)\n{Mem[ROB[h].Destination]  ROB[h].Value;}\nelse /* put the result in the register destination */\n{Regs[d]  ROB[h].Value;};\nROB[h].Busy  no; /* free up ROB entry */\n/* free up dest register if no one else writing it */\nif (RegisterStat[d].Reorder==h) {RegisterStat[d].Busy  no;};\nFigure 3.18 Steps in the algorithm and what is required for each step. For the issuing instruction, rd is the des-\ntination, rs and rt are the sources, r is the reservation station allocated, b is the assigned ROB entry, and h is the\nhead entry of the ROB. RS is the reservation station data structure. The value returned by a reservation station is called\nthe result. Register-Stat is the register data structure, Regs represents the actual registers, and ROB is the\nreorder buffer data structure.\n216\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 249,
        "text": "the ROB. This difference ensures that memory is not updated until an instruc-\ntion is no longer speculative.\nFigure 3.18 has one significant simplification for stores, which is unneeded\nin practice. Figure 3.18 requires stores to wait in the Write Result stage for the\nregister source operand whose value is to be stored; the value is then moved\nfrom the Vk field of the store\u2019s reservation station to the Value field of the\nstore\u2019s ROB entry. In reality, however, the value to be stored need not arrive\nuntil just before the store commits and can be placed directly into the store\u2019s\nROB entry by the sourcing instruction. This is accomplished by having the\nhardware track when the source value to be stored is available in the store\u2019s\nROB entry and searching the ROB on every instruction completion to look\nfor dependent stores.\nThis addition is not complicated, but adding it has two effects: we would need\nto add a field to the ROB, and Figure 3.18, which is already in a small font, would\nbe even longer! Although Figure 3.18 makes this simplification, in our examples,\nwe will allow the store to pass through the Write Result stage and simply wait for\nthe value to be ready when it commits.\nLike Tomasulo\u2019s algorithm, we must avoid hazards through memory. WAW\nand WAR hazards through memory are eliminated with speculation because the\nactual updating of memory occurs in order, when a store is at the head of the\nROB, so no earlier loads or stores can still be pending. RAW hazards through\nmemory are maintained by two restrictions:\n1. Not allowing a load to initiate the second step of its execution if any active ROB\nentry occupied by a store has a Destination field that matches the value of the A\nfield of the load\n2. Maintaining the program order for the computation of an effective address of a\nload with respect to all earlier stores\nTogether, these two restrictions ensure that any load that accesses a memory\nlocation written to by an earlier store cannot perform the memory access until\nthe store has written the data. Some speculative processors will actually bypass\nthe value from the store to the load directly when such a RAW hazard occurs.\nAnother approach is to predict potential collisions using a form of value prediction;\nwe consider this in Section 3.9.\nAlthough this explanation of speculative execution has focused on floating\npoint, the techniques easily extend to the integer registers and functional units.\nIndeed, because such programs tend to have code where the branch behavior is\nless predictable, speculation may be more useful in integer programs. Additionally,\nthese techniques can be extended to work in a multiple-issue processor by allowing\nmultiple instructions to issue and commit every clock. In fact, speculation is\nprobably most interesting in such processors because less ambitious techniques\ncan probably exploit sufficient ILP within basic blocks when assisted by a\ncompiler.\n3.6\nHardware-Based Speculation\n\u25a0\n217"
    },
    {
        "page": 250,
        "text": "3.7\nExploiting ILP Using Multiple Issue\nand Static Scheduling\nThe techniques of the preceding sections can be used to eliminate data, control\nstalls, and achieve an ideal CPI of one. To improve performance further, we want\nto decrease the CPI to less than one, but the CPI cannot be reduced below one if we\nissue only one instruction every clock cycle.\nThe goal of the multiple-issue processors, discussed in the next few sections, is\nto allow multiple instructions to issue in a clock cycle. Multiple-issue processors\ncome in three major flavors:\n1. Statically scheduled superscalar processors\n2. VLIW (very long instruction word) processors\n3. Dynamically scheduled superscalar processors\nThe two types of superscalar processors issue varying numbers of instructions per\nclock and use in-order execution if they are statically scheduled or out-of-order\nexecution if they are dynamically scheduled.\nVLIW processors, in contrast, issue a fixed number of instructions formatted\neither as one large instruction or as a fixed instruction packet with the parallelism\namong instructions explicitly indicated by the instruction. VLIW processors are\ninherently statically scheduled by the compiler. When Intel and HP created the\nIA-64 architecture, described in Appendix H, they also introduced the name EPIC\n(explicitly parallel instruction computer) for this architectural style.\nAlthough statically scheduled superscalars issue a varying rather than a\nfixed number of instructions per clock, they are actually closer in concept to\nVLIWs because both approaches rely on the compiler to schedule code for\nthe processor. Because of the diminishing advantages of a statically scheduled\nsuperscalar as the issue width grows, statically scheduled superscalars are used\nprimarily for narrow issue widths, normally just two instructions. Beyond that\nwidth, most designers choose to implement either a VLIW or a dynamically\nscheduled superscalar. Because of the similarities in hardware and required\ncompiler technology, we focus on VLIWs in this section, and we will see them\nagain in Chapter 7. The insights of this section are easily extrapolated to a stat-\nically scheduled superscalar.\nFigure 3.19 summarizes the basic approaches to multiple issue and their\ndistinguishing characteristics and shows processors that use each approach.\nThe Basic VLIW Approach\nVLIWs use multiple, independent functional units. Rather than attempting to issue\nmultiple, independent instructions to the units, a VLIW packages the multiple\noperations into one very long instruction or requires that the instructions in the\n218\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 251,
        "text": "issue packet satisfy the same constraints. Because there is no fundamental differ-\nence in the two approaches, we will just assume that multiple operations are placed\nin one instruction, as in the original VLIW approach.\nBecause the advantage of a VLIW increases as the maximum issue rate grows,\nwe focus on a wider issue processor. Indeed, for simple two-issue processors, the\noverhead of a superscalar is probably minimal. Many designers would probably\nargue that a four-issue processor has manageable overhead, but as we will see later\nin this chapter, the growth in overhead is a major factor limiting wider issue\nprocessors.\nLet\u2019s consider a VLIW processor with instructions that contain five operations,\nincluding one integer operation (which could also be a branch), two floating-point\noperations, and two memory references. The instruction would have a set of fields\nfor each functional unit\u2014perhaps 16\u201324 bits per unit, yielding an instruction\nlength of between 80 and 120 bits. By comparison, the Intel Itanium 1 and 2 con-\ntain six operations per instruction packet (i.e., they allow concurrent issue of two\nthree-instruction bundles, as Appendix H describes).\nTo keep the functional units busy, there must be enough parallelism in a code\nsequence to fill the available operation slots. This parallelism is uncovered by\nunrolling loops and scheduling the code within the single larger loop body. If\nthe unrolling generates straight-line code, then local scheduling techniques, which\noperate on a single basic block, can be used. If finding and exploiting the parallel-\nism require scheduling code across branches, a substantially more complex global\nCommon\nname\nIssue\nstructure\nHazard\ndetection\nScheduling\nDistinguishing\ncharacteristic\nExamples\nSuperscalar\n(static)\nDynamic\nHardware\nStatic\nIn-order execution\nMostly in the embedded\nspace: MIPS and ARM,\nincluding the Cortex-A53\nSuperscalar\n(dynamic)\nDynamic\nHardware\nDynamic\nSome out-of-order\nexecution, but no\nspeculation\nNone at the present\nSuperscalar\n(speculative)\nDynamic\nHardware\nDynamic with\nspeculation\nOut-of-order execution\nwith speculation\nIntel Core i3, i5, i7; AMD\nPhenom; IBM Power 7\nVLIW/LIW\nStatic\nPrimarily\nsoftware\nStatic\nAll hazards determined\nand indicated by compiler\n(often implicitly)\nMost examples are in signal\nprocessing, such as the TI\nC6x\nEPIC\nPrimarily\nstatic\nPrimarily\nsoftware\nMostly static\nAll hazards determined\nand indicated explicitly\nby the compiler\nItanium\nFigure 3.19 The five primary approaches in use for multiple-issue processors and the primary characteristics\nthat distinguish them. This chapter has focused on the hardware-intensive techniques, which are all some form of\nsuperscalar. Appendix H focuses on compiler-based approaches. The EPIC approach, as embodied in the IA-64\narchitecture, extends many of the concepts of the early VLIW approaches, providing a blend of static and dynamic\napproaches.\n3.7 Exploiting ILP Using Multiple Issue and Static Scheduling\n\u25a0\n219"
    },
    {
        "page": 252,
        "text": "scheduling algorithm must be used. Global scheduling algorithms are not only\nmore complex in structure, but they also must deal with significantly more com-\nplicated trade-offs in optimization, because moving code across branches is\nexpensive.\nIn Appendix H, we discuss trace scheduling, one of these global scheduling\ntechniquesdevelopedspecificallyforVLIWs;wewill alsoexplorespecialhardware\nsupport that allows some conditional branches to be eliminated, extending the use-\nfulness of local scheduling and enhancing the performance of global scheduling.\nFor now, we will rely on loop unrolling to generate long, straight-line code\nsequences so that we can use local scheduling to build up VLIW instructions\nand focus on how well these processors operate.\nExample\nSuppose we have a VLIW that could issue two memory references, two FP oper-\nations, and one integer operation or branch in every clock cycle. Show an unrolled\nversion of the loop x[i] = x[i] + s (see page 158 for the RISC-V code) for such\na processor. Unroll as many times as necessary to eliminate any stalls.\nAnswer\nFigure 3.20 shows the code. The loop has been unrolled to make seven copies of\nthe body, which eliminates all stalls (i.e., completely empty issue cycles), and runs\nin 9 cycles for the unrolled and scheduled loop. This code yields a running rate of\nseven results in 9 cycles, or 1.29 cycles per result, nearly twice as fast as the\ntwo-issue superscalar of Section 3.2 that used unrolled and scheduled code.\nMemory\nreference 1\nMemory\nreference 2\nFP operation 1\nFP operation 2\nInteger\noperation/branch\nfld f0,0(x1)\nfld f6,-8(x1)\nfld f10,-16(x1) fld f14,-24(x1)\nfld f18,-32(x1) fld f22,-40(x1) fadd.d f4,f0,f2\nfadd.d f8,f6,f2\nfld f26,-48(x1)\nfadd.d f12,f0,f2\nfadd.d f16,f14,f2\nfadd.d f20,f18,f2\nfadd.d f24,f22,f2\nfsd f4,0(x1)\nfsd f8,-8(x1)\nfadd.d f28,f26,f24\nfsd f12,-16(x1) fsd f16,-24(x1)\naddi x1,x1,-56\nfsd f20,24(x1)\nfsd f24,16(x1)\nfsd f28,8(x1)\nbne x1,x2,Loop\nFigure 3.20 VLIW instructions that occupy the inner loop and replace the unrolled sequence. This code takes 9\ncycles assuming correct branch prediction. The issue rate is 23 operations in 9 clock cycles, or 2.5 operations per cycle.\nThe efficiency, the percentage of available slots that contained an operation, is about 60%. To achieve this issue rate\nrequires a larger number of registers than RISC-V would normally use in this loop. The preceding VLIW code sequence\nrequires at least eight FP registers, whereas the same code sequence for the base RISC-V processor can use as few as\ntwo FP registers or as many as five when unrolled and scheduled.\n220\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 253,
        "text": "For the original VLIW model, there were both technical and logistical problems\nthat made the approach less efficient. The technical problems were the increase in\ncode size and the limitations of the lockstep operation. Two different elements com-\nbine to increase code size substantially for a VLIW. First, generating enough oper-\nations in a straight-line code fragment requires ambitiously unrolling loops (as in\nearlier examples), thereby increasing code size. Second, whenever instructions are\nnot full, the unused functional units translate to wasted bits in the instruction encod-\ning. In Appendix H, we examine software scheduling approaches, such as software\npipelining,thatcanachievethebenefitsofunrollingwithoutasmuchcodeexpansion.\nTo combat this code size increase, clever encodings are sometimes used. For\nexample, there may be only one large immediate field for use by any functional\nunit. Another technique is to compress the instructions in main memory and expand\nthem when they are read into the cache or are decoded. In Appendix H, we show\nother techniques, as well as document the significant code expansion seen in IA-64.\nEarly VLIWs operated in lockstep; there was no hazard-detection hardware at\nall. This structure dictated that a stall in any functional unit pipeline must cause the\nentire processor to stall because all the functional units had to be kept synchro-\nnized. Although a compiler might have been able to schedule the deterministic\nfunctional units to prevent stalls, predicting which data accesses would encounter\na cache stall and scheduling them were very difficult to do. Thus caches needed to\nbe blocking and causing all the functional units to stall. As the issue rate and num-\nber of memory references became large, this synchronization restriction became\nunacceptable. In more recent processors, the functional units operate more inde-\npendently, and the compiler is used to avoid hazards at issue time, while hardware\nchecks allow for unsynchronized execution once instructions are issued.\nBinary code compatibility has also been a major logistical problem for general-\npurpose VLIWs or those that run third-party software. In a strict VLIW approach,\nthe code sequence makes use of both the instruction set definition and the detailed\npipeline structure, including both functional units and their latencies. Thus differ-\nent numbers of functional units and unit latencies require different versions of the\ncode. This requirement makes migrating between successive implementations, or\nbetween implementations with different issue widths, more difficult than it is for a\nsuperscalar design. Of course, obtaining improved performance from a new super-\nscalar design may require recompilation. Nonetheless, the ability to run old binary\nfiles is a practical advantage for the superscalar approach. In the domain-specific\narchitectures, which we examine in Chapter 7, this problem is not serious because\napplications are written specifically for an architectural configuration.\nThe EPIC approach, of which the IA-64 architecture is the primary example,\nprovides solutions to many of the problems encountered in early general-purpose\nVLIW designs, including extensions for more aggressive software speculation and\nmethods to overcome the limitation of hardware dependence while preserving\nbinary compatibility.\nThe major challenge for all multiple-issue processors is to try to exploit large\namounts of ILP. When the parallelism comes from unrolling simple loops in FP\n3.7 Exploiting ILP Using Multiple Issue and Static Scheduling\n\u25a0\n221"
    },
    {
        "page": 254,
        "text": "programs, the original loop probably could have been run efficiently on a vector\nprocessor (described in the next chapter). It is not clear that a multiple-issue pro-\ncessor is preferred over a vector processor for such applications; the costs are sim-\nilar, and the vector processor is typically the same speed or faster. The potential\nadvantages of a multiple-issue processor versus a vector processor are the former\u2019s\nability to extract some parallelism from less structured code and to easily cache all\nforms of data. For these reasons, multiple-issue approaches have become the pri-\nmary method for taking advantage of instruction-level parallelism, and vectors\nhave become primarily an extension to these processors.\n3.8\nExploiting ILP Using Dynamic Scheduling, Multiple Issue,\nand Speculation\nSofarwe have seen howthe individualmechanisms ofdynamic scheduling,multiple\nissue, and speculation work. In this section, we put all three together, which yields a\nmicroarchitecture quite similar to those in modern microprocessors. For simplicity\nwe consider only an issue rate of two instructions per clock, but the concepts are no\ndifferent from modern processors that issue three or more instructions per clock.\nLet\u2019s assume we want to extend Tomasulo\u2019s algorithm to support multiple-\nissue superscalar pipeline with separate integer, load/store, and floating-point units\n(both FP multiply and FP add), each of which can initiate an operation on every\nclock. We do not want to issue instructions to the reservation stations out of order\nbecause this could lead to a violation of the program semantics. To gain the full\nadvantage of dynamic scheduling, we will allow the pipeline to issue any combi-\nnation of two instructions in a clock, using the scheduling hardware to actually\nassign operations to the integer and floating-point unit. Because the interaction\nof the integer and floating-point instructions is crucial, we also extend Tomasulo\u2019s\nscheme to deal with both the integer and floating-point functional units and reg-\nisters, as well as incorporating speculative execution. As Figure 3.21 shows, the\nbasic organization is similar to that of a processor with speculation with one issue\nper clock, except that the issue and completion logic must be enhanced to allow\nmultiple instructions to be processed per clock.\nIssuing multiple instructions per clock in a dynamically scheduled processor\n(with or without speculation) is very complex for the simple reason that the mul-\ntiple instructions may depend on one another. Because of this, the tables must be\nupdated for the instructions in parallel; otherwise, the tables will be incorrect or the\ndependence may be lost.\nTwo different approaches have been used to issue multiple instructions per\nclock in a dynamically scheduled processor, and both rely on the observation that\nthe key is assigning a reservation station and updating the pipeline control tables.\nOne approach is to run this step in half a clock cycle so that two instructions can be\nprocessed in one clock cycle; this approach cannot be easily extended to handle\nfour instructions per clock, unfortunately.\n222\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 255,
        "text": "A second alternative is to build the logic necessary to handle two or more\ninstructions at once, including any possible dependences between the instructions.\nModern superscalar processors that issue four or more instructions per clock may\ninclude both approaches: They both pipeline and widen the issue logic. A key\nobservation is that we cannot simply pipeline away the problem. By making\ninstruction issues take multiple clocks because new instructions are issuing every\nclock cycle, we must be able to assign the reservation station and to update the\npipeline tables so that a dependent instruction issuing on the next clock can use\nthe updated information.\nFrom instruction unit\nInteger and FP registers\nReservation\nstations\nFP adders\nFP multipliers\n3\n2\n1\n2\n1\nCommon data bus (CDB)\nOperation bus\nOperand\nbuses\nAddress unit\nLoad buffers\nMemory unit\nData\nReg #\nReorder buffer\nStore\ndata\nAddress\nLoad\ndata\nStore\naddress\nFloating-point\noperations\nLoad/store\noperations\nInstruction\nqueue\nInteger unit\n2\n1\nFigure 3.21 The basic organization of a multiple issue processor with speculation. In this case, the organization\ncould allow a FP multiply, FP add, integer, and load/store to all issues simultaneously (assuming one issue per clock\nper functional unit). Note that several datapaths must be widened to support multiple issues: the CDB, the operand\nbuses, and, critically, the instruction issue logic, which is not shown in this figure. The last is a difficult problem, as we\ndiscuss in the text.\n3.8\nExploiting ILP Using Dynamic Scheduling, Multiple Issue, and Speculation\n\u25a0\n223"
    },
    {
        "page": 256,
        "text": "This issue step is one of the most fundamental bottlenecks in dynamically\nscheduled superscalars. To illustrate the complexity of this process, Figure 3.22\nshows the issue logic for one case: issuing a load followed by a dependent FP oper-\nation. The logic is based on that in Figure 3.18 on page 197, but represents only one\ncase. In a modern superscalar, every possible combination of dependent instruc-\ntions that is allowed to issue in the same clock cycle must be considered. Because\nthe number of possibilities climbs as the square of the number of instructions that\ncan be issued in a clock, the issue step is a likely bottleneck for attempts to go\nbeyond four instructions per clock.\nWe can generalize the detail of Figure 3.22 to describe the basic strategy for\nupdating the issue logic and the reservation tables in a dynamically scheduled\nsuperscalar with up to n issues per clock as follows:\n1. Assign a reservation station and a reorder buffer for every instruction that might\nbe issued in the next issue bundle. This assignment can be done before the\ninstruction types are known simply by preallocating the reorder buffer entries\nsequentially to the instructions in the packet using n available reorder buffer\nentries and by ensuring that enough reservation stations are available to issue\nthe whole bundle, independent of what it contains. By limiting the number\nof instructions of a given class (say, one FP, one integer, one load, one store),\nthe necessary reservation stations can be preallocated. Should sufficient reser-\nvation stations not be available (such as when the next few instructions in the\nprogram are all of one instruction type), the bundle is broken, and only a subset\nof the instructions, in the original program order, is issued. The remainder of the\ninstructions in the bundle can be placed in the next bundle for potential issue.\n2. Analyze all the dependences among the instructions in the issue bundle.\n3. If an instruction in the bundle depends on an earlier instruction in the bundle,\nuse the assigned reorder buffer number to update the reservation table for\nthe dependent instruction. Otherwise, use the existing reservation table and\nreorder buffer information to update the reservation table entries for the issuing\ninstruction.\nOf course, what makes the preceding very complicated is that it is all done in par-\nallel in a single clock cycle!\nAt the back-end of the pipeline, we must be able to complete and commit mul-\ntiple instructions per clock. These steps are somewhat easier than the issue problems\nbecause multiple instructions that can actually commit in the same clock cycle must\nhave already dealt with and resolved any dependences. As we will see, designers\nhave figured out how to handle this complexity: The Intel i7, which we examine in\nSection 3.12, uses essentially the scheme we have described for speculative mul-\ntiple issue, including a large number of reservation stations, a reorder buffer, and\na load and store buffer that is also used to handle nonblocking cache misses.\nFrom a performance viewpoint, we can show how the concepts fit together\nwith an example.\n224\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 257,
        "text": "Action or bookkeeping\nComments\nif (RegisterStat[rs1].Busy)/*in-flight instr. writes rs*/\n{h  RegisterStat[rs1].Reorder;\nif (ROB[h].Ready)/* Instr completed already */\n{RS[x1].Vj  ROB[h].Value; RS[x1].Qj  0;}\nelse {RS[x1].Qj  h;} /* wait for instruction */\n} else {RS[x1].Vj  Regs[rs]; RS[x1].Qj  0;};\nRS[x1].Busy  yes; RS[x1].Dest  b1;\nROB[b1].Instruction  Load; ROB[b1].Dest  rd1;\nROB[b1].Ready  no;\nRS[r].A  imm1; RegisterStat[rt1].Reorder  b1;\nRegisterStat[rt1].Busy  yes; ROB[b1].Dest  rt1;\nUpdating the reservation tables for\nthe load instruction, which has a\nsingle source operand. Because this\nis the first instruction in this issue\nbundle, it looks no different than\nwhat would normally happen for a\nload.\nRS[x2].Qj  b1;} /* wait for load instruction */\nBecause we know that the first\noperand of the FP operation is from\nthe load, this step simply updates the\nreservation station to point to the\nload. Notice that the dependence\nmust be analyzed on the fly and the\nROB entries must be allocated\nduring this issue step so that the\nreservation tables can be correctly\nupdated.\nif (RegisterStat[rt2].Busy) /*in-flight instr writes rt*/\n{h  RegisterStat[rt2].Reorder;\nif (ROB[h].Ready)/* Instr completed already */\n{RS[x2].Vk  ROB[h].Value; RS[x2].Qk  0;}\nelse {RS[x2].Qk  h;} /* wait for instruction */\n} else {RS[x2].Vk  Regs[rt2]; RS[x2].Qk  0;};\nRegisterStat[rd2].Reorder  b2;\nRegisterStat[rd2].Busy  yes;\nROB[b2].Dest  rd2;\nBecause we assumed that the\nsecond operand of the FP\ninstruction was from a prior issue\nbundle, this step looks like it would\nin the single-issue case. Of course, if\nthis instruction were dependent on\nsomething in the same issue bundle,\nthe tables would need to be updated\nusing the assigned reservation\nbuffer.\nRS[x2].Busy  yes; RS[x2].Dest  b2;\nROB[b2].Instruction  FP operation; ROB[b2].Dest  rd2;\nROB[b2].Ready  no;\nThis section simply updates the\ntables for the FP operation and is\nindependent of the load. Of course,\nif further instructions in this issue\nbundle depended on the FP\noperation (as could happen with a\nfour-issue superscalar), the updates\nto the reservation tables for those\ninstructions would be effected by\nthis instruction.\nFigure 3.22 The issue steps for a pair of dependent instructions (called 1 and 2), where instruction 1 is FP load\nand instruction 2 is an FP operation whose first operand is the result of the load instruction; x1 and x2 are the\nassigned reservation stations for the instructions; and b1 and b2 are the assigned reorder buffer entries. For the\nissuing instructions, rd1 and rd2 are the destinations; rs1, rs2, and rt2 are the sources (the load has only one\nsource); x1 and x2 are the reservation stations allocated; and b1 and b2 are the assigned ROB entries. RS is the\nreservation station data structure. RegisterStat is the register data structure, Regs represents the actual regis-\nters, and ROB is the reorder buffer data structure. Notice that we need to have assigned reorder buffer entries for this\nlogic to operate properly, and recall that all these updates happen in a single clock cycle in parallel, not sequentially.\n3.8\nExploiting ILP Using Dynamic Scheduling, Multiple Issue, and Speculation\n\u25a0\n225"
    },
    {
        "page": 258,
        "text": "Example\nConsider the execution of the following loop, which increments each element of an\ninteger array, on a two-issue processor, once without speculation and once with\nspeculation:\nLoop:\nld\nx2,0(x1)\n//x2=array element\naddi\nx2,x2,1\n//increment x2\nsd\nx2,0(x1)\n//store result\naddi\nx1,x1,8\n//increment pointer\nbne\nx2,x3,Loop\n//branch if not last\nAssume that there are separate integer functional units for effective address cal-\nculation, for ALU operations, and for branch condition evaluation. Create a table\nfor the first three iterations of this loop for both processors. Assume that up to two\ninstructions of any type can commit per clock.\nAnswer\nFigures 3.23 and 3.24 show the performance for a two-issue, dynamically\nscheduled processor, without and with speculation. In this case, where a branch\nIteration\nnumber\nInstructions\nIssues at\nclock cycle\nnumber\nExecutes at\nclock cycle\nnumber\nMemory access at\nclock cycle\nnumber\nWrite CDB at\nclock cycle\nnumber\nComment\n1\nld\nx2,0(x1)\n1\n2\n3\n4\nFirst issue\n1\naddi x2,x2,1\n1\n5\n6\nWait for ld\n1\nsd\nx2,0(x1)\n2\n3\n7\nWait for addi\n1\naddi x1,x1,8\n2\n3\n4\nExecute directly\n1\nbne\nx2,x3,Loop\n3\n7\nWait for addi\n2\nld\nx2,0(x1)\n4\n8\n9\n10\nWait for bne\n2\naddi x2,x2,1\n4\n11\n12\nWait for ld\n2\nsd\nx2,0(x1)\n5\n9\n13\nWait for addi\n2\naddi x1,x1,8\n5\n8\n9\nWait for bne\n2\nbne\nx2,x3,Loop\n6\n13\nWait for addi\n3\nld\nx2,0(x1)\n7\n14\n15\n16\nWait for bne\n3\naddi x2,x2,1\n7\n17\n18\nWait for ld\n3\nsd\nx2,0(x1)\n8\n15\n19\nWait for addi\n3\naddi x1,x1,8\n8\n14\n15\nWait for bne\n3\nbne\nx2,x3,Loop\n9\n19\nWait for addi\nFigure 3.23 The time of issue, execution, and writing result for a dual-issue version of our pipeline without spec-\nulation. Note that the ld following the bne cannot start execution earlier because it must wait until the branch out-\ncome is determined. This type of program, with data-dependent branches that cannot be resolved earlier, shows the\nstrength of speculation. Separate functional units for address calculation, ALU operations, and branch-condition eval-\nuation allow multiple instructions to execute in the same cycle. Figure 3.24 shows this example with speculation.\n226\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 259,
        "text": "can be a critical performance limiter, speculation helps significantly. The third\nbranch in the speculative processor executes in clock cycle 13, whereas it exe-\ncutes in clock cycle 19 on the nonspeculative pipeline. Because the completion\nrate on the nonspeculative pipeline is falling behind the issue rate rapidly, the\nnonspeculative pipeline will stall when a few more iterations are issued. The\nperformance of the nonspeculative processor could be improved by allowing\nload instructions to complete effective address calculation before a branch is\ndecided, but unless speculative memory accesses are allowed, this improve-\nment will gain only 1 clock per iteration.\nThis example clearly shows how speculation can be advantageous when there are\ndata-dependent branches, which otherwise would limit performance. This advan-\ntage depends, however, on accurate branch prediction. Incorrect speculation does\nnot improve performance; in fact, it typically harms performance and, as we shall\nsee, dramatically lowers energy efficiency.\nIteration\nnumber\nInstructions\nIssues\nat clock\nnumber\nExecutes\nat clock\nnumber\nRead\naccess\nat clock\nnumber\nWrite\nCDB at\nclock\nnumber\nCommits\nat clock\nnumber\nComment\n1\nld\nx2,0(x1)\n1\n2\n3\n4\n5\nFirst issue\n1\naddi x2,x2,1\n1\n5\n6\n7\nWait for ld\n1\nsd\nx2,0(x1)\n2\n3\n7\nWait for addi\n1\naddi x1,x1,8\n2\n3\n4\n8\nCommit in order\n1\nbne\nx2,x3,Loop\n3\n7\n8\nWait for addi\n2\nld\nx2,0(x1)\n4\n5\n6\n7\n9\nNo execute delay\n2\naddi x2,x2,1\n4\n8\n9\n10\nWait for ld\n2\nsd\nx2,0(x1)\n5\n6\n10\nWait for addi\n2\naddi x1,x1,8\n5\n6\n7\n11\nCommit in order\n2\nbne\nx2,x3,Loop\n6\n10\n11\nWait for addi\n3\nld\nx2,0(x1)\n7\n8\n9\n10\n12\nEarliest possible\n3\naddi x2,x2,1\n7\n11\n12\n13\nWait for ld\n3\nsd\nx2,0(x1)\n8\n9\n13\nWait for addi\n3\naddi x1,x1,8\n8\n9\n10\n14\nExecutes earlier\n3\nbne\nx2,x3,Loop\n9\n13\n14\nWait for addi\nFigure 3.24 The time of issue, execution, and writing result for a dual-issue version of our pipeline with specu-\nlation. Note that the ld following the bne can start execution early because it is speculative.\n3.8\nExploiting ILP Using Dynamic Scheduling, Multiple Issue, and Speculation\n\u25a0\n227"
    },
    {
        "page": 260,
        "text": "3.9\nAdvanced Techniques for Instruction Delivery\nand Speculation\nIn\na\nhigh-performance\npipeline,\nespecially\none\nwith\nmultiple\nissues,\npredicting branches well is not enough; we actually have to be able to deliver a\nhigh-bandwidth instruction stream. In recent multiple-issue processors, this has\nmeant delivering 4\u20138 instructions every clock cycle. We look at methods for\nincreasing instruction delivery bandwidth first. We then turn to a set of key issues\nin implementing advanced speculation techniques, including the use of register\nrenaming versus reorder buffers, the aggressiveness of speculation, and a tech-\nnique called value prediction, which attempts to predict the result of a computation\nand which could further enhance ILP.\nIncreasing Instruction Fetch Bandwidth\nA multiple-issue processor will require that the average number of instructions\nfetched every clock cycle be at least as large as the average throughput.\nOf course, fetching these instructions requires wide enough paths to the\ninstruction cache, but the most difficult aspect is handling branches. In this\nsection, we look at two methods for dealing with branches and then discuss\nhow modern processors integrate the instruction prediction and prefetch\nfunctions.\nBranch-Target Buffers\nTo reduce the branch penalty for our simple five-stage pipeline, as well as for dee-\nper pipelines, we must know whether the as-yet-undecoded instruction is a branch\nand, if so, what the next program counter (PC) should be. If the instruction is a\nbranch and we know what the next PC should be, we can have a branch penalty\nof zero. A branch-prediction cache that stores the predicted address for the next\ninstruction after a branch is called a branch-target buffer or branch-target cache.\nFigure 3.25 shows a branch-target buffer.\nBecause a branch-target buffer predicts the next instruction address and will\nsend it out before decoding the instruction, we must know whether the fetched\ninstruction is predicted as a taken branch. If the PC of the fetched instruction\nmatches an address in the prediction buffer, then the corresponding predicted\nPC is used as the next PC. The hardware for this branch-target buffer is essentially\nidentical to the hardware for a cache.\nIf a matching entry is found in the branch-target buffer, fetching begins imme-\ndiately at the predicted PC. Note that unlike a branch-prediction buffer, the predic-\ntive entry must be matched to this instruction because the predicted PC will be sent\n228\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 261,
        "text": "out before it is known whether this instruction is even a branch. If the processor did\nnot check whether the entry matched this PC, then the wrong PC would be sent out\nfor instructions that were not branches, resulting in worse performance. We need to\nstore only the predicted-taken branches in the branch-target buffer because an unta-\nken branch should simply fetch the next sequential instruction, as if it were not a\nbranch.\nFigure 3.26 shows the steps when using a branch-target buffer for a simple five-\nstage pipeline. As we can see in this figure, there will be no branch delay if a\nbranch-prediction entry is found in the buffer and the prediction is correct. Other-\nwise, there will be a penalty of at least two clock cycles. Dealing with the mispre-\ndictions and misses is a significant challenge because we typically will have to halt\ninstruction fetch while we rewrite the buffer entry. Thus we want to make this pro-\ncess fast to minimize the penalty.\nLook up\nPredicted PC\nNumber of\nentries\nin branch-\ntarget\nbuffer\nNo:  instruction is not\npredicted to be a taken\nbranch; proceed normally\n=\nYes:  then instruction is taken branch and predicted\nPC should be used as the next PC\nPC of instruction to fetch\nFigure 3.25 A branch-target buffer. The PC of the instruction being fetched is matched against a set of instruc-\ntion addresses stored in the first column; these represent the addresses of known branches. If the PC matches one\nof these entries, then the instruction being fetched is a taken branch, and the second field, predicted PC, contains the\nprediction for the next PC after the branch. Fetching begins immediately at that address. The third field, which is\noptional, may be used for extra prediction state bits.\n3.9\nAdvanced Techniques for Instruction Delivery and Speculation\n\u25a0\n229"
    },
    {
        "page": 262,
        "text": "To evaluate how well a branch-target buffer works, we first must determine the\npenalties in all possible cases. Figure 3.27 contains this information for a simple\nfive-stage pipeline.\nExample\nDetermine the total branch penalty for a branch-target buffer assuming the penalty\ncycles for individual mispredictions in Figure 3.27. Make the following assump-\ntions about the prediction accuracy and hit rate:\n\u25a0\nPrediction accuracy is 90% (for instructions in the buffer).\n\u25a0\nHit rate in the buffer is 90% (for branches predicted taken).\nIF\nID\nEX\nSend PC to memory and\nbranch-target buffer\nEntry found in\nbranch-target\nbuffer?\nNo\nNo\nNormal\ninstruction\nexecution\nYes\nSend out\npredicted\nPC\nIs\ninstruction\na taken\nbranch?\nTaken\nbranch?\nEnter\nbranch instruction\naddress and next\nPC into branch-\ntarget buffer\nMispredicted branch,\nkill fetched instruction;\nrestart fetch at other\ntarget; delete entry\nfrom target buffer\nBranch correctly\npredicted;\ncontinue execution\nwith no stalls\nYes\nNo\nYes\nFigure 3.26 The steps involved in handling an instruction with a branch-target buffer.\n230\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 263,
        "text": "Answer\nWe compute the penalty by looking at the probability of two events: the branch is\npredicted taken but ends up being not taken, and the branch is taken but is not found\nin the buffer. Both carry a penalty of two cycles.\nProbability branch inbuffer, but actually nottaken\n\u00f0\n\u00de \u00bc Percentbuffer hitrate\n\u0003Percentincorrect predictions\n\u00bc 90%\u000310% \u00bc 0:09\nProbability branchnot inbuffer, but actually taken\n\u00f0\n\u00de \u00bc 10%\nBranchpenalty \u00bc 0:09 + 0:10\n\u00f0\n\u00de\u00032\nBranch penalty \u00bc 0:38\nThe improvement from dynamic branch prediction will grow as the pipeline\nlength, and thus the branch delay grows; in addition, better predictors will yield\na greater performance advantage. Modern high-performance processors have\nbranch misprediction delays on the order of 15 clock cycles; clearly, accurate pre-\ndiction is critical!\nOne variation on the branch-target buffer is to store one or more target instruc-\ntions instead of, or in addition to, the predicted target address. This variation has\ntwo potential advantages. First, it allows the branch-target buffer access to take\nlonger than the time between successive instruction fetches, possibly allowing a\nlarger branch-target buffer. Second, buffering the actual target instructions allows\nus to perform an optimization called branch folding. Branch folding can be used to\nobtain 0-cycle unconditional branches and sometimes 0-cycle conditional\nbranches. As we will see, the Cortex A-53 uses a single-entry branch target cache\nthat stores the predicted target instructions.\nConsider a branch-target buffer that buffers instructions from the predicted\npath and is being accessed with the address of an unconditional branch. The\nonly function of the unconditional branch is to change the PC. Thus, when\nthe branch-target buffer signals a hit and indicates that the branch is\nInstruction in buffer\nPrediction\nActual branch\nPenalty cycles\nYes\nTaken\nTaken\n0\nYes\nTaken\nNot taken\n2\nNo\nTaken\n2\nNo\nNot taken\n0\nFigure 3.27 Penalties for all possible combinations of whether the branch is in the\nbuffer and what it actually does, assuming we store only taken branches in\nthe buffer. There is no branch penalty if everything is correctly predicted and the\nbranch is found in the target buffer. If the branch is not correctly predicted, the penalty\nis equal to 1 clock cycle to update the buffer with the correct information (during which\nan instruction cannot be fetched) and 1 clock cycle, if needed, to restart fetching the\nnext correct instruction for the branch. If the branch is not found and taken, a 2-cycle\npenalty is encountered, during which time the buffer is updated.\n3.9\nAdvanced Techniques for Instruction Delivery and Speculation\n\u25a0\n231"
    },
    {
        "page": 264,
        "text": "unconditional, the pipeline can simply substitute the instruction from the\nbranch-target buffer in place of the instruction that is returned from the cache\n(which is the unconditional branch). If the processor is issuing multiple\ninstructions per cycle, then the buffer will need to supply multiple instructions\nto obtain the maximum benefit. In some cases, it may be possible to eliminate\nthe cost of a conditional branch.\nSpecialized Branch Predictors: Predicting Procedure Returns,\nIndirect Jumps, and Loop Branches\nAs we try to increase the opportunity and accuracy of speculation, we face the chal-\nlenge of predicting indirect jumps, that is, jumps whose destination address varies\nat runtime. High-level language programs will generate such jumps for indirect\nprocedure calls, select or case statements, and FORTRAN-computed gotos,\nalthough many indirect jumps simply come from procedure returns. For example,\nfor the SPEC95 benchmarks, procedure returns account for more than 15% of the\nbranches and the vast majority of the indirect jumps on average. For object-\noriented languages such as C++ and Java, procedure returns are even more fre-\nquent. Thus focusing on procedure returns seems appropriate.\nThough procedure returns can be predicted with a branch-target buffer, the\naccuracy of such a prediction technique can be low if the procedure is called\nfrom multiple sites and the calls from one site are not clustered in time. For\nexample, in SPEC CPU95, an aggressive branch predictor achieves an accu-\nracy of less than 60% for such return branches. To overcome this problem,\nsome designs use a small buffer of return addresses operating as a stack. This\nstructure caches the most recent return addresses, pushing a return address on\nthe stack at a call and popping one off at a return. If the cache is sufficiently\nlarge (i.e., as large as the maximum call depth), it will predict the returns per-\nfectly. Figure 3.28 shows the performance of such a return buffer with 0\u201316\nelements for a number of the SPEC CPU95 benchmarks. We will use a similar\nreturn predictor when we examine the studies of ILP in Section 3.10. Both the\nIntel Core processors and the AMD Phenom processors have return address\npredictors.\nIn large server applications, indirect jumps also occur for various function calls\nand control transfers. Predicting the targets of such branches is not as simple as in a\nprocedure return. Some processors have opted to add specialized predictors for all\nindirect jumps, whereas others rely on a branch target buffer.\nAlthough a simple predictor like gshare does a good job of predicting many\nconditional branches, it is not tailored to predicting loop branches, especially\nfor long running loops. As we observed earlier, the Intel Core i7 920 used a spe-\ncialized loop branch predictor. With the emergence of tagged hybrid predictors,\nwhich are as good at predicting loop branches, some recent designers have opted\nto put the resources into larger tagged hybrid predictors rather than a separate loop\nbranch predictor.\n232\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 265,
        "text": "Integrated Instruction Fetch Units\nTo meet the demands of multiple-issue processors, many recent designers have\nchosen to implement an integrated instruction fetch unit as a separate autonomous\nunit that feeds instructions to the rest of the pipeline. Essentially, this amounts to\nrecognizing that characterizing instruction fetch as a simple single pipe stage given\nthe complexities of multiple issue is no longer valid.\nInstead, recent designs have used an integrated instruction fetch unit that inte-\ngrates several functions:\n1. Integrated branch prediction\u2014The branch predictor becomes part of the\ninstruction fetch unit and is constantly predicting branches, so as to drive\nthe fetch pipeline.\nMisprediction frequency\n70%\n60%\n50%\n40%\n30%\n20%\n0\n10%\n0%\nReturn address buffer entries\n1\n2\n4\n8\n16\nGo\nm88ksim\ncc1\nCompress\nXlisp\nIjpeg\nPerl\nVortex\nFigure 3.28 Prediction accuracy for a return address buffer operated as a stack on a\nnumber of SPEC CPU95 benchmarks. The accuracy is the fraction of return addresses\npredicted correctly. A buffer of 0 entries implies that the standard branch prediction is\nused. Because call depths are typically not large, with some exceptions, a modest buffer\nworks well. These data come from Skadron et al. (1999) and use a fix-up mechanism to\nprevent corruption of the cached return addresses.\n3.9\nAdvanced Techniques for Instruction Delivery and Speculation\n\u25a0\n233"
    },
    {
        "page": 266,
        "text": "2. Instruction prefetch\u2014To deliver multiple instructions per clock, the instruction\nfetch unit will likely need to fetch ahead. The unit autonomously manages the\nprefetching of instructions (see Chapter 2 for a discussion of techniques for\ndoing this), integrating it with branch prediction.\n3. Instruction memory access and buffering\u2014When fetching multiple instructions\nper cycle, a variety of complexities are encountered, including the difficulty that\nfetching multiple instructions may require accessing multiple cache lines. The\ninstruction fetch unit encapsulates this complexity, using prefetch to try to hide\nthe cost of crossing cache blocks. The instruction fetch unit also provides buff-\nering, essentially acting as an on-demand unit to provide instructions to the\nissue stage as needed and in the quantity needed.\nVirtually all high-end processors now use a separate instruction fetch unit con-\nnected to the rest of the pipeline by a buffer containing pending instructions.\nSpeculation: Implementation Issues and Extensions\nIn this section, we explore five issues that involve the design trade-offs and chal-\nlenges in multiple-issue and speculation, starting with the use of register renaming,\nthe approach that is sometimes used instead of a reorder buffer. We then discuss\none important possible extension to speculation on control flow: an idea called\nvalue prediction.\nSpeculation Support: Register Renaming Versus Reorder Buffers\nOne alternative to the use of a reorder buffer (ROB) is the explicit use of a\nlarger physical set of registers combined with register renaming. This approach\nbuilds on the concept of renaming used in Tomasulo\u2019s algorithm and extends it.\nIn Tomasulo\u2019s algorithm, the values of the architecturally visible registers\n(x0, . . . r31 and f0, . . . f31) are contained, at any point in execution, in some\ncombination of the register set and the reservation stations. With the addition of\nspeculation, register values may also temporarily reside in the ROB. In either case,\nif the processor does not issue new instructions for a period of time, all existing\ninstructions will commit, and the register values will appear in the register file,\nwhich directly corresponds to the architecturally visible registers.\nIn the register-renaming approach, an extended set of physical registers is used\nto hold both the architecturally visible registers as well as temporary values. Thus\nthe extended registers replace most of the function of the ROB and the reservation\nstations; only a queue to ensure that instructions complete in order is needed.\nDuring instruction issue, a renaming process maps the names of architectural\nregisters to physical register numbers in the extended register set, allocating a\nnew unused register for the destination. WAW and WAR hazards are avoided\nby renaming of the destination register, and speculation recovery is handled\n234\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 267,
        "text": "because a physical register holding an instruction destination does not become the\narchitectural register until the instruction commits.\nThe renaming map is a simple data structure that supplies the physical register\nnumber of the register that currently corresponds to the specified architectural reg-\nister, a function performed by the register status table in Tomasulo\u2019s algorithm.\nWhen an instruction commits, the renaming table is permanently updated to indi-\ncate that a physical register corresponds to the actual architectural register, thus\neffectively finalizing the update to the processor state. Although an ROB is not\nnecessary with register renaming, the hardware must still track instructions in a\nqueue-like structure and update the renaming table in strict order.\nAn advantage of the renaming approach versus the ROB approach is that\ninstruction commit is slightly simplified because it requires only two simple\nactions: (1) record that the mapping between an architectural register number\nand physical register number is no longer speculative, and (2) free up any physical\nregisters being used to hold the \u201colder\u201d value of the architectural register. In a\ndesign with reservation stations, a station is freed up when the instruction using\nit completes execution, and a ROB entry is freed up when the corresponding\ninstruction commits.\nWith register renaming, deallocating registers is more complex because before\nwe free up a physical register, we must know that it no longer corresponds to an\narchitectural register and that no further uses of the physical register are outstand-\ning. A physical register corresponds to an architectural register until the architec-\ntural register is rewritten, causing the renaming table to point elsewhere. That is, if\nno renaming entry points to a particular physical register, then it no longer corre-\nsponds to an architectural register. There may, however, still be outstanding uses of\nthe physical register. The processor can determine whether this is the case by\nexamining the source register specifiers of all instructions in the functional unit\nqueues. If a given physical register does not appear as a source and it is not des-\nignated as an architectural register, it may be reclaimed and reallocated.\nAlternatively, the processor can simply wait until another instruction that\nwrites the same architectural register commits. At that point, there can be no further\nuses of the older value outstanding. Although this method may tie up a physical\nregister slightly longer than necessary, it is easy to implement and is used in most\nrecent superscalars.\nOne question you may be asking is how do we ever know which registers are\nthe architectural registers if they are constantly changing? Most of the time when\nthe program is executing, it does not matter. There are clearly cases, however,\nwhere another process, such as the operating system, must be able to know exactly\nwhere the contents of a certain architectural register reside. To understand how this\ncapability is provided, assume the processor does not issue instructions for some\nperiod of time. Eventually all instructions in the pipeline will commit, and the map-\nping between the architecturally visible registers and physical registers will\nbecome stable. At that point, a subset of the physical registers contains the archi-\ntecturally visible registers, and the value of any physical register not associated\nwith an architectural register is unneeded. It is then easy to move the architectural\n3.9\nAdvanced Techniques for Instruction Delivery and Speculation\n\u25a0\n235"
    },
    {
        "page": 268,
        "text": "registers to a fixed subset of physical registers so that the values can be commu-\nnicated to another process.\nBoth register renaming and reorder buffers continue to be used in high-end pro-\ncessors, which now feature the ability to have as many as 100 or more instructions\n(including loads and stores waiting on the cache) in flight. Whether renaming or a\nreorder buffer is used, the key complexity bottleneck for a dynamically scheduled\nsuperscalar remains issuing bundles of instructions with dependences within the\nbundle. In particular, dependent instructions in an issue bundle must be issued with\nthe assigned virtual registers of the instructions on which they depend. A strategy\nfor instruction issue with register renaming similar to that used for multiple issue\nwith reorder buffers (see page 205) can be deployed, as follows:\n1. The issue logic reserves enough physical registers for the entire issue bundle\n(say, four registers for a four-instruction bundle with at most one register result\nper instruction).\n2. The issue logic determines what dependences exist within the bundle. If a\ndependence does not exist within the bundle, the register renaming structure\nis used to determine the physical register that holds, or will hold, the result\non which instruction depends. When no dependence exists within the bundle,\nthe result is from an earlier issue bundle, and the register renaming table will\nhave the correct register number.\n3. If an instruction depends on an instruction that is earlier in the bundle, then the\npre-reserved physical register in which the result will be placed is used to update\nthe information for the issuing instruction.\nNote that just as in the reorder buffer case, the issue logic must both determine\ndependences within the bundle and update the renaming tables in a single clock,\nand as before, the complexity of doing this for a larger number of instructions per\nclock becomes a chief limitation in the issue width.\nThe Challenge of More Issues per Clock\nWithout speculation, there is little motivation to try to increase the issue rate\nbeyond two, three, or possibly four issues per clock because resolving branches\nwould limit the average issue rate to a smaller number. Once a processor includes\naccurate branch prediction and speculation, we might conclude that increasing the\nissue rate would be attractive. Duplicating the functional units is straightforward\nassuming silicon capacity and power; the real complications arise in the issue step\nand correspondingly in the commit step. The Commit step is the dual of the issue\nstep, and the requirements are similar, so let\u2019s take a look at what has to happen for\na six-issue processor using register renaming.\nFigure 3.29 shows a six-instruction code sequence and what the issue step must\ndo. Remember that this must all occur in a single clock cycle, if the processor is to\nmaintain a peak rate of six issues per clock! All the dependences must be detected,\n236\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 269,
        "text": "the physical registers must be assigned, and the instructions must be rewritten\nusing the physical register numbers: in one clock. This example makes it clear\nwhy issue rates have grown from 3\u20134 to only 4\u20138 in the past 20 years. The com-\nplexity of the analysis required during the issue cycle grows as the square of the\nissue width, and a new processor is typically targeted to have a higher clock rate\nthan in the last generation! Because register renaming and the reorder buffer\napproaches are duals, the same complexities arise independent of the implemen-\ntation scheme.\nHow Much to Speculate\nOne of the significant advantages of speculation is its ability to uncover events that\nwould otherwise stall the pipeline early, such as cache misses. This potential\nadvantage, however, comes with a significant potential disadvantage. Speculation\nis not free. It takes time and energy, and the recovery of incorrect speculation fur-\nther reduces performance. In addition, to support the higher instruction execution\nrate needed to benefit from speculation, the processor must have additional\nresources, which take silicon area and power. Finally, if speculation causes an\nexceptional event to occur, such as a cache or translation lookaside buffer\n(TLB) miss, the potential for significant performance loss increases, if that event\nwould not have occurred without speculation.\nTo maintain most of the advantage while minimizing the disadvantages, most\npipelines with speculation will allow only low-cost exceptional events (such as a\nfirst-level cache miss) to be handled in speculative mode. If an expensive exceptional\nevent occurs, suchasasecond-levelcache missora TLBmiss, theprocessor willwait\nInstr. #\nInstruction\nPhysical register assigned\nor destination\nInstruction with physical\nregister numbers\nRename map\nchanges\n1\nadd x1,x2,x3\np32\nadd p32,p2,p3\nx1-> p32\n2\nsub x1,x1,x2\np33\nsub p33,p32,p2\nx1->p33\n3\nadd x2,x1,x2\np34\nadd p34,p33,x2\nx2->p34\n4\nsub x1,x3,x2\np35\nsub p35,p3,p34\nx1->p35\n5\nadd x1,x1,x2\np36\nadd p36,p35,p34\nx1->p36\n6\nsub x1,x3,x1\np37\nsub p37,p3,p36\nx1->p37\nFigure 3.29 An example of six instructions to be issued in the same clock cycle and what has to happen. The\ninstructions are shown in program order: 1\u20136; they are, however, issued in 1 clock cycle! The notation pi is used\nto refer to a physical register; the contents of that register at any point is determined by the renaming map. For sim-\nplicity, we assume that the physical registers holding the architectural registers x1, x2, and x3 are initially p1, p2,\nand p3 (they could be any physical register). The instructions are issued with physical register numbers, as shown in\ncolumn four. The rename map, which appears in the last column, shows how the map would change if the instruc-\ntions were issued sequentially. The difficulty is that all this renaming and replacement of architectural registers by\nphysical renaming registers happens effectively in 1 cycle, not sequentially. The issue logic must find all the depen-\ndences and \u201crewrite\u201d the instruction in parallel.\n3.9\nAdvanced Techniques for Instruction Delivery and Speculation\n\u25a0\n237"
    },
    {
        "page": 270,
        "text": "until the instruction causing the event is no longer speculative before handling the\nevent. Although this may slightly degrade the performance of some programs, it\navoids significant performance losses in others, especially those that suffer from a\nhigh frequency of such events coupled with less-than-excellent branch prediction.\nIn the 1990s the potential downsides of speculation were less obvious. As pro-\ncessors have evolved, the real costs of speculation have become more apparent,\nand the limitations of wider issue and speculation have been obvious. We return\nto this issue shortly.\nSpeculating Through Multiple Branches\nIn the examples we have considered in this chapter, it has been possible to resolve a\nbranch before having to speculate on another. Three different situations can benefit\nfrom speculating on multiple branches simultaneously: (1) a very high branch fre-\nquency, (2) significant clustering of branches, and (3) long delays in functional\nunits. In the first two cases, achieving high performance may mean that multiple\nbranches are speculated, and it may even mean handling more than one branch per\nclock. Database programs and other less structured integer computations, often\nexhibit these properties, making speculation on multiple branches important. Like-\nwise, long delays in functional units can raise the importance of speculating on\nmultiple branches as a way to avoid stalls from the longer pipeline delays.\nSpeculating on multiple branches slightly complicates the process of specula-\ntion recovery but is straightforward otherwise. As of 2017, no processor has yet\ncombined full speculation with resolving multiple branches per cycle, and it\nis unlikely that the costs of doing so would be justified in terms of performance\nversus complexity and power.\nSpeculation and the Challenge of Energy Efficiency\nWhat is the impact of speculation on energy efficiency? At first glance, one might\nargue that using speculation always decreases energy efficiency because whenever\nspeculation is wrong, it consumes excess energy in two ways:\n1. Instructions that are speculated and whose results are not needed generate\nexcess work for the processor, wasting energy.\n2. Undoing the speculation and restoring the state of the processor to continue exe-\ncution at the appropriate address consumes additional energy that would not be\nneeded without speculation.\nCertainly, speculation will raise the power consumption, and if we could control\nspeculation, it would be possible to measure the cost (or at least the dynamic power\ncost). But, if speculation lowers the execution time by more than it increases the\naverage power consumption, then the total energy consumed may be less.\nThus, to understand the impact of speculation on energy efficiency, we need\nto look at how often speculation is leading to unnecessary work. If a significant\nnumber of unneeded instructions is executed, it is unlikely that speculation will\n238\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 271,
        "text": "improve running time by a comparable amount. Figure 3.30 shows the fraction of\ninstructions that are executed from misspeculation for a subset of the SPEC2000\nbenchmarks using a sophisticated branch predictor. As we can see, this fraction of\nexecuted misspeculated instructions is small in scientific code and significant\n(about 30% on average) in integer code. Thus it is unlikely that speculation is\nenergy-efficient for integer applications, and the end of Dennard scaling makes\nimperfect speculation more problematic. Designers could avoid speculation, try\nto reduce the misspeculation, or think about new approaches, such as only spec-\nulating on branches that are known to be highly predictable.\nAddress Aliasing Prediction\nAddress aliasing prediction is a technique that predicts whether two stores or a load\nand a store refer to the same memory address. If two such references do not refer to\nthe same address, then they may be safely interchanged. Otherwise, we must wait\nuntil the memory addresses accessed by the instructions are known. Because we\nneed not actually predict the address values, only whether such values conflict,\nthe prediction can be reasonably accurate with small predictors. Address prediction\nrelies on the ability of a speculative processor to recover after a misprediction; that\nis, if the actual addresses that were predicted to be different (and thus not alias) turn\nout to be the same (and thus are aliases), the processor simply restarts the sequence,\n0%\n164.gzip\n175.vpr\n176.gcc\n181.mcf\n186.crafty\n168.wupwise\n171.swim\n172.mgrid\n173.applu\n177.mesa\nMisspeculation\n45%\n40%\n35%\n30%\n25%\n20%\n15%\n10%\n5%\nFigure 3.30 The fraction of instructions that are executed as a result of misspeculation is typically much higher\nfor integer programs (the first five) versus FP programs (the last five).\n3.9\nAdvanced Techniques for Instruction Delivery and Speculation\n\u25a0\n239"
    },
    {
        "page": 272,
        "text": "just as though it had mispredicted a branch. Address value speculation has been\nused in several processors already and may become universal in the future.\nAddress prediction is a simple and restricted form of value prediction, which\nattempts to predict the value that will be produced by an instruction. Value predic-\ntion could, if it were highly accurate, eliminate data flow restrictions and achieve\nhigher rates of ILP. Despite many researchers focusing on value prediction in the\npast 15 years in dozens of papers, the results have never been sufficiently attractive\nto justify general value prediction in real processors.\n3.10\nCross-Cutting Issues\nHardware Versus Software Speculation\nThe hardware-intensive approaches to speculation in this chapter and the software\napproaches of Appendix H provide alternative approaches to exploiting ILP. Some\nof the trade-offs, and the limitations, for these approaches are listed here:\n\u25a0\nTo speculate extensively, we must be able to disambiguate memory references.\nThis capability is difficult to do at compile time for integer programs that con-\ntain pointers. In a hardware-based scheme, dynamic runtime disambiguation of\nmemory addresses is done using the techniques we saw earlier for Tomasulo\u2019s\nalgorithm. This disambiguation allows us to move loads past stores at runtime.\nSupport for speculative memory references can help overcome the conserva-\ntism of the compiler, but unless such approaches are used carefully, the over-\nhead of the recovery mechanisms may swamp the advantages.\n\u25a0\nHardware-based speculation works better when control flow is unpredictable\nand when hardware-based branch prediction is superior to software-based\nbranch prediction done at compile time. These properties hold for many integer\nprograms, where the misprediction rates for dynamic predictors are usually less\nthan one-half of those for static predictors. Because speculated instructions\nmay slow down the computation when the prediction is incorrect, this differ-\nence is significant. One result of this difference is that even statically scheduled\nprocessors normally include dynamic branch predictors.\n\u25a0\nHardware-based speculation maintains a completely precise exception model\neven for speculated instructions. Recent software-based approaches have\nadded special support to allow this as well.\n\u25a0\nHardware-based speculation does not require compensation or bookkeeping\ncode, which is needed by ambitious software speculation mechanisms.\n\u25a0\nCompiler-based approaches may benefit from the ability to see further into the\ncode sequence, resulting in better code scheduling than a purely hardware-\ndriven approach.\n\u25a0\nHardware-based speculation with dynamic scheduling does not require differ-\nent code sequences to achieve good performance for different implementations\n240\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 273,
        "text": "of an architecture. Although this advantage is the hardest to quantify, it may be\nthe most important one in the long run. Interestingly, this was one of the moti-\nvations for the IBM 360/91. On the other hand, more recent explicitly parallel\narchitectures, such as IA-64, have added flexibility that reduces the hardware\ndependence inherent in a code sequence.\nThe major disadvantage of supporting speculation in hardware is the complex-\nity and additional hardware resources required. This hardware cost must be eval-\nuated against both the complexity of a compiler for a software-based approach and\nthe amount and usefulness of the simplifications in a processor that relies on such a\ncompiler.\nSome designers have tried to combine the dynamic and compiler-based\napproaches to achieve the best of each. Such a combination can generate interest-\ning and obscure interactions. For example, if conditional moves are combined with\nregister renaming, a subtle side effect appears. A conditional move that is annulled\nmust still copy a value to the destination register because it was renamed earlier in\nthe instruction pipeline. These subtle interactions complicate the design and ver-\nification process and can also reduce performance.\nThe Intel Itanium processor was the most ambitious computer ever designed\nbased on the software support for ILP and speculation. It did not deliver on the\nhopes of the designers, especially for general-purpose, nonscientific code. As\ndesigners\u2019 ambitions for exploiting ILP were reduced in light of the difficulties\ndescribed on page 244, most architectures settled on hardware-based mechanisms\nwith issue rates of three to four instructions per clock.\nSpeculative Execution and the Memory System\nInherent in processors that support speculative execution or conditional instruc-\ntions is the possibility of generating invalid addresses that would not occur without\nspeculative execution. Not only would this be incorrect behavior if protection\nexceptions were taken, but also the benefits of speculative execution would be\nswamped by false exception overhead. Therefore the memory system must identify\nspeculatively executed instructions and conditionally executed instructions and\nsuppress the corresponding exception.\nBy similar reasoning, we cannot allow such instructions to cause the cache\nto stall on a miss because, again, unnecessary stalls could overwhelm the\nbenefits of speculation. Thus these processors must be matched with nonblock-\ning caches.\nIn reality, the penalty of a miss that goes to DRAM is so large that speculated\nmisses are handled only when the next level is on-chip cache (L2 or L3). Figure 2.5\non page 84 shows that for some well-behaved scientific programs, the compiler can\nsustain multiple outstanding L2 misses to cut the L2 miss penalty effectively. Once\nagain, for this to work, the memory system behind the cache must match the goals\nof the compiler in number of simultaneous memory accesses.\n3.10\nCross-Cutting Issues\n\u25a0\n241"
    },
    {
        "page": 274,
        "text": "3.11\nMultithreading: Exploiting Thread-Level Parallelism\nto Improve Uniprocessor Throughput\nThe topic we cover in this section, multithreading, is truly a cross-cutting topic,\nbecause it has relevance to pipelining and superscalars, to graphics processing\nunits (Chapter 4), and to multiprocessors (Chapter 5). A thread is like a process\nin that it has state and a current program counter, but threads typically share the\naddress space of a single process, allowing a thread to easily access data of other\nthreads within the same process. Multithreading is a technique whereby multiple\nthreads share a processor without requiring an intervening process switch. The\nability to switch between threads rapidly is what enables multithreading to be used\nto hide pipeline and memory latencies.\nIn the next chapter, we will see how multithreading provides the same advan-\ntages in GPUs. Finally, Chapter 5 will explore the combination of multithreading\nand multiprocessing. These topics are closely interwoven because multithreading\nis a primary technique for exposing more parallelism to the hardware. In a strict\nsense, multithreading uses thread-level parallelism, and thus is properly the subject\nof Chapter 5, but its role in both improving pipeline utilization and in GPUs moti-\nvates us to introduce the concept here.\nAlthough increasing performance by using ILP has the great advantage\nthat it is reasonably transparent to the programmer, as we have seen, ILP can\nbe quite limited or difficult to exploit in some applications. In particular, with\nreasonable instruction issue rates, cache misses that go to memory or off-chip\ncaches are unlikely to be hidden by available ILP. Of course, when the processor\nis stalled waiting on a cache miss, the utilization of the functional units drops\ndramatically.\nBecause attempts to cover long memory stalls with more ILP have limited\neffectiveness, it is natural to ask whether other forms of parallelism in an applica-\ntion could be used to hide memory delays. For example, an online transaction pro-\ncessing system has natural parallelism among the multiple queries and updates that\nare presented by requests. Of course, many scientific applications contain natural\nparallelism because they often model the three-dimensional, parallel structure of\nnature, and that structure can be exploited by using separate threads. Even desktop\napplications that use modern Windows-based operating systems often have mul-\ntiple active applications running, providing a source of parallelism.\nMultithreading allows multiple threads to share the functional units of a single\nprocessor in an overlapping fashion. In contrast, a more general method to exploit\nthread-level parallelism (TLP) is with a multiprocessor that has multiple indepen-\ndent threads operating at once and in parallel. Multithreading, however, does not\nduplicate the entire processor as a multiprocessor does. Instead, multithreading\nshares most of the processor core among a set of threads, duplicating only private\nstate, such as the registers and program counter. As we will see in Chapter 5, many\nrecent processors incorporate both multiple processor cores on a single chip and\nprovide multithreading within each core.\n242\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 275,
        "text": "Duplicating the per-thread state of a processor core means creating a separate\nregister file and a separate PC for each thread. The memory itself can be shared\nthrough the virtual memory mechanisms, which already support multiprogram-\nming. In addition, the hardware must support the ability to change to a different\nthread relatively quickly; in particular, a thread switch should be much more effi-\ncient than a process switch, which typically requires hundreds to thousands of pro-\ncessor cycles. Of course, for multithreading hardware to achieve performance\nimprovements, a program must contain multiple threads (we sometimes say that\nthe application is multithreaded) that could execute in concurrent fashion. These\nthreads are identified either by a compiler (typically from a language with paral-\nlelism constructs) or by the programmer.\nThere are three main hardware approaches to multithreading: fine-grained,\ncoarse-grained, and simultaneous. Fine-grained multithreading switches between\nthreads on each clock cycle, causing the execution of instructions from multiple\nthreads to be interleaved. This interleaving is often done in a round-robin fashion,\nskipping any threads that are stalled at that time. One key advantage of fine-\ngrained multithreading is that it can hide the throughput losses that arise from\nboth short and long stalls because instructions from other threads can be executed\nwhen one thread stalls, even if the stall is only for a few cycles. The primary\ndisadvantage of fine-grained multithreading is that it slows down the execution\nof an individual thread because a thread that is ready to execute without stalls\nwill be delayed by instructions from other threads. It trades an increase in multi-\nthreaded throughput for a loss in the performance (as measured by latency) of a\nsingle thread.\nThe SPARC T1 through T5 processors (originally made by Sun, now made by\nOracle and Fujitsu) use fine-grained multithreading. These processors were tar-\ngeted at multithreaded workloads such as transaction processing and web services.\nThe T1 supported 8 cores per processor and 4 threads per core, while the T5\nsupports 16 cores and 128 threads per core. Later versions (T2\u2013T5) also supported\n4\u20138 processors. The NVIDIA GPUs, which we look at in the next chapter, also\nmake use of fine-grained multithreading.\nCoarse-grained multithreading was invented as an alternative to fine-grained\nmultithreading. Coarse-grained multithreading switches threads only on costly\nstalls, such as level two or three cache misses. Because instructions from other\nthreads will be issued only when a thread encounters a costly stall, coarse-grained\nmultithreading relieves the need to have thread-switching be essentially free and is\nmuch less likely to slow down the execution of any one thread.\nCoarse-grained multithreading suffers, however, from a major drawback: it is\nlimited in its ability to overcome throughput losses, especially from shorter stalls.\nThis limitation arises from the pipeline start-up costs of coarse-grained multi-\nthreading. Because a processor with coarse-grained multithreading issues instruc-\ntions from a single thread, when a stall occurs, the pipeline will see a bubble before\nthe new thread begins executing. Because of this start-up overhead, coarse-grained\nmultithreading is much more useful for reducing the penalty of very high-cost\nstalls, where pipeline refill is negligible compared to the stall time. Several research\n3.11\nMultithreading: Exploiting Thread-Level Parallelism to Improve Uniprocessor Throughput\n\u25a0\n243"
    },
    {
        "page": 276,
        "text": "projects have explored coarse-grained multithreading, but no major current proces-\nsors use this technique.\nThe most common implementation of multithreading is called simultaneous\nmultithreading (SMT). Simultaneous multithreading is a variation on fine-grained\nmultithreading that arises naturally when fine-grained multithreading is implemen-\nted on top of a multiple-issue, dynamically scheduled processor. As with other\nforms of multithreading, SMT uses thread-level parallelism to hide long-latency\nevents in a processor, thereby increasing the usage of the functional units. The\nkey insight in SMT is that register renaming and dynamic scheduling allow mul-\ntiple instructions from independent threads to be executed without regard to the\ndependences among them; the resolution of the dependences can be handled by\nthe dynamic scheduling capability.\nFigure 3.31 conceptually illustrates the differences in a processor\u2019s ability to\nexploit the resources of a superscalar for the following processor configurations:\n\u25a0\nA superscalar with no multithreading support\n\u25a0\nA superscalar with coarse-grained multithreading\nSuperscalar\nCoarse MT\nFine MT\nSMT\nTime\nExecution slots\nFigure 3.31 How four different approaches use the functional unit execution slots of a superscalar processor. The\nhorizontal dimension represents the instruction execution capability in each clock cycle. The vertical dimension rep-\nresents a sequence of clock cycles. An empty (white) box indicates that the corresponding execution slot is unused in\nthat clock cycle. The shades of gray and black correspond to four different threads in the multithreading processors.\nBlack is also used to indicate the occupied issue slots in the case of the superscalar without multithreading support.\nThe Sun T1 and T2 (aka Niagara) processors are fine-grained, multithreaded processors, while the Intel Core i7 and\nIBM Power7 processors use SMT. The T2 has 8 threads, the Power7 has 4, and the Intel i7 has 2. In all existing SMTs,\ninstructions issue from only one thread at a time. The difference in SMT is that the subsequent decision to execute an\ninstruction is decoupled and could execute the operations coming from several different instructions in the same\nclock cycle.\n244\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 277,
        "text": "\u25a0\nA superscalar with fine-grained multithreading\n\u25a0\nA superscalar with simultaneous multithreading\nIn the superscalar without multithreading support, the use of issue slots is lim-\nited by a lack of ILP, including ILP to hide memory latency. Because of the length\nof L2 and L3 cache misses, much of the processor can be left idle.\nIn the coarse-grained multithreaded superscalar, the long stalls are partially\nhidden by switching to another thread that uses the resources of the processor. This\nswitching reduces the number of completely idle clock cycles. In a coarse-grained\nmultithreaded processor, however, thread switching occurs only when there is a\nstall. Because the new thread has a start-up period, there are likely to be some fully\nidle cycles remaining.\nIn the fine-grained case, the interleaving of threads can eliminate fully empty\nslots. In addition, because the issuing thread is changed on every clock cycle, longer\nlatency operations can be hidden. Because instruction issue and execution are con-\nnected,athread canissueonlyasmany instructionsasareready.Withanarrowissue\nwidth, this is not a problem (a cycle is either occupied or not), which is why fine-\ngrained multithreading works perfectly for a single issue processor, and SMT would\nmake no sense. Indeed, in the Sun T2, there are two issues per clock, but they are\nfrom different threads. This eliminates the need to implement the complex dynamic\nscheduling approach and relies instead on hiding latency with more threads.\nIf one implements fine-grained threading on top of a multiple-issue, dynami-\ncally schedule processor, the result is SMT. In all existing SMT implementations,\nall issues come from one thread, although instructions from different threads can\ninitiate execution in the same cycle, using the dynamic scheduling hardware to\ndetermine what instructions are ready. Although Figure 3.31 greatly simplifies\nthe real operation of these processors, it does illustrate the potential performance\nadvantages of multithreading in general and SMT in wider issue, dynamically\nscheduled processors.\nSimultaneous multithreading uses the insight that a dynamically scheduled\nprocessor already has many of the hardware mechanisms needed to support the\nmechanism, including a large virtual register set. Multithreading can be built on\ntop of an out-of-order processor by adding a per-thread renaming table, keeping\nseparate PCs, and providing the capability for instructions from multiple threads\nto commit.\nEffectiveness of Simultaneous Multithreading on Superscalar\nProcessors\nA key question is, how much performance can be gained by implementing SMT?\nWhen this question was explored in 2000\u20132001, researchers assumed that dynamic\nsuperscalars would get much wider in the next five years, supporting six to eight\nissues per clock with speculative dynamic scheduling, many simultaneous loads\nand stores, large primary caches, and four to eight contexts with simultaneous issue\n3.11\nMultithreading: Exploiting Thread-Level Parallelism to Improve Uniprocessor Throughput\n\u25a0\n245"
    },
    {
        "page": 278,
        "text": "and retirement from multiple contexts. No processor has gotten close to this\ncombination.\nAs a result, simulation research results that showed gains for multiprogrammed\nworkloads of two or more times are unrealistic. In practice, the existing implemen-\ntations of SMT offer only two to four contexts with fetching and issue from only\none, and up to four issues per clock. The result is that the gain from SMT is also\nmore modest.\nEsmaeilzadeh et al. (2011) did an extensive and insightful set of measurements\nthat examined both the performance and energy benefits of using SMT in a single\ni7 920 core running a set of multithreaded applications. The Intel i7 920 supported\nSMT with two threads per core, as does the recent i7 6700. The changes between\nthe i7 920 and the 6700 are relatively small and are unlikely to significantly change\nthe results as shown in this section.\nThe benchmarks used consist of a collection of parallel scientific applications\nand a set of multithreaded Java programs from the DaCapo and SPEC Java suite, as\nsummarized in Figure 3.32. Figure 3.31 shows the ratios of performance and\nenergy efficiency for these benchmarks when run on one core of a i7 920 with\nSMT turned off and on. (We plot the energy efficiency ratio, which is the inverse\nof energy consumption, so that, like speedup, a higher ratio is better.)\nThe harmonic mean of the speedup for the Java benchmarks is 1.28, despite the\ntwo benchmarks that see small gains. These two benchmarks, pjbb2005 and trade-\nbeans, while multithreaded, have limited parallelism. They are included because\nthey are typical of a multithreaded benchmark that might be run on an SMT pro-\ncessor with the hope of extracting some performance, which they find in limited\namounts. The PARSEC benchmarks obtain somewhat better speedups than the full\nset of Java benchmarks (harmonic mean of 1.31). If tradebeans and pjbb2005 were\nomitted, the Java workload would actually have significantly better speedup (1.39)\nthan the PARSEC benchmarks. (See the discussion of the implication of using har-\nmonic mean to summarize the results in the caption of Figure 3.33.)\nEnergy consumption is determined by the combination of speedup and increase\nin power consumption. For the Java benchmarks, on average, SMT delivers the\nsame energy efficiency as non-SMT (average of 1.0), but it is brought down by\nthe two poor performing benchmarks; without pjbb2005 and tradebeans, the aver-\nage energy efficiency for the Java benchmarks is 1.06, which is almost as good as\nthe PARSEC benchmarks. In the PARSEC benchmarks, SMT reduces energy by\n1(1/1.08)\u00bc7%. Such energy-reducing performance enhancements are very dif-\nficult to find. Of course, the static power associated with SMT is paid in both cases,\nthus the results probably slightly overstate the energy gains.\nThese results clearly show that SMT with extensive support in an aggressive\nspeculative processor can improve performance in an energy-efficient fashion. In\n2011, the balance between offering multiple simpler cores and fewer more sophis-\nticated cores has shifted in favor of more cores, with each core typically being a\nthree- to four-issue superscalar with SMT supporting two to four threads. Indeed,\nEsmaeilzadeh et al. (2011) show that the energy improvements from SMT are even\nlarger on the Intel i5 (a processor similar to the i7, but with smaller caches and a\n246\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 279,
        "text": "lower clock rate) and the Intel Atom (an 80x86 processor originally designed for\nthe netbook and PMD market, now focused on low-end PCs, and described in\nSection 3.13).\n3.12\nPutting It All Together: The Intel Core i7 6700\nand ARM Cortex-A53\nIn this section, we explore the design of two multiple issue processors: the ARM\nCortex-A53 core, which is used as the basis for several tablets and cell phones, and\nthe Intel Core i7 6700, a high-end, dynamically scheduled, speculative processor\nintended for high-end desktops and server applications. We begin with the simpler\nprocessor.\nblackscholes\nPrices a portfolio of options with the Black-Scholes PDE\nbodytrack\nTracks a markerless human body\ncanneal\nMinimizes routing cost of a chip with cache-aware simulated annealing\nfacesim\nSimulates motions of a human face for visualization purposes\nferret\nSearch engine that finds a set of images similar to a query image\nfluidanimate\nSimulates physics of fluid motion for animation with SPH algorithm\nraytrace\nUses physical simulation for visualization\nstreamcluster\nComputes an approximation for the optimal clustering of data points\nswaptions\nPrices a portfolio of swap options with the Heath\u2013Jarrow\u2013Morton framework\nvips\nApplies a series of transformations to an image\nx264\nMPG-4 AVC/H.264 video encoder\neclipse\nIntegrated development environment\nlusearch\nText search tool\nsunflow\nPhoto-realistic rendering system\ntomcat\nTomcat servlet container\ntradebeans\nTradebeans Daytrader benchmark\nxalan\nAn XSLT processor for transforming XML documents\npjbb2005\nVersion of SPEC JBB2005 (but fixed in problem size rather than time)\nFigure 3.32 The parallel benchmarks used here to examine multithreading, as well as in Chapter 5 to\nexamine multiprocessing with an i7. The top half of the chart consists of PARSEC benchmarks collected by\nBienia et al. (2008). The PARSEC benchmarks are meant to be indicative of compute-intensive, parallel appli-\ncations that would be appropriate for multicore processors. The lower half consists of multithreaded Java\nbenchmarks from the DaCapo collection (see Blackburn et al., 2006) and pjbb2005 from SPEC. All of these\nbenchmarks contain some parallelism; other Java benchmarks in the DaCapo and SPEC Java workloads use\nmultiple threads but have little or no true parallelism and, hence, are not used here. See Esmaeilzadeh et al.\n(2011) for additional information on the characteristics of these benchmarks, relative to the measurements\nhere and in Chapter 5.\n3.12\nPutting It All Together: The Intel Core i7 6700 and ARM Cortex-A53\n\u25a0\n247"
    },
    {
        "page": 280,
        "text": "The ARM Cortex-A53\nThe A53 is a dual-issue, statically scheduled superscalar with dynamic issue detec-\ntion, which allows the processor to issue two instructions per clock. Figure 3.34\nshows the basic pipeline structure of the pipeline. For nonbranch, integer instruc-\ntions, there are eight stages: F1, F2, D1, D2, D3/ISS, EX1, EX2, and WB, as\ndescribed in the caption. The pipeline is in order, so an instruction can initiate exe-\ncution only when its results are available and when proceeding instructions have\ninitiated. Thus, if the next two instructions are dependent, both can proceed to the\nappropriate execution pipeline, but they will be serialized when they get to the\nbeginning of that pipeline. When the scoreboard-based issue logic indicates that\nthe result from the first instruction is available, the second instruction can issue.\n2.00\n1.75\n1.50\n1.25\n1.00\n0.75\ni7 SMT performance and energy efficiency ratio\nEclipse\nSunflow\nTomcat\nXalan\nBlackscholes\nBodytrack\nCanneal\nFerret\nFluidanimate\nRaytrace\nStreamcluster\nSwaptions\n\u00d7264\nEnergy efficiency\nSpeedup\nLusearch\nTradebeans\nPjbb2005\nFacesim\nVips\nFigure 3.33 The speedup from using multithreading on one core on an i7 processor averages 1.28 for the Java\nbenchmarks and 1.31 for the PARSEC benchmarks (using an unweighted harmonic mean, which implies a work-\nload where the total time spent executing each benchmark in the single-threaded base set was the same). The\nenergy efficiency averages 0.99 and 1.07, respectively (using the harmonic mean). Recall that anything above 1.0 for\nenergy efficiency indicates that the feature reduces execution time by more than it increases average power. Two of\nthe Java benchmarks experience little speedup and have significant negative energy efficiency because of this issue.\nTurbo Boost is off in all cases. These data were collected and analyzed by Esmaeilzadeh et al. (2011) using the Oracle\n(Sun) HotSpot build 16.3-b01 Java 1.6.0 Virtual Machine and the gcc v4.4.1 native compiler.\n248\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 281,
        "text": "The four cycles of instruction fetch include an address generation unit that pro-\nduces the next PC either by incrementing the last PC or from one of four predictors:\n1. A single-entry branch target cache containing two instruction cache fetches (the\nnext two instructions following the branch, assuming the prediction is correct).\nThis target cache is checked during the first fetch cycle, if it hits; then the next\ntwo instructions are supplied from the target cache. In case of a hit and a correct\nprediction, the branch is executed with no delay cycles.\n2. A 3072-entry hybrid predictor, used for all instructions that do not hit in the\nbranch target cache, and operating during F3. Branches handled by this predic-\ntor incur a 2-cycle delay.\n3. A 256-entry indirect branch predictor that operates during F4; branches pre-\ndicted by this predictor incur a three-cycle delay when predicted correctly.\n4. An 8-deep return stack, operating during F4 and incurring a three-cycle delay.\nFloating Point execute\nInteger execute and load-store\nInstruction fetch & predict\nInstruction Decode\nAGU\n+\nTLB\nInstruction\ncache\nF1\nF3\nF2\nWriteback\nD1\nIss\nEx1\nEx2\nWr\nALU pipe 0\nALU pipe 1\nMAC pipe\nDivide pipe\nLoad pipe\nMUL/DIV/SQRT pipe\nALU pipe\nHybrid\npredictor\nIndirect\npredictor\nEarly\ndecode\n13-Entry\ninstruction\nqueue\nF4\nD2\nMain\ndecode\nLate\ndecode\nD3\nIssue\nInteger\nregister\nfile\nStore pipe\nNEON\nregister\nfile\nF1\nF2\nF3\nF4\nF5\nFigure 3.34 The basic structure of the A53 integer pipeline is 8 stages: F1 and F2 fetch the instruction, D1 and D2\ndo the basic decoding, and D3 decodes some more complex instructions and is overlapped with the first stage of\nthe execution pipeline (ISS). After ISS, the Ex1, EX2, and WB stages complete the integer pipeline. Branches use four\ndifferent predictors, depending on the type. The floating-point execution pipeline is 5 cycles deep, in addition to the 5\ncycles needed for fetch and decode, yielding 10 stages in total.\n3.12\nPutting It All Together: The Intel Core i7 6700 and ARM Cortex-A53\n\u25a0\n249"
    },
    {
        "page": 282,
        "text": "Branch decisions are made in ALU pipe 0, resulting in a branch misprediction\npenalty of 8 cycles. Figure 3.35 shows the misprediction rate for SPECint2006.\nThe amount of work that is wasted depends on both the misprediction rate and\nthe issue rate sustained during the time that the mispredicted branch was followed.\nAs Figure 3.36 shows, wasted work generally follows the misprediction rate,\nthough it may be larger or occasionally shorter.\nPerformance of the A53 Pipeline\nThe A53 has an ideal CPI of 0.5 because of its dual-issue structure. Pipeline stalls\ncan arise from three sources:\n1. Functional hazards, which occur because two adjacent instructions selected for\nissue simultaneously use the same functional pipeline. Because the A53 is stat-\nically scheduled, the compiler should try to avoid such conflicts. When such\nhmmer\n0%\nh264ref libquantum perlbench\nsjeng\nbzip2\ngobmk\nxalancbmk\ngcc\nastar\nomnetpp\nmcf\n2%\n4%\n6%\n8%\n10%\n12%\nBranch misprediction rate\n14%\n16%\n18%\n20%\n22%\nFigure 3.35 Misprediction rate of the A53 branch predictor for SPECint2006.\n250\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 283,
        "text": "instructions appear sequentially, they will be serialized at the beginning of the\nexecution pipeline, when only the first instruction will begin execution.\n2. Data hazards, which are detected early in the pipeline and may stall either both\ninstructions (if the first cannot issue, the second is always stalled) or the second\nof a pair. Again, the compiler should try to prevent such stalls when possible.\n3. Control hazards, which arise only when branches are mispredicted.\nBoth TLB misses and cache misses also cause stalls. On the instruction side, a\nTLB or cache miss causes a delay in filling the instruction queue, likely leading to a\ndownstream stall of the pipeline. Of course, this depends on whether it is an L1\nmiss, which might be largely hidden if the instruction queue was full at the time\nof the miss, or an L2 miss, which takes considerably longer. On the data side, a\ncache or TLB miss will cause the pipeline to stall because the load or store that\nhmmer\n0%\nh264ref libquantum perlbench\nsjeng\nbzip2\ngobmk\nxalancbmk\ngcc\nastar\nomnetpp\nmcf\n2%\n4%\n6%\n8%\n10%\n12%\n% Wasted work\n14%\n16%\n18%\n20%\n22%\nFigure 3.36 Wasted work due to branch misprediction on the A53. Because the A53 is an in-order machine, the\namount of wasted work depends on a variety of factors, including data dependences and cache misses, both of which\nwill cause a stall.\n3.12\nPutting It All Together: The Intel Core i7 6700 and ARM Cortex-A53\n\u25a0\n251"
    },
    {
        "page": 284,
        "text": "caused the miss cannot proceed down the pipeline. All other subsequent instruc-\ntions will thus be stalled. Figure 3.37 shows the CPI and the estimated contribu-\ntions from various sources.\nThe A53 uses a shallow pipeline and a reasonably aggressive branch predictor,\nleading to modest pipeline losses, while allowing the processor to achieve high\nclock rates at modest power consumption. In comparison with the i7, the A53 con-\nsumes approximately 1/200 the power for a quad core processor!\nThe Intel Core i7\nThe i7 uses an aggressive out-of-order speculative microarchitecture with deep\npipelines with the goal of achieving high instruction throughput by combining mul-\ntiple issue and high clock rates. The first i7 processor was introduced in 2008; the i7\n6700 is the sixth generation. The basic structure of the i7 is similar, but successive\nhmmer\nh264ref\nlibquantum perlbench\nsjeng\nbzip2\ngobmk\nxalancbmk\ngcc\nastar\nomnetpp\nmcf\nMemory hierarchy stalls\nPipeline stalls\nIdeal CPI\n0.97\n1.04\n1.07\n1.17\n1.22\n1.33\n1.39\n1.75\n1.76\n2.14\n3.37\n8.56\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nFigure 3.37 The estimated composition of the CPI on the ARM A53 shows that pipeline stalls are significant but\nare outweighed by cache misses in the poorest performing programs. This estimate is obtained by using the L1 and\nL2 miss rates and penalties to compute the L1 and L2 generated stalls per instruction. These are subtracted from the\nCPI measured by a detailed simulator to obtain the pipeline stalls. Pipeline stalls include all three hazards.\n252\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 285,
        "text": "generations have enhanced performance by changing cache strategies (e.g., the\naggressiveness of prefetching), increasing memory bandwidth, expanding the num-\nber of instructions in flight, enhancing branch prediction, and improving graphics\nsupport. The early i7 microarchitectures used reservations stations and reorder\nbuffers for their out-of-order, speculative pipeline. Later microarchitectures, includ-\ning the i7 6700, use register renaming, with the reservations stations acting as func-\ntional unit queues and the reorder buffer simply tracking control information.\nFigure 3.38 shows the overall structure of the i7 pipeline. We will examine the\npipeline by starting with instruction fetch and continuing on to instruction commit,\nfollowing steps labeled in the figure.\n1. Instruction fetch\u2014The processor uses a sophisticated multilevel branch predic-\ntor to achieve a balance between speed and prediction accuracy. There is also a\nreturn address stack to speed up function return. Mispredictions cause a penalty\nof about 17 cycles. Using the predicted address, the instruction fetch unit fetches\n16 bytes from the instruction cache.\n2. The 16 bytes are placed in the predecode instruction buffer\u2014In this step, a pro-\ncess called macro-op fusion is executed. Macro-op fusion takes instruction\ncombinations such as compare followed by a branch and fuses them into a sin-\ngle operation, which can issue and dispatch as one instruction. Only certain spe-\ncial cases can be fused, since we must know that the only use of the first result is\nby the second instruction (i.e., compare and branch). In a study of the Intel Core\narchitecture (which has many fewer buffers), Bird et al. (2007) discovered that\nmacrofusion had a significant impact on the performance of integer programs\nresulting in an 8%\u201310% average increase in performance with a few programs\nshowing negative results. There was little impact on FP programs; in fact, about\nhalf of the SPECFP benchmarks showed negative results from macro-op fusion.\nThe predecode stage also breaks the 16 bytes into individual x86 instructions.\nThis predecode is nontrivial because the length of an x86 instruction can be\nfrom 1 to 17 bytes and the predecoder must look through a number of bytes\nbefore it knows the instruction length. Individual x86 instructions (including\nsome fused instructions) are placed into the instruction queue.\n3. Micro-op decode\u2014Individual x86 instructions are translated into micro-ops.\nMicro-ops are simple RISC-V-like instructions that can be executed directly\nby the pipeline; this approach of translating the x86 instruction set into simple\noperations that are more easily pipelined was introduced in the Pentium Pro in\n1997 and has been used since. Three of the decoders handle x86 instructions\nthat translate directly into one micro-op. For x86 instructions that have more\ncomplex semantics, there is a microcode engine that is used to produce the\nmicro-op sequence; it can produce up to four micro-ops every cycle and con-\ntinues until the necessary micro-op sequence has been generated. The micro-\nops are placed according to the order of the x86 instructions in the 64-entry\nmicro-op buffer.\n3.12\nPutting It All Together: The Intel Core i7 6700 and ARM Cortex-A53\n\u25a0\n253"
    },
    {
        "page": 286,
        "text": "4. The micro-op buffer preforms loop stream detection and microfusion\u2014If\nthere is a small sequence of instructions (less than 64 instructions) that com-\nprises a loop, the loop stream detector will find the loop and directly issue\nthe micro-ops from the buffer, eliminating the need for the instruction fetch\nand instruction decode stages to be activated. Microfusion combines\n256 KB unified l2\ncache (4-way)\nRegister alias table and allocator\n224-Entry reorder buffer\n97-Entry reservation station\nRetirement\nregister file\nALU\nshift\nSSE\nshuffle\nALU\n128-bit\nFMUL\nFDIV\n128-bit\nFMUL\nFDIV\n128-bit\nFMUL\nFDIV\nSSE\nshuffle\nALU\nSSE\nshuffle\nALU\nMemory order buffer\n(72 load; 56 stores pending)\nALU\nshift\nALU\nshift\nLoad\naddress\nStore\naddress\nStore\ndata\nStore\n& load\nMicro\n-code\nComplex\nmacro-op\ndecoder\n64-Entry micro-op loop stream detect buffer\nSimple\nmacro-op\ndecoder\nSimple\nmacro-op\ndecoder\nSimple\nmacro-op\ndecoder\n128-Entry\ninst. TLB\n(8-way)\nInstruction\nfetch\nhardware\nInstruction queue\n32 KB Inst. cache (8-way associative)\nPre-decode+macro-op\nfusion, fetch buffer\n64-Entry data TLB\n(4-way associative)\n32-KB dual-ported data\ncache (8-way associative)\n1536-Entry unified\nL2 TLB (12-way)\n8 MB all core shared and inclusive L3\ncache (16-way associative)\nUncore arbiter (handles scheduling and\nclock/power state differences)\nFigure 3.38 The Intel Core i7 pipeline structure shown with the memory system components. The total pipeline\ndepth is 14 stages, with branch mispredictions typically costing 17 cycles, with the extra few cycles likely due to the\ntime to reset the branch predictor. The six independent functional units can each begin execution of a ready micro-op\nin the same cycle. Up to four micro-ops can be processed in the register renaming table.\n254\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 287,
        "text": "instruction pairs such as ALU operation and a dependent store and issues\nthem to a single reservation station (where they can still issue indepen-\ndently), thus increasing the usage of the buffer. Micro-op fusion produces\nsmaller gains for integer programs and larger ones for FP, but the results vary\nwidely. The different results for integer and FP programs with macro and\nmicro fusion, probably arise from the patterns recognized and fused and\nthe frequency of occurrence in integer versus FP programs. In the i7, which\nhas a much larger number of reorder buffer entries, the benefits from both\ntechniques are likely to be smaller.\n5. Perform the basic instruction issue\u2014Looking up the register location in the\nregister tables, renaming the registers, allocating a reorder buffer entry, and\nfetching any results from the registers or reorder buffer before sending\nthe micro-ops to the reservation stations. Up to four micro-ops can be pro-\ncessed every clock cycle; they are assigned the next available reorder buffer\nentries.\n6. The i7 uses a centralized reservation station shared by six functional units. Up to\nsix micro-ops may be dispatched to the functional units every clock cycle.\n7. Micro-ops are executed by the individual function units, and then results are\nsent back to any waiting reservation station as well as to the register retirement\nunit, where they will update the register state once it is known that the instruc-\ntion is no longer speculative. The entry corresponding to the instruction in the\nreorder buffer is marked as complete.\n8. When one or more instructions at the head of the reorder buffer have been\nmarked as complete, the pending writes in the register retirement unit are exe-\ncuted, and the instructions are removed from the reorder buffer.\nIn addition to the changes in the branch predictor, the major changes between the\nfirst generation i7 (the 920, Nehalem microarchitecture) and the sixth generation\n(i7 6700, Skylake microarchitecture) are in the sizes of the various buffers, renam-\ning registers, and resources so as to allow many more outstanding instructions.\nFigure 3.39 summarizes these differences.\nPerformance of the i7\nIn earlier sections, we examined the performance of the i7\u2019s branch predictor\nand also the performance of SMT. In this section, we look at single-thread pipeline\nperformance. Because of the presence of aggressive speculation as well as non-\nblocking caches, it is difficult to accurately attribute the gap between idealized per-\nformance and actual performance. The extensive queues and buffers on the 6700\nreduce the probability of stalls because of a lack of reservation stations, renaming\nregisters, or reorder buffers significantly. Indeed, even on the earlier i7 920 with\nnotably fewer buffers, only about 3% of the loads were delayed because no reser-\nvation station was available.\n3.12\nPutting It All Together: The Intel Core i7 6700 and ARM Cortex-A53\n\u25a0\n255"
    },
    {
        "page": 288,
        "text": "Thus most losses come either from branch mispredicts or cache misses. The\ncost of a branch mispredict is 17 cycles, whereas the cost of an L1 miss is about\n10 cycles. An L2 miss is slightly more than three times as costly as an L1 miss, and\nan L3 miss costs about 13 times what an L1 miss costs (130\u2013135 cycles). Although\nthe processor will attempt to find alternative instructions to execute during L2 and\nL3 misses, it is likely that some of the buffers will fill before a miss completes,\ncausing the processor to stop issuing instructions.\nFigure 3.40 shows the overall CPI for the 19 SPECCPUint2006 benchmarks\ncompared to the CPI for the earlier i7 920. The average CPI on the i7 6700 is\n0.71, whereas it is almost 1.5 times better on the i7 920, at 1.06. This difference\nderives from improved branch prediction and a reduction in the demand miss rates\n(see Figure 2.26 on page 135).\nTo understand how the 6700 achieves the significant improvement in CPI, let\u2019s\nlook at the benchmarks that achieve the largest improvement. Figure 3.41 shows\nthe five benchmarks that have a CPI ratio on the 920 that is at least 1.5 times higher\nthan that of the 6700. Interestingly, three other benchmarks show a significant\nimprovement in branch prediction accuracy (1.5 or more); however, those three\nbenchmarks (HMMER, LIBQUANTUM, and SJENG) show equal or slightly\nhigher L1 demand miss rates on the i7 6700. These misses likely arise because\nthe aggressive prefetching is replacing cache blocks that are actually used. This\ntype of behavior reminds designers of the challenges of maximizing performance\nin complex speculative multiple issue processors: rarely can significant perfor-\nmance be achieved by tuning only one part of the microarchitecture!\nResource\ni7 920 (Nehalem)\ni7 6700 (Skylake)\nMicro-op queue (per thread)\n28\n64\nReservation stations\n36\n97\nInteger registers\nNA\n180\nFP registers\nNA\n168\nOutstanding load buffer\n48\n72\nOutstanding store buffer\n32\n56\nReorder buffer\n128\n256\nFigure 3.39 The buffers and queues in the first generation i7 and the latest\ngeneration i7. Nehalem used a reservation station plus reorder buffer organization.\nIn later microarchitectures, the reservation stations serve as scheduling resources,\nand register renaming is used rather than the reorder buffer; the reorder buffer in\nthe Skylake microarchitecture serves only to buffer control information. The choices\nof the size of various buffers and renaming registers, while appearing sometimes arbi-\ntrary, are likely based on extensive simulation.\n256\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 289,
        "text": "hmmer\n0\n0.71\n1.06\n0.54\n0.74\n0.68\n1.23\ni7 6700\ni7 920\nh264ref\nlibquantum\nperlbench\nsjeng\nbzip2\ngobmk\nxalancbmk\ngcc\nastar\nomnetpp\nmcf\n0.5\n1\n1.5\n2\n2.5\n3\nCycles per instruction\n0.81\n1.02\n0.42\n0.59\n0.47\n0.61\n0.41\n0.44\n0.38\n0.65\n0.60\n0.92\n0.76 0.77\n1.44\n2.67\n1.37\n2.12\nFigure 3.40 The CPI for the SPECCPUint2006 benchmarks on the i7 6700 and the i7 920. The data in this section\nwere collected by Professor Lu Peng and PhD student Qun Liu, both of Louisiana State University.\nBenchmark\nCPI ratio (920/6700)\nBranch mispredict\nratio (920/6700)\nL1 demand miss\nratio (920/6700)\nASTAR\n1.51\n1.53\n2.14\nGCC\n1.82\n2.54\n1.82\nMCF\n1.85\n1.27\n1.71\nOMNETPP\n1.55\n1.48\n1.96\nPERLBENCH\n1.70\n2.11\n1.78\nFigure 3.41 An analysis of the five integer benchmarks with the largest performance gap between the i7 6700\nand 920. These five benchmarks show an improvement in the branch prediction rate and a reduction in the L1\ndemand miss rate.\n3.12\nPutting It All Together: The Intel Core i7 6700 and ARM Cortex-A53\n\u25a0\n257"
    },
    {
        "page": 290,
        "text": "3.13\nFallacies and Pitfalls\nOur few fallacies focus on the difficulty of predicting performance and energy effi-\nciency and extrapolating from single measures such as clock rate or CPI. We also\nshow that different architectural approaches can have radically different behaviors\nfor different benchmarks.\nFallacy\nIt is easy to predict the performance and energy efficiency of two different\nversions of the same instruction set architecture, if we hold the technology\nconstant.\nIntel offers a processor for the low-end Netbook and PMD space called the Atom\n230, which implements both the 64-bit and 32-bit versions of the x86 architecture.\nThe Atom is a statically scheduled, 2-issue superscalar, quite similar in its micro-\narchitecture to the ARM A8, a single-core predecessor of the A53. Interestingly,\nboth the Atom 230 and the Core i7 920 have been fabricated in the same 45 nm\nIntel technology. Figure 3.42 summarizes the Intel Core i7 920, the ARM Cortex-\nA8, and the Intel Atom 230. These similarities provide a rare opportunity to\ndirectly compare two radically different microarchitectures for the same instruction\nset while holding constant the underlying fabrication technology. Before we do the\ncomparison, we need to say a little more about the Atom 230.\nThe Atom processors implement the x86 architecture using the standard tech-\nnique of translating x86 instructions into RISC-like instructions (as every x86\nimplementation since the mid-1990s has done). Atom uses a slightly more pow-\nerful microoperation, which allows an arithmetic operation to be paired with a load\nor a store; this capability was added to later i7s by the use of macrofusion. This\nmeans that on average for a typical instruction mix, only 4% of the instructions\nrequire more than one microoperation. The microoperations are then executed\nin a 16-deep pipeline capable of issuing two instructions per clock, in order, as\nin the ARM A8. There are dual-integer ALUs, separate pipelines for FP add\nand other FP operations, and two memory operation pipelines, supporting more\ngeneral dual execution than the ARM A8 but still limited by the in-order issue\ncapability. The Atom 230 has a 32 KiB instruction cache and a 24 KiB data cache,\nboth backed by a shared 512 KiB L2 on the same die. (The Atom 230 also supports\nmultithreading with two threads, but we will consider only single-threaded\ncomparisons.)\nWe might expect that these two processors, implemented in the same technol-\nogy and with the same instruction set, would exhibit predictable behavior, in terms\nof relative performance and energy consumption, meaning that power and perfor-\nmance would scale close to linearly. We examine this hypothesis using three sets of\nbenchmarks. The first set is a group of Java single-threaded benchmarks that come\nfrom the DaCapo benchmarks and the SPEC JVM98 benchmarks (see\nEsmaeilzadeh et al. (2011) for a discussion of the benchmarks and measurements).\nThe second and third sets of benchmarks are from SPEC CPU2006 and consist of\nthe integer and FP benchmarks, respectively.\n258\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 291,
        "text": "As we can see in Figure 3.43, the i7 significantly outperforms the Atom. All\nbenchmarks are at least four times faster on the i7, two SPECFP benchmarks are\nover 10 times faster, and one SPECINT benchmark runs over eight times faster!\nBecause the ratio of clock rates of these two processors is 1.6, most of the advan-\ntage comes from a much lower CPI for the i7 920: a factor of 2.8 for the Java bench-\nmarks, a factor of 3.1 for the SPECINT benchmarks, and a factor of 4.3 for the\nSPECFP benchmarks.\nBut the average power consumption for the i7 920 is just under 43 W, while\nthe average power consumption of the Atom is 4.2 W, or about one-tenth of the\npower! Combining the performance and power leads to an energy efficiency\nadvantage for the Atom that is typically more than 1.5 times better and often\n2 times better! This comparison of two processors using the same underlying\ntechnology makes it clear that the performance advantages of an aggressive\nArea\nSpecific characteristic\nIntel i7 920\nARM A8\nIntel Atom 230\nFour cores, each\nwith FP\nOne core, no FP\nOne core, with FP\nPhysical chip\nproperties\nClock rate\n2.66 GHz\n1 GHz\n1.66 GHz\nThermal design power\n130 W\n2 W\n4 W\nPackage\n1366-pin BGA\n522-pin BGA\n437-pin BGA\nMemory system\nTLB\nTwo-level\nTwo-level\nAll four-way set\nassociative\nAll four-way set\nassociative\nOne-level fully\nassociative\n128 I/64 D\n16 I/16 D\n512 L2\n32 I/32 D\n64 L2\nCaches\nThree-level\n32 KiB/32 KiB\nTwo-level\nTwo-level\n256 KiB\n16/16 or 32/32 KiB\n32/24 KiB\n2\u20138 MiB\n128 KiB\u20131 MiB\n512 KiB\nPeak memory BW\n17 GB/s\n12 GB/sec\n8 GB/s\nPipeline structure\nPeak issue rate\n4 ops/clock with\nfusion\n2 ops/clock\n2 ops/clock\nPipe line scheduling\nSpeculating out of\norder\nIn-order dynamic\nissue\nIn-order dynamic\nissue\nBranch prediction\nTwo-level\nTwo-level\n512-entry BTB\n4 K global history\n8-entry return stack\nTwo-level\nFigure 3.42 An overview of the four-core Intel i7 920, an example of a typical ARM A8 processor chip (with a\n256 MiB L2, 32 KiB L1s, and no floating point), and the Intel ARM 230, clearly showing the difference in design\nphilosophy between a processor intended for the PMD (in the case of ARM) or netbook space (in the case of\nAtom) and a processor for use in servers and high-end desktops. Remember, the i7 includes four cores, each of\nwhich is higher in performance than the one-core A8 or Atom. All these processors are implemented in a comparable\n45 nm technology.\n3.13\nFallacies and Pitfalls\n\u25a0\n259"
    },
    {
        "page": 292,
        "text": "11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nFop\nLuindex\nantlr\nBloat\n_201_compress\n_202_jess\n_209_db\n_213_javac\n_212_mpegaudio\n_228_jack\n400.perlbench\n401.bzip2\n403.gcc\n429.mcf\n445.gobmk\n456.hmmer\n458.sjeng\n462.libquantum\n464.h264ref\n470.omnetpp\n473.astar\n483.xalancbmk\n416.gamess\n433.milc\n434.zeusmp\n435.gromacs\n436.cactus ADM\n437.leslie3d\n444.namd\n447.dealll\n450.soplex\n453.povray\n454.calculix\n459.gams FDTD\n465.tonto\n470.ibm\n482.sphinx3\ni7 920 and Atom 230 performance and energy ratio\nEnergy efficiency\nSpeedup\nFigure 3.43 The relative performance and energy efficiency for a set of single-threaded benchmarks shows the i7 920 is 4 to over 10 times\nfaster than the Atom 230 but that it is about 2 times less power-efficient on average! Performance is shown in the columns as i7 relative to\nAtom, which is execution time (i7)/execution time (Atom). Energy is shown with the line as Energy (Atom)/Energy (i7). The i7 never beats the\nAtom in energy efficiency, although it is essentially as good on four benchmarks, three of which are floating point. The data shown here were\ncollected by Esmaeilzadeh et al. (2011). The SPEC benchmarks were compiled with optimization using the standard Intel compiler, while the Java\nbenchmarks use the Sun (Oracle) Hotspot Java VM. Only one core is active on the i7, and the rest are in deep power saving mode. Turbo Boost is\nused on the i7, which increases its performance advantage but slightly decreases its relative energy efficiency.\n260\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 293,
        "text": "superscalar with dynamic scheduling and speculation come with a significant dis-\nadvantage in energy efficiency.\nFallacy\nProcessors with lower CPIs will always be faster.\nFallacy\nProcessors with faster clock rates will always be faster.\nThe key is that it is the product of CPI and clock rate that determines performance.\nA high clock rate obtained by deeply pipelining the processor must maintain a low\nCPI to get the full benefit of the faster clock. Similarly, a simple processor with a\nhigh clock rate but a low CPI may be slower.\nAs we saw in the previous fallacy, performance and energy efficiency can\ndiverge significantly among processors designed for different environments even\nwhen they have the same ISA. In fact, large differences in performance can show\nup even within a family of processors from the same company all designed for\nhigh-end applications. Figure 3.44 shows the integer and FP performance of\ntwo different implementations of the x86 architecture from Intel, as well as a ver-\nsion of the Itanium architecture, also by Intel.\nThe Pentium 4 was the most aggressively pipelined processor ever built by\nIntel. It used a pipeline with over 20 stages, had seven functional units, and cached\nmicro-ops rather than x86 instructions. Its relatively inferior performance, given\nthe aggressive implementation, was a clear indication that the attempt to exploit\nmore ILP (there could easily be 50 instructions in flight) had failed. The Pentium\u2019s\npower consumption was similar to the i7, although its transistor count was lower,\nas its primary caches were half as large as the i7, and it included only a 2 MiB sec-\nondary cache with no tertiary cache.\nThe Intel Itanium is a VLIW-style architecture, which despite the potential\ndecrease in complexity compared to dynamically scheduled superscalars, never\nattained competitive clock rates with the mainline x86 processors (although it\nappears to achieve an overall CPI similar to that of the i7). In examining these\nresults, the reader should be aware that they use different implementation technol-\nogies, giving the i7 an advantage in terms of transistor speed and hence clock rate\nfor an equivalently pipelined processor. Nonetheless, the wide variation in\nProcessor\nImplementation\ntechnology\nClock\nrate\nPower\nSPECCInt2006\nbase\nSPECCFP2006\nbaseline\nIntel Pentium 4 670\n90 nm\n3.8 GHz\n115 W\n11.5\n12.2\nIntel Itanium 2\n90 nm\n1.66 GHz\n104 W\napprox. 70 W one\ncore\n14.5\n17.3\nIntel i7 920\n45 nm\n3.3 GHz\n130 W total\napprox. 80 W one\ncore\n35.5\n38.4\nFigure 3.44 Three different Intel processors vary widely. Although the Itanium processor has two cores and the i7\nfour, only one core is used in the benchmarks; the Power column is the thermal design power with estimates for only\none core active in the multicore cases.\n3.13\nFallacies and Pitfalls\n\u25a0\n261"
    },
    {
        "page": 294,
        "text": "performance\u2014more than three times between the Pentium and i7\u2014is astonishing.\nThe next pitfall explains where a significant amount of this advantage comes from.\nPitfall\nSometimes bigger and dumber is better.\nMuch of the attention in the early 2000s went to building aggressive processors to\nexploit ILP, including the Pentium 4 architecture, which used the deepest pipeline\never seen in a microprocessor, and the Intel Itanium, which had the highest peak\nissue rate per clock ever seen. What quickly became clear was that the main lim-\nitation in exploiting ILP often turned out to be the memory system. Although spec-\nulative out-of-order pipelines were fairly good at hiding a significant fraction of the\n10- to 15-cycle miss penalties for a first-level miss, they could do very little to hide\nthe penalties for a second-level miss that, when going to main memory, were likely\nto be 50\u2013100 clock cycles.\nThe result was that these designs never came close to achieving the peak\ninstruction throughput despite the large transistor counts and extremely sophisti-\ncated and clever techniques. Section 3.15 discusses this dilemma and the turning\naway from more aggressive ILP schemes to multicore, but there was another\nchange that exemplified this pitfall. Instead of trying to hide even more memory\nlatency with ILP, designers simply used the transistors to build much larger caches.\nBoth the Itanium 2 and the i7 use three-level caches compared to the two-level\ncache of the Pentium 4, and the third-level caches are 9 and 8 MiB compared to\nthe 2 MiB second-level cache of the Pentium 4. Needless to say, building larger\ncaches is a lot easier than designing the 20+-stage Pentium 4 pipeline, and based\non the data in Figure 3.44, doing so seems to be more effective.\nPitfall\nAnd sometimes smarter is better than bigger and dumber.\nOne of the more surprising results of the past decade has been in branch prediction.\nThe emergence of hybrid tagged predictors has shown that a more sophisticated pre-\ndictor can outperform the simple gshare predictor with the same number of bits (see\nFigure 3.8 on page 171). One reason this result is so surprising is that the tagged\npredictor actually stores fewer predictions, because it also consumes bits to store\ntags, whereas gshare has only a large array of predictions. Nonetheless, it appears\nthat the advantage gained by not misusing a prediction for one branch on another\nbranch more than justifies the allocation of bits to tags versus predictions.\nPitfall\nBelieving that there are large amounts of ILP available, if only we had the right\ntechniques.\nThe attempts to exploit large amounts of ILP failed for several reasons, but one\nof the most important ones, which some designers did not initially accept, is\nthat it is hard to find large amounts of ILP in conventionally structured pro-\ngrams, even with speculation. A famous study by David Wall in 1993 (see\nWall, 1993) analyzed the amount of ILP available under a variety of idealistic\nconditions. We summarize his results for a processor configuration with\nroughly five to ten times the capability of the most advanced processors in\n2017. Wall\u2019s study extensively documented a variety of different approaches,\n262\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 295,
        "text": "and the reader interested in the challenge of exploiting ILP should read the\ncomplete study.\nThe aggressive processor we consider has the following characteristics:\n1. Up to 64 instruction issues and dispatches per clock with no issue restric-\ntions, or 8 times the total issue width of the widest processor in 2016 (the\nIBM Power8) and with up to 32 times as many loads and stores allowed\nper clock! As we have discussed, there are serious complexity and power\nproblems with large issue rates.\n2. A tournament predictor with 1K entries and a 16-entry function return pre-\ndictor. This predictor is comparable to the best predictors in 2016; the pre-\ndictor is not a primary bottleneck. Mispredictions are handled in one cycle,\nbut they limit the ability to speculate.\n3. Perfect disambiguation of memory references done dynamically\u2014this is\nambitious but perhaps attainable for small window sizes.\n4. Register renaming with 64 additional integer and 64 additional FP registers,\nwhich is somewhat less than the most aggressive processor in 2011. Because\nthe study assumes a latency of only one cycle for all instructions (versus 15\nor more on processors like the i7 or Power8), the effective number of rename\nregisters is about five times larger than either of those processors.\nFigure 3.45 shows the result for this configuration as we vary the window size.\nThis configuration is more complex and expensive than existing implementations,\nespecially in terms of the number of instruction issues. Nonetheless, it gives a useful\nupper limit on what future implementations might yield. The data in these figures are\nlikely to be very optimistic for another reason. There are no issue restrictions among\nthe 64 instructions: for example, they may all be memory references. No one would\neven contemplate this capability in a processor for the near future. In addition,\nremember that in interpreting these results, cache misses and non-unit latencies were\nnot taken into account, and both these effects have significant impacts.\nThe most startling observation in Figure 3.45 is that with the preceding realistic\nprocessor constraints, the effect of the window size for the integer programs is not\nas severe as for FP programs. This result points to the key difference between these\ntwo types of programs. The availability of loop-level parallelism in two of the FP\nprograms means that the amount of ILP that can be exploited is higher, but for\ninteger programs other factors\u2014such as branch prediction, register renaming,\nand less parallelism, to start with\u2014are all important limitations. This observation\nis critical because most of the market growth in the past decade\u2014transaction pro-\ncessing, web servers, and the like\u2014depended on integer performance, rather than\nfloating point.\nWall\u2019s study was not believed by some, but 10 years later, the reality had\nsunk in, and the combination of modest performance increases with significant\nhardware resources and major energy issues coming from incorrect speculation\nforced a change in direction. We will return to this discussion in our concluding\nremarks.\n3.13\nFallacies and Pitfalls\n\u25a0\n263"
    },
    {
        "page": 296,
        "text": "3.14\nConcluding Remarks: What\u2019s Ahead?\nAs 2000 began the focus on exploiting instruction-level parallelism was at its peak.\nIn the first five years of the new century, it became clear that the ILP approach had\nlikely peaked and that new approaches would be needed. By 2005 Intel and all the\nother major processor manufacturers had revamped their approach to focus on mul-\nticore. Higher performance would be achieved through thread-level parallelism\nrather than instruction-level parallelism, and the responsibility for using the\n10\n10\n10\n8\n9\n15\n15\n13\n8\n10\n11\n12\n12\n11\n9\n14\n22\n35\n52\n47\n9\n12\n15\n16\n17\n56\n45\n34\n22\n14\ngcc\nespresso\nli\nfpppp\nBenchmarks\ndoduc\ntomcatv\n0\n10\n20\nInstruction issues per cycle\n30\n40\n50\n60\nInfinite\n256\n128\n64\n32\nWindow size\nFigure 3.45 The amount of parallelism available versus the window size for a variety of integer and floating-\npoint programs with up to 64 arbitrary instruction issues per clock. Although there are fewer renaming registers\nthan the window size, the fact that all operations have 1-cycle latency and that the number of renaming registers\nequals the issue width allows the processor to exploit parallelism within the entire window.\n264\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 297,
        "text": "processor efficiently would largely shift from the hardware to the software and the\nprogrammer. This change was the most significant change in processor architec-\nture since the early days of pipelining and instruction-level parallelism some 25+\nyears earlier.\nDuring the same period, designers began to explore the use of more data-level\nparallelism as another approach to obtaining performance. SIMD extensions\nenabled desktop and server microprocessors to achieve moderate performance\nincreases for graphics and similar functions. More importantly, graphics proces-\nsing units (GPUs) pursued aggressive use of SIMD, achieving significant perfor-\nmance advantages for applications with extensive data-level parallelism. For\nscientific applications, such approaches represent a viable alternative to the more\ngeneral, but less efficient, thread-level parallelism exploited in multicores. The\nnext chapter explores these developments in the use of data-level parallelism.\nMany researchers predicted a major retrenchment in the use of ILP, predicting\nthat two issue superscalar processors and larger numbers of cores would be the\nfuture. The advantages, however, of slightly higher issue rates and the ability of\nspeculative dynamic scheduling to deal with unpredictable events, such as level-\none cache misses, led to moderate ILP (typically about 4 issues/clock) being the\nprimary building block in multicore designs. The addition of SMT and its effective-\nness (both for performance and energy efficiency) further cemented the position of\nthe moderate issue, out-of-order, speculative approaches. Indeed, even in the\nembedded market, the newest processors (e.g., the ARM Cortex-A9 and Cortex-\nA73) have introduced dynamic scheduling, speculation, and wider issues rates.\nIt is highly unlikely that future processors will try to increase the width of issue\nsignificantly. It is simply too inefficient from the viewpoint of silicon utilization\nand power efficiency. Consider the data in Figure 3.46 that show the five proces-\nsors in the IBM Power series. Over more than a decade, there has been a modest\nimprovement in the ILP support in the Power processors, but the dominant portion\nPower4\nPower5\nPower6\nPower7\nPower8\nIntroduced\n2001\n2004\n2007\n2010\n2014\nInitial clock rate (GHz)\n1.3\n1.9\n4.7\n3.6\n3.3 GHz\nTransistor count (M)\n174\n276\n790\n1200\n4200\nIssues per clock\n5\n5\n7\n6\n8\nFunctional units per core\n8\n8\n9\n12\n16\nSMT threads per core\n0\n2\n2\n4\n8\nCores/chip\n2\n2\n2\n8\n12\nSMT threads per core\n0\n2\n2\n4\n8\nTotal on-chip cache (MiB)\n1.5\n2\n4.1\n32.3\n103.0\nFigure 3.46 Characteristics of five generations of IBM Power processors. All except the Power6, which is static and\nin-order, were dynamically scheduled; all the processors support two load/store pipelines. The Power6 has the same\nfunctional units as the Power5 except for a decimal unit. Power7 and Power8 use embedded DRAM for the L3 cache.\nPower9 has been described briefly; it further expands the caches and supports off-chip HBM.\n3.14\nConcluding Remarks: What\u2019s Ahead?\n\u25a0\n265"
    },
    {
        "page": 298,
        "text": "of the increase in transistor count (a factor of more than 10 from the Power4 to the\nPower8) went to increasing the caches and the number of cores per die. Even the\nexpansion in SMT support seems to be more of a focus than is an increase in the ILP\nthroughput: The ILP structure from Power4 to Power8 went from 5 issues to 8,\nfrom 8 functional units to 16 (but not increasing from the original 2 load/store\nunits), whereas the SMT support went from nonexistent to 8 threads/processor.\nA similar trend can be observed across the six generations of i7 processors, where\nalmost all the additional silicon has gone to supporting more cores. The next two\nchapters focus on approaches that exploit data-level and thread-level parallelism.\n3.15\nHistorical Perspective and References\nSection M.5 (available online) features a discussion on the development of\npipelining and instruction-level parallelism. We provide numerous references\nfor further reading and exploration of these topics. Section M.5 covers both\nChapter 3 and Appendix H.\nCase Studies and Exercises by Jason D. Bakos\nand Robert P. Colwell\nCase Study: Exploring the Impact of Microarchitectural\nTechniques\nConcepts illustrated by this case study\n\u25a0\nBasic Instruction Scheduling, Reordering, Dispatch\n\u25a0\nMultiple Issue and Hazards\n\u25a0\nRegister Renaming\n\u25a0\nOut-of-Order and Speculative Execution\n\u25a0\nWhere to Spend Out-of-Order Resources\nYou are tasked with designing a new processor microarchitecture and you are try-\ning to determine how best to allocate your hardware resources. Which of the hard-\nware and software techniques you learned in Chapter 3 should you apply? You\nhave a list of latencies for the functional units and for memory, as well as some\nrepresentative code. Your boss has been somewhat vague about the performance\nrequirements of your new design, but you know from experience that, all else being\nequal, faster is usually better. Start with the basics. Figure 3.47 provides a sequence\nof instructions and list of latencies.\n3.1\n[10] <3.1, 3.2> What is the baseline performance (in cycles, per loop iteration) of\nthe code sequence in Figure 3.47 if no new instruction\u2019s execution could be\n266\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 299,
        "text": "initiated until the previous instruction\u2019s execution had completed? Ignore front-end\nfetch and decode. Assume for now that execution does not stall for lack of the next\ninstruction, but only one instruction/cycle can be issued. Assume the branch is\ntaken, and that there is a one-cycle branch delay slot.\n3.2\n[10] <3.1, 3.2> Think about what latency numbers really mean\u2014they indicate the\nnumber of cycles a given function requires to produce its output. If the overall pipe-\nline stalls for the latency cycles of each functional unit, then you are at least guar-\nanteed that any pair of back-to-back instructions (a \u201cproducer\u201d followed by a\n\u201cconsumer\u201d) will execute correctly. But not all instruction pairs have a pro-\nducer/consumer relationship. Sometimes two adjacent instructions have nothing\nto do with each other. How many cycles would the loop body in the code sequence\nin Figure 3.47 require if the pipeline detected true data dependences and only\nstalled on those, rather than blindly stalling everything just because one functional\nunit is busy? Show the code with <stall> inserted where necessary to accom-\nmodate stated latencies. (Hint: an instruction with latency +2 requires\ntwo <stall> cycles to be inserted into the code sequence.) Think of it this\nLatencies beyond single cycle\nMemory LD\n+3\nMemory SD\n+1\nInteger ADD, SUB\n+0\nBranches\n+1\nfadd.d\n+2\nfmul.d\n+4\nfdiv.d\n+10\nLoop:\nfld\nf2,0(Rx)\nI0:\nfmul.d\nf2,f0,f2\nI1:\nfdiv.d\nf8,f2,f0\nI2:\nfld\nf4,0(Ry)\nI3:\nfadd.d\nf4,f0,f4\nI4:\nfadd.d\nf10,f8,f2\nI5:\nfsd\nf4,0(Ry)\nI6:\naddi\nRx,Rx,8\nI7:\naddi\nRy,Ry,8\nI8:\nsub\nx20,x4,Rx\nI9:\nbnz\nx20,Loop\nFigure 3.47 Code and latencies for Exercises 3.1 through 3.6.\nCase Studies and Exercises by Jason D. Bakos and Robert P. Colwell\n\u25a0\n267"
    },
    {
        "page": 300,
        "text": "way: a one-cycle instruction has latency 1+0, meaning zero extra wait states. So,\nlatency 1+1 implies one stall cycle; latency 1+N has N extra stall cycles.\n3.3\n[15] <3.1, 3.2> Consider a multiple-issue design. Suppose you have two execu-\ntion pipelines, each capable of beginning execution of one instruction per cycle,\nand enough fetch/decode bandwidth in the front end so that it will not stall your\nexecution. Assume results can be immediately forwarded from one execution unit\nto another, or to itself. Further assume that the only reason an execution pipeline\nwould stall is to observe a true data dependency. Now how many cycles does the\nloop require?\n3.4\n[10] <3.1, 3.2> In the multiple-issue design of Exercise 3.3, you may have rec-\nognized some subtle issues. Even though the two pipelines have the exact same\ninstruction repertoire, they are neither identical nor interchangeable, because there\nis an implicit ordering between them that must reflect the ordering of the instruc-\ntions in the original program. If instruction N+1 begins execution in Execution\nPipe 1 at the same time that instruction N begins in Pipe 0, and N+1 happens\nto require a shorter execution latency than N, then N+1 will complete before N\n(even though program ordering would have implied otherwise). Recite at least\ntwo reasons why that could be hazardous and will require special considerations\nin the microarchitecture. Give an example of two instructions from the code in\nFigure 3.47 that demonstrate this hazard.\n3.5\n[20] <3.1, 3.2> Reorder the instructions to improve performance of the code in\nFigure 3.47. Assume the two-pipe machine in Exercise 3.3 and that the out-of-\norder completion issues of Exercise 3.4 have been dealt with successfully. Just\nworry about observing true data dependences and functional unit latencies for\nnow. How many cycles does your reordered code take?\n3.6\n[10/10/10] <3.1, 3.2> Every cycle that does not initiate a new operation in a\npipe is a lost opportunity, in the sense that your hardware is not living up to its\npotential.\na. [10] <3.1, 3.2> In your reordered code from Exercise 3.5, what fraction of all\ncycles, counting both pipes, were wasted (did not initiate a new op)?\nb. [10] <3.1, 3.2> Loop unrolling is one standard compiler technique for finding\nmore parallelism in code, in order to minimize the lost opportunities for perfor-\nmance. Hand-unroll two iterations of the loop in your reordered code from Exer-\ncise 3.5.\nc. [10] <3.1, 3.2> What speedup did you obtain? (For this exercise, just color the\nN+1 iteration\u2019s instructions green to distinguish them from the Nth iteration\u2019s\ninstructions; if you were actually unrolling the loop, you would have to reassign\nregisters to prevent collisions between the iterations.)\n3.7\n[15] <3.4> Computers spend most of their time in loops, so multiple loop itera-\ntions are great places to speculatively find more work to keep CPU resources busy.\nNothing is ever easy, though; the compiler emitted only one copy of that loop\u2019s\ncode, so even though multiple iterations are handling distinct data, they will\n268\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 301,
        "text": "appear to use the same registers. To keep multiple iterations\u2019 register usages from\ncolliding, we rename their registers. Figure 3.48 shows example code that we\nwould like our hardware to rename. A compiler could have simply unrolled the\nloop and used different registers to avoid conflicts, but if we expect our hardware\nto unroll the loop, it must also do the register renaming. How? Assume your hard-\nware has a pool of temporary registers (call them T registers, and assume that there\nare 64 of them, T0 through T63) that it can substitute for those registers designated\nby the compiler. This rename hardware is indexed by the src (source) register\ndesignation, and the value in the table is the T register of the last destination that\ntargeted that register. (Think of these table values as producers, and the src reg-\nisters are the consumers; it doesn\u2019t much matter where the producer puts its result\nas long as its consumers can find it.) Consider the code sequence in Figure 3.48.\nEvery time you see a destination register in the code, substitute the next available\nT, beginning with T9. Then update all the src registers accordingly, so that true\ndata dependences are maintained. Show the resulting code. (Hint: see Figure 3.49.)\n3.8\n[20] <3.4> Exercise 3.7 explored simple register renaming: when the hardware\nregister renamer sees a source register, it substitutes the destination T register of\nthe last instruction to have targeted that source register. When the rename table sees\na destination register, it substitutes the next available T for it, but superscalar\ndesigns need to handle multiple instructions per clock cycle at every stage in\nthe machine, including the register renaming. A SimpleScalar processor would\ntherefore look up both src register mappings for each instruction and allocate\na new dest mapping per clock cycle. Superscalar processors must be able to\ndo that as well, but they must also ensure that any dest-to-src relationships\nbetween the two concurrent instructions are handled correctly. Consider the sample\ncode sequence in Figure 3.50. Assume that we would like to simultaneously\nLoop:\nfld\nf2,0(Rx)\nI0:\nfmul.d\nf5,f0,f2\nI1:\nfdiv.d\nf8,f0,f2\nI2:\nfld\nf4,0(Ry)\nI3:\nfadd.d\nf6,f0,f4\nI4:\nfadd.d\nf10,f8,f2\nI5:\nsd\nf4,0(Ry)\nFigure 3.48 Sample code for register renaming practice.\nI0:\nfld\nT9,0(Rx)\nI1:\nfmul.d\nT10,F0,T9\n. . .\nFigure 3.49 Expected output of register renaming.\nCase Studies and Exercises by Jason D. Bakos and Robert P. Colwell\n\u25a0\n269"
    },
    {
        "page": 302,
        "text": "rename the first two instructions. Further assume that the next two available T reg-\nisters to be used are known at the beginning of the clock cycle in which these two\ninstructions are being renamed. Conceptually, what we want is for the first instruc-\ntion to do its rename table lookups and then update the table per its destination\u2019s T\nregister. Then the second instruction would do exactly the same thing, and any\ninter-instruction dependency would thereby be handled correctly. But there\u2019s\nnot enough time to write that T register designation into the renaming table and\nthen look it up again for the second instruction, all in the same clock cycle. That\nregister substitution must instead be done live (in parallel with the register rename\ntable update). Figure 3.51 shows a circuit diagram, using multiplexers and com-\nparators, that will accomplish the necessary on-the-fly register renaming. Your task\nis to show the cycle-by-cycle state of the rename table for every instruction of the\ncode shown in Figure 3.50. Assume the table starts out with every entry equal to its\nindex (T0=0; T1=1, \u2026) (Figure 3.51).\nI0:\nfmul.d\nf5,f0,f2\nI1:\nfadd.d\nf9,f5,f4\nI2:\nfadd.d\nf5,f5,f2\nI3:\nfdiv.d\nf2,f9,f0\nFigure 3.50 Sample code for superscalar register renaming.\nRename table\n0\n1\n2\n3\n4\n5\nNext available T register\ndst=F4\nsrc1=F1\nsrc2=F2\ndst=F1\nsrc1=F2\nsrc2=F3\ndst= T9\nsrc1 = T19\nsrc2 = T38\ndst = T10\nsrc1= T9\nsrc2= T19\n(Similar mux\nfor src2)\nY\nN\nThis 9 appears\nin the rename\ntable in next\nclock cycle\nI1 dst =I2 src?\n(As per instr 1)\nI1\nI2\n19\n29\n8\n9\n62\n63\n9\n10\n. . . \n. . . \n. . . \n21\n38\nFigure 3.51 Initial state of the register renaming table.\n270\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 303,
        "text": "3.9\n[5] <3.4> If you ever get confused about what a register renamer has to do, go\nback to the assembly code you\u2019re executing, and ask yourself what has to happen\nfor the right result to be obtained. For example, consider a three-way superscalar\nmachine renaming these three instructions concurrently:\naddi x1, x1, x1\naddi x1, x1, x1\naddi x1, x1, x1\nIf the value of x1 starts out as 5, what should its value be when this sequence has\nexecuted?\n3.10\n[20] <3.4, 3.7> Very long instruction word (VLIW) designers have a few basic\nchoices to make regarding architectural rules for register use. Suppose a VLIW is\ndesigned with self-draining execution pipelines: once an operation is initiated, its\nresults will appear in the destination register at most L cycles later (where L is the\nlatency of the operation). There are never enough registers, so there is a temptation\nto wring maximum use out of the registers that exist. Consider Figure 3.52. If loads\nhave a 1+2 cycle latency, unroll this loop once, and show how a VLIW capable of\ntwo loads and two adds per cycle can use the minimum number of registers, in the\nabsence of any pipeline interruptions or stalls. Give an example of an event that, in\nthe presence of self-draining pipelines, could disrupt this pipelining and yield\nwrong results.\n3.11\n[10/10/10] <3.3> Assume a five-stage single-pipeline microarchitecture (fetch,\ndecode, execute, memory, write-back) and the code in Figure 3.53. All ops are\none cycle except LW and SW, which are 1+2 cycles, and branches, which are\n1+1 cycles. There is no forwarding. Show the phases of each instruction per clock\ncycle for one iteration of the loop.\na. [10] <3.3> How many clock cycles per loop iteration are lost to branch\noverhead?\nb. [10] <3.3> Assume a static branch predictor, capable of recognizing a back-\nward branch in the Decode stage. Now how many clock cycles are wasted\non branch overhead?\nLoop:\nlw\nx1,0(x2);\nlw\nx3,8(x2)\n<stall>\n<stall>\naddi\nx10,x1,1;\naddi\nx11,x3,1\nsw\nx1,0(x2);\nsw\nx3,8(x2)\naddi\nx2,x2,8\nsub\nx4,x3,x2\nbnz\nx4,Loop\nFigure 3.52 Sample VLIW code with two adds, two loads, and two stalls.\nCase Studies and Exercises by Jason D. Bakos and Robert P. Colwell\n\u25a0\n271"
    },
    {
        "page": 304,
        "text": "c. [10] <3.3> Assume a dynamic branch predictor. How many cycles are lost on a\ncorrect prediction?\n3.12\n[15/20/20/10/20] <3.4, 3.6> Let\u2019s consider what dynamic scheduling might\nachieve here. Assume a microarchitecture as shown in Figure 3.54. Assume that\nthe arithmetic-logical units (ALUs) can do all arithmetic ops (fmul.d,\nfdiv.d, fadd.d, addi, sub) and branches, and that the Reservation Station\n(RS) can dispatch, at most, one operation to each functional unit per cycle (one op\nto each ALU plus one memory op to the fld/ fsd).\na. [15] <3.4> Suppose all of the instructions from the sequence in Figure 3.47 are\npresent in the RS, with no renaming having been done. Highlight any instruc-\ntions in the code where register renaming would improve performance. (Hint:\nlook for read-after-write and write-after-write hazards. Assume the same func-\ntional unit latencies as in Figure 3.47.)\nb. [20] <3.4> Suppose the register-renamed version of the code from part (a) is\nresident in the RS in clock cycle N, with latencies as given in Figure 3.47. Show\nhow the RS should dispatch these instructions out of order, clock by clock, to\nLoop:\nlw x1,0(x2)\naddi\nx1,x1, 1\nsw\nx1,0(x2)\naddi\nx2,x2,4\nsub\nx4,x3,x2\nbnz\nx4,Loop\nFigure 3.53 Code loop for Exercise 3.11.\nReservation\nstation\nALU 0\nInstructions\nfrom decoder\n1\n2\nALU 1\nLD/ST\nMem\nFigure 3.54 Microarchitecture for Exercise 3.12.\n272\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 305,
        "text": "obtain optimal performance on this code. (Assume the same RS restrictions as in\npart (a). Also assume that results must be written into the RS before they\u2019re\navailable for use\u2014no bypassing.) How many clock cycles does the code\nsequence take?\nc. [20] <3.4> Part (b) lets the RS try to optimally schedule these instructions. But\nin reality, the whole instruction sequence of interest is not usually present in the\nRS. Instead, various events clear the RS, and as a new code sequence streams in\nfrom the decoder, the RS must choose to dispatch what it has. Suppose that the\nRS is empty. In cycle 0, the first two register-renamed instructions of this\nsequence appear in the RS. Assume it takes one clock cycle to dispatch any\nop, and assume functional unit latencies are as they were for Exercise 3.2. Fur-\nther assume that the front end (decoder/register-renamer) will continue to supply\ntwo new instructions per clock cycle. Show the cycle-by-cycle order of dispatch\nof the RS. How many clock cycles does this code sequence require now?\nd. [10] <3.4> If you wanted to improve the results of part (c), which would have\nhelped most: (1) Another ALU? (2) Another LD/ST unit? (3) Full bypassing of\nALU results to subsequent operations? or (4) Cutting the longest latency in half?\nWhat\u2019s the speedup?\ne. [20] <3.6> Now let\u2019s consider speculation, the act of fetching, decoding, and\nexecuting beyond one or more conditional branches. Our motivation to do this\nis twofold: the dispatch schedule we came up with in part (c) had lots of nops,\nand we know computers spend most of their time executing loops (which implies\nthe branch back to the top of the loop is pretty predictable). Loops tell us where to\nfind more work to do; our sparse dispatch schedule suggests we have opportuni-\nties to do some of that work earlier than before. In part (d) you found the critical\npath through the loop. Imagine folding a second copy of that path onto the sched-\nule you got in part (b). How many more clock cycles would be required to do two\nloops\u2019 worth of work (assuming all instructions are resident in the RS)? (Assume\nall functional units are fully pipelined.)\nExercises\n3.13\n[25] <3.7, 3.8> In this exercise, you will explore performance trade-offs between\nthree processors that each employ different types of multithreading (MT). Each of\nthese processors is superscalar, uses in-order pipelines, requires a fixed three-cycle\nstall following all loads and branches, and has identical L1 caches. Instructions\nfrom the same thread issued in the same cycle are read in program order and must\nnot contain any data or control dependences.\n\u25a0\nProcessor A is a superscalar simultaneous MT architecture, capable of issuing\nup to two instructions per cycle from two threads.\n\u25a0\nProcessor B is a fine-grained MT architecture, capable of issuing up to four\ninstructions per cycle from a single thread and switches threads on any\npipeline stall.\nCase Studies and Exercises by Jason D. Bakos and Robert P. Colwell\n\u25a0\n273"
    },
    {
        "page": 306,
        "text": "\u25a0\nProcessor C is a coarse-grained MT architecture, capable of issuing up to eight\ninstructions per cycle from a single thread and switches threads on an L1\ncache miss.\nOur application is a list searcher, which scans a region of memory for a specific\nvalue stored in R9 between the address range specified in R16 and R17. It is par-\nallelized by evenly dividing the search space into four equal-sized contiguous\nblocks and assigning one search thread to each block (yielding four threads). Most\nof each thread\u2019s runtime is spent in the following unrolled loop body:\nloop: lw x1,0(x16)\nlw x2,8(x16)\nlw x3,16(x16)\nlw x4,24(x16)\nlw x5,32(x16)\nlw x6,40(x16)\nlw x7,48(x16)\nlw x8,56(x16)\nbeq x9,x1,match0\nbeq x9,x2,match1\nbeq x9,x3,match2\nbeq x9,x4,match3\nbeq x9,x5,match4\nbeq x9,x6,match5\nbeq x9,x7,match6\nbeq x9,x8,match7\nDADDIU x16,x16,#64\nblt x16,x17,loop\nAssume the following:\n\u25a0\nA barrier is used to ensure that all threads begin simultaneously.\n\u25a0\nThe first L1 cache miss occurs after two iterations of the loop.\n\u25a0\nNone of the BEQAL branches is taken.\n\u25a0\nThe BLT is always taken.\n\u25a0\nAll three processors schedule threads in a round-robin fashion.\nDetermine how many cycles are required for each processor to complete the first\ntwo iterations of the loop.\n3.14\n[25/25/25] <3.2, 3.7> In this exercise, we look at how software techniques can\nextract instruction-level parallelism (ILP) in a common vector loop. The following\nloop is the so-called DAXPY loop (double-precision aX plus Y) and is the central\noperation in Gaussian elimination. The following code implements the DAXPY\noperation, Y\u00bcaX+Y, for a vector length 100. Initially, R1 is set to the base address\nof array X and R2 is set to the base address of Y:\naddi\nx4,x1,#800 ; x1 = upper bound for X\n274\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 307,
        "text": "foo: fld\nF2,0(x1)\n; (F2) = X(i)\nfmul.d\nF4,F2,F0\n; (F4) = a*X(i)\nfld\nF6,0(x2)\n; (F6) = Y(i)\nfadd.d\nF6,F4,F6\n; (F6) = a*X(i) + Y(i)\nfsd\nF6,0(x2)\n; Y(i) = a*X(i) + Y(i)\naddi\nx1,x1,#8\n; increment X index\naddi\nx2,x2,#8\n; increment Y index\nsltu\nx3,x1,x4\n; test: continue loop?\nbnez\nx3,foo\n; loop if needed\nAssume the functional unit latencies as shown in the following table. Assume a\none-cycle delayed branch that resolves in the ID stage. Assume that results are fully\nbypassed.\nInstruction producing result\nInstruction using result\nLatency in clock cycles\nFP multiply\nFP ALU op\n6\nFP add\nFP ALU op\n4\nFP multiply\nFP store\n5\nFP add\nFP store\n4\nInteger operations and all loads\nAny\n2\na. [25] <3.2> Assume a single-issue pipeline. Show how the loop would look\nboth unscheduled by the compiler and after compiler scheduling for both\nfloating-point operation and branch delays, including any stalls or idle clock\ncycles. What is the execution time (in cycles) per element of the result vector,\nY, unscheduled and scheduled? How much faster must the clock be for proces-\nsor hardware alone to match the performance improvement achieved by the\nscheduling compiler? (Neglect any possible effects of increased clock speed\non memory system performance.)\nb. [25] <3.2> Assume a single-issue pipeline. Unroll the loop as many times as\nnecessary to schedule it without any stalls, collapsing the loop overhead instruc-\ntions. How many times must the loop be unrolled? Show the instruction sched-\nule. What is the execution time per element of the result?\nc. [25] <3.7> Assume a VLIW processor with instructions that contain five\noperations, as shown in Figure 3.20. We will compare two degrees of loop\nunrolling. First, unroll the loop 6 times to extract ILP and schedule it without\nany stalls (i.e., completely empty issue cycles), collapsing the loop overhead\ninstructions, and then repeat the process but unroll the loop 10 times. Ignore\nthe branch delay slot. Show the two schedules. What is the execution time per\nelement of the result vector for each schedule? What percent of the operation\nslots are used in each schedule? How much does the size of the code differ\nbetween the two schedules? What is the total register demand for the two\nschedules?\nCase Studies and Exercises by Jason D. Bakos and Robert P. Colwell\n\u25a0\n275"
    },
    {
        "page": 308,
        "text": "3.15\n[20/20] <3.4, 3.5, 3.7, 3.8> In this exercise, we will look at how variations on\nTomasulo\u2019s algorithm perform when running the loop from Exercise 3.14. The\nfunctional units (FUs) are described in the following table.\nFU type\nCycles in EX\nNumber of FUs\nNumber of reservation stations\nInteger\n1\n1\n5\nFP adder\n10\n1\n3\nFP multiplier\n15\n1\n2\nAssume the following:\n\u25a0\nFunctional units are not pipelined.\n\u25a0\nThere is no forwarding between functional units; results are communicated by\nthe common data bus (CDB).\n\u25a0\nThe execution stage (EX) does both the effective address calculation and the\nmemory access for loads and stores. Thus, the pipeline is IF/ID/IS/EX/WB.\n\u25a0\nLoads require one clock cycle.\n\u25a0\nThe issue (IS) and write-back (WB) result stages each require one clock cycle.\n\u25a0\nThere are five load buffer slots and five store buffer slots.\n\u25a0\nAssume that the Branch on Not Equal to Zero (BNEZ) instruction requires one\nclock cycle.\na. [20] <3.4\u20133.5> For this problem use the single-issue Tomasulo MIPS pipeline\nof Figure 3.10 with the pipeline latencies from the preceding table. Show the\nnumber of stall cycles for each instruction and what clock cycle each instruction\nbegins execution (i.e., enters its first EX cycle) for three iterations of the loop.\nHow many cycles does each loop iteration take? Report your answer in the form\nof a table with the following column headers:\n\u25a0\nIteration (loop iteration number)\n\u25a0\nInstruction\n\u25a0\nIssues (cycle when instruction issues)\n\u25a0\nExecutes (cycle when instruction executes)\n\u25a0\nMemory access (cycle when memory is accessed)\n\u25a0\nWrite CDB (cycle when result is written to the CDB)\n\u25a0\nComment (description of any event on which the instruction is waiting)\nShow three iterations of the loop in your table. You may ignore the first\ninstruction.\nb. [20] <3.7, 3.8> Repeat part (a) but this time assume a two-issue Tomasulo\nalgorithm and a fully pipelined floating-point unit (FPU).\n3.16\n[10] <3.4> Tomasulo\u2019s algorithm has a disadvantage: only one result can compute\nper clock per CDB. Use the hardware configuration and latencies from the previous\nquestion and find a code sequence of no more than 10 instructions where Toma-\nsulo\u2019s algorithm must stall due to CDB contention. Indicate where this occurs in\nyour sequence.\n276\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 309,
        "text": "3.17\n[20] <3.3> An (m,n) correlating branch predictor uses the behavior of the most\nrecent m executed branches to choose from 2m predictors, each of which is an n-\nbit predictor. A two-level local predictor works in a similar fashion, but only keeps\ntrack of the past behavior of each individual branch to predict future behavior.\nThere is a design trade-off involved with such predictors: correlating predictors\nrequire little memory for history, which allows them to maintain 2-bit predictors\nfor a large number of individual branches (reducing the probability of branch\ninstructions reusing the same predictor), while local predictors require substan-\ntially more memory to keep history and are thus limited to tracking a relatively\nsmall number of branch instructions. For this exercise, consider a (1,2) correlating\npredictor that can track four branches (requiring 16 bits) versus a (1,2) local pre-\ndictor that can track two branches using the same amount of memory. For the fol-\nlowing branch outcomes, provide each prediction, the table entry used to make the\nprediction, any updates to the table as a result of the prediction, and the final mis-\nprediction rate of each predictor. Assume that all branches up to this point have\nbeen taken. Initialize each predictor to the following:\nCorrelating predictor\nEntry\nBranch\nLast outcome\nPrediction\n0\n0\nT\nT with one misprediction\n1\n0\nNT\nNT\n2\n1\nT\nNT\n3\n1\nNT\nT\n4\n2\nT\nT\n5\n2\nNT\nT\n6\n3\nT\nNT with one misprediction\n7\n3\nNT\nNT\nLocal predictor\nEntry\nBranch\nLast 2 outcomes (right is most recent)\nPrediction\n0\n0\nT,T\nT with one misprediction\n1\n0\nT,NT\nNT\n2\n0\nNT,T\nNT\n3\n0\nNT\nT\n4\n1\nT,T\nT\n5\n1\nT,NT\nT with one misprediction\n6\n1\nNT,T\nNT\n7\n1\nNT,NT\nNT\nCase Studies and Exercises by Jason D. Bakos and Robert P. Colwell\n\u25a0\n277"
    },
    {
        "page": 310,
        "text": "3.18\n[10] <3.9> Suppose we have a deeply pipelined processor, for which we imple-\nment a branch-target buffer for the conditional branches only. Assume that the mis-\nprediction penalty is always four cycles and the buffer miss penalty is always three\ncycles. Assume a 90% hit rate, 90% accuracy, and 15% branch frequency. How\nmuch faster is the processor with the branch-target buffer versus a processor that\nhas a fixed two-cycle branch penalty? Assume a base clock cycle per instruction\n(CPI) without branch stalls of one.\n3.19\n[10/5] <3.9> Consider a branch-target buffer that has penalties of zero, two, and\ntwo clock cycles for correct conditional branch prediction, incorrect prediction,\nand a buffer miss, respectively. Consider a branch-target buffer design that distin-\nguishes conditional and unconditional branches, storing the target address for a\nconditional branch and the target instruction for an unconditional branch.\na. [10] <3.9> What is the penalty in clock cycles when an unconditional branch is\nfound in the buffer?\nb. [10] <3.9> Determine the improvement from branch folding for unconditional\nbranches. Assume a 90% hit rate, an unconditional branch frequency of 5%, and\na two-cycle penalty for a buffer miss. How much improvement is gained by this\nenhancement? How high must the hit rate be for this enhancement to provide a\nperformance gain?\nBranch PC (word address)\nOutcome\n454\nT\n543\nNT\n777\nNT\n543\nNT\n777\nNT\n454\nT\n777\nNT\n454\nT\n543\nT\n278\n\u25a0\nChapter Three Instruction-Level Parallelism and Its Exploitation"
    },
    {
        "page": 311,
        "text": "This page intentionally left blank"
    },
    {
        "page": 312,
        "text": "4.1\nIntroduction\n282\n4.2\nVector Architecture\n283\n4.3\nSIMD Instruction Set Extensions for Multimedia\n304\n4.4\nGraphics Processing Units\n310\n4.5\nDetecting and Enhancing Loop-Level Parallelism\n336\n4.6\nCross-Cutting Issues\n345\n4.7\nPutting It All Together: Embedded Versus Server GPUs\nand Tesla Versus Core i7\n346\n4.8\nFallacies and Pitfalls\n353\n4.9\nConcluding Remarks\n357\n4.10\nHistorical Perspective and References\n357\nCase Study and Exercises by Jason D. Bakos\n357"
    },
    {
        "page": 313,
        "text": "4\nData-Level Parallelism in\nVector, SIMD, and GPU\nArchitectures\nWe call these algorithms data parallel algorithms because their\nparallelism comes from simultaneous operations across large sets\nof data rather than from multiple threads of control.\nW. Daniel Hillis and Guy L. Steele,\n\u201cData parallel algorithms,\u201d Commun. ACM (1986)\nIf you were plowing a field, which would you rather use: two strong\noxen or 1024 chickens?\nSeymour Cray, Father of the Supercomputer\n(arguing for two powerful vector processors\nversus many simple processors)\nComputer Architecture. https://doi.org/10.1016/B978-0-12-811905-1.00004-3\n\u00a9 2019 Elsevier Inc. All rights reserved."
    },
    {
        "page": 314,
        "text": "4.1\nIntroduction\nA question for the single instruction multiple data (SIMD) architecture, which\nChapter 1 introduced, has always been just how wide a set of applications has sig-\nnificant data-level parallelism (DLP). Five years after the SIMD classification was\nproposed (Flynn, 1966), the answer is not only the matrix-oriented computations of\nscientific computing but also the media-oriented image and sound processing and\nmachine learning algorithms, as we will see in Chapter 7. Since a multiple instruc-\ntion multiple data (MIMD) architecture needs to fetch one instruction per data oper-\nation, single instruction multiple data (SIMD) is potentially more energy-efficient\nsince a single instruction can launch many data operations. These two answers make\nSIMD attractive for personal mobile devices as well as for servers. Finally, perhaps\nthe biggest advantage of SIMD versus MIMD is that the programmer continues to\nthink sequentially yet achieves parallel speedup by having parallel data operations.\nThis chapter covers three variations of SIMD: vector architectures, multimedia\nSIMD instruction set extensions, and graphics processing units (GPUs).1\nThe first variation, which predates the other two by more than 30 years, extends\npipelined execution of many data operations. These vector architectures are easier\nto understand and to compile to than other SIMD variations, but they were consid-\nered too expensive for microprocessors until recently. Part of that expense was in\ntransistors, and part was in the cost of sufficient dynamic random access memory\n(DRAM) bandwidth, given the widespread reliance on caches to meet memory per-\nformance demands on conventional microprocessors.\nThe second SIMD variation borrows from the SIMD name to mean basically\nsimultaneous parallel data operations and is now found in most instruction set\narchitectures that support multimedia applications. For x86 architectures, the\nSIMD instruction extensions started with the MMX (multimedia extensions) in\n1996, which were followed by several SSE (streaming SIMD extensions) versions\nin the next decade, and they continue until this day with AVX (advanced vector\nextensions). To get the highest computation rate from an x86 computer, you often\nneed to use these SIMD instructions, especially for floating-point programs.\nThe third variation on SIMD comes from the graphics accelerator community,\noffering higher potential performance than is found in traditional multicore com-\nputers today. Although GPUs share features with vector architectures, they have\ntheir own distinguishing characteristics, in part because of the ecosystem in which\nthey evolved. This environment has a system processor and system memory in\naddition to the GPU and its graphics memory. In fact, to recognize those distinc-\ntions, the GPU community refers to this type of architecture as heterogeneous.\n1 This chapter is based on material in Appendix F, \u201cVector Processors,\u201d by Krste Asanovic, and Appendix G, \u201cHardware\nand Software for VLIW and EPIC\u201d from the 5th edition of this book; on material in Appendix A, \u201cGraphics and Computing\nGPUs,\u201d by John Nickolls and David Kirk, from the 5th edition of Computer Organization and Design; and to a lesser extent\non material in \u201cEmbracing and Extending 20th-Century Instruction Set Architectures,\u201d by Joe Gebis and David Patterson,\nIEEE Computer, April 2007.\n282\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 315,
        "text": "For problems with lots of data parallelism, all three SIMD variations share the\nadvantage of being easier on the programmer than classic parallel MIMD\nprogramming.\nThegoalofthischapterisforarchitectstounderstandwhyvectorismoregeneral\nthan multimedia SIMD, as well as the similarities and differences between vector\nand GPU architectures.Because vector architectures are supersetsof the multimedia\nSIMD instructions, including a better model for compilation, and because GPUs\nshare several similaritieswithvector architectures, we start with vector architectures\nto set the foundation for the following two sections. The next section introduces\nvector architectures, and Appendix G goes much deeper into the subject.\n4.2\nVector Architecture\nThe most efficient way to execute a vectorizable application is a vector\nprocessor.\nJim Smith,\nInternational Symposium on Computer Architecture (1994)\nVector architectures grab sets of data elements scattered about memory, place them\ninto large sequential register files, operate on data in those register files, and then\ndisperse the results back into memory. A single instruction works on vectors of\ndata, which results in dozens of register-register operations on independent data\nelements.\nThese large register files act as compiler-controlled buffers, both to hide mem-\nory latency and to leverage memory bandwidth. Because vector loads and stores\nare deeply pipelined, the program pays the long memory latency only once per vec-\ntor load or store versus once per element, thus amortizing the latency over, say, 32\nelements. Indeed, vector programs strive to keep the memory busy.\nThe power wall leads architects to value architectures that can deliver good\nperformance without the energy and design complexity costs of highly out-of-\norder superscalar processors. Vector instructions are a natural match to this trend\nbecause architects can use them to increase performance of simple in-order scalar\nprocessors without greatly raising energy demands and design complexity. In prac-\ntice, developers can express many of the programs that ran well on complex out-of-\norder designs more efficiently as data-level parallelism in the form of vector\ninstructions, as Kozyrakis and Patterson (2002) showed.\nRV64V Extension\nWe begin with a vector processor consisting of the primary components that\nFigure 4.1 shows. It is loosely based on the 40-year-old Cray-1, which was one\nof the first supercomputers. At the time of the writing of this edition, the RISC-\nV vector instruction set extension RVV was still under development. (The vector\nextension by itself is called RVV, so RV64V refers to the RISC-V base instructions\n4.2\nVector Architecture\n\u25a0\n283"
    },
    {
        "page": 316,
        "text": "plus the vector extension.) We show a subset of RV64V, trying to capture its\nessence in a few pages.\nThe primary components of the instruction set architecture of RV64V are the\nfollowing:\n\u25a0\nVector registers\u2014Each vector register holds a single vector, and RV64V has\n32 of them, each 64 bits wide. The vector register file needs to provide enough\nports to feed all the vector functional units. These ports will allow a high degree\nof overlap among vector operations to different vector registers. The read and\nwrite ports, which total at least 16 read ports and 8 write ports, are connected to\nthe functional unit inputs or outputs by a pair of crossbar switches. One way to\nMain memory\nVector\nregisters\nScalar\nregisters\nFP add/subtract\nFP multiply\nFP divide\nInteger\nLogical\nVector\nload/store\nFigure 4.1 The basic structure of a vector architecture, RV64V, which includes a\nRISC-V scalar architecture. There are also 32 vector registers, and all the functional units\nare vector functional units. The vector and scalar registers have a significant number of\nread and write ports to allow multiple simultaneous vector operations. A set of crossbar\nswitches (thick gray lines) connects these ports to the inputs and outputs of the vector\nfunctional units.\n284\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 317,
        "text": "increase the register file bandwidth is to compose it from multiple banks, which\nwork well with relatively long vectors.\n\u25a0\nVector functional units\u2014Each unit is fully pipelined in our implementation,\nand it can start a new operation on every clock cycle. A control unit is needed\nto detect hazards, both structural hazards for functional units and data hazards\non register accesses. Figure 4.1 shows that we assume an implementation of\nRV64V has five functional units. For simplicity, we focus on the floating-point\nfunctional units in this section.\n\u25a0\nVector load/store unit\u2014The vector memory unit loads or stores a vector to\nor from memory. The vector loads and stores are fully pipelined in our\nhypothetical RV64V implementation so that words can be moved between\nthe vector registers and memory with a bandwidth of one word per clock\ncycle, after an initial latency. This unit would also normally handle scalar\nloads and stores.\n\u25a0\nA set of scalar registers\u2014Scalar registers can likewise provide data as input to\nthe vector functional units, as well as compute addresses to pass to the vector\nload/store unit. These are the normal 31 general-purpose registers and 32\nfloating-point registers of RV64G. One input of the vector functional units\nlatches scalar values as they are read out of the scalar register file.\nFigure 4.2 lists the RV64V vector instructions we use in this section. The\ndescription in Figure 4.2 assumes that the input operands are all vector registers,\nbut there are also versions of these instructions where an operand can be a scalar\nregister (xi or fi). RV64V uses the suffix .vv when both are vectors, .vs\nwhen the second operand is a scalar, and .sv when the first is a scalar register.\nThus these three are all valid RV64V instructions: vsub.vv, vsub.vs, and\nvsub.sv. (Add and other commutative operations have only the first two\nversions, as vadd.sv and vadd.sv would be redundant.) Because the\noperands determine the version of the instruction, we usually let the assembler\nsupply the appropriate suffix. The vector functional unit gets a copy of the sca-\nlar value at instruction issue time.\nAlthough the traditional vector architectures didn\u2019t support narrow data types\nefficiently, vectors naturally accommodate varying data sizes (Kozyrakis and\nPatterson, 2002). Thus, if a vector register has 32 64-bit elements, then 128\u000116-\nbit elements, and even 256\u00018-bit elements are equally valid views. Such hardware\nmultiplicity is why a vector architecture canbe useful for multimedia applications as\nwell as for scientific applications.\nNote that the RV64V instructions in Figure 4.2 omit the data type and size! An\ninnovation of RV64V is to associate a data type and data size with each vector\nregister, rather than the normal approach of the instruction supplying that informa-\ntion. Thus, before executing the vector instructions, a program configures the\nvector registers being used to specify their data type and widths. Figure 4.3 lists\nthe options for RV64V.\n4.2\nVector Architecture\n\u25a0\n285"
    },
    {
        "page": 318,
        "text": "Mnemonic\nName\nDescription\nvadd\nADD\nAdd elements of V[rs1] and V[rs2], then put each result in V[rd]\nvsub\nSUBtract\nSubtract elements of V[rs2] frpm V[rs1], then put each result in V[rd]\nvmul\nMULtiply\nMultiply elements of V[rs1] and V[rs2], then put each result in V[rd]\nvdiv\nDIVide\nDivide elements of V[rs1] by V[rs2], then put each result in V[rd]\nvrem\nREMainder\nTake remainder of elements of V[rs1] by V[rs2], then put each result in V[rd]\nvsqrt\nSQuare RooT\nTake square root of elements of V[rs1], then put each result in V[rd]\nvsll\nShift Left\nShift elements of V[rs1] left by V[rs2], then put each result in V[rd]\nvsrl\nShift Right\nShift elements of V[rs1] right by V[rs2], then put each result in V[rd]\nvsra\nShift Right\nArithmetic\nShift elements of V[rs1] right by V[rs2] while extending sign bit, then put each result in\nV[rd]\nvxor\nXOR\nExclusive OR elements of V[rs1] and V[rs2], then put each result in V[rd]\nvor\nOR\nInclusive OR elements of V[rs1] and V[rs2], then put each result in V[rd]\nvand\nAND\nLogical AND elements of V[rs1] and V[rs2], then put each result in V[rd]\nvsgnj\nSiGN source\nReplace sign bits of V[rs1] with sign bits of V[rs2], then put each result in V[rd]\nvsgnjn\nNegative SiGN\nsource\nReplace sign bits of V[rs1] with complemented sign bits of V[rs2], then put each result\nin V[rd]\nvsgnjx\nXor SiGN\nsource\nReplace sign bits of V[rs1] with xor of sign bits of V[rs1] and V[rs2], then put each\nresult in V[rd]\nvld\nLoad\nLoad vector register V[rd] from memory starting at address R[rs1]\nvlds\nStrided Load\nLoad V[rd] from address at R[rs1] with stride in R[rs2] (i.e., R[rs1]+i\u0001R[rs2])\nvldx\nIndexed Load\n(Gather)\nLoad V[rs1] with vector whose elements are at R[rs2]+V[rs2] (i.e., V[rs2] is an index)\nvst\nStore\nStore vector register V[rd] into memory starting at address R[rs1]\nvsts\nStrided Store\nStore V[rd] into memory at address R[rs1] with stride in R[rs2] (i.e., R[rs1]+i\u0001R[rs2])\nvstx\nIndexed Store\n(Scatter)\nStore V[rs1] into memory vector whose elements are at R[rs2]+V[rs2] ( i.e., V[rs2] is\nan index)\nvpeq\nCompare \u00bc\nCompare elements of V[rs1] and V[rs2]. When equal, put a 1 in the corresponding 1-bit\nelement of p[rd]; otherwise, put 0\nvpne\nCompare !\u00bc\nCompare elements of V[rs1] and V[rs2]. When not equal, put a 1 in the corresponding\n1-bit element of p[rd]; otherwise, put 0\nvplt\nCompare <\nCompare elements of V[rs1] and V[rs2]. When less than, put a 1 in the corresponding 1-\nbit element of p[rd]; otherwise, put 0\nvpxor\nPredicate XOR\nExclusive OR 1-bit elements of p[rs1] and p[rs2], then put each result in p[rd]\nvpor\nPredicate OR\nInclusive OR 1-bit elements of p[rs1] and p[rs2], then put each result in p[rd]\nvpand\nPredicate AND\nLogical AND 1-bit elements of p[rs1] and p[rs2], then put each result in p[rd]\nsetvl\nSet Vector\nLength\nSet vl and the destination register to the smaller of mvl and the source regsiter\nFigure 4.2 The RV64V vector instructions. All use the R instruction format. Each vector operation with two operands\nis shown with both operands being vector (.vv), but there are also versions where the second operand is a scalar\nregister (.vs) and, when it makes a difference, where the first operand is a scalar register and the second is a vector\nregister (.sv). The type and width of the operands are determined by configuring each vector register rather than\nbeing supplied by the instruction. In addition to the vector registers and predicate registers, there are two vector\ncontrol and status registers (CSRs), vl and vctype, discussed below. The strided and indexed data transfers are also\nexplained later. Once completed, RV64 will surely have more instructions, but the ones in this figure will be included."
    },
    {
        "page": 319,
        "text": "One reason for dynamic register typing is that many instructions are required\nfor a conventional vector architecture that supports such variety. Given the com-\nbinations of data types and sizes in Figure 4.3, if not for dynamic register typing,\nFigure 4.2 would be several pages long!\nDynamic typing also lets programs disable unused vector registers. As a conse-\nquence, enabled vector registers are allocated all the vector memory as long vectors.\nFor example, assume we have 1024 bytes of vector memory, if 4 vector registers\nare enabled and they are type 64\u2013bit floats, the processor would give each vector\nregister 256 bytes or 256/8\u00bc32 elements. This valiue is called the maximum vector\nlength (mvl), which is set by the processor and cannot be changed by sofware.\nOne complaint about vector architectures is that their larger state means slower\ncontext switch time. Our implementation of RV64V increases state a factor of 3:\nfrom 2\u000132\u00018\u00bc512 bytes to 2\u000132\u00011024\u00bc1536 bytes. A pleasant side effect\nof dynamic register typing is that the program can configure vector registers as dis-\nabled when they are not being used, so there is no need to save and restore them on\na context switch.\nA third benefit of dynamic register typing is that conversions between different\nsize operands can be implicit depending on the configuration of the registers rather\nthan as additional explicit conversion instructions. We\u2019ll see an example of this\nbenefit in the next section.\nThe names vld and vst denote vector load and vector store, and they load or\nstore an entire vectors of data. One operand is the vector register to be loaded or\nstored; the other operand, which is a RV64G general-purpose register, is the start-\ning address of the vector in memory. Vector needs more registers beyond the vector\nregisters themselves. The vector-length register vl is used when the natural vector\nlength is not equal to mvl, the vector-type register vctype records register types,\nand the predicate registers pi are used when loops involve IF statements. We\u2019ll see\nthem in action in the following example.\nWith a vector instruction, the system can perform the operations on the vector\ndataelements inmany ways, including operating onmany elements simultaneously.\nThis flexibility lets vector designs use slow but wide execution units to achieve\nhigh performance at low power. Furthermore, the independence of elements within\na vector instruction set allows scaling of functional units without performing addi-\ntional costly dependency checks, as superscalar processors require.\nInteger\n8, 16, 32, and 64 bits\nFloating point\n16, 32, and 64 bits\nFigure 4.3 Data sizes supported for RV64V assuming it also has the single- and\ndouble-precision floating-point extensions RVS and RVD. Adding RVV to such a\nRISC-V design means the scalar unit must also add RVH, which is a scalar instruction\nextension to support half-precision (16-bit) IEEE 754 floating point. Because RV32V\nwould not have doubleword scalar operations, it could drop 64-bit integers from the\nvector unit. If a RISC-V implementation didn\u2019t include RVS or RVD, it could omit the vec-\ntor floating-point instructions.\n4.2\nVector Architecture\n\u25a0\n287"
    },
    {
        "page": 320,
        "text": "How Vector Processors Work: An Example\nWe can best understand a vector processor by looking at a vector loop for RV64V.\nLet\u2019s take a typical vector problem, which we use throughout this section:\nY = a \u0001 X + Y\nX and Y are vectors, initially resident in memory, and a is a scalar. This problem is\nthe SAXPY or DAXPY loop that forms the inner loop of the Linpack benchmark\n(Dongarra et al., 2003). (SAXPY stands for single-precision a\u0001X plus Y, and\nDAXPY for double precision a\u0001X plus Y.) Linpack is a collection of linear alge-\nbra routines, and the Linpack benchmark consists of routines for performing\nGaussian elimination.\nFor now, let us assume that the number of elements, or length, of a vector reg-\nister (32) matches the length of the vector operation we are interested in. (This\nrestriction will be lifted shortly.)\nExample\nShow the code for RV64G and RV64V for the DAXPY loop. For this example,\nassume that X and Y have 32 elements and the starting addresses of X and Y\nare in x5 and x6, respectively. (A subsequent example covers when they do\nnot have 32 elements.)\nAnswer\nHere is the RISC-V code:\nfld\nf0,a\n# Load scalar a\naddi\nx28,x5,#256\n# Last address to load\nLoop: fld\nf1,0(x5)\n# Load X[i]\nfmul.d f1,f1,f0\n# a \u0001 X[i]\nfld\nf2,0(x6)\n# Load Y[i]\nfadd.d f2,f2,f1\n# a \u0001 X[i] + Y[i]\nfsd\nf2,0(x6)\n# Store into Y[i]\naddi\nx5,x5,#8\n# Increment index to X\naddi\nx6,x6,#8\n# Increment index to Y\nbne\nx28,x5,Loop\n# Check if done\nHere is the RV64V code for DAXPY:\nvsetdcfg\n4*FP64\n# Enable 4 DP FP vregs\nfld\nf0,a\n# Load scalar a\nvld\nv0,x5\n# Load vector X\nvmul\nv1,v0,f0\n# Vector-scalar mult\nvld\nv2,x6\n# Load vector Y\nvadd\nv3,v1,v2\n# Vector-vector add\nvst\nv3,x6\n# Store the sum\nvdisable\n# Disable vector regs\nNote that the assembler determines which version of the vector operations to gen-\nerate. Because the multiply has a scalar operand, it generates vmul.vs, whereas\nthe add doesn\u2019t, so it generates vadd.vv.\n288\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 321,
        "text": "The initial instruction configures the first four vector registers to hold 64-bit\nfloating-point data. The last instruction disables all vector registers. If a context\nswitch happened after the last instruction, there is no additional state to save.\nThemostdramaticdifferencebetweentheprecedingscalarandvectorcodeisthat\nthe vector processor greatly reduces the dynamic instruction bandwidth, executing\nonly 8 instructions versus 258 for RV64G. This reduction occurs because the vector\noperations work on 32 elements and the overhead instructions that constitute nearly\nhalf the loop on RV64G are not present in the RV64V code. When the compiler\nproduces vector instructions for such a sequence, and the resulting code spends\nmuch of its time running in vector mode, the code is said to be vectorized or vector-\nizable. Loops can be vectorized when they do not have dependences between\niterations of a loop, which are called loop-carried dependences (see Section 4.5).\nAnother important difference between RV64G and RV64V is the frequency of\npipeline interlocks for a simple implementation of RV64G. In the straightforward\nRV64G code, every fadd.d must wait for a fmul.d, and every fsd must wait\nfor the fadd.d. On the vector processor, each vector instruction will stall only\nforthefirstelement ineach vector, andthensubsequentelementswillflow smoothly\ndown the pipeline. Thus pipeline stalls are required only once per vector instruction,\nrather than once per vector element. Vector architects call forwarding of element-\ndependent operations chaining, in that the dependent operations are \u201cchained\u201d\ntogether. In this example, the pipeline stall frequency on RV64G will be about\n32\u0001 higher than it is on RV64V. Software pipelining, loop unrolling (Appendix\nH), or out-of-order execution can reduce the pipeline stalls on RV64G; however,\nthe large difference in instruction bandwidth cannot be reduced substantially.\nLet\u2019s show off the dynamic register typing before discussing performance of\nthe code.\nExample\nA common use of multiply-accumulate operations is to multiply using narrow data\nand to accumulate at a wider size to increase the accuracy of a sum of products.\nShow how the preceding code would change if X and a were single-precision\ninstead of a double-precision floating point. Next, show the changes to this code\nif we switch X, Y, and a from floating-point type to integers.\nAnswer\nThe changes are underlined in the following code. Amazingly, the same code works\nwith two small changes: the configuration instruction includes one single-precision\nvector, and the scalar load is now single-precision:\nvsetdcfg 1*FP32,3*FP64\n# 1 32b, 3 64b vregs\nflw\nf0,a\n# Load scalar a\nvld\nv0,x5\n# Load vector X\nvmul\nv1,v0,f0\n# Vector-scalar mult\nvld\nv2,x6\n# Load vector Y\nvadd\nv3,v1,v2\n# Vector-vector add\nvst\nv3,x6\n# Store the sum\nvdisable\n# Disable vector regs\n4.2\nVector Architecture\n\u25a0\n289"
    },
    {
        "page": 322,
        "text": "Note that RV64V hardware will implicitly perform a conversion from the narrower\nsingle-precision to the wider double-precision in this setup.\nThe switch to integers is almost as easy, but we must now use an integer load\ninstruction and integer register to hold the scalar value:\nvsetdcfg 1*X32,3*X64\n# 1 32b, 3 64b int reg\nlw\nx7,a\n# Load scalar a\nvld\nv0,x5\n# Load vector X\nvmul\nv1,v0,x7\n# Vector-scalar mult\nvld\nv2,x6\n# Load vector Y\nvadd\nv3,v1,v2\n# Vector-vector add\nvst\nv3,x6\n# Store the sum\nvdisable\n# Disable vector regs\nVector Execution Time\nThe execution time of a sequence of vector operations primarily depends on three\nfactors: (1) the length of the operand vectors, (2) structural hazards among the\noperations, and (3) the data dependences. Given the vector length and the initiation\nrate, which is the rate at which a vector unit consumes new operands and produces\nnew results, we can compute the time for a single vector instruction.\nAll modern vector computers have vector functional units with multiple par-\nallel pipelines (or lanes) that can produce two or more results per clock cycle,\nbut they may also have some functional units that are not fully pipelined. For sim-\nplicity, our RV64V implementation has one lane with an initiation rate of one ele-\nment per clock cycle for individual operations. Thus the execution time in clock\ncycles for a single vector instruction is approximately the vector length.\nTo simplify the discussion of vector execution and vector performance, we use\nthe notion of a convoy, which is the set of vector instructions that could potentially\nexecute together. The instructions in a convoy must not contain any structural haz-\nards; if such hazards were present, the instructions would need to be serialized and\ninitiated in different convoys. Thus the vld and the following vmul in the pre-\nceding example can be in the same convoy. As we will soon see, you can estimate\nperformance of a section of code by counting the number of convoys. To keep this\nanalysis simple, we assume that a convoy of instructions must complete execution\nbefore any other instructions (scalar or vector) can begin execution.\nIt might seem that in addition to vector instruction sequences with structural\nhazards, sequences with read-after-write dependency hazards should also be in\nseparate convoys. However, chaining allows them to be in the same convoy since\nit allows a vector operation to start as soon as the individual elements of its vector\nsource operand become available: the results from the first functional unit in the\nchain are \u201cforwarded\u201d to the second functional unit. In practice, we often imple-\nment chaining by allowing the processor to read and write a particular vector reg-\nister at the same time, albeit to different elements. Early implementations of\n290\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 323,
        "text": "chaining worked just like forwarding in scalar pipelining, but this restricted the\ntiming of the source and destination instructions in the chain. Recent implementa-\ntions use flexible chaining, which allows a vector instruction to chain to essentially\nany other active vector instruction, assuming that we don\u2019t generate a structural\nhazard. All modern vector architectures support flexible chaining, which we\nassume throughout this chapter.\nTo turn convoys into execution time, we need a metric to estimate the length of\na convoy. It is called a chime, which is simply the unit of time taken to execute one\nconvoy. Thus a vector sequence that consists of m convoys executes in m chimes;\nfor a vector length of n, for our simple RV64V implementation, this is approxi-\nmately m\u0001n clock cycles.\nThe chime approximation ignores some processor-specific overheads, many of\nwhich are dependent on vector length. Therefore measuring time in chimes is a\nbetter approximation for long vectors than for short ones. We will use the chime\nmeasurement, rather than clock cycles per result, to indicate explicitly that we are\nignoring certain overheads.\nIf we know the number of convoys in a vector sequence, we know the execution\ntimeinchimes.Onesourceofoverheadignoredinmeasuringchimesisanylimitation\non initiating multiple vector instructions in a single clock cycle. If only one vector\ninstruction can be initiated in a clock cycle (the reality in most vector processors),\nthe chime count will underestimate the actual execution time of a convoy. Because\nthe length of vectors is typically much greater than the number of instructions in\nthe convoy, we will simply assume that the convoy executes in one chime.\nExample\nShow how the following code sequence lays out in convoys, assuming a single\ncopy of each vector functional unit:\nvld\nv0,x5\n# Load vector X\nvmul\nv1,v0,f0\n# Vector-scalar multiply\nvld\nv2,x6\n# Load vector Y\nvadd\nv3,v1,v2\n# Vector-vector add\nvst\nv3,x6\n# Store the sum\nHow many chimes will this vector sequence take? How many cycles per\nFLOP (floating-point operation) are needed, ignoring vector instruction issue\noverhead?\nAnswer\nThe first convoy starts with the first vld instruction. The vmul is dependent on the\nfirst vld, but chaining allows it to be in the same convoy.\nThe second vld instruction must be in a separate convoy because there is a\nstructural hazard on the load/store unit for the prior vld instruction. The vadd\nis dependent on the second vld, but it can again be in the same convoy via chain-\ning. Finally, the vst has a structural hazard on the vld in the second convoy, so it\nmust go in the third convoy. This analysis leads to the following layout of vector\ninstructions into convoys:\n4.2\nVector Architecture\n\u25a0\n291"
    },
    {
        "page": 324,
        "text": "1. vld\nvmul\n2. vld\nvadd\n3. vst\nThe sequence requires three convoys. Because the sequence takes three chimes and\nthere are two floating-point operations per result, the number of cycles per FLOP is\n1.5 (ignoring any vector instruction issue overhead). Note that, although we allow\nthe vld and vmul both to execute in the first convoy, most vector machines will\ntake 2 clock cycles to initiate the instructions.\nThis example shows that the chime approximation is reasonably accurate for\nlong vectors. For example, for 32-element vectors, the time in chimes is 3, so\nthe sequence would take about 32\u00013 or 96 clock cycles. The overhead of issuing\nconvoys in two separate clock cycles would be small.\nAnother source of overhead is far more significant than the issue limitation. The\nmost important source of overhead ignored by the chime model is vector start-up\ntime, which is the latency in clock cycles until the pipeline is full. The start-up time\nis principally determined by the pipelining latency of the vector functional unit. For\nRV64V, we will use the same pipeline depths as the Cray-1, although latencies in\nmore modern processors have tended to increase, especially for vector loads. All\nfunctional units are fully pipelined. The pipeline depths are 6 clock cycles for\nfloating-point add, 7 for floating-point multiply, 20 for floating-point divide,\nand 12 for vector load.\nGiven these vector basics, the next several sections will give optimizations that\neither improve the performance or increase the types of programs that can run well\non vector architectures. In particular, they will answer these questions:\n\u25a0\nHow can a vector processor execute a single vector faster than one element per\nclock cycle? Multiple elements per clock cycle improve performance.\n\u25a0\nHow does a vector processor handle programs where the vector lengths are not\nthe same as the maximum vector length (mvl)? Because most application vec-\ntors don\u2019t match the architecture vector length, we need an efficient solution to\nthis common case.\n\u25a0\nWhat happens when there is an IF statement inside the code to be vectorized?\nMore code can vectorize if we can efficiently handle conditional statements.\n\u25a0\nWhat does a vector processor need from the memory system? Without suffi-\ncient memory bandwidth, vector execution can be futile.\n\u25a0\nHow does a vector processor handle multiple dimensional matrices? This pop-\nular data structure must vectorize for vector architectures to do well.\n\u25a0\nHow does a vector processor handle sparse matrices? This popular data struc-\nture must vectorize also.\n292\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 325,
        "text": "\u25a0\nHow do you program a vector computer? Architectural innovations that are a\nmismatch to programming languages and their compilers may not get\nwidespread use.\nThe rest of this section introduces each of these optimizations of the vector archi-\ntecture, and Appendix G goes into greater depth.\nMultiple Lanes: Beyond One Element per Clock Cycle\nA critical advantage of a vector instruction set is that it allows software to pass a\nlarge amount of parallel work to hardware using only a single short instruction.\nOne vector instruction can include scores of independent operations yet be\nencoded in the same number of bits as a conventional scalar instruction. The par-\nallel semantics of a vector instruction allow an implementation to execute these\nelemental operations using a deeply pipelined functional unit, as in the RV64V\nimplementation we\u2019ve studied so far; an array of parallel functional units; or a com-\nbination of parallel and pipelined functional units. Figure 4.4 illustrates how to\nimprove vector performance by using parallel pipelines to execute a vector add\ninstruction.\nThe RV64V instruction set has the property that all vector arithmetic instruc-\ntions only allow element N of one vector register to take part in operations with\nelement N from other vector registers. This dramatically simplifies the design of\na highly parallel vector unit, which can be structured as multiple parallel lanes.\nAs with a traffic highway, we can increase the peak throughput of a vector unit\nby adding more lanes. Figure 4.5 shows the structure of a four-lane vector unit.\nThus going to four lanes from one lane reduces the number of clocks for a chime\nfrom 32 to 8. For multiple lanes to be advantageous, both the applications and the\narchitecture must support long vectors; otherwise, they will execute so quickly that\nyou\u2019ll run out of instruction bandwidth, requiring ILP techniques (see Chapter 3) to\nsupply enough vector instructions.\nEach lane contains one portion of the vector register file and one execution\npipeline from each vector functional unit. Each vector functional unit executes\nvector instructions at the rate of one element group per cycle using multiple\npipelines, one per lane. The first lane holds the first element (element 0) for all\nvector registers, and so the first element in any vector instruction will have its\nsource and destination operands located in the first lane. This allocation allows\nthe arithmetic pipeline local to the lane to complete the operation without commu-\nnicating with other lanes. Avoiding interlane communication reduces the wiring\ncost and register file ports required to build a highly parallel execution unit and\nhelps explain why vector computers can complete up to 64 operations per clock\ncycle (2 arithmetic units and 2 load/store units across 16 lanes).\nAdding multiple lanes is a popular technique to improve vector performance as\nit requires little increase in control complexity and does not require changes to\nexisting machine code. It also allows designers to trade off die area, clock rate,\nvoltage, and energy without sacrificing peak performance. If the clock rate of a\n4.2\nVector Architecture\n\u25a0\n293"
    },
    {
        "page": 326,
        "text": "vector processor is halved, doubling the number of lanes will retain the same peak\nperformance.\nVector-Length Registers: Handling Loops Not Equal to 32\nA vector register processor has a natural vector length determined by the maximum\nvector length (mvl). This length, which was 32 in our example above, is unlikely\n(A)\n(B)\nElement group\nFigure 4.4 Using multiple functional units to improve the performance of a single vector add instruction,\nC5A+B. The vector processor (A) on the left has a single add pipeline and can complete one addition per clock\ncycle. The vector processor (B) on the right has four add pipelines and can complete four additions per clock cycle.\nThe elements within a single vector add instruction are interleaved across the four pipelines. The set of elements that\nmove through the pipelines together is termed an element group. Reproduced with permission from Asanovic, K.,\n1998. Vector Microprocessors (Ph.D. thesis). Computer Science Division, University of California, Berkeley.\n294\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 327,
        "text": "to match the real vector length in a program. Moreover, in a real program, the\nlength of a particular vector operation is often unknown at compile time. In fact,\na single piece of code may require different vector lengths. For example, consider\nthis code:\nfor (i=0; i <n; i=i+1)\nY[i] = a * X[i] + Y[i];\nThe size of all the vector operations depends on n, which may not even be known\nuntil run time. The value of n might also be a parameter to a procedure containing\nthe preceding loop and therefore subject to change during execution.\nLane 1\nLane 2\nLane 3\nLane 0\nFP add\npipe 0\nVector\nregisters:\nelements\n0, 4, 8, . . . \nFP mul.\npipe 0\nFP mul.\npipe 1\nVector load-store unit\nFP mul.\npipe 2\nFP mul.\npipe 3\nVector\nregisters:\nelements\n1, 5, 9, . . . \nVector\nregisters:\nelements\n2, 6, 10, . . . \nVector\nregisters:\nelements\n3, 7, 11, . . . \nFP add\npipe 1\nFP add\npipe 2\nFP add\npipe 3\nFigure 4.5 Structure of a vector unit containing four lanes. The vector register mem-\nory is divided across the lanes, with each lane holding every fourth element of each\nvector register. The figure shows three vector functional units: an FP add, an FP multiply,\nand a load-store unit. Each of the vector arithmetic units contains four execution pipe-\nlines, one per lane, which act in concert to complete a single vector instruction. Note\nhow each section of the vector register file needs to provide only enough ports for pipe-\nlines local to its lane. This figure does not show the path to provide the scalar operand\nfor vector-scalar instructions, but the scalar processor (or Control Processor) broadcasts\na scalar value to all lanes.\n4.2\nVector Architecture\n\u25a0\n295"
    },
    {
        "page": 328,
        "text": "The solution to these problems is to add a vector-length register (vl). The vl\ncontrols the length of any vector operation, including a vector load or store. The\nvalue in the vl, however, cannot be greater than the maximum vector length (mvl).\nThis solves our problem as long as the real length is less than or equal to the max-\nimum vector length (mvl). This parameter means the length of vector registers can\ngrow in later computer generations without changing the instruction set. As we will\nsee in the next section, multimedia SIMD extensions have no equivalent of mvl, so\nthey expand the instruction set every time they increase their vector length.\nWhat if the value of n is not known at compile time and thus may be greater\nthan the mvl? To tackle the second problem where the vector is longer than the\nmaximum length, a technique called strip mining is traditionally used. Strip mining\nis the generation of code such that each vector operation is done for a size less than\nor equal to the mvl. One loop handles any number of iterations that is a multiple of\nthe mvl and another loop that handles any remaining iterations and must be less\nthan the mvl. RISC-V has a better solution than a separate loop for strip mining.\nThe instruction setvl writes the smaller of the mvl and the loop variable n into\nvl (and to another register). If the number of iterations of the loop is larger than n,\nthen the fastest the loop can compute is mvl values at time, so setvl sets vl to\nmvl. If n is smaller than mvl, it should compute only on the last n elements in this\nfinal iteration of the loop, so setvl sets vl to n. setvl also writes another scalar\nregister to help with later loop bookkeeping. Below is the RV64V code for vector\nDAXPY for any value of n.\nvsetdcfg 2 DP FP\n# Enable 2 64b Fl.Pt. registers\nfld\nf0,a\n# Load scalar a\nloop: setvl\nt0,a0\n# vl = t0 = min(mvl,n)\nvld\nv0,x5\n# Load vector X\nslli\nt1,t0,3\n# t1 = vl * 8 (in bytes)\nadd\nx5,x5,t1\n# Increment pointer to X by vl*8\nvmul\nv0,v0,f0\n# Vector-scalar mult\nvld\nv1,x6\n# Load vector Y\nvadd\nv1,v0,v1\n# Vector-vector add\nsub\na0,a0,t0\n# n \u0003= vl (t0)\nvst\nv1,x6\n# Store the sum into Y\nadd\nx6,x6,t1\n# Increment pointer to Y by vl*8\nbnez\na0,loop\n# Repeat if n != 0\nvdisable\n# Disable vector regs\nPredicate Registers: Handling IF Statements in Vector Loops\nFrom Amdahl\u2019s law, we know that the speedup on programs with low to moderate\nlevels of vectorization will be very limited. The presence of conditionals (IF state-\nments) inside loops and the use of sparse matrices are two main reasons for lower\nlevels of vectorization. Programs that contain IF statements in loops cannot be run\n296\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 329,
        "text": "in vector mode using the techniques we have discussed up to now because the IF\nstatements introduce control dependences into a loop. Likewise, we cannot imple-\nment sparse matrices efficiently using any of the capabilities we have seen so far.\nWe examine strategies for dealing with conditional execution here, leaving the dis-\ncussion of sparse matrices for later.\nConsider the following loop written in C:\nfor (i = 0; i < 64;\ni=i+1)\nif (X[i] != 0)\nX[i] = X[i] \u2013 Y[i];\nThis loop cannot normally be vectorized because of the conditional execution of\nthe body; however, if the inner loop could be run for the iterations for which X[i]6\u00bc\n0, then the subtraction could be vectorized.\nThe common extension for this capability is vector-mask control. In RV64V,\npredicate registers hold the mask and essentially provide conditional execution of\neach element operation in a vector instruction. These registers use a Boolean vector\nto control the execution of a vector instruction, just as conditionally executed\ninstructions use a Boolean condition to determine whether to execute a scalar\ninstruction (see Chapter 3). When the predicate register p0 is set, all following vec-\ntor instructions operate only on the vector elements whose corresponding entries in\nthe predicate register are 1. The entries in the destination vector register that cor-\nrespond to a 0 in the mask register are unaffected by the vector operation. Like\nvector registers, predicate registers are configured and can be disabled. Enabling\na predicate register initializes it to all 1 s, meaning that subsequent vector instruc-\ntions operate on all vector elements. We can now use the following code for the\nprevious loop, assuming that the starting addresses of X and Y are in x5 and\nx6, respectively:\nvsetdcfg\n2*FP64\n# Enable 2 64b FP vector regs\nvsetpcfgi\n1\n# Enable 1 predicate register\nvld\nv0,x5\n# Load vector X into v0\nvld\nv1,x6\n# Load vector Y into v1\nfmv.d.x\nf0,x0\n# Put (FP) zero into f0\n0\n..\n(m \u22121)\nm\n..\n(m\u22121)\n+MVL\n(m+MVL) \n.. \n(m\u22121)\n+2\u00d7MVL\n(m+2\u00d7MVL) \n.. \n(m\u22121)\n+3\u00d7 MVL\n. . .\n(n\u2212MVL)\n.. \n(n\u22121)\nRange of i\nValue of j\nn/MVL\n1\n2\n3\n. . .\n0\n. . .\n. . .\nFigure 4.6 A vector of arbitrary length processed with strip mining. All blocks but the\nfirst are of length MVL, utilizing the full power of the vector processor. In this figure, we\nuse the variable m for the expression (n % MVL). (The C operator % is modulo.)\n4.2\nVector Architecture\n\u25a0\n297"
    },
    {
        "page": 330,
        "text": "vpne\np0,v0,f0\n# Set p0(i) to 1 if v0(i)!=f0\nvsub\nv0,v0,v1\n# Subtract under vector mask\nvst\nv0,x5\n# Store the result in X\nvdisable\n# Disable vector registers\nvpdisable\n# Disable predicate registers\nCompiler writers use the term IF-conversion to transform an IF statement into a\nstraight-line code sequence using conditional execution.\nUsing a vector-mask register does have overhead, however. With scalar archi-\ntectures, conditionally executed instructions still require execution time when\nthe condition is not satisfied. Nonetheless, the elimination of a branch and the asso-\nciated control dependences can make a conditional instruction faster even if it\nsometimes does useless work. Similarly, vector instructions executed with a vector\nmask still take the same execution time, even for the elements where the mask\nis zero. Likewise, despite a significant number of zeros in the mask, using\nvector-mask control may still be significantly faster than using scalar mode.\nAs we will see in Section 4.4, one difference between vector processors and\nGPUs is the way they handle conditional statements. Vector processors make the\npredicate registers part of the architectural state and rely on compilers to manipulate\nmask registers explicitly. In contrast, GPUs get the same effect using hardware to\nmanipulate internal mask registers that are invisible to GPU software. In both cases,\nthe hardware spends the time to execute a vector element whether the corresponding\nmask bit is 0 or 1, so the GFLOPS rate drops when masks are used.\nMemory Banks: Supplying Bandwidth for Vector\nLoad/Store Units\nThe behavior of the load/store vector unit is significantly more complicated than\nthat of the arithmetic functional units. The start-up time for a load is the time to\nget the first word from memory into a register. If the rest of the vector can be sup-\nplied without stalling, then the vector initiation rate is equal to the rate at which new\nwords are fetched or stored. Unlike simpler functional units, the initiation rate may\nnot necessarily be 1 clock cycle because memory bank stalls can reduce effective\nthroughput.\nTypically, penalties for start-ups on load/store units are higher than\nthose for arithmetic units\u2014over 100 clock cycles on many processors. For\nRV64V, we assume a start-up time of 12 clock cycles, the same as the Cray-\n1. (Recent vector computers use caches to bring down latency of vector loads\nand stores.)\nTo maintain an initiation rate of one word fetched or stored per clock cycle,\nthe memory system must be capable of producing or accepting this much data.\nSpreading accesses across multiple independent memory banks usually delivers\nthe desired rate. As we will soon see, having significant numbers of banks is\nuseful for dealing with vector loads or stores that access rows or columns\nof data.\n298\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 331,
        "text": "Most vector processors use memory banks, which allow several independent\naccesses rather than simple memory interleaving for three reasons:\n1. Many vector computers support many loads or stores per clock cycle, and the\nmemory bank cycle time is usually several times larger than the processor cycle\ntime. To support simultaneous accesses from multiple loads or stores, the mem-\nory system needs multiple banks and needs to be able to control the addresses to\nthe banks independently.\n2. Most vector processors support the ability to load or store data words that are\nnot sequential. In such cases, independent bank addressing, rather than inter-\nleaving, is required.\n3. Most vector computers support multiple processors sharing the same memory\nsystem, so each processor will be generating its own separate stream of\naddresses.\nIn combination, these features lead to the desire for a large number of independent\nmemory banks, as the following example shows.\nExample\nThe largest configuration of a Cray T90 (Cray T932) has 32 processors, each capa-\nble of generating 4 loads and 2 stores per clock cycle. The processor clock cycle is\n2.167 ns, while the cycle time of the SRAMs used for the memory system is 15 ns.\nCalculate the minimum number of memory banks required to allow all processors\nto run at the full memory bandwidth.\nAnswer\nThe maximum number of memory references each cycle is 192: 32 processors\ntimes 6 references per processor. Each SRAM bank is busy for 15/2.167\u00bc6.92\nclock cycles, which we round up to 7 processor clock cycles. Therefore we require\na minimum of 192\u00017\u00bc1344 memory banks!\nThe Cray T932 actually has 1024 memory banks, so the early models could not\nsustain the full bandwidth to all processors simultaneously. A subsequent memory\nupgrade replaced the 15 ns asynchronous SRAMs with pipelined synchronous\nSRAMs that more than halved the memory cycle time, thereby providing sufficient\nbandwidth.\nTaking a higher-level perspective, vector load/store units play a similar role to\nprefetch units in scalar processors in that both try to deliver data bandwidth by\nsupplying processors with streams of data.\nStride: Handling Multidimensional Arrays in Vector\nArchitectures\nThe position in memory of adjacent elements in a vector may not be sequential.\nConsider this straightforward code for matrix multiply in C:\n4.2\nVector Architecture\n\u25a0\n299"
    },
    {
        "page": 332,
        "text": "for (i = 0; i < 100;\ni=i+1)\nfor (j = 0; j < 100;\nj=j+1) {\nA[i][j] = 0.0;\nfor (k = 0; k < 100; k=k+1)\nA[i][j] = A[i][j] + B[i][k] * D[k][j];\n}\nWe could vectorize the multiplication of each row of B with each column of D and\nstrip-mine the inner loop with k as the index variable.\nTo do so, we must consider how to address adjacent elements in B and adjacent\nelements in D. When an array is allocated memory, it is linearized and must be laid\nout in either row-major order (as in C) or column-major order (as in Fortran). This\nlinearization means that either the elements in the row or the elements in the col-\numn are not adjacent in memory. For example, the preceding C code allocates in\nrow-major order, so the elements of D that are accessed by iterations in the inner\nloop are separated by the row size times 8 (the number of bytes per entry) for a total\nof 800 bytes. In Chapter 2, we saw that blocking could improve locality in cache-\nbased systems. For vector processors without caches, we need another technique to\nfetch elements of a vector that are not adjacent in memory.\nThis distance separating elements to be gathered into a single vector register is\ncalled the stride. In this example, matrix D has a stride of 100 double words (800\nbytes), and matrix B would have a stride of 1 double word (8 bytes). For column-\nmajor order, which is used by Fortran, the strides would be reversed. Matrix D\nwould have a stride of 1, or 1 double word (8 bytes), separating successive\nelements, while matrix B would have a stride of 100, or 100 double words (800\nbytes). Thus, without reordering the loops, the compiler can\u2019t hide the long\ndistances between successive elements for both B and D.\nOnce a vector is loaded into a vector register, it acts as if it had logically adja-\ncent elements. Thus a vector processor can handle strides greater than one, called\nnonunit strides, using only vector load and vector store operations with stride\ncapability. This ability to access nonsequential memory locations and to reshape\nthem into a dense structure is one of the major advantages of a vector\narchitecture.\nCaches inherently deal with unit-stride data; increasing block size can help\nreduce miss rates for large scientific datasets with unit stride, but increasing block\nsize can even have a negative effect for data that are accessed with nonunit strides.\nWhile blocking techniques can solve some of these problems (see Chapter 2), the\nability to access noncontiguous data efficiently remains an advantage for vector\nprocessors on certain problems, as we will see in Section 4.7.\nOn RV64V, where the addressable unit is a byte, the stride for our example\nwould be 800. The value must be computed dynamically because the size of\nthe matrix may not be known at compile time or\u2014just like vector length\u2014may\nchange for different executions of the same statement. The vector stride, like\nthe vector starting address, can be put in a general-purpose register. Then the\nRV64V instruction VLDS (load vector with stride) fetches the vector into a vector\n300\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 333,
        "text": "register. Likewise, when storing a nonunit stride vector, use the instruction VSTS\n(store vector with stride).\nSupporting strides greater than one complicates the memory system. Once we\nintroduce nonunit strides, it becomes possible to request accesses from the same\nbank frequently. When multiple accesses contend for a bank, a memory bank\nconflict occurs, thereby stalling one access. A bank conflict and thus a stall will\noccur if\nNumber of banks\nLeast common multiple Stride, Number of banks\n\u00f0\n\u00de < Bankbusy time\nExample\nSuppose we have 8 memory banks with a bank busy time of 6 clocks and a total\nmemory latency of 12 cycles. How long will it take to complete a 64-element\nvector load with a stride of 1? With a stride of 32?\nAnswer\nBecause the number of banks is larger than the bank busy time, for a stride of 1, the\nload will take 12+64\u00bc76 clock cycles, or 1.2 clock cycles per element. The worst\npossible stride is a value that is a multiple of the number of memory banks, as in\nthis case with a stride of 32 and 8 memory banks. Every access to memory (after the\nfirst one) will collide with the previous access and will have to wait for the 6-clock-\ncycle bank busy time. The total time will be 12+1+6 * 63\u00bc391 clock cycles, or\n6.1 clock cycles per element, slowing it down by a factor of 5!\nGather-Scatter: Handling Sparse Matrices in Vector\nArchitectures\nAs previously mentioned, sparse matrices are commonplace, so it is important to\nhave techniques to allow programs with sparse matrices to execute in vector mode.\nIn a sparse matrix, the elements of a vector are usually stored in some compacted\nform and then accessed indirectly. Assuming a simplified sparse structure, we\nmight see code that looks like this:\nfor\n(i = 0; i < n;\ni=i+1)\nA[K[i]] = A[K[i]] + C[M[i]];\nThis code implements a sparse vector sum on the arrays A and C, using index vec-\ntors K and M to designate the nonzero elements of A and C. (A and C must have the\nsame number of nonzero elements\u2014n of them\u2014so K and M are the same size.)\nThe primary mechanism for supporting sparse matrices is gather-scatter oper-\nations using index vectors. The goal of such operations is to support moving\nbetween a compressed representation (i.e., zeros are not included) and normal\nrepresentation (i.e., the zeros are included) of a sparse matrix. A gather operation\ntakes an index vector and fetches the vector whose elements are at the addresses\ngiven by adding a base address to the offsets given in the index vector. The result is\na dense vector in a vector register. After these elements are operated on in a dense\n4.2\nVector Architecture\n\u25a0\n301"
    },
    {
        "page": 334,
        "text": "form, the sparse vector can be stored in an expanded form by a scatter store, using\nthe same index vector. Hardware support for such operations is called gather-scat-\nter, and it appears on nearly all modern vector processors. The RV64V instructions\nare vldi (load vector indexed or gather) and vsti (store vector indexed or\nscatter). For example, if x5, x6, x7, and x28 contain the starting addresses of\nthe vectors in the previous sequence, we can code the inner loop with vector\ninstructions such as:\nvsetdcfg\n4*FP64\n# 4\n64b\nFP vector registers\nvld\nv0, x7\n# Load K[]\nvldx\nv1, x5, v0)\n# Load A[K[]]\nvld\nv2, x28\n# Load M[]\nvldi\nv3, x6, v2)\n# Load C[M[]]\nvadd\nv1, v1, v3\n# Add them\nvstx\nv1, x5, v0)\n# Store A[K[]]\nvdisable\n# Disable vector registers\nThis technique allows code with sparse matrices to run in vector mode. A\nsimple vectorizing compiler could not automatically vectorize the preceding\nsource code because the compiler would not know that the elements of K are\ndistinct values, and thus that no dependences exist. Instead, a programmer directive\nwould tell the compiler that it was safe to run the loop in vector mode.\nAlthough indexed loads and stores (gather and scatter) can be pipelined, they\ntypically run much more slowly than nonindexed loads or stores, because the mem-\nory banks are not known from the start of the instruction. The register file must also\nprovide communication between the lanes of a vector unit to support gather and\nscatter.\nEach element of a gather or scatter has an individual address, so they can\u2019t be\nhandled in groups, and there can be conflicts at many places throughout the mem-\nory system. Thus each individual access incurs significant latency even on cache-\nbased systems. However, as Section 4.7 shows, a memory system can deliver better\nperformance by designing for this case and by using more hardware resources\nversus when architects have a laissez-faire attitude toward such unpredictable\naccesses.\nAs we will see in Section 4.4, all loads are gathers and all stores are scatters in\nGPUs in that no separate instructions restrict addresses to be sequential. To turn the\npotentially slow gathers and scatters into the more efficient unit-stride accesses to\nmemory, the GPU hardware must recognize the sequential addresses during\nexecution and the GPU programmer to ensure that all the addresses in a gather\nor scatter are to adjacent locations.\nProgramming Vector Architectures\nAn advantage of vector architectures is that compilers can tell programmers at\ncompile time whether a section of code will vectorize or not, often giving hints\n302\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 335,
        "text": "as to why it did not vectorize the code. This straightforward execution model\nallows experts in other domains to learn how to improve performance by revising\ntheir code or by giving hints to the compiler when it\u2019s okay to assume indepen-\ndence between operations, such as for gather-scatter data transfers. It is this\ndialogue between the compiler and the programmer, with each side giving hints\nto the other on how to improve performance, that simplifies programming of vector\ncomputers.\nToday, the main factor that affects the success with which a program runs in\nvector mode is the structure of the program itself: Do the loops have true data\ndependences (see Section 4.5), or can they be restructured so as not to have such\ndependences? This factor is influenced by the algorithms chosen and, to some\nextent, by how they are coded.\nAs an indication of the level of vectorization achievable in scientific programs,\nlet\u2019s look at the vectorization levels observed for the Perfect Club benchmarks.\nFigure 4.7 shows the percentage of operations executed in vector mode for two\nversions of the code running on the Cray Y-MP. The first version is that obtained\nwith just compiler optimization on the original code, while the second version uses\nextensive hints from a team of Cray Research programmers. Several studies of the\nperformance of applications on vector processors show a wide variation in the level\nof compiler vectorization.\nBenchmark\nname\nOperations executed\nin vector mode,\ncompiler-optimized\nOperations executed\nin vector mode,\nwith programmer aid\nSpeedup\nfrom hint\noptimization\nBDNA\n96.1%\n97.2%\n1.52\nMG3D\n95.1%\n94.5%\n1.00\nFLO52\n91.5%\n88.7%\nN/A\nARC3D\n91.1%\n92.0%\n1.01\nSPEC77\n90.3%\n90.4%\n1.07\nMDG\n87.7%\n94.2%\n1.49\nTRFD\n69.8%\n73.7%\n1.67\nDYFESM\n68.8%\n65.6%\nN/A\nADM\n42.9%\n59.6%\n3.60\nOCEAN\n42.8%\n91.2%\n3.92\nTRACK\n14.4%\n54.6%\n2.52\nSPICE\n11.5%\n79.9%\n4.06\nQCD\n4.2%\n75.1%\n2.15\nFigure 4.7 Level of vectorization among the Perfect Club benchmarks when exe-\ncuted on the Cray Y-MP (Vajapeyam, 1991). The first column shows the vectorization\nlevel obtained with the compiler without hints, and the second column shows the\nresults after the codes have been improved with hints from a team of Cray Research\nprogrammers.\n4.2\nVector Architecture\n\u25a0\n303"
    },
    {
        "page": 336,
        "text": "The hint-rich versions show significant gains in vectorization level for codes\nthat the compiler could not vectorize well by itself, with all codes now above 50%\nvectorization. The median vectorization improved from about 70% to about 90%.\n4.3\nSIMD Instruction Set Extensions for Multimedia\nSIMD Multimedia Extensions started with the simple observation that many media\napplications operate on narrower data types than the 32-bit processors were opti-\nmized for. Graphics systems would use 8 bits to represent each of the three primary\ncolors plus 8 bits for transparency. Depending on the application, audio samples are\nusually represented with 8 or 16 bits. By partitioning the carry chains within, say, a\n256-bit adder, a processor could perform simultaneous operations on short vectors\nof thirty-two 8-bit operands, sixteen 16-bit operands, eight 32-bit operands, or four\n64-bit operands. Theadditional cost of such partitioned adderswas small. Figure 4.8\nsummarizes typical multimedia SIMD instructions. Like vector instructions, a\nSIMD instruction specifies the same operation on vectors of data. Unlike vector\nmachines with large register files such as the RISC-V RV64V vector registers,\nwhich can hold, say, thirty-two 64-bit elements in each of 32 vector registers, SIMD\ninstructions tend to specify fewer operands and thus use much smaller register files.\nIn contrast to vector architectures, which offer an elegant instruction set that is\nintended to be the target of a vectorizing compiler, SIMD extensions have three\nmajor omissions: no vector length register, no strided or gather/scatter data transfer\ninstructions, and no mask registers.\n1. Multimedia SIMD extensions fix the number of data operands in the opcode,\nwhich has led to the addition of hundreds of instructions in the MMX, SSE,\nand AVX extensions of the x86 architecture. Vector architectures have a\nvector-length register that specifies the number of operands for the current oper-\nation. These variable-length vector registers easily accommodate programs that\nnaturally have shorter vectors than the maximum size the architecture supports.\nMoreover, vector architectures have an implicit maximum vector length in the\nInstruction category\nOperands\nUnsigned add/subtract\nThirty-two 8-bit, sixteen 16-bit, eight 32-bit, or four 64-bit\nMaximum/minimum\nThirty-two 8-bit, sixteen 16-bit, eight 32-bit, or four 64-bit\nAverage\nThirty-two 8-bit, sixteen 16-bit, eight 32-bit, or four 64-bit\nShift right/left\nThirty-two 8-bit, sixteen 16-bit, eight 32-bit, or four 64-bit\nFloating point\nSixteen 16-bit, eight 32-bit, four 64-bit, or two 128-bit\nFigure 4.8 Summary of typical SIMD multimedia support for 256-bit-wide opera-\ntions. Note that the IEEE 754-2008 floating-point standard added half-precision (16-\nbit) and quad-precision (128-bit) floating-point operations.\n304\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 337,
        "text": "architecture, which combined with the vector length register avoids the use of\nmany opcodes.\n2. Up until recently, multimedia SIMD did not offer the more sophisticated\naddressing modes of vector architectures, namely strided accesses and\ngather-scatter accesses. These features increase the number of programs that\na vector compiler can successfully vectorize (see Section 4.7).\n3. Although this is changing, multimedia SIMD usually did not offer the mask reg-\nisters to support conditional execution of elements as in vector architectures.\nSuch omissions make it harder for the compiler to generate SIMD code and\nincrease the difficulty of programming in SIMD assembly language.\nFor the x86 architecture, the MMX instructions added in 1996 repurposed the\n64-bit floating-point registers, so the basic instructions could perform eight 8-bit\noperations or four 16-bit operations simultaneously. These were joined by parallel\nMAX and MIN operations, a wide variety of masking and conditional instructions,\noperations typically found in digital signal processors, and ad hoc instructions that\nwere believed to be useful in important media libraries. Note that MMX reused the\nfloating-point data-transfer instructions to access memory.\nThe Streaming SIMD Extensions (SSE) successor in 1999 added 16 separate\nregisters (XMM registers) that were 128 bits wide, so now instructions could simul-\ntaneously perform sixteen 8-bit operations, eight 16-bit operations, or four 32-bit\noperations. It also performed parallel single-precision floating-point arithmetic.\nBecause SSE had separate registers, it needed separate data transfer instructions.\nIntel soon added double-precision SIMD floating-point data types via SSE2 in\n2001, SSE3 in 2004, and SSE4 in 2007. Instructions with four single-precision\nfloating-point operations or two parallel double-precision operations increased\nthe peak floating-point performance of the x86 computers, as long as programmers\nplaced the operands side by side. With each generation, they also added ad hoc\ninstructions whose aim was to accelerate specific multimedia functions perceived\nto be important.\nThe Advanced Vector Extensions (AVX), added in 2010, doubled the width of\nthe registers again to 256 bits (YMM registers) and thereby offered instructions that\ndouble the number of operations on all narrower data types. Figure 4.9 shows AVX\ninstructions useful for double-precision floating-point computations. AVX2 in\n2013 added 30 new instructions such as gather (VGATHER) and vector shifts\n(VPSLL, VPSRL, VPSRA). AVX-512 in 2017 doubled the width again to 512 bits\n(ZMM registers), doubled the number of the registers again to 32, and added about\n250 new instructions including scatter (VPSCATTER) and mask registers\n(OPMASK). AVX includes preparations to extend registers to 1024 bits in future\neditions of the architecture.\nIn general, the goal of these extensions has been to accelerate carefully written\nlibraries rather than for the compiler to generate them (see Appendix H), but recent\nx86 compilers are trying to generate such code, particularly for floating-point-\nintensive applications. Since the opcode determines the width of the SIMD regis-\nter, every time the width doubles, so must the number of SIMD instructions.\n4.3\nSIMD Instruction Set Extensions for Multimedia\n\u25a0\n305"
    },
    {
        "page": 338,
        "text": "Given these weaknesses, why are multimedia SIMD extensions so popular?\nFirst, they initially cost little to add to the standard arithmetic unit and they were\neasy to implement. Second, they require scant extra processor state compared to\nvector architectures, which is always a concern for context switch times. Third,\nyou need a lot of memory bandwidth to support a vector architecture, which\nmany computers don\u2019t have. Fourth, SIMD does not have to deal with problems\nin virtual memory when a single instruction can generate 32 memory accesses\nand any of which can cause a page fault. The original SIMD extensions used\nseparate data transfers per SIMD group of operands that are aligned in memory,\nand so they cannot cross page boundaries. Another advantage of short, fixed-\nlength \u201cvectors\u201d of SIMD is that it is easy to introduce instructions that can help\nwith new media standards, such as instructions that perform permutations or\ninstructions that consume either fewer or more operands than vectors can pro-\nduce. Finally, there was concern about how well vector architectures can work\nwith caches. More recent vector architectures have addressed all of these prob-\nlems. The overarching issue, however, is that due the overiding importance of\nbackwards binary compatability, once an architecture gets started on the SIMD\npath it\u2019s very hard to get off it.\nExample\nTo get an idea about what multimedia instructions look like, assume we added a\n256-bit SIMD multimedia instruction extension to RISC-V, tentatively called\nRVP for \u201cpacked.\u201d We concentrate on floating-point in this example. We add\nthe suffix \u201c4D\u201d on instructions that operate on four double-precision operands\nat once. Like vector architectures, you can think of a SIMD Processor as having\nlanes, four in this case. RV64P expands the F registers to be the full width, in this\ncase 256 bits. This example shows the RISC-V SIMD code for the DAXPY loop,\nAVX instruction\nDescription\nVADDPD\nAdd four packed double-precision operands\nVSUBPD\nSubtract four packed double-precision operands\nVMULPD\nMultiply four packed double-precision operands\nVDIVPD\nDivide four packed double-precision operands\nVFMADDPD\nMultiply and add four packed double-precision operands\nVFMSUBPD\nMultiply and subtract four packed double-precision operands\nVCMPxx\nCompare four packed double-precision operands for EQ, NEQ, LT, LE, GT, GE, \u2026\nVMOVAPD\nMove aligned four packed double-precision operands\nVBROADCASTSD\nBroadcast one double-precision operand to four locations in a 256-bit register\nFigure 4.9 AVX instructions for x86 architecture useful in double-precision floating-point programs. Packed-\ndouble for 256-bit AVX means four 64-bit operands executed in SIMD mode. As the width increases with AVX, it\nis increasingly important to add data permutation instructions that allow combinations of narrow operands from\ndifferent parts of the wide registers. AVX includes instructions that shuffle 32-bit, 64-bit, or 128-bit operands within\na 256-bit register. For example, BROADCAST replicates a 64-bit operand four times in an AVX register. AVX also\nincludes a large variety of fused multiply-add/subtract instructions; we show just two here.\n306\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 339,
        "text": "with the changes to the RISC-V code for SIMD underlined. We assume that the\nstarting addresses of X and Y are in x5 and x6, respectively.\nAnswer\nHere is the RISC-V SIMD code:\nfld\nf0,a\n#Load scalar a\nsplat.4D\nf0,f0\n#Make 4 copies of a\naddi\nx28,x5,#256\n#Last address to load\nLoop: fld.4D\nf1,0(x5)\n#Load X[i] ... X[i+3]\nfmul.4D\nf1,f1,f0\n#a\u0001X[i] ... a\u0001X[i+3]\nfld.4D\nf2,0(x6)\n#Load Y[i] ... Y[i+3]\nfadd.4D\nf2,f2,f1\n# a\u0001X[i]+Y[i]...\n# a\u0001X[i+3]+Y[i+3]\nfsd.4D\nf2,0(x6)\n#Store Y[i]... Y[i+3]\naddi\nx5,x5,#32\n#Increment index to X\naddi\nx6,x6,#32\n#Increment index to Y\nbne\nx28,x5,Loop\n#Check if done\nThe changes were replacing every RISC-V double-precision instruction with its 4D\nequivalent, increasing the increment from 8 to 32, and adding the splat instruc-\ntion that makes 4 copies of a in the 256 bits of f0. While not as dramatic as the\n32\u0001 reduction of dynamic instruction bandwidth of RV64V, RISC-V SIMD does\nget almost a 4\u0001 reduction: 67 versus 258 instructions executed for RV64G. This\ncode knows the number of elements. That number is often determined at run time,\nwhich would require an extra strip-mine loop to handle the case when the number is\nnot a modulo of 4.\nProgramming Multimedia SIMD Architectures\nGiven the ad hoc nature of the SIMD multimedia extensions, the easiest way to use\nthese instructions has been through libraries or by writing in assembly language.\nRecent extensions have become more regular, giving compilers a more reason-\nable target. By borrowing techniques from vectorizing compilers, compilers are\nstarting to produce SIMD instructions automatically. For example, advanced com-\npilers today can generate SIMD floating-point instructions to deliver much higher\nperformance for scientific codes. However, programmers must be sure to align all\nthe data in memory to the width of the SIMD unit on which the code is run to prevent\nthe compiler from generating scalar instructions for otherwise vectorizable code.\nThe Roofline Visual Performance Model\nOne visual, intuitive way to compare potential floating-point performance of var-\niations of SIMD architectures is the Roofline model (Williams et al., 2009). The\nhorizontal and diagonal lines of the graphs it produces give this simple model its\nname and indicate its value (see Figure 4.11). It ties together floating-point perfor-\nmance, memory performance, and arithmetic intensity in a two-dimensional graph.\n4.3\nSIMD Instruction Set Extensions for Multimedia\n\u25a0\n307"
    },
    {
        "page": 340,
        "text": "Arithmetic intensity is the ratio of floating-point operations per byte of memory\naccessed. It can be calculated by taking the total number of floating-point opera-\ntions for a program divided by the total number of data bytes transferred to main\nmemory during program execution. Figure 4.10 shows the relative arithmetic\nintensity of several example kernels.\nPeak floating-point performance can be found using the hardware specifica-\ntions. Many of the kernels in this case study do not fit in on-chip caches, so peak\nmemory performance is defined by the memory system behind the caches. Note\nthat we need the peak memory bandwidth that is available to the processors,\nnot just at the DRAM pins as in Figure 4.27 on page 328. One way to find the\n(delivered) peak memory performance is to run the Stream benchmark.\nFigure 4.11 shows the Roofline model for the NEC SX-9 vector processor on\nthe left and the Intel Core i7 920 multicore computer on the right. The vertical Y-\naxis is achievable floating-point performance from 2 to 256 GFLOPS/s. The hor-\nizontal X-axis is arithmetic intensity, varying from 1/8 FLOP/DRAM byte\naccessed to 16 FLOP/DRAM byte accessed in both graphs. Note that the graph\nis a log-log scale, and that Rooflines are done just once for a computer.\nFor a given kernel, we can find a point on the X-axis based on its arithmetic\nintensity. If we drew a vertical line through that point, the performance of the ker-\nnel on that computer must lie somewhere along that line. We can plot a horizontal\nline showing peak floating-point performance of the computer. Obviously, the\nactual floating-point performance can be no higher than the horizontal line because\nthat is a hardware limit.\nHow could we plot the peak memory performance? Because the X-axis is\nFLOP/byte and the Y-axis is FLOP/s, bytes/s is just a diagonal line at a 45-degree\nangle in this figure. Thus we can plot a third line that gives the maximum floating-\npoint performance that the memory system of that computer can support for a given\nArithmetic intensity \nO(N) \nO(log(N)) \nO(1) \nSparse\nmatrix\n(SpMV)\nStructured\ngrids\n(Stencils,\nPDEs)\nStructured\ngrids\n(Lattice\nmethods)\nSpectral\nmethods\n(FFTs)\nDense\nmatrix\n(BLAS3)\nN-body\n(Particle\nmethods)\nFigure 4.10 Arithmetic intensity, specified as the number of floating-point opera-\ntions to run the program divided by the number of bytes accessed in main memory\n(Williams et al., 2009). Some kernels have an arithmetic intensity that scales with prob-\nlem size, such as a dense matrix, but there are many kernels with arithmetic intensities\nindependent of problem size.\n308\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 341,
        "text": "arithmetic intensity. We can express the limits as a formula to plot these lines in the\ngraphs in Figure 4.11:\nAttainable GFLOPs=s \u00bc Min Peak Memory BW\n\u00f0\n\u0001Arithmetic Intensity, Peak Floating\u0003Point Perf:\u00de\nThe \u201cRoofline\u201d sets an upper bound on performance of a kernel depending on its\narithmetic intensity. If we think of arithmetic intensity as a pole that hits the roof,\neither it hits the flat part of the roof, which means performance is computationally\nlimited, or it hits the slanted part of the roof, which means performance is ulti-\nmately limited by memory bandwidth. In Figure 4.11, the vertical dashed line\non the right (arithmetic intensity of 4) is an example of the former and the vertical\ndashed line on the left (arithmetic intensity of 1/4) is an example of the latter. Given\na Roofline model of a computer, you can apply it repeatedly, because it doesn\u2019t\nvary by kernel.\nNote that the \u201cridge point,\u201d where the diagonal and horizontal roofs meet,\noffers an interesting insight into a computer. If it is far to the right, then only kernels\nwith very high arithmetic intensity can achieve the maximum performance of that\ncomputer. If it is far to the left, then almost any kernel can potentially hit the max-\nimum performance. As we will see, this vector processor has both much higher\n1/8\n1/2\nArithmetic intensity\n1/4\n1\n2\n4\n8\n16\n1/8\n1/2\nArithmetic intensity\n1/4\n1\n2\n4\n8\n16\n256\n128\n64\n32\n16\n8\n4\n2\n256\n128\n64\n32\n16\n8\n4\n2\nDouble precision GLFOP/s\nDouble precision GLFOP/s\nNEC SX-9 CPU\nIntel Core i7 920\n(Nehalem)\n102.4GFLOP/s\n42.66 GFLOP/s\n162 GB/s\n(Stream)\n16.4GB/s\n(Stream)\nFigure 4.11 Roofline model for one NEC SX-9 vector processor on the left and the Intel Core i7 920 multicore\ncomputer with SIMD extensions on the right (Williams et al., 2009). This Roofline is for unit-stride memory accesses\nand double-precision floating-point performance. NEC SX-9 is a vector supercomputer announced in 2008 that\ncost millions of dollars. It has a peak DP FP performance of 102.4 GFLOP/s and a peak memory bandwidth of\n162 GB/s from the Stream benchmark. The Core i7 920 has a peak DP FP performance of 42.66 GFLOP/s and a peak\nmemory bandwidth of 16.4 GB/s. The dashed vertical lines at an arithmetic intensity of 4 FLOP/byte show that both\nprocessors operate at peak performance. In this case, the SX-9 at 102.4 FLOP/s is 2.4\u0001 faster than the Core i7 at\n42.66 GFLOP/s. At an arithmetic intensity of 0.25 FLOP/byte, the SX-9 is 10\u0001 faster at 40.5 GFLOP/s versus\n4.1 GFLOP/s for the Core i7.\n4.3\nSIMD Instruction Set Extensions for Multimedia\n\u25a0\n309"
    },
    {
        "page": 342,
        "text": "memory bandwidth and a ridge point far to the left as compared to other SIMD\nProcessors.\nFigure 4.11 shows that the peak computational performance of the SX-9 is\n2.4\u0001 faster than Core i7, but the memory performance is 10\u0001 faster. For programs\nwith an arithmetic intensity of 0.25, the SX-9 is 10\u0001 faster (40.5 versus\n4.1 GFLOP/s). The higher memory bandwidth moves the ridge point from 2.6\nin the Core i7 to 0.6 on the SX-9, which means many more programs can reach\nthe peak computational performance on the vector processor.\n4.4\nGraphics Processing Units\nPeople can buy a GPU chip with thousands of parallel floating-point units for a few\nhundred dollars and plug it into their desk side PC. Such affordability and conve-\nnience makes high performance computing available to many. The interest in\nGPU computing blossomed when this potential was combined with a programming\nlanguage that made GPUs easier to program. Therefore many programmers of sci-\nentific and multimedia applications today are pondering whether to use GPUs or\nCPUs. For programmers interested in machine learning, which is the subject of\nChapter 7, GPUs are currently the preferred platform.\nGPUs and CPUs do not go back in computer architecture genealogy to a com-\nmon ancestor; there is no \u201cmissing link\u201d that explains both. As Section 4.10\ndescribes, the primary ancestors of GPUs are graphics accelerators, as doing\ngraphics well is the reason why GPUs exist. While GPUs are moving toward\nmainstream computing, they can\u2019t abandon their responsibility to continue to excel\nat graphics. Thus the design of GPUs may make more sense when architects ask,\ngiven the hardware invested to do graphics well, how can we supplement it to\nimprove the performance of a wider range of applications?\nNote that this section concentrates on using GPUs for computing. To see how\nGPU computing combines with the traditional role of graphics acceleration, see\n\u201cGraphics and Computing GPUs,\u201d by John Nickolls and David Kirk (Appendix\nA in the 5th edition of Computer Organization and Design by the same authors\nas this book).\nBecause the terminology and some hardware features are quite different\nfrom vector and SIMD architectures, we believe it will be easier if we start with\nthe simplified programming model for GPUs before we describe the architecture.\nProgramming the GPU\nCUDA is an elegant solution to the problem of representing parallelism\nin algorithms, not all algorithms, but enough to matter. It seems to res-\nonate in some way with the way we think and code, allowing an easier,\nmore natural expression of parallelism beyond the task level.\nVincent Natol,\n\u201cKudos for CUDA,\u201d HPC Wire (2010)\n310\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 343,
        "text": "The challenge for the GPU programmer is not simply getting good perfor-\nmance on the GPU, but also in coordinating the scheduling of computation\non the system processor and the GPU and the transfer of data between system\nmemory and GPU memory. Moreover, as we see will see later in this section,\nGPUs have virtually every type of parallelism that can be captured by the\nprogramming\nenvironment:\nmultithreading,\nMIMD,\nSIMD,\nand\neven\ninstruction-level.\nNVIDIA decided to develop a C-like language and programming environment\nthat would improve the productivity of GPU programmers by attacking both the\nchallenges of heterogeneous computing and of multifaceted parallelism. The name\nof their system is CUDA, for Compute Unified Device Architecture. CUDA pro-\nduces C/C++ for the system processor (host) and a C and C++ dialect for the GPU\n(device, thus the D in CUDA). A similar programming language is OpenCL, which\nseveral companies are developing to offer a vendor-independent language for mul-\ntiple platforms.\nNVIDIA decided that the unifying theme of all these forms of parallelism\nis the CUDA Thread. Using this lowest level of parallelism as the program-\nming primitive, the compiler and the hardware can gang thousands of CUDA\nThreads together to utilize the various styles of parallelism within a GPU: mul-\ntithreading, MIMD, SIMD, and instruction-level parallelism. Therefore NVI-\nDIA classifies the CUDA programming model as single instruction, multiple\nthread (SIMT). For reasons we will soon see, these threads are blocked\ntogether and executed in groups of threads, called a Thread Block. We call\nthe hardware that executes a whole block of threads a multithreaded SIMD\nProcessor.\nWe need just a few details before we can give an example of a CUDA\nprogram:\n\u2022\nTo distinguish between functions for the GPU (device) and functions for the\nsystem processor (host), CUDA uses __device__ or __global__ for\nthe former and __host__ for the latter.\n\u2022\nCUDA variables declared with __device__ are allocated to the GPU\nMemory (see below), which is accessible by all multithreaded SIMD\nProcessors.\n\u2022\nThe extended function call syntax for the function name that runs on the\nGPU is\nname < <<dimGrid, dimBlock>> > (\u2026 parameter list\u2026)\nwhere dimGrid and dimBlock specify the dimensions of the code (in\nThread Blocks) and the dimensions of a block (in threads).\n\u2022\nIn addition to the identifier for blocks (blockIdx) and the identifier for each\nthread in a block (threadIdx), CUDA provides a keyword for the number of\nthreads per block (blockDim), which comes from the dimBlock parameter\nin the preceding bullet.\n4.4\nGraphics Processing Units\n\u25a0\n311"
    },
    {
        "page": 344,
        "text": "Before seeing the CUDA code, let\u2019s start with conventional C code for the DAXPY\nloop from Section 4.2:\n// Invoke DAXPY\ndaxpy(n, 2.0, x, y);\n// DAXPY in C\nvoid daxpy(int n, double a, double *x, double *y)\n{\nfor\n(int i = 0; i < n; ++i)\ny[i] = a*x[i] + y[i];\n}\nFollowing is the CUDA version. We launch n threads, one per vector element, with\n256 CUDA Threads per Thread Block in a multithreaded SIMD Processor. The\nGPU function starts by calculating the corresponding element index i based on\nthe block ID, the number of threads per block, and the thread ID. As long as this\nindex is within the array (i < n), it performs the multiply and add.\n// Invoke DAXPY with 256 threads per Thread Block\n__host__\nint nblocks = (n+ 255) / 256;\ndaxpy<<<nblocks, 256>>>(n, 2.0, x, y);\n// DAXPY in CUDA\n__global__\nvoid daxpy(int n, double a, double *x, double *y)\n{\nint i = blockIdx.x*blockDim.x + threadIdx.x;\nif (i < n) y[i] = a*x[i] + y[i];\n}\nComparing the C and CUDA codes, we see a common pattern to parallelizing\ndata-parallel CUDA code. The C version has a loop where each iteration is inde-\npendent from the others, allowing the loop to be transformed straightforwardly into\na parallel code where each loop iteration becomes a separate thread. (As previously\nmentioned and described in detail in Section 4.5, vectorizing compilers also rely on\na lack of dependences between iterations of a loop, which are called loop-carried\ndependences.) The programmer determines the parallelism in CUDA explicitly by\nspecifying the grid dimensions and the number of threads per SIMD Processor. By\nassigning a single thread to each element, there is no need to synchronize between\nthreads when writing results to memory.\nThe GPU hardware handles parallel execution and thread management; it is not\ndone by applications or by the operating system. To simplify scheduling by the\nhardware, CUDA requires that Thread Blocks be able to execute independently\nand in any order. Different Thread Blocks cannot communicate directly, although\nthey can coordinate using atomic memory operations in global memory.\nAs we will soon see, many GPU hardware concepts are not obvious in CUDA.\nWriting efficient GPU code requires that programmers think in terms of SIMD\n312\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 345,
        "text": "operations, even though the CUDA programming model looks like MIMD. Per-\nformance programmers must keep the GPU hardware in mind when writing in\nCUDA. That could hurt programmer productivity, but then most programmers\nare using GPUs instead of CPUs to get performance. For reasons explained shortly,\nthey know that they need to keep groups of 32 threads together in control flow to\nget the best performance from multithreaded SIMD Processors and to create many\nmore threads per multithreaded SIMD Processor to hide latency to DRAM. They\nalso need to keep the data addresses localized in one or a few blocks of memory to\nget the expected memory performance.\nLike many parallel systems, a compromise between productivity and perfor-\nmance is for CUDA to include intrinsics to give programmers explicit control over\nthe hardware. The struggle between productivity on the one hand versus allowing the\nprogrammer to be able to express anything that the hardware can do on the other\nhand happens often in parallel computing. It will be interesting to see how the lan-\nguage evolves in this classic productivity-performance battle as well as to see\nwhether CUDA becomes popular for other GPUs or even other architectural styles.\nNVIDIA GPU Computational Structures\nThe uncommon heritage mentioned above helps explain why GPUs have their own\narchitectural style and their own terminology independent from CPUs. One obsta-\ncle to understanding GPUs has been the jargon, with some terms even having\nmisleading names. This obstacle has been surprisingly difficult to overcome, as\nthe many rewrites of this chapter can attest.\nTo try to bridge the twin goals of making the architecture of GPUs understand-\nable and learning the many GPU terms with nontraditional definitions, our\napproach is to use the CUDA terminology for software but initially use more\ndescriptive terms for the hardware, sometimes borrowing terms from OpenCL.\nOnce we explain the GPU architecture in our terms, we\u2019ll map them into the\nofficial jargon of NVIDIA GPUs.\nFrom left to right, Figure 4.12 lists the descriptive term used in this section, the\nclosest term from mainstream computing, the official NVIDIA GPU jargon in case\nyou are interested, and then a short explanation of the term. The rest of this section\nexplains the microarchitectural features of GPUs using the descriptive terms on the\nleft in the figure.\nWe use NVIDIA systems as our example as they are representative of GPU archi-\ntectures. Specifically, we follow the terminology of the preceding CUDA parallel pro-\ngramminglanguageandusetheNVIDIAPascalGPUastheexample(seeSection4.7).\nLike vector architectures, GPUs work well only with data-level parallel\nproblems. Both styles have gather-scatter data transfers and mask registers, and\nGPU processors have even more registers than do vector processors. Sometimes,\nGPUs implement certain features in hardware that vector processors would imple-\nment in software. This difference is because vector processors have a scalar\nprocessor that can execute a software function. Unlike most vector architectures,\n4.4\nGraphics Processing Units\n\u25a0\n313"
    },
    {
        "page": 346,
        "text": "Type\nDescriptive\nname\nClosest old term\noutside of GPUs\nOfficial\nCUDA/NVIDIA\nGPU term\nShort explanation\nProgram abstractions\nVectorizable\nLoop\nVectorizable Loop\nGrid\nA vectorizable loop, executed on the GPU, made up\nof one or more Thread Blocks (bodies of vectorized\nloop) that can execute in parallel\nBody of\nVectorized\nLoop\nBody of a (Strip-\nMined)\nVectorized Loop\nThread Block\nA vectorized loop executed on a multithreaded\nSIMD Processor, made up of one or more threads of\nSIMD instructions. They can communicate via local\nmemory\nSequence of\nSIMD Lane\nOperations\nOne iteration of a\nScalar Loop\nCUDA Thread\nA vertical cut of a thread of SIMD instructions\ncorresponding to one element executed by one SIMD\nLane. Result is stored depending on mask and\npredicate register\nMachine object\nA Thread of\nSIMD\nInstructions\nThread of Vector\nInstructions\nWarp\nA traditional thread, but it only contains SIMD\ninstructions that are executed on a multithreaded\nSIMD Processor. Results stored depending on a per-\nelement mask\nSIMD\nInstruction\nVector Instruction\nPTX\nInstruction\nA single SIMD instruction executed across SIMD\nLanes\nProcessing hardware\nMultithreaded\nSIMD\nProcessor\n(Multithreaded)\nVector Processor\nStreaming\nMultiprocessor\nA multithreaded SIMD Processor executes threads of\nSIMD instructions, independent of other SIMD\nProcessors\nThread Block\nScheduler\nScalar Processor\nGiga Thread\nEngine\nAssigns multiple Thread Blocks (bodies of\nvectorized loop) to multithreaded SIMD Processors\nSIMD Thread\nScheduler\nThread Scheduler\nin a Multithreaded\nCPU\nWarp\nScheduler\nHardware unit that schedules and issues threads of\nSIMD instructions when they are ready to execute;\nincludes a scoreboard to track SIMD Thread\nexecution\nSIMD Lane\nVector Lane\nThread\nProcessor\nA SIMD Lane executes the operations in a thread of\nSIMD instructions on a single element. Results\nstored depending on mask\nMemory hardware\nGPU Memory\nMain Memory\nGlobal\nMemory\nDRAM memory accessible by all multithreaded\nSIMD Processors in a GPU\nPrivate Memory\nStack or Thread\nLocal Storage\n(OS)\nLocal Memory\nPortion of DRAM memory private to each SIMD\nLane\nLocal Memory\nLocal Memory\nShared\nMemory\nFast local SRAM for one multithreaded SIMD\nProcessor, unavailable to other SIMD Processors\nSIMD Lane\nRegisters\nVector Lane\nRegisters\nThread\nProcessor\nRegisters\nRegisters in a single SIMD Lane allocated across a\nfull Thread Block (body of vectorized loop)\nFigure 4.12 Quick guide to GPU terms used in this chapter. We use the first column for hardware terms. Four\ngroups cluster these 11 terms. From top to bottom: program abstractions, machine objects, processing hardware,\nand memory hardware. Figure 4.21 on page 312 associates vector terms with the closest terms here, and\nFigure 4.24 on page 317 and Figure 4.25 on page 318 reveal the official CUDA/NVIDIA and AMD terms and definitions\nalong with the terms used by OpenCL.\n314\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 347,
        "text": "GPUs also rely on multithreading within a single multithreaded SIMD Processor to\nhide memory latency (see Chapters 2 and 3). However, efficient code for both vec-\ntor architectures and GPUs requires programmers to think in groups of SIMD\noperations.\nA Grid is the code that runs on a GPU that consists of a set of Thread Blocks.\nFigure 4.12 draws the analogy between a grid and a vectorized loop and between a\nThread Block and the body of that loop (after it has been strip-mined, so that it is a\nfull computation loop). To give a concrete example, let\u2019s suppose we want to mul-\ntiply two vectors together, each 8192 elements long: A = B * C. We\u2019ll return to this\nexample throughout this section. Figure 4.13 shows the relationship between this\nexample and these first two GPU terms. The GPU code that works on the whole\n8192 element multiply is called a Grid (or vectorized loop). To break it down into\nmore manageable sizes, a Grid is composed of Thread Blocks (or body of a\nvectorized loop), each with up to 512 elements. Note that a SIMD instruction\nexecutes 32 elements at a time. With 8192 elements in the vectors, this example\nthus has 16 Thread Blocks because 16\u00bc8192 \u0004 512. The Grid and Thread Block\nare programming abstractions implemented in GPU hardware that help program-\nmers organize their CUDA code. (The Thread Block is analogous to a strip-mined\nvector loop with a vector length of 32.)\nA Thread Block is assigned to a processor that executes that code, which we\ncall a multithreaded SIMD Processor, by the Thread Block Scheduler. The\nprogrammer tells the Thread Block Scheduler, which is implemented in hardware,\nhow many Thread Blocks to run. In this example, it would send 16 Thread Blocks\nto multithreaded SIMD Processors to compute all 8192 elements of this loop\nFigure 4.14 shows a simplified block diagram of a multithreaded SIMD\nProcessor. It is similar to a vector processor, but it has many parallel functional\nunits instead of a few that are deeply pipelined, as in a vector processor. In the pro-\ngramming example in Figure 4.13, each multithreaded SIMD Processor is assigned\n512 elements of the vectors to work on. SIMD Processors are full processors with\nseparate PCs and are programmed using threads (see Chapter 3).\nThe GPU hardware then contains a collection of multithreaded SIMD Proces-\nsors that execute a Grid of Thread Blocks (bodies of vectorized loop); that is, a\nGPU is a multiprocessor composed of multithreaded SIMD Processors.\nA GPU can have from one to several dozen multithreaded SIMD Processors. For\nexample, the Pascal P100 system has 56, while the smaller chips may have as few as\none or two. To provide transparent scalability across models of GPUs with a differing\nnumber of multithreaded SIMD Processors, the Thread Block Scheduler assigns\nThread Blocks (bodies of a vectorized loop) to multithreaded SIMD Processors.\nFigure4.15showsthefloorplanoftheP100implementationofthePascalarchitecture.\nDropping down one more level of detail, the machine object that the hardware\ncreates, manages, schedules, and executes is a thread of SIMD instructions. It is a\ntraditional thread that contains exclusively SIMD instructions. These threads of\nSIMD instructions have their own PCs, and they run on a multithreaded SIMD\nProcessor. The SIMD Thread Scheduler knows which threads of SIMD instruc-\ntions are ready to run and then sends them off to a dispatch unit to be run on\n4.4\nGraphics Processing Units\n\u25a0\n315"
    },
    {
        "page": 348,
        "text": "Figure 4.13 The mapping of a Grid (vectorizable loop), Thread Blocks (SIMD basic blocks), and threads of SIMD\ninstructions to a vector-vector multiply, with each vector being 8192 elements long. Each thread of SIMD instruc-\ntions calculates 32 elements per instruction, and in this example, each Thread Block contains 16 threads of SIMD\ninstructions and the Grid contains 16 Thread Blocks. The hardware Thread Block Scheduler assigns Thread Blocks\nto multithreaded SIMD Processors, and the hardware Thread Scheduler picks which thread of SIMD instructions\nto run each clock cycle within a SIMD Processor. Only SIMD Threads in the same Thread Block can communicate\nvia local memory. (The maximum number of SIMD Threads that can execute simultaneously per Thread Block is\n32 for Pascal GPUs.)\n316\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 349,
        "text": "the multithreaded SIMD Processor. Thus GPU hardware has two levels of hard-\nware schedulers: (1) the Thread Block Scheduler that assigns Thread Blocks (bod-\nies of vectorized loops) to multithreaded SIMD Processors and (2) the SIMD\nThread Scheduler within a SIMD Processor, which schedules when threads of\nSIMD instructions should run.\nThe SIMD instructions of these threads are 32 wide, so each thread of SIMD\ninstructions in this example would compute 32 of the elements of the computation.\nIn this example, Thread Blocks would contain 512/32\u00bc16 SIMD Threads (see\nFigure 4.13).\nBecause the thread consists of SIMD instructions, the SIMD Processor must\nhave parallel functional units to perform the operation. We call them SIMD Lanes,\nand they are quite similar to the Vector Lanes in Section 4.2.\nInstruction\ncache\nInstruction register\nRegi-\nsters\n1K \u00d7 32\nLoad\nstore\nunit\nLoad\nstore\nunit\nLoad\nstore\nunit\nLoad\nstore\nunit\nAddress coalescing unit\nInterconnection network\nLocal memory\n64 KB\nTo global\n memory\nLoad\nstore\nunit\nLoad\nstore\nunit\nLoad\nstore\nunit\nLoad\nstore\nunit\nLoad\nstore\nunit\nLoad\nstore\nunit\nLoad\nstore\nunit\nLoad\nstore\nunit\nLoad\nstore\nunit\nLoad\nstore\nunit\nLoad\nstore\nunit\nLoad\nstore\nunit\nReg\n \n1K \u00d7 32\nReg\n \n1K \u00d732\nReg\n \n1K \u00d732\nReg\n \n1K \u00d732\nReg\n \n1K \u00d732\nReg\n \n1K \u00d732\nReg\n \n1K \u00d732\nReg\n \n1K\u00d732\nReg\n \n1K\u00d7 32\nReg\n \n1K\u00d7 32\nReg\n \n1K\u00d7 32\nReg\n \n1K\u00d7 32\nReg\n \n1K\u00d7 32\nReg\n \n1K\u00d7 32\nReg\n \n1K\u00d7 32\nWarp scheduler\nSIMD lanes\n(thread\nprocessors)\nFigure 4.14 Simplified block diagram of a multithreaded SIMD Processor. It has 16 SIMD Lanes. The SIMD Thread\nScheduler has, say, 64 independent threads of SIMD instructions that it schedules with a table of 64 program counters\n(PCs). Note that each lane has 1024 32-bit registers.\n4.4\nGraphics Processing Units\n\u25a0\n317"
    },
    {
        "page": 350,
        "text": "With the Pascal GPU, each 32-wide thread of SIMD instructions is mapped\nto 16 physical SIMD Lanes, so each SIMD instruction in a thread of SIMD instruc-\ntions takes 2 clock cycles to complete. Each thread of SIMD instructions is\nexecuted in lock step and scheduled only at the beginning. Staying with the anal-\nogy of a SIMD Processor as a vector processor, you could say that it has 16 lanes,\nthe vector length is 32, and the chime is 2 clock cycles. (This wide but shallow\nnature is why we use the more accurate term SIMD Processor rather than vector.)\nNotethatthe number oflanes ina GPU SIMDProcessor can beanythinguptothe\nnumber of threads in a Thread Block, just as the number of lanes in a vector processor\ncan vary between 1 and the maximum vector length. For example, across GPU gen-\nerations, the number of lanes per SIMD Processor has fluctuated between 8 and 32.\nBecause by definition the threads of SIMD instructions are independent, the\nSIMD Thread Scheduler can pick whatever thread of SIMD instructions is ready,\nand need not stick with the next SIMD instruction in the sequence within a thread.\nThe SIMD Thread Scheduler includes a scoreboard (see Chapter 3) to keep track of\nup to 64 threads of SIMD instructions to see which SIMD instruction is ready to go.\nThe latency of memory instructions is variable because of hits and misses in the\ncaches and the TLB, thus the requirement of a scoreboard to determine when these\ninstructions are complete. Figure 4.16 shows the SIMD Thread Scheduler picking\nthreads of SIMD instructions in a different order over time. The assumption\nof GPU architects is that GPU applications have so many threads of SIMD instruc-\ntions that multithreading can both hide the latency to DRAM and increase utiliza-\ntion of multithreaded SIMD Processors.\nFigure 4.15 Full-chip block diagram of the Pascal P100 GPU. It has 56 multithreaded SIMD Processors, each with an\nL1 cache and local memory, 32 L2 units, and a memory-bus width of 4096 data wires. (It has 60 blocks, with four spares\nto improve yield.) The P100 has 4 HBM2 ports supporting up to 16 GB of capacity. It contains 15.4 billion transistors.\n318\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 351,
        "text": "Continuing our vector multiply example, each multithreaded SIMD Processor\nmust load 32 elements of two vectors from memory into registers, perform the\nmultiply by reading and writing registers, and store the product back from reg-\nisters into memory. To hold these memory elements, a SIMD Processor has\nbetween an impressive 32,768\u201365,536 32-bit registers (1024 per lane in\nFigure 4.14), depending on the model of the Pascal GPU. Just like a vector pro-\ncessor, these registers are divided logically across the Vector Lanes or, in this\ncase, SIMD Lanes.\nEach SIMD Thread is limited to no more than 256 registers, so you might think\nof a SIMD Thread as having up to 256 vector registers, with each vector register\nhaving 32 elements and each element being 32 bits wide. (Because double-\nprecision floating-point operands use two adjacent 32-bit registers, an alternative\nview is that each SIMD Thread has 128 vector registers of 32 elements, each of\nwhich is 64 bits wide.)\nThere is a trade-off between register use and maximum number of threads;\nfewer registers per thread means more threads are possible, and more registers\nSIMD thread 8 instruction 11\nSIMD thread 1 instruction 42\nSIMD thread 3 instruction 95\nSIMD thread 8 instruction 12\nTime\nSIMD thread scheduler\nSIMD thread 1 instruction 43\nSIMD thread 3 instruction 96\nPhoto: Judy Schoonmaker\nFigure 4.16 Scheduling of threads of SIMD instructions. The scheduler selects a ready\nthread of SIMD instructions and issues an instruction synchronously to all the SIMD\nLanes executing the SIMD Thread. Because threads of SIMD instructions are indepen-\ndent, the scheduler may select a different SIMD Thread each time.\n4.4\nGraphics Processing Units\n\u25a0\n319"
    },
    {
        "page": 352,
        "text": "mean fewer threads. That is, not all SIMD Threads need to have the maximum\nnumber of registers. Pascal architects believe much of this precious silicon area\nwould be idle if all threads had the maximum number of registers.\nTo be able to execute many threads of SIMD instructions, each is dynamically\nallocated a set of the physical registers on each SIMD Processor when threads of\nSIMD instructions are created and freed when the SIMD Thread exits. For example,\na programmer can have a Thread Block that uses 36 registers per thread with, say, 16\nSIMD Threads alongside another Thread Block that has 20 registers per thread with\n32 SIMD Threads. Subsequent Thread Blocks may show up in any order, and the\nregisters have to be allocated on demand. While this variability can lead to fragmen-\ntation and make some registers unavailable, in practice most Thread Blocks use the\nsame number of registers for a given vectorizable loop (\u201cgrid\u201d). The hardware must\nknow where the registers for each Thread Block are in the large register file, and this\nis recorded on a per Thread-Block basis. This flexibility requires routing, arbitration,\nand banking in the hardware because a specific register for a given Thread Block\ncould end up in any location in the register file.\nNote that a CUDA Thread is just a vertical cut of a thread of SIMD instructions,\ncorresponding to one element executed by one SIMD Lane. Beware that CUDA\nThreads are very different from POSIX Threads; you can\u2019t make arbitrary system\ncalls from a CUDA Thread.\nWe\u2019re now ready to see what GPU instructions look like.\nNVIDA GPU Instruction Set Architecture\nUnlike most system processors, the instruction set target of the NVIDIA compilers\nis an abstraction of the hardware instruction set. PTX (Parallel Thread Execution)\nprovides a stable instruction set for compilers as well as compatibility across gen-\nerations of GPUs. The hardware instruction set is hidden from the programmer.\nPTX instructions describe the operations on a single CUDA Thread and usually\nmap one-to-one with hardware instructions, but one PTX instruction can expand\nto many machine instructions, and vice versa. PTX uses an unlimited number of\nwrite-once registers and the compiler must run a register allocation procedure to\nmap the PTX registers to a fixed number of read-write hardware registers available\non the actual device. The optimizer runs subsequently and can reduce register use\neven further. This optimizer also eliminates dead code, folds instructions together,\nand calculates places where branches might diverge and places where diverged\npaths could converge.\nAlthough there is some similarity between the x86 microarchitecture and PTX,\nin that both translate to an internal form (microinstructions for x86), the difference\nis that this translation happens in hardware at runtime during execution on the x86\nversus in software and load time on a GPU.\nThe format of a PTX instruction is\nopcode.type d, a, b, c;\nwhere d is the destination operand; a, b, and c are source operands; and the\noperation type is one of the following:\n320\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 353,
        "text": "Source operands are 32-bit or 64-bit registers or a constant value. Destinations are\nregisters, except for store instructions.\nFigure 4.17 shows the basic PTX instruction set. All instructions can be pred-\nicated by 1-bit predicate registers, which can be set by a set predicate instruction\n(setp). The control flow instructions are functions call and return, thread\nexit, branch, and barrier synchronization for threads within a Thread Block\n(bar.sync). Placing a predicate in front of a branch instruction gives us con-\nditional branches. The compiler or PTX programmer declares virtual registers as\n32-bit or 64-bit typed or untyped values. For example, R0, R1, ... are for 32-bit\nvalues and RD0, RD1, ... are for 64-bit registers. Recall that the assignment of\nvirtual registers to physical registers occurs at load time with PTX.\nThe following sequence of PTX instructions is for one iteration of our DAXPY\nloop on page 292:\nshl.u32 R8, blockIdx, 8\n; Thread Block ID * Block size\n;(256 or 28)\nadd.u32 R8, R8, threadIdx ; R8 = i = my CUDA Thread ID\nshl.u32 R8, R8, 3\n; byte offset\nld.global.f64 RD0, [X+R8]; RD0 = X[i]\nld.global.f64 RD2, [Y+R8]; RD2 = Y[i]\nmul.f64 RD0, RD0, RD4\n; Product in RD0 = RD0 * RD4\n; (scalar a)\nadd.f64 RD0, RD0, RD2\n; Sum in RD0 = RD0 + RD2 (Y[i])\nst.global.f64 [Y+R8], RD0; Y[i] = sum (X[i]*a + Y[i])\nAs demonstrated above, the CUDA programming model assigns one CUDA Thread\nto each loop iteration and offers a unique identifier number to each Thread Block\n(blockIdx) and one to each CUDA Thread within a block (threadIdx). Thus\nit creates 8192 CUDA Threads and uses the unique number to address each element\nwithin the array, so there is no incrementing or branching code. The first three PTX\ninstructions calculate that unique element byte offset in R8, which is added to the\nbase of the arrays. The following PTX instructions load two double-precision\nfloating-point operands, multiply and add them, and store the sum. (We\u2019ll describe\nthe PTX code corresponding to the CUDA code \u201cif (i < n)\u201d below.)\nNote that unlike vector architectures, GPUs don\u2019t have separate instructions for\nsequential data transfers, strided data transfers, and gather-scatter data transfers.\nType\n.type specifier\nUntyped bits 8, 16, 32, and 64 bits\n.b8, .b16, .b32, .b64\nUnsigned integer 8, 16, 32, and 64 bits\n.u8, .u16, .u32, .u64\nSigned integer 8, 16, 32, and 64 bits\n.s8, .s16, .s32, .s64\nFloating Point 16, 32, and 64 bits\n.f16, .f32, .f64\n4.4\nGraphics Processing Units\n\u25a0\n321"
    },
    {
        "page": 354,
        "text": "Group\nInstruction\nExample\nMeaning\nComments\nArithmetic\narithmetic .type = .s32, .u32, .f32, .s64, .u64, .f64\nadd.type\nadd.f32 d, a, b\nd = a + b;\nsub.type\nsub.f32 d, a, b\nd = a \u2013 b;\nmul.type\nmul.f32 d, a, b\nd = a * b;\nmad.type\nmad.f32 d, a, b, c\nd = a * b + c;\nmultiply-add\ndiv.type\ndiv.f32 d, a, b\nd = a / b;\nmultiple microinstructions\nrem.type\nrem.u32 d, a, b\nd = a % b;\ninteger remainder\nabs.type\nabs.f32 d, a\nd = jaj;\nneg.type\nneg.f32 d, a\nd = 0 \u2013 a;\nmin.type\nmin.f32 d, a, b\nd = (a < b)? a:b;\nfloating selects non-NaN\nmax.type\nmax.f32 d, a, b\nd = (a > b)? a:b;\nfloating selects non-NaN\nsetp.cmp.type\nsetp.lt.f32 p, a, b\np = (a < b);\ncompare and set predicate\nnumeric .cmp = eq, ne, lt, le, gt, ge; unordered cmp = equ, neu, ltu, leu, gtu, geu, num, nan\nmov.type\nmov.b32 d, a\nd = a;\nmove\nselp.type\nselp.f32 d, a, b, p\nd = p? a: b;\nselect with predicate\ncvt.dtype.atype\ncvt.f32.s32 d, a\nd = convert(a);\nconvert atype to dtype\nSpecial function\nspecial .type = .f32 (some .f64)\nrcp.type\nrcp.f32 d, a\nd = 1/a;\nreciprocal\nsqrt.type\nsqrt.f32 d, a\nd = sqrt(a);\nsquare root\nrsqrt.type\nrsqrt.f32 d, a\nd = 1/sqrt(a);\nreciprocal square root\nsin.type\nsin.f32 d, a\nd = sin(a);\nsine\ncos.type\ncos.f32 d, a\nd = cos(a);\ncosine\nlg2.type\nlg2.f32 d, a\nd = log(a)/log(2)\nbinary logarithm\nex2.type\nex2.f32 d, a\nd = 2 ** a;\nbinary exponential\nLogical\nlogic.type = .pred,.b32, .b64\nand.type\nand.b32 d, a, b\nd = a & b;\nor.type\nor.b32 d, a, b\nd = a j b;\nxor.type\nxor.b32 d, a, b\nd = a ^b;\nnot.type\nnot.b32 d, a, b\nd = \u0005a;\none\u2019s complement\ncnot.type\ncnot.b32 d, a, b\nd = (a==0)? 1:0;\nC logical not\nshl.type\nshl.b32 d, a, b\nd = a << b;\nshift left\nshr.type\nshr.s32 d, a, b\nd = a >> b;\nshift right\nMemory access\nmemory.space = .global, .shared, .local, .const; .type = .b8, .u8, .s8, .b16, .b32, .b64\nld.space.type\nld.global.b32 d, [a+off]\nd = *(a+off);\nload from memory space\nst.space.type\nst.shared.b32 [d+off], a\n*(d+off) = a;\nstore to memory space\ntex.nd.dtyp.btype\ntex.2d.v4.f32.f32 d, a, b\nd = tex2d(a, b);\ntexture lookup\natom.spc.op.type\natom.global.add.u32 d,[a], b\natom.global.cas.b32 d,[a], b, c\natomic { d = *a;\n*a = op(*a, b); }\natomic read-modify-write\noperation\natom.op = and, or, xor, add, min, max, exch, cas; .spc = .global; .type = .b32\nControl flow\nbranch\n@p bra target\nif (p) goto target;\nconditional branch\ncall\ncall (ret), func, (params)\nret = func(params);\ncall function\nret\nret\nreturn;\nreturn from function call\nbar.sync\nbar.sync d\nwait for threads\nbarrier synchronization\nexit\nexit\nexit;\nterminate thread execution\nFigure 4.17 Basic PTX GPU thread instructions.\n322\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 355,
        "text": "All data transfers are gather-scatter! To regain the efficiency of sequential (unit-\nstride) data transfers, GPUs include special Address Coalescing hardware to rec-\nognize when the SIMD Lanes within a thread of SIMD instructions are collectively\nissuing sequential addresses. That runtime hardware then notifies the Memory\nInterface Unit to request a block transfer of 32 sequential words. To get this impor-\ntant performance improvement, the GPU programmer must ensure that adjacent\nCUDA Threads access nearby addresses at the same time so that they can be coa-\nlesced into one or a few memory or cache blocks, which our example does.\nConditional Branching in GPUs\nJust like the case with unit-stride data transfers, there are strong similarities\nbetween how vector architectures and GPUs handle IF statements, with the former\nimplementing the mechanism largely in software with limited hardware support\nand the latter making use of even more hardware. As we will see, in addition to\nexplicit predicate registers, GPU branch hardware uses internal masks, a branch\nsynchronization stack, and instruction markers to manage when a branch diverges\ninto multiple execution paths and when the paths converge.\nAt the PTX assembler level, control flow of one CUDA Thread is described by\nthe PTX instructions branch, call, return, and exit, plus individual per-thread-lane\npredication of each instruction, specified by the programmer with per-thread-lane\n1-bit predicate registers. The PTX assembler analyzes the PTX branch graph and\noptimizes it to the fastest GPU hardware instruction sequence. Each can make its\nown decision on a branch and does not need to be in lock step.\nAt the GPU hardware instruction level, control flow includes branch, jump,\njump indexed, call, call indexed, return, exit, and special instructions that manage\nthe branch synchronization stack. GPU hardware provides each SIMD Thread with\nits own stack; a stack entry contains an identifier token, a target instruction address,\nand a target thread-active mask. There are GPU special instructions that push stack\nentries for a SIMD Thread and special instructions and instruction markers that pop\na stack entry or unwind the stack to a specified entry and branch to the target\ninstruction address with the target thread-active mask. GPU hardware instructions\nalso have an individual per-lane predication (enable/disable), specified with a 1-bit\npredicate register for each lane.\nThe PTX assembler typically optimizes a simple outer-level IF-THEN-ELSE\nstatement coded with PTX branch instructions to solely predicated GPU instruc-\ntions, without any GPU branch instructions. A more complex control flow often\nresults in a mixture of predication and GPU branch instructions with special\ninstructions and markers that use the branch synchronization stack to push a stack\nentry when some lanes branch to the target address, while others fall through. NVI-\nDIA says a branch diverges when this happens. This mixture is also used when a\nSIMD Lane executes a synchronization marker or converges, which pops a stack\nentry and branches to the stack-entry address with the stack-entry thread-\nactive mask.\n4.4\nGraphics Processing Units\n\u25a0\n323"
    },
    {
        "page": 356,
        "text": "The PTX assembler identifies loop branches and generates GPU branch\ninstructions that branch to the top of the loop, along with special stack instructions\nto handle individual lanes breaking out of the loop and converging the SIMD Lanes\nwhen all lanes have completed the loop. GPU indexed jump and indexed call\ninstructions push entries on the stack so that when all lanes complete the switch\nstatement or function call, the SIMD Thread converges.\nA GPU set predicate instruction (setp in Figure 4.17) evaluates the condi-\ntional part of the IF statement. The PTX branch instruction then depends on that\npredicate. If the PTX assembler generates predicated instructions with no GPU\nbranch instructions, it uses a per-lane predicate register to enable or disable each\nSIMD Lane for each instruction. The SIMD instructions in the threads inside the\nTHEN part of the IF statement broadcast operations to all the SIMD Lanes. Those\nlanes with the predicate set to 1 perform the operation and store the result, and the\nother SIMD Lanes don\u2019t perform an operation or store a result. For the ELSE\nstatement, the instructions use the complement of the predicate (relative to the\nTHEN statement), so the SIMD Lanes that were idle now perform the operation\nand store the result while their formerly active siblings don\u2019t. At the end of\nthe ELSE statement, the instructions are unpredicated so the original computation\ncan proceed. Thus, for equal length paths, an IF-THEN-ELSE operates at 50%\nefficiency or less.\nIF statements can be nested, thus the use of a stack, and the PTX assembler\ntypically generates a mix of predicated instructions and GPU branch and special\nsynchronization instructions for complex control flow. Note that deep nesting\ncan mean that most SIMD Lanes are idle during execution of nested conditional\nstatements. Thus, doubly nested IF statements with equal-length paths run at\n25% efficiency, triply nested at 12.5% efficiency, and so on. The analogous\ncase would be a vector processor operating where only a few of the mask bits\nare ones.\nDropping down a level of detail, the PTX assembler sets a \u201cbranch synchro-\nnization\u201d marker on appropriate conditional branch instructions that pushes the\ncurrent active mask on a stack inside each SIMD Thread. If the conditional branch\ndiverges (some lanes take the branch but some fall through), it pushes a stack entry\nand sets the current internal active mask based on the condition. A branch synchro-\nnization marker pops the diverged branch entry and flips the mask bits before the\nELSE portion. At the end of the IF statement, the PTX assembler adds another\nbranch synchronization marker that pops the prior active mask off the stack into\nthe current active mask.\nIf all the mask bits are set to 1, then the branch instruction at the end of the\nTHEN skips over the instructions in the ELSE part. There is a similar optimization\nfor the THEN part in case all the mask bits are 0 because the conditional branch\njumps over the THEN instructions. Parallel IF statements and PTX branches often\nuse branch conditions that are unanimous (all lanes agree to follow the same path)\nsuch that the SIMD Thread does not diverge into a different individual lane control\nflow. The PTX assembler optimizes such branches to skip over blocks of instruc-\ntions that are not executed by any lane of a SIMD Thread. This optimization is\n324\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 357,
        "text": "useful in conditional error checking, for example, where the test must be made but\nis rarely taken.\nThe code for a conditional statement similar to the one in Section 4.2 is\nif (X[i] != 0)\nX[i] = X[i] \u2013 Y[i];\nelse X[i] = Z[i];\nThis IF statement could compile to the following PTX instructions (assuming\nthat R8 already has the scaled thread ID), with *Push, *Comp, *Pop indicating\nthe branch synchronization markers inserted by the PTX assembler that push the\nold mask, complement the current mask, and pop to restore the old mask:\nld.global.f64 RD0, [X+R8]\n; RD0 = X[i]\nsetp.neq.s32 P1, RD0, #0\n;P1 is predicate reg 1\n@!P1, bra ELSE1, *Push\n; Push old mask, set new\n; mask bits if P1 false, go to ELSE1\nld.global.f64 RD2, [Y+R8]\n; RD2 = Y[i]\nsub.f64 RD0, RD0, RD2\n; Difference in RD0\nst.global.f64 [X+R8], RD0\n; X[i] = RD0\n@P1, bra ENDIF1, *Comp\n; complement mask bits\n; if P1 true, go to ENDIF1\nELSE1: ld.global.f64 RD0, [Z+R8] ; RD0 = Z[i]\nst.global.f64 [X+R8], RD0 ; X[i] = RD0\nENDIF1:<next instruction>, *Pop\n; pop to restore old mask\nOnce again, normally all instructions in the IF-THEN-ELSE statement are exe-\ncuted by a SIMD Processor. It\u2019s just that only some of the SIMD Lanes are enabled\nfor the THEN instructions and some lanes for the ELSE instructions. As previously\nmentioned, in the surprisingly common case that the individual lanes agree on the\npredicated branch\u2014such as branching on a parameter value that is the same for all\nlanes so that all active mask bits are 0s or all are 1s\u2014the branch skips the THEN\ninstructions or the ELSE instructions.\nThis flexibility makes it appear that an element has its own program counter;\nhowever, in the slowest case, only one SIMD Lane could store its result every 2\nclock cycles, with the rest idle. The analogous slowest case for vector architec-\ntures is operating with only one mask bit set to 1. This flexibility can lead naive\nGPU programmers to poor performance, but it can be helpful in the early stages\nof program development. Keep in mind, however, that the only choice for a\nSIMD Lane in a clock cycle is to perform the operation specified in the PTX\ninstruction or be idle; two SIMD Lanes cannot simultaneously execute different\ninstructions.\nThis flexibility also helps explain the name CUDA Thread given to each ele-\nment in a thread of SIMD instructions, because it gives the illusion of acting inde-\npendently. A naive programmer may think that this thread abstraction means GPUs\nhandle conditional branches more gracefully. Some threads go one way, the rest go\n4.4\nGraphics Processing Units\n\u25a0\n325"
    },
    {
        "page": 358,
        "text": "another, which seems true as long as you\u2019re not in a hurry. Each CUDA Thread is\neither executing the same instruction as every other thread in the Thread Block or it\nis idle. This synchronization makes it easier to handle loops with conditional\nbranches because the mask capability can turn off SIMD Lanes and it detects\nthe end of the loop automatically.\nThe resulting performance sometimes belies that simple abstraction. Writing\nprograms that operate SIMD Lanes in this highly independent MIMD mode is like\nwriting programs that use lots of virtual address space on a computer with a smaller\nphysical memory. Both are correct, but they may run so slowly that the program-\nmer will not be pleased with the result.\nConditional execution is a case where GPUs do in runtime hardware what vec-\ntor architectures do at compile time. Vector compilers do a double IF-conversion,\ngenerating four different masks. The execution is basically the same as GPUs, but\nthere are some more overhead instructions executed for vectors. Vector architec-\ntures have the advantage of being integrated with a scalar processor, allowing them\nto avoid the time for the 0 cases when they dominate a calculation. Although it will\ndepend on the speed of the scalar processor versus the vector processor, the cross-\nover point when it\u2019s better to use scalar might be when less than 20% of the mask\nbits are 1s. One optimization available at runtime for GPUs, but not at compile time\nfor vector architectures, is to skip the THEN or ELSE parts when mask bits are all\n0s or all 1s.\nThus the efficiency with which GPUs execute conditional statements comes\ndown to how frequently the branches will diverge. For example, one calculation\nof eigenvalues has deep conditional nesting, but measurements of the code show\nthat around 82% of clock cycle issues have between 29 and 32 out of the 32 mask\nbits set to 1, so GPUs execute this code more efficiently than one might expect.\nNote that the same mechanism handles the strip-mining of vector loops\u2014when\nthe number of elements doesn\u2019t perfectly match the hardware. The example at the\nbeginning of this section shows that an IF statement checks to see if this SIMD\nLane element number (stored in R8 in the preceding example) is less than the limit\n(i < n), and it sets masks appropriately.\nNVIDIA GPU Memory Structures\nFigure 4.18 shows the memory structures of an NVIDIA GPU. Each SIMD Lane in\na multithreaded SIMD Processor is given a private section of off-chip DRAM,\nwhich we call the private memory. It is used for the stack frame, for spilling\nregisters, and for private variables that don\u2019t fit in the registers. SIMD Lanes do\nnot share private memories. GPUs cache this private memory in the L1 and L2\ncaches to aid register spilling and to speed up function calls.\nWe call the on-chip memory that is local to each multithreaded SIMD Proces-\nsor local memory. It is a small scratchpad memory with low latency (a few dozen\nclocks) and high bandwidth (128 bytes/clock) where the programmer can store\ndata that needs to be reused, either by the same thread or another thread in the same\n326\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 359,
        "text": "Thread Block. Local memory is limited in size, typically to 48 KiB. It carries no\nstate between Thread Blocks executed on the same processor. It is shared by the\nSIMD Lanes within a multithreaded SIMD Processor, but this memory is not\nshared between multithreaded SIMD Processors. The multithreaded SIMD\nProcessor dynamically allocates portions of the local memory to a Thread Block\nwhen it creates the Thread Block, and frees the memory when all the threads of the\nThread Block exit. That portion of local memory is private to that Thread Block.\nFinally, we call the off-chip DRAM shared by the whole GPU and all Thread\nBlocks GPU Memory. Our vector multiply example used only GPU Memory.\nThe system processor, called the host, can read or write GPU Memory. Local\nmemory is unavailable to the host, as it is private to each multithreaded SIMD\nProcessor. Private memories are unavailable to the host as well.\nCUDA thread\nThread block\nPer-block\nlocal memory\nGrid 0 \n. . . \nGrid 1 \n. . . \nGPU memory\nSequence\nInter-grid synchronization\nPer-CUDA thread private memory\nFigure 4.18 GPU memory structures. GPU memory is shared by all Grids (vectorized\nloops), local memory is shared by all threads of SIMD instructions within a Thread Block\n(body of a vectorized loop), and private memory is private to a single CUDA Thread.\nPascal allows preemption of a Grid, which requires that all local and private memory\nbe able to be saved in and restored from global memory. For completeness sake,\nthe GPU can also access CPU memory via the PCIe bus. This path is commonly used\nfor a final result when its address is in host memory. This option eliminates a final copy\nfrom the GPU memory to the host memory.\n4.4\nGraphics Processing Units\n\u25a0\n327"
    },
    {
        "page": 360,
        "text": "Rather than rely on large caches to contain the whole working sets of an\napplication,GPUstraditionallyusesmallerstreamingcachesand,becausetheirwork-\ning sets can be hundreds of megabytes, rely on extensive multithreading of threads of\nSIMDinstructionstohidethelonglatencytoDRAM.Giventheuseofmultithreading\nto hide DRAM latency, the chip area used for large L2 and L3 caches in system pro-\ncessorsisspentinsteadoncomputingresourcesandonthelargenumberofregistersto\nhold the state of many threads of SIMD instructions. In contrast, as mentioned, vector\nloads and stores amortize the latency across many elements because they pay the\nlatency only once and then pipeline the rest of the accesses.\nAlthough hiding memory latency behind many threads was the original philos-\nophy of GPUs and vector processors, all recent GPUs and vector processors have\ncaches to reduce latency. The argument follows Little\u2019s Law from queuing theory:\nthe longer the latency, the more threads need to run during a memory access, which\nin turn requires more registers. Thus GPU caches are added to lower average\nlatency and thereby mask potential shortages of the number of registers.\nTo improve memory bandwidth and reduce overhead, as mentioned, PTX data\ntransfer instructions in cooperation with the memory controller coalesce individual\nparallel thread requests from the same SIMD Thread together into a single memory\nblock request when the addresses fall in the same block. These restrictions are\nplaced on the GPU program, somewhat analogous to the guidelines for system pro-\ncessor programs to engage hardware prefetching (see Chapter 2). The GPU mem-\nory controller will also hold requests and send ones together to the same open page\nto improve memory bandwidth (see Section 4.6). Chapter 2 describes DRAM in\nsufficient detail for readers to understand the potential benefits of grouping related\naddresses.\nInnovations in the Pascal GPU Architecture\nThe multithreaded SIMD Processor of Pascal is more complicated than the simpli-\nfied version in Figure 4.20. To increase hardware utilization, each SIMD Processor\nhas two SIMD Thread Schedulers, each with multiple instruction dispatch units\n(some GPUs have four thread schedulers). The dual SIMD Thread Scheduler\nselects two threads of SIMD instructions and issues one instruction from each\nto two sets of 16 SIMD Lanes, 16 load/store units, or 8 special function units. With\nmultiple execution units available, two threads of SIMD instructions are scheduled\neach clock cycle, allowing 64 lanes to be active. Because the threads are indepen-\ndent, there is no need to check for data dependences in the instruction stream. This\ninnovation would be analogous to a multithreaded vector processor that can issue\nvector instructions from two independent threads. Figure 4.19 shows the Dual\nScheduler issuing instructions, and Figure 4.20 shows the block diagram of the\nmultithreaded SIMD Processor of a Pascal GP100 GPU.\nEach new generation of GPU typically adds some new features that increase\nperformance or make it easier for programmers. Here are the four main innovations\nof Pascal:\n328\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 361,
        "text": "\u25a0\nFast single-precision, double-precision, and half-precision floating-point\narithmetic\u2014Pascal GP100 chip has significant floating-point performance\nin three sizes, all part of the IEEE standard for floating-point. The single-\nprecision floating-point of the GPU runs at a peak of 10 TeraFLOP/s.\nDouble-precision is roughly half-speed at 5 TeraFLOP/s, and half-precision\nis about double-speed at 20 TeraFLOP/s when expressed as 2-element vectors.\nThe atomic memory operations include floating-point add for all three sizes.\nPascal GP100 is the first GPU with such high performance for half-precision.\n\u25a0\nHigh-bandwidth memory\u2014The next innovation of the Pascal GP100 GPU is\nthe use of stacked, high-bandwidth memory (HBM2). This memory has a wide\nbus with 4096 data wires running at 0.7 GHz offering a peak bandwidth of 732\nGB/s, which is more than twice as fast as previous GPUs.\n\u25a0\nHigh-speed chip-to-chip interconnect\u2014Given the coprocessor nature of\nGPUs, the PCI bus can be a communications bottleneck when trying to use\nmultiple GPUs with one CPU. Pascal GP100 introduces the NVLink commu-\nnications channel that supports data transfers of up to 20 GB/s in each direc-\ntion. Each GP100 has 4 NVLink channels, providing a peak aggregate chip-to-\nchip bandwidth of 160 GB/s per chip. Systems with 2, 4, and 8 GPUs are\navailable for multi-GPU applications, where each GPU can perform load, store,\nand atomic operations to any GPU connected by NVLink. Additionally, an\nNVLink channel can communicate with the CPU in some cases. For example,\nthe IBM Power9 CPU supports CPU-GPU communication. In this chip,\nNVLink provides a coherent view of memory between all GPUs and CPUs\nconnected together. It also provides cache-to-cache communication instead\nof memory-to-memory communication.\nSIMD thread scheduler\nInstruction dispatch unit\nSIMD thread 8 instruction 11\nSIMD thread 2 instruction 42\nSIMD thread 14 instruction 95\nSIMD thread 8 instruction 12\nTime\nSIMD thread 2 instruction 43\nSIMD thread 14 instruction 96\nSIMD thread scheduler\nInstruction dispatch unit\nSIMD thread 9 instruction 11\nSIMD thread 3 instruction 33\nSIMD thread 15 instruction 95\nSIMD thread 9 instruction 12\nSIMD thread 15 instruction 96\nSIMD thread 3 instruction 34\nFigure 4.19 Block diagram of Pascal\u2019s dual SIMD Thread scheduler. Compare this\ndesign to the single SIMD Thread design in Figure 4.16.\n4.4\nGraphics Processing Units\n\u25a0\n329"
    },
    {
        "page": 362,
        "text": "\u25a0\nUnified virtual memory and paging support\u2014The Pascal GP100 GPU adds\npage-fault capabilities within a unified virtual address space. This feature\nallows a single virtual address for every data structure that is identical across\nall the GPUs and CPUs in a single system. When a thread accesses an address\nthat is remote, a page of memory is transferred to the local GPU for subsequent\nuse. Unified memory simplifies the programming model by providing demand\npaging instead of explicit memory copying between the CPU and GPU or\nInstruction Cache\nTexture / L1 Cache\n64KB Shared Memory\nDispatch\nUnits\nDispatch\nUnits\nDispatch\nUnits\nDispatch\nUnits\nSIMD Thread Scheduler\nRegister File (32,768 \u00d7 32-bit)\nSIMD Thread Scheduler\nInstruction Buffer\nInstruction Buffer\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nLD/ST\nSFU\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nLD/ST\nSFU\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nLD/ST\nSFU\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nLD/ST\nSFU\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nLD/ST\nSFU\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nLD/ST\nSFU\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nLD/ST\nSFU\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nLD/ST\nSFU\nDispatch\nUnits\nDispatch\nUnits\nDispatch\nUnits\nDispatch\nUnits\nRegister File (32,768 \u00d7 32-bit)\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nLD/ST\nSFU\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nLD/ST\nSFU\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nLD/ST\nSFU\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nLD/ST\nSFU\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nLD/ST\nSFU\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nLD/ST\nSFU\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nLD/ST\nSFU\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nSIMD\nLane\nSIMD\nLane\nDP\nUnit\nLD/ST\nSFU\nTex\nTex\nTex\nTex\nFigure 4.20 Block diagram of the multithreaded SIMD Processor of a Pascal GPU. Each of the 64 SIMD Lanes\n(cores) has a pipelined floating-point unit, a pipelined integer unit, some logic for dispatching instructions and oper-\nands to these units, and a queue for holding results. The 64 SIMD Lanes interact with 32 double-precision ALUs (DP\nunits) that perform 64-bit floating-point arithmetic, 16 load-store units (LD/STs), and 16 special function units (SFUs)\nthat calculate functions such as square roots, reciprocals, sines, and cosines.\n330\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 363,
        "text": "between GPUs. It also allows allocating far more memory than exists on the\nGPU to solve problems with large memory requirements. As with any virtual\nmemory system, care must be taken to avoid excessive page movement.\nSimilarities and Differences Between Vector\nArchitectures and GPUs\nAs we have seen, there really are many similarities between vector architectures\nand GPUs. Along with the quirky jargon of GPUs, these similarities have contrib-\nuted to the confusion in architecture circles about how novel GPUs really are. Now\nthat you\u2019ve seen what is under the covers of vector computers and GPUs, you can\nappreciate both the similarities and the differences. Because both architectures are\ndesigned to execute data-level parallel programs, but take different paths, this com-\nparison is in depth in order to provide a better understanding of what is needed for\nDLP hardware. Figure 4.21 shows the vector term first and then the closest equiv-\nalent in a GPU.\nA SIMD Processor is like a vector processor. The multiple SIMD Processors\nin GPUs act as independent MIMD cores, just as many vector computers have\nmultiple vector processors. This view will consider the NVIDIA Tesla P100 as\na 56-core machine with hardware support for multithreading, where each core\nhas 64 lanes. The biggest difference is multithreading, which is fundamental to\nGPUs and missing from most vector processors.\nLooking at the registers in the two architectures, the RV64V register file in our\nimplementation holds entire vectors\u2014that is, a contiguous block of elements. In\ncontrast, a single vector in a GPU will be distributed across the registers of all SIMD\nLanes. A RV64V processor has 32 vector registers with perhaps 32 elements, or\n1024 elementstotal. AGPU threadof SIMD instructions hasupto256 registers with\n32 elements each, or 8192 elements. These extra GPU registers support\nmultithreading.\nFigure 4.22 is a block diagram of the execution units of a vector processor on\nthe left and a multithreaded SIMD Processor of a GPU on the right. For pedagogic\npurposes, we assume the vector processor has four lanes and the multithreaded\nSIMD Processor also has four SIMD Lanes. This figure shows that the four SIMD\nLanes act in concert much like a four-lane vector unit, and that a SIMD Processor\nacts much like a vector processor.\nIn reality, there are many more lanes in GPUs, so GPU \u201cchimes\u201d are shorter.\nWhile a vector processor might have 2 to 8 lanes and a vector length of, say, 32\u2014\nmaking a chime 4 to 16 clock cycles\u2014a multithreaded SIMD Processor might have\n8 or 16 lanes. A SIMD Thread is 32 elements wide, so a GPU chime would just be 2\nor 4 clock cycles. This difference is why we use \u201cSIMD Processor\u201d as the more\ndescriptive term because it is closer to a SIMD design than it is to a traditional vec-\ntor processor design.\n4.4\nGraphics Processing Units\n\u25a0\n331"
    },
    {
        "page": 364,
        "text": "Type\nVector term\nClosest CUDA/NVIDIA\nGPU term\nComment\nProgram\nabstractions\nVectorized\nLoop\nGrid\nConcepts are similar, with the GPU using the less descriptive term\nChime\n\u2014\nBecause a vector instruction (PTX instruction) takes just 2 cycles\non Pascal to complete, a chime is short in GPUs. Pascal has two\nexecution units that support the most common floating-point\ninstructions that are used alternately, so the effective issue rate is 1\ninstruction every clock cycle\nMachine objects\nVector\nInstruction\nPTX Instruction\nA PTX instruction of a SIMD Thread is broadcast to all SIMD\nLanes, so it is similar to a vector instruction\nGather/\nScatter\nGlobal load/store (ld.\nglobal/st.global)\nAll GPU loads and stores are gather and scatter, in that each SIMD\nLane sends a unique address. It\u2019s up to the GPU Coalescing Unit to\nget unit-stride performance when addresses from the SIMD Lanes\nallow it\nMask\nRegisters\nPredicate Registers and\nInternal Mask Registers\nVector mask registers are explicitly part of the architectural state,\nwhile GPU mask registers are internal to the hardware. The GPU\nconditional hardware adds a new feature beyond predicate registers\nto manage masks dynamically\nProcessing and memory hardware\nVector\nProcessor\nMultithreaded SIMD\nProcessor\nThese are similar, but SIMD Processors tend to have many lanes,\ntaking a few clock cycles per lane to complete a vector, while\nvector architectures have few lanes and take many cycles to\ncomplete a vector. They are also multithreaded where vectors\nusually are not\nControl\nProcessor\nThread Block Scheduler\nThe closest is the Thread Block Scheduler that assigns Thread\nBlocks to a multithreaded SIMD Processor. But GPUs have no\nscalar-vector operations and no unit-stride or strided data transfer\ninstructions, which Control Processors often provide in vector\narchitectures\nScalar\nProcessor\nSystem Processor\nBecause of the lack of shared memory and the high latency to\ncommunicate over a PCI bus (1000s of clock cycles), the system\nprocessor in a GPU rarely takes on the same tasks that a scalar\nprocessor does in a vector architecture\nVector Lane\nSIMD Lane\nVery similar; both are essentially functional units with registers\nVector\nRegisters\nSIMD Lane Registers\nThe equivalent of a vector register is the same register in all 16\nSIMD Lanes of a multithreaded SIMD Processor running a thread\nof SIMD instructions. The number of registers per SIMD Thread is\nflexible, but the maximum is 256 in Pascal, so the maximum\nnumber of vector registers is 256\nMain\nMemory\nGPU Memory\nMemory for GPU versus system memory in vector case\nFigure 4.21 GPU equivalent to vector terms.\n332\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 365,
        "text": "The closest GPU term to a vectorized loop is Grid, and a PTX instruction is the\nclosest to a vector instruction because a SIMD Thread broadcasts a PTX instruction\nto all SIMD Lanes.\nWith respect to memory access instructions in the two architectures, all GPU\nloads are gather instructions and all GPU stores are scatter instructions. If data\naddresses of CUDA Threads refer to nearby addresses that fall into the same\ncache/memory block at the same time, the Address Coalescing Unit of the GPU\nwill ensure high memory bandwidth. The explicit unit-stride load and store instruc-\ntions of vector architectures versus the implicit unit stride of GPU programming is\nwhy writing efficient GPU code requires that programmers think in terms of SIMD\noperations, even though the CUDA programming model looks like MIMD.\nBecause CUDA Threads can generate their own addresses, strided as well as\ngather-scatter, addressing vectors are found in both vector architectures and GPUs.\nPC\nMask\nFU0\n0\n1\n2\n3\n4\n60\n61\n62\n63\n5\n6\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n7\nFU1\nFU2\nFU3\nMask\nMask\nInstruction\ncache\nInstruction register\nVector load/store unit\nMemory interface\nunit\nControl\nprocessor\nVector registers\nMask\nMask\nMask\nMask\nMask\n0\n0\n0\n0\n1\n1023\n1023\n1023\n1023\n1\n1\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n1\nInstruction\ncache\nInstruction register\nSIMD load/store unit\nRegisters\nAddress coalescing unit\nMemory interface unit\nSIMD thread scheduler\nDispatch unit\nPC\nPC\nPC\nPC\nFU0\nFU1\nFU2\nFU3\nFigure 4.22 A vector processor with four lanes on the left and a multithreaded SIMD Processor of a GPU with four\nSIMD Lanes on the right. (GPUs typically have 16 or 32 SIMD Lanes.) The Control Processor supplies scalar operands\nfor scalar-vector operations, increments addressing for unit and nonunit stride accesses to memory, and performs\nother accounting-type operations. Peak memory performance occurs only in a GPU when the Address Coalescing\nUnit can discover localized addressing. Similarly, peak computational performance occurs when all internal mask bits\nare set identically. Note that the SIMD Processor has one PC per SIMD Thread to help with multithreading.\n4.4\nGraphics Processing Units\n\u25a0\n333"
    },
    {
        "page": 366,
        "text": "As we mentioned several times, the two architectures take very different\napproaches to hiding memory latency. Vector architectures amortize it across all\nthe elements of the vector by having a deeply pipelined access, so you pay the\nlatency only once per vector load or store. Therefore vector loads and stores are\nlike a block transfer between memory and the vector registers. In contrast, GPUs\nhide memory latency using multithreading. (Some researchers are investigating\nadding multithreading to vector architectures to try to capture the best of both\nworlds.)\nWith respect to conditional branch instructions, both architectures implement\nthem using mask registers. Both conditional branch paths occupy time and/or space\neven when they do not store a result. The difference is that the vector compiler\nmanages mask registers explicitly in software while the GPU hardware and assem-\nbler manages them implicitly using branch synchronization markers and an inter-\nnal stack to save, complement, and restore masks.\nThe Control Processor of a vector computer plays an important role in the exe-\ncution of vector instructions. It broadcasts operations to all the Vector Lanes and\nbroadcasts a scalar register value for vector-scalar operations. It also does implicit\ncalculations that are explicit in GPUs, such as automatically incrementing memory\naddresses for unit-stride and nonunit-stride loads and stores. The Control Processor\nis missing in the GPU. The closest analogy is the Thread Block Scheduler, which\nassigns Thread Blocks (bodies of vector loop) to multithreaded SIMD Processors.\nThe runtime hardware mechanisms in a GPU that both generate addresses and then\ndiscover if they are adjacent, which is commonplace in many DLP applications, are\nlikely less power-efficient than using a Control Processor.\nThe scalar processor in a vector computer executes the scalar instructions of a\nvector program; that is, it performs operations that would be too slow to do in the\nvector unit. Although the system processor that is associated with a GPU is the\nclosest analogy to a scalar processor in a vector architecture, the separate address\nspaces plus transferring over a PCIe bus means thousands of clock cycles of\noverhead to use them together. The scalar processor can be slower than a vector\nprocessor for floating-point computations in a vector computer, but not by the same\nratio as the system processor versus a multithreaded SIMD Processor (given the\noverhead).\nTherefore each \u201cvector unit\u201d in a GPU must do computations that you would\nexpect to do using a scalar processor in a vector computer. That is, rather than\ncalculate on the system processor and communicate the results, it can be faster\nto disable all but one SIMD Lane using the predicate registers and built-in masks\nand do the scalar work with one SIMD Lane. The relatively simple scalar pro-\ncessor in a vector computer is likely to be faster and more power-efficient than\nthe GPU solution. If system processors and GPUs become more closely tied\ntogether in the future, it will be interesting to see if system processors can play\nthe same role as scalar processors do for vector and multimedia SIMD\narchitectures.\n334\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 367,
        "text": "Similarities and Differences Between Multimedia SIMD\nComputers and GPUs\nAt a high level, multicore computers with multimedia SIMD instruction extensions\ndo share similarities with GPUs. Figure 4.23 summarizes the similarities and\ndifferences.\nBoth are multiprocessors whose processors use multiple SIMD Lanes, although\nGPUs have more processors and many more lanes. Both use hardware multithread-\ning to improve processor utilization, although GPUs have hardware support for\nmany more threads. Both have roughly 2:1 performance ratios between peak perfor-\nmance of single-precision and double-precision floating-point arithmetic. Both use\ncaches, although GPUs use smaller streaming caches, and multicore computers\nuse large multilevel caches that try to contain whole working sets completely. Both\nuse a 64-bit address space, although the physical main memory is much smaller in\nGPUs. Both support memory protection at the page level as well as demand paging,\nwhich allows them to address far more memory than they have on board.\nIn addition to the large numerical differences in processors, SIMD Lanes, hard-\nware thread support, and cache sizes, there are many architectural differences. The\nscalar processor and multimedia SIMD instructions are tightly integrated in tradi-\ntional computers; they are separated by an I/O bus in GPUs, and they even have\nseparate main memories. The multiple SIMD Processors in a GPU use a single\naddress space and can support a coherent view of all memory on some systems\ngiven support from CPU vendors (such as the IBM Power9). Unlike GPUs, mul-\ntimedia SIMD instructions historically did not support gather-scatter memory\naccesses, which Section 4.7 shows is a significant omission.\nFeature\nMulticore with SIMD\nGPU\nSIMD Processors\n4\u20138\n8\u201332\nSIMD Lanes/Processor\n2\u20134\nup to 64\nMultithreading hardware support for SIMD Threads\n2\u20134\nup to 64\nTypical ratio of single-precision to double-precision performance\n2:1\n2:1\nLargest cache size\n40 MB\n4 MB\nSize of memory address\n64-bit\n64-bit\nSize of main memory\nup to 1024 GB\nup to 24 GB\nMemory protection at level of page\nYes\nYes\nDemand paging\nYes\nYes\nIntegrated scalar processor/SIMD Processor\nYes\nNo\nCache coherent\nYes\nYes on some systems\nFigure 4.23 Similarities and differences between multicore with multimedia SIMD extensions and recent GPUs.\n4.4\nGraphics Processing Units\n\u25a0\n335"
    },
    {
        "page": 368,
        "text": "Summary\nNow that the veil has been lifted, we can see that GPUs are really just multi-\nthreaded SIMD Processors, although they have more processors, more lanes\nper processor, and more multithreading hardware than do traditional multicore\ncomputers. For example, the Pascal P100 GPU has 56 SIMD Processors with\n64 lanes per processor and hardware support for 64 SIMD Threads. Pascal\nembraces instruction-level parallelism by issuing instructions from two SIMD\nThreads to two sets of SIMD Lanes. GPUs also have less cache memory\u2014Pas-\ncal\u2019s L2 cache is 4 MiB\u2014and it can be coherent with a cooperative distant scalar\nprocessor or distant GPUs.\nThe CUDA programming model wraps up all these forms of parallelism\naround a single abstraction, the CUDA Thread. Thus the CUDA programmer\ncan think of programming thousands of threads, although they are really executing\neach block of 32 threads on the many lanes of the many SIMD Processors. The\nCUDA programmer who wants good performance keeps in mind that these threads\nare organized in blocks and executed 32 at a time and that addresses need to be to\nadjacent addresses to get good performance from the memory system.\nAlthough we\u2019ve used CUDA and the NVIDIA GPU in this section, rest assured\nthat the same ideas are found in the OpenCL programming language and in GPUs\nfrom other companies.\nNow that you understand better how GPUs work, we reveal the real jargon.\nFigures 4.24 and 4.25 match the descriptive terms and definitions of this section\nwith the official CUDA/NVIDIA and AMD terms and definitions. We also include\nthe OpenCL terms. We believe the GPU learning curve is steep in part because of\nusing terms such as \u201cstreaming multiprocessor\u201d for the SIMD Processor, \u201cthread\nprocessor\u201d for the SIMD Lane, and \u201cshared memory\u201d for local memory\u2014\nespecially because local memory is not shared between SIMD Processors! We\nhope that this two-step approach gets you up that curve quicker, even if it\u2019s a\nbit indirect.\n4.5\nDetecting and Enhancing Loop-Level Parallelism\nLoops in programs are the fountainhead of many of the types of parallelism we\npreviously discussed here and in Chapter 5. In this section, we discuss compiler\ntechnology used for discovering the amount of parallelism that we can exploit\nin a program as well as hardware support for these compiler techniques. We define\nprecisely when a loop is parallel (or vectorizable), how a dependence can prevent a\nloop from being parallel, and techniques for eliminating some types of depen-\ndences. Finding and manipulating loop-level parallelism is critical to exploiting\nboth DLP and TLP, as well as the more aggressive static ILP approaches (e.g.,\nVLIW) that we examine in Appendix H.\n336\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 369,
        "text": "Loop-level parallelism is normally investigated at the source level or\nclose to it, while most analysis of ILP is done once instructions have been\ngenerated by the compiler. Loop-level analysis involves determining what\ndependences exist among the operands in a loop across the iterations of that\nloop. For now, we will consider only data dependences, which arise when an\noperand is written at some point and read at a later point. Name dependences\nalso exist and may be removed by the renaming techniques discussed in\nChapter 3.\nThe analysis of loop-level parallelism focuses on determining whether data\naccesses in later iterations are dependent on data values produced in earlier itera-\ntions; such dependence is called a loop-carried dependence. Most of the examples\nType\nMore\ndescriptive\nname used in\nthis book\nOfficial\nCUDA/\nNVIDIA\nterm\nShort explanation and AMD and\nOpenCL terms\nOfficial CUDA/NVIDIA definition\nProgram abstractions\nVectorizable\nloop\nGrid\nA vectorizable loop, executed on the\nGPU, made up of one or more \u201cThread\nBlocks\u201d (or bodies of vectorized loop)\nthat can execute in parallel. OpenCL\nname is \u201cindex range.\u201d AMD name is\n\u201cNDRange\u201d\nA Grid is an array of Thread Blocks\nthat can execute concurrently,\nsequentially, or a mixture\nBody of\nVectorized\nloop\nThread\nBlock\nA vectorized loop executed on a\nmultithreaded SIMD Processor, made\nup of one or more threads of SIMD\ninstructions. These SIMD Threads can\ncommunicate via local memory. AMD\nand OpenCL name is \u201cwork group\u201d\nA Thread Block is an array of CUDA\nThreads that execute concurrently\nand can cooperate and communicate\nvia shared memory and barrier\nsynchronization. A Thread Block has\na Thread Block ID within its Grid\nSequence of\nSIMD Lane\noperations\nCUDA\nThread\nA vertical cut of a thread of SIMD\ninstructions corresponding to one\nelement executed by one SIMD Lane.\nResult is stored depending on mask.\nAMD and OpenCL call a CUDA Thread\na \u201cwork item\u201d\nA CUDA Thread is a lightweight\nthread that executes a sequential\nprogram and that can cooperate with\nother CUDA Threads executing in\nthe same Thread Block. A CUDA\nThread has a thread ID within its\nThread Block\nMachine object\nA thread of\nSIMD\ninstructions\nWarp\nA traditional thread, but it contains just\nSIMD instructions that are executed on a\nmultithreaded SIMD Processor. Results\nare stored depending on a per-element\nmask. AMD name is \u201cwavefront\u201d\nA warp is a set of parallel CUDA\nThreads (e.g., 32) that execute the\nsame instruction together in a\nmultithreaded SIMT/SIMD\nProcessor\nSIMD\ninstruction\nPTX\ninstruction\nA single SIMD instruction executed\nacross the SIMD Lanes. AMD name is\n\u201cAMDIL\u201d or \u201cFSAIL\u201d instruction\nA PTX instruction specifies an\ninstruction executed by a CUDA\nThread\nFigure 4.24 Conversion from terms used in this chapter to official NVIDIA/CUDA and AMD jargon. OpenCL names\nare given in the book\u2019s definitions.\n4.5\nDetecting and Enhancing Loop-Level Parallelism\n\u25a0\n337"
    },
    {
        "page": 370,
        "text": "Type\nMore\ndescriptive\nname used in\nthis book\nOfficial\nCUDA/\nNVIDIA term\nShort explanation and AMD and\nOpenCL terms\nOfficial CUDA/NVIDIA definition\nProcessing hardware\nMultithreaded\nSIMD\nprocessor\nStreaming\nmultiprocessor\nMultithreaded SIMD Processor that\nexecutes thread of SIMD\ninstructions, independent of other\nSIMD Processors. Both AMD and\nOpenCL call it a \u201ccompute unit.\u201d\nHowever, the CUDA programmer\nwrites program for one lane rather\nthan for a \u201cvector\u201d of multiple SIMD\nLanes\nA streaming multiprocessor (SM) is\na multithreaded SIMT/SIMD\nProcessor that executes warps of\nCUDA Threads. A SIMT program\nspecifies the execution of one\nCUDA Thread, rather than a vector\nof multiple SIMD Lanes\nThread Block\nScheduler\nGiga Thread\nEngine\nAssigns multiple bodies of\nvectorized loop to multithreaded\nSIMD Processors. AMD name is\n\u201cUltra-Threaded Dispatch Engine\u201d\nDistributes and schedules Thread\nBlocks of a grid to streaming\nmultiprocessors as resources\nbecome available\nSIMD Thread\nscheduler\nWarp\nscheduler\nHardware unit that schedules and\nissues threads of SIMD instructions\nwhen they are ready to execute;\nincludes a scoreboard to track SIMD\nThread execution. AMD name is\n\u201cWork Group Scheduler\u201d\nA warp scheduler in a streaming\nmultiprocessor schedules warps for\nexecution when their next\ninstruction is ready to execute\nSIMD Lane\nThread\nprocessor\nHardware SIMD Lane that executes\nthe operations in a thread of SIMD\ninstructions on a single element.\nResults are stored depending on\nmask. OpenCL calls it a \u201cprocessing\nelement.\u201d AMD name is also \u201cSIMD\nLane\u201d\nA thread processor is a datapath and\nregister file portion of a streaming\nmultiprocessor that executes\noperations for one or more lanes of a\nwarp\nMemory hardware\nGPU Memory\nGlobal\nmemory\nDRAM memory accessible by all\nmultithreaded SIMD Processors in a\nGPU. OpenCL calls it \u201cglobal\nmemory\u201d\nGlobal memory is accessible by all\nCUDA Threads in any Thread Block\nin any grid; implemented as a region\nof DRAM, and may be cached\nPrivate\nmemory\nLocal memory\nPortion of DRAM memory private\nto each SIMD Lane. Both AMD and\nOpenCL call it \u201cprivate memory\u201d\nPrivate \u201cthread-local\u201d memory for a\nCUDA Thread; implemented as a\ncached region of DRAM\nLocal memory\nShared\nmemory\nFast local SRAM for one\nmultithreaded SIMD Processor,\nunavailable to other SIMD\nProcessors. OpenCL calls it \u201clocal\nmemory.\u201d AMD calls it \u201cgroup\nmemory\u201d\nFast SRAM memory shared by the\nCUDA Threads composing a Thread\nBlock, and private to that Thread\nBlock. Used for communication\namong CUDA Threads in a Thread\nBlock at barrier synchronization\npoints\nSIMD Lane\nregisters\nRegisters\nRegisters in a single SIMD Lane\nallocated across body of vectorized\nloop. AMD also calls them\n\u201cregisters\u201d\nPrivate registers for a CUDA\nThread; implemented as\nmultithreaded register file for certain\nlanes of several warps for each\nthread processor\nFigure 4.25 Conversion from terms used in this chapter to official NVIDIA/CUDA and AMD jargon. Note that our\ndescriptive terms \u201clocal memory\u201d and \u201cprivate memory\u201d use the OpenCL terminology. NVIDIA uses SIMT (single-\ninstruction multiple-thread) rather than SIMD to describe a streaming multiprocessor. SIMT is preferred over SIMD\nbecause the per-thread branching and control flow are unlike any SIMD machine."
    },
    {
        "page": 371,
        "text": "we considered in Chapters 2 and 3 had no loop-carried dependences and thus are\nloop-level parallel. To see that a loop is parallel, let us first look at the source\nrepresentation:\nfor (i=999; i>=0; i=i-1)\nx[i] = x[i] + s;\nIn this loop, the two uses of x[i] are dependent, but this dependence is within a\nsingle iteration and is not loop-carried. There is a loop-carried dependence between\nsuccessive uses of i in different iterations, but this dependence involves an induc-\ntion variable that can be easily recognized and eliminated. We saw examples of\nhow to eliminate dependences involving induction variables during loop unrolling\nin Section 2.2 of Chapter 2, and we will look at additional examples later in this\nsection.\nBecause finding loop-level parallelism involves recognizing structures\nsuch as loops, array references, and induction variable computations, a com-\npiler can do this analysis more easily at or near the source level, in contrast to\nthe machine-code level. Let\u2019s look at a more complex example.\nExample\nConsider a loop like this one:\nfor (i=0; i<100; i=i+1) {\nA[i+1]\n=\nA[i]\n+\nC[i];\n/* S1 */\nB[i+1]\n=\nB[i]\n+\nA[i+1]; /* S2 */\n}\nAssume that A, B, and C are distinct, nonoverlapping arrays. (In practice, the arrays\nmay sometimes be the same or may overlap. Because the arrays may be passed as\nparameters to a procedure that includes this loop, determining whether arrays over-\nlap or are identical often requires sophisticated, interprocedural analysis of the pro-\ngram.) What are the data dependences among the statements S1 and S2 in\nthe loop?\nAnswer\nThere are two different dependences:\n1. S1 uses a value computed by S1 in an earlier iteration, because iteration i com-\nputes A[i+1], which is read in iteration i+1. The same is true of S2 for B[i] and\nB[i+1].\n2. S2 uses the value A[i+1] computed by S1 in the same iteration.\nThese two dependences are distinct and have different effects. To see how they\ndiffer, let\u2019s assume that only one of these dependences exists at a time. Because\nthe dependence of statement S1 is on an earlier iteration of S1, this dependence\nis loop-carried. This dependence forces successive iterations of this loop to execute\nin series.\n4.5\nDetecting and Enhancing Loop-Level Parallelism\n\u25a0\n339"
    },
    {
        "page": 372,
        "text": "The second dependence (S2 depending on S1) is within an iteration\nand is not loop-carried. Thus, if this were the only dependence, multiple iter-\nations of the loop would execute in parallel, as long as each pair of state-\nments in an iteration were kept in order. We saw this type of dependence\nin an example in Section 2.2, where unrolling could expose the parallelism.\nThese intra-loop dependences are common; for example, a sequence of vector\ninstructions that uses chaining exhibits exactly this sort of dependence.\nIt is also possible to have a loop-carried dependence that does not prevent\nparallelism, as the next example shows.\nExample\nConsider a loop like this one:\nfor (i=0; i<100; i=i+1) {\nA[i]\n=\nA[i]\n+\nB[i];\n/* S1 */\nB[i+1]\n=\nC[i]\n+\nD[i]; /* S2 */\n}\nWhat are the dependences between S1 and S2? Is this loop parallel? If not, show\nhow to make it parallel.\nAnswer\nStatement S1 uses the value assigned in the previous iteration by statement S2, so\nthere is a loop-carried dependence between S2 and S1. Despite this loop-carried\ndependence, this loop can be made parallel. Unlike the earlier loop, this depen-\ndence is not circular; neither statement depends on itself, and although S1 depends\non S2, S2 does not depend on S1. A loop is parallel if it can be written without a\ncycle in the dependences because the absence of a cycle means that the depen-\ndences give a partial ordering on the statements.\nAlthough there are no circular dependences in the preceding loop, it must be\ntransformed to conform to the partial ordering and expose the parallelism. Two\nobservations are critical to this transformation:\n1. There is no dependence from S1 to S2. If there were, then there would be a\ncycle in the dependences and the loop would not be parallel. Because this other\ndependence is absent, interchanging the two statements will not affect the exe-\ncution of S2.\n2. On the first iteration of the loop, statement S2 depends on the value of B[0]\ncomputed prior to initiating the loop.\nThese two observations allow us to replace the preceding loop with the following\ncode sequence:\nA[0] = A[0] + B[0];\nfor (i=0; i<99; i=i+1) {\n340\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 373,
        "text": "B[i+1]\n=\nC[i]\n+\nD[i];\nA[i+1]\n=\nA[i+1]\n+\nB[i+1];\n}\nB[100]\n=\nC[99]\n+\nD[99];\nThe dependence between the two statements is no longer loop-carried so that\niterations of the loop may be overlapped, provided the statements in each iteration\nare kept in order.\nOur analysis needs to begin by finding all loop-carried dependences. This depen-\ndence information is inexact, in the sense that it tells us that such dependence may\nexist. Consider the following example:\nfor\n(i=0;i<100;i=i+1) {\nA[i] =\nB[i]\n+\nC[i]\nD[i] =\nA[i]\n*\nE[i]\n}\nThe second reference to A in this example need not be translated to a load\ninstruction because we know that the value is computed and stored by the previous\nstatement. Thus the second reference to A can simply be a reference to the register\ninto which A was computed. Performing this optimization requires knowing that\nthe two references are always to the same memory address and that there is no\nintervening access to the same location. Normally, data dependence analysis tells\nthat only one reference may depend on another; a more complex analysis is\nrequired to determine that two references must be to the exact same address. In\nthe preceding example, a simple version of this analysis suffices because the\ntwo references are in the same basic block.\nOften loop-carried dependences are in the form of a recurrence. A recurrence\noccurs when a variable is defined based on the value of that variable in an earlier\niteration, usually the one immediately preceding, as in the following code fragment:\nfor\n(i=1;i<100;i=i+1) {\nY[i] =\nY[i-1]\n+\nY[i];\n}\nDetecting a recurrence can be important for two reasons: some architectures\n(especially vector computers) have special support for executing recurrences,\nand in an ILP context, it may still be possible to exploit a fair amount of parallelism.\nFinding Dependences\nClearly, finding the dependences in a program is important both to determine\nwhich loops might contain parallelism and to eliminate name dependences. The\ncomplexity of dependence analysis arises also because of the presence of arrays\n4.5\nDetecting and Enhancing Loop-Level Parallelism\n\u25a0\n341"
    },
    {
        "page": 374,
        "text": "and pointers in languages such as C or C++, or pass-by-reference parameter pass-\ning in Fortran. Because scalar variable references explicitly refer to a name, they\ncan usually be analyzed quite easily with aliasing because of pointers and reference\nparameters causing some complications and uncertainty in the analysis.\nHow does the compiler detect dependences in general? Nearly all dependence\nanalysis algorithms work on the assumption that array indices are affine. In sim-\nplest terms, a one-dimensional array index is affine if it can be written in the form\na\u0001i+b, where a and b are constants and i is the loop index variable. The index of a\nmultidimensional array is affine if the index in each dimension is affine. Sparse\narray accesses, which typically have the form x[y[i]], are one of the major exam-\nples of nonaffine accesses.\nDetermining whether there is a dependence between two references to the same\narray in a loop is thus equivalent to determining whether two affine functions can\nhave the identical value for different indices between the bounds of the loop. For\nexample, suppose we have stored to an array element with index value a\u0001i+b and\nloaded from the same array with index value c\u0001i+d, where i is the for-loop index\nvariable that runs from m to n. A dependence exists if two conditions hold:\n1. There are two iteration indices, j and k, that are both within the limits of the for-\nloop. That is, m \u0006 j \u0006 n, m \u0006 k \u0006 n.\n2. The loop stores into an array element indexed by a\u0001j+b and later fetches from\nthat same array element when it is indexed by c\u0001k+d, that is, a\u0001j+b\u00bcc\u0001k+d.\nIn general, we cannot determine whether dependence exists at compile time. For\nexample, the values of a, b, c, and d may not be known (they could be values in\nother arrays), making it impossible to tell if a dependence exists. In other cases, the\ndependence testing may be very expensive but decidable at compile time; for\nexample, the accesses may depend on the iteration indices of multiple nested loops.\nMany programs, however, contain primarily simple indices where a, b, c, and d are\nall constants. For these cases, it is possible to devise reasonable compile time tests\nfor dependence.\nAs an example, a simple and sufficient test for the absence of a dependence is\nthe greatest common divisor (GCD) test. It is based on the observation that if a\nloop-carried dependence exists, then GCD (c, a) must divide (d \u2013 b). (Recall that\nan integer, x, divides another integer, y, if we get an integer quotient when we do\nthe division y/x and there is no remainder.)\nExample\nUse the GCD test to determine whether dependences exist in the following loop:\nfor\n(i=0; i<100; i=i+1) {\nX[2*i+3]\n=\nX[2*i]\n*\n5.0;\n}\nAnswer\nGiven the values a\u00bc2, b\u00bc3, c\u00bc2, and d\u00bc0, then GCD(a, c)\u00bc2, and d\u0003b\u00bc\u00033.\nBecause 2 does not divide \u00033, no dependence is possible.\n342\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 375,
        "text": "The GCD test is sufficient to guarantee that no dependence exists; however, there\nare cases where the GCD test succeeds but no dependence exists. This can arise, for\nexample, because the GCD test does not consider the loop bounds.\nIn general, determining whether a dependence actually exists is NP-complete.\nIn practice, however, many common cases can be analyzed precisely at low cost.\nRecently, approaches using a hierarchy of exact tests increasing in generality and\ncost have been shown to be both accurate and efficient. (A test is exact if it pre-\ncisely determines whether a dependence exists. Although the general case is\nNP-complete, there exist exact tests for restricted situations that are much cheaper.)\nIn addition to detecting the presence of a dependence, a compiler wants to clas-\nsify the type of dependence. This classification allows a compiler to recognize\nname dependences and eliminate them at compile time by renaming and copying.\nExample\nThe following loop has multiple types of dependences. Find all the true depen-\ndences, output dependences, and antidependences, and eliminate the output\ndependences and antidependences by renaming.\nfor (i=0; i<100; i=i+1) {\nY[i]\n=\nX[i] / c;\n/*\nS1\n*/\nX[i]\n=\nX[i] + c;\n/*\nS2\n*/\nZ[i]\n=\nY[i] + c;\n/*\nS3\n*/\nY[i]\n=\nc - Y[i];\n/*\nS4\n*/\n}\nAnswer\nThe following dependences exist among the four statements:\n1. There are true dependences from S1 to S3 and from S1 to S4 because of Y[i].\nThese are not loop-carried, so they do not prevent the loop from being consid-\nered parallel. These dependences will force S3 and S4 to wait for S1 to\ncomplete.\n2. There is an antidependence from S1 to S2, based on X[i].\n3. There is an antidependence from S3 to S4 for Y[i].\n4. There is an output dependence from S1 to S4, based on Y[i].\nThe following version of the loop eliminates these false (or pseudo) dependences.\nfor (i=0; i<100; i=i+1 {\nT[i] = X[i] / c; /* Y renamed to T to remove output\ndependence */\nX1[i] = X[i] + c;/* X renamed to X1 to remove\nantidependence */\nZ[i] = T[i] + c;/* Y renamed to T to remove\nantidependence */\nY[i] = c - T[i];\n}\n4.5\nDetecting and Enhancing Loop-Level Parallelism\n\u25a0\n343"
    },
    {
        "page": 376,
        "text": "After the loop, the variable X has been renamed X1. In code that follows the\nloop, the compiler can simply replace the name X by X1. In this case, renaming\ndoes not require an actual copy operation, as it can be done by substituting names\nor by register allocation. In other cases, however, renaming will require copying.\nDependence analysis is a critical technology for exploiting parallelism, as well as\nfor the transformation-like blocking that Chapter 2 covers. For detecting loop-level\nparallelism, dependence analysis is the basic tool. Effectively compiling programs\nfor vector computers, SIMD computers, or multiprocessors depends critically on\nthis analysis. The major drawback of dependence analysis is that it applies only\nunder a limited set of circumstances, namely, among references within a single\nloop nest and using affine index functions. Thus there are many situations where\narray-oriented dependence analysis cannot tell us what we want to know; for\nexample, analyzing accesses done with pointers, rather than with array indices\ncan be much harder. (This is one reason why Fortran is still preferred over C\nand C++ for many scientific applications designed for parallel computers.) Simi-\nlarly, analyzing references across procedure calls is extremely difficult. Thus,\nwhile analysis of code written in sequential languages remains important, we also\nneed approaches such as OpenMP and CUDA that write explicitly parallel loops.\nEliminating Dependent Computations\nAs previously mentioned, one of the most important forms of dependent compu-\ntations is a recurrence. A dot product is a perfect example of a recurrence:\nfor (i=9999; i>=0; i=i-1)\nsum\n=\nsum + x[i] * y[i];\nThis loop is not parallel because it has a loop-carried dependence on the var-\niable sum. We can, however, transform it to a set of loops, one of which is\ncompletely parallel and the other partly parallel. The first loop will execute the\ncompletely parallel portion of this loop. It looks like this:\nfor (i=9999;\ni>=0;\ni=i-1)\nsum[i]\n=\nx[i]\n*\ny[i];\nNotice that sum has been expanded from a scalar into a vector quantity (a trans-\nformation called scalar expansion) and that this transformation makes this new\nloop completely parallel. When we are done, however, we need to do the reduce\nstep, which sums up the elements of the vector. It looks like this:\nfor (i=9999; i>=0; i=i-1)\nfinalsum = finalsum + sum[i];\nAlthough this loop is not parallel, it has a very specific structure called a reduc-\ntion. Reductions are common in linear algebra, and as we will see in Chapter 6,\n344\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 377,
        "text": "they are also a key part of the primary parallelism primitive MapReduce used in\nwarehouse-scale computers. In general, any function can be used as a reduction\noperator, and common cases include operators such as max and min.\nReductions are sometimes handled by special hardware in a vector and SIMD\narchitecture that allows the reduce step to be done much faster than it could be done\nin scalar mode. These work by implementing a technique similar to what can be\ndone in a multiprocessor environment. While the general transformation works\nwith any number of processors, suppose for simplicity we have 10 processors.\nIn the first step of reducing the sum, each processor executes the following (with\np as the processor number ranging from 0 to 9):\nfor (i=999; i>=0; i=i-1)\nfinalsum[p] = finalsum[p] + sum[i+1000*p];\nThis loop, which sums up 1000 elements on each of the 10 processors, is\ncompletely parallel. A simple scalar loop can then complete the summation of\nthe last 10 sums. Similar approaches are used in vector processors and SIMD\nProcessors.\nIt is important to observe that the preceding transformation relies on associa-\ntivity of addition. Although arithmetic with unlimited range and precision is asso-\nciative, computer arithmetic is not associative, for either integer arithmetic,\nbecause of limited range, or floating-point arithmetic, because of both range\nand precision. Thus using these restructuring techniques can sometimes lead to\nerroneous behavior, although such occurrences are rare. For this reason, most\ncompilers require that optimizations that rely on associativity be explicitly enabled.\n4.6\nCross-Cutting Issues\nEnergy and DLP: Slow and Wide Versus Fast and Narrow\nA fundamental power advantage of data-level parallel architectures comes from the\nenergyequationin Chapter 1.Assumingample data-level parallelism,theperformance\nisthesameifwehalvetheclockrateanddoubletheexecutionresources:twicethenum-\nberoflanesforavectorcomputer,widerregistersandALUsformultimediaSIMD,and\nmore SIMD Lanes for GPUs. If we can lower the voltage while dropping the clock\nrate,wecanactuallyreduceenergyaswellasthepowerforthecomputationwhilemain-\ntaining the same peak performance. Thus GPUs tend to have lower clock rates than\nsystem processors, which rely on high clock rates for performance (see Section 4.7).\nCompared to out-of-order processors, DLP processors can have simpler control\nlogic to launch a large number of operations per clock cycle; for example, the con-\ntrol is identical for all lanes in vector processors, and there is no logic to decide on\nmultiple instruction issues or speculative execution logic. They also fetch and\ndecode far fewer instructions. Vector architectures can also make it easier to turn\noff unused portions of the chip. Each vector instruction explicitly describes all the\nresources it needs for a number of cycles when the instruction issues.\n4.6\nCross-Cutting Issues\n\u25a0\n345"
    },
    {
        "page": 378,
        "text": "Banked Memory and Graphics Memory\nSection 4.2 noted the importance of substantial memory bandwidth for vector\narchitectures to support unit stride, nonunit stride, and gather-scatter accesses.\nTo achieve the highest memory performance, stacked DRAMs are used in the\ntop-end GPUs from AMD and NVIDIA. Intel also uses stacked DRAM in its Xeon\nPhi product. Also known as high bandwidth memory (HBM, HBM2), the memory\nchips are stacked and placed in the same package as the processing chip. The exten-\nsive width (typically 1024\u20134096 data wires) provides high bandwidth, while plac-\ning the memory chips in the same package as the processor chip reduces latency\nand power consumption. The capacity of stacked DRAM is typically 8\u201332 GB.\nGiven all the potential demands on the memory from both the computation\ntasks and the graphics acceleration tasks, the memory system could see a large\nnumber of uncorrelated requests. Unfortunately, this diversity hurts memory per-\nformance. To cope, the GPU\u2019s memory controller maintains separate queues of\ntraffic bound for different banks, waiting until there is enough traffic to justify\nopening a row and transferring all requested data at once. This delay improves\nbandwidth but stretches latency, and the controller must ensure that no processing\nunits starve while waiting for data, for otherwise neighboring processors could\nbecome idle. Section 4.7 shows that gather-scatter techniques and memory-\nbank-aware access techniques can deliver substantial increases in performance\nversus conventional cache-based architectures.\nStrided Accesses and TLB Misses\nOne problem with strided accesses is how they interact with the translation looka-\nside buffer (TLB) for virtual memory in vector architectures or GPUs. (GPUs also\nuse TLBs for memory mapping.) Depending on how the TLB is organized and the\nsize of the array being accessed in memory, it is even possible to get one TLB miss\nfor every access to an element in the array! The same type of collision can happen\nwith caches, but the performance impact is probably less.\n4.7\nPutting It All Together: Embedded Versus Server GPUs\nand Tesla Versus Core i7\nGiven the popularity of graphics applications, GPUs are now found in both mobile\nclients and traditional servers and heavy-duty desktop computers. Figure 4.26 lists\nthe key characteristics of the NVIDIA Tegra Parker system on a chip for embedded\nclients, which is popular in automobiles, and the Pascal GPU for servers. GPU\nserver engineers hope to be able to do live animation within five years after a movie\nis released. GPU-embedded engineers in turn want to do what a server or game\nconsole does today on their hardware within five more years.\nThe NVIDIA Tegra P1 has six ARMv8 cores and a smaller Pascal GPU (capa-\nble of 750 GFLOPS) and 50 GB/s of memory bandwidth. It is the key component\n346\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 379,
        "text": "of the NVIDIA DRIVE PX2 computing platform that is used in cars for autono-\nmous driving. The NVIDIA Tegra X1 is the previous generation and is used in\nseveral high-end tablets, such as the Google Pixel C and the NVIDIA Shield\nTV. It has a Maxwell-class GPU capable of 512 GFLOPS.\nThe NVIDIA Tesla P100 is the Pascal GPU discussed extensively in this chap-\nter. (Tesla is Nvidia\u2019s name for products targeting general-purpose computing.)\nThe clock rate is 1.4 GHz, and it includes 56 SIMD Processors. The path to\nHBM2 memory is 4096-bits wide, and it transfers data on both the rising and\nfalling edge of a 0.715 GHz clock, which means a peak memory bandwidth\nof 732 GB/s. It connects to the host system processor and memory via a PCI\nExpress\u000116 Gen 3 link, which has a peak bidirectional rate of 32 GB/s.\nAll physical characteristics of the P100 die are impressively large: it contains\n15.3 billion transistors, the die size is 645 mm2 in a 16-nm TSMC process, and the\ntypical power is 300 W.\nComparison of a GPU and a MIMD With Multimedia SIMD\nA group of Intel researchers published a paper (Lee et al., 2010) comparing a quad-\ncore Intel i7 with multimedia SIMD extensions to the Tesla GTX 280. Although\nthe study did not compare the latest versions of CPUs and GPUs, it was the most\nNVIDIA Tegra 2\nNVIDIA Tesla P100\nMarket\nAutomotive, Embedded,\nConsole, Tablet\nDesktop, server\nSystem processor\nSix-Core ARM (2 Denver2\n+4 A57)\nNot applicable\nSystem interface\nNot applicable\nPCI Express\u000116 Gen 3\nSystem interface\nbandwidth\nNot applicable\n16 GB/s (each direction),\n32 GB/s (total)\nClock rate\n1.5 GHz\n1.4 GHz\nSIMD multiprocessors\n2\n56\nSIMD Lanes/SIMD\nmultiprocessor\n128\n64\nMemory interface\n128-bit LP-DDR4\n4096-bit HBM2\nMemory bandwidth\n50 GB/s\n732 GB/s\nMemory capacity\nup to 16 GB\nup to 16 GB\nTransistors\n7 billion\n15.3 billion\nProcess\nTSMC 16 nm FinFET\nTSMC 16 nm FinFET\nDie area\n147 mm2\n645 mm2\nPower\n20 W\n300 W\nFigure 4.26 Key features of the GPUs for embedded clients and servers.\nComparison of a GPU and a MIMD With Multimedia SIMD\n\u25a0\n347"
    },
    {
        "page": 380,
        "text": "in-depth comparison of the two styles in that it explained the reasons behind the\ndifferences in performance. Moreover, the current versions of these architectures\nshare many similarities to the ones in the study.\nFigure 4.27 lists the characteristics of the two systems. Both products were pur-\nchased in the fall of 2009. The Core i7 is in Intel\u2019s 45-nanometer semiconductor\ntechnology, while the GPU is in TSMC\u2019s 65-nanometer technology. Although it\nmight have been fairer to have a comparison done by a neutral party or by both\ninterested parties, the purpose of this section is not to determine how much faster\none product is than the other, but to try to understand the relative value of features\nof these two contrasting architecture styles.\nThe rooflines of the Core i7 920 and GTX 280 in Figure 4.28 illustrate the dif-\nferences in the computers. The 920 has a slower clock rate than the 960 (2.66 GHz\nvs. 3.2 GHz), but the rest of the system is the same. Not only does the GTX 280\nhave much higher memory bandwidth and double-precision floating-point perfor-\nmance, but also its double-precision ridge point is considerably to the left. As pre-\nviously mentioned, it is much easier to hit peak computational performance the\nfurther the ridge point of the roofline is to the left. The double-precision ridge point\nCore i7-960\nGTX 280\nRatio 280/i7\nNumber of processing elements (cores or SMs)\n4\n30\n7.5\nClock frequency (GHz)\n3.2\n1.3\n0.41\nDie size\n263\n576\n2.2\nTechnology\nIntel 45 nm\nTSMC 65 nm\n1.6\nPower (chip, not module)\n130\n130\n1.0\nTransistors\n700 M\n1400 M\n2.0\nMemory bandwidth (GB/s)\n32\n141\n4.4\nSingle-precision SIMD width\n4\n8\n2.0\nDouble-precision SIMD width\n2\n1\n0.5\nPeak single-precision scalar FLOPS (GFLOP/S)\n26\n117\n4.6\nPeak single-precision SIMD FLOPS (GFLOP/S)\n102\n311\u2013933\n3.0\u20139.1\n(SP 1 add or multiply)\nN.A.\n(311)\n(3.0)\n(SP 1 instruction fused multiply-adds)\nN.A.\n(622)\n(6.1)\n(Rare SP dual issue fused multiply-add and multiply)\nN.A.\n(933)\n(9.1)\nPeak double-precision SIMD FLOPS (GFLOP/S)\n51\n78\n1.5\nFigure 4.27 Intel Core i7-960 and NVIDIA GTX 280. The rightmost column shows the ratios of GTX 280 to Core i7. For\nsingle-precision SIMD FLOPS on the GTX 280, the higher speed (933) comes from a very rare case of dual issuing of\nfused multiply-add and multiply. More reasonable is 622 for single fused multiply-adds. Note that these memory\nbandwidths are higher than in Figure 4.28 because these are DRAM pin bandwidths and those in Figure 4.28 are\nat the processors as measured by a benchmark program. From Table 2 in Lee, W.V., et al., 2010. Debunking the\n100\u0001 GPU vs. CPU myth: an evaluation of throughput computing on CPU and GPU. In: Proc. 37th Annual Int\u2019l. Sym-\nposium on Computer Architecture (ISCA), June 19\u201323, 2010, Saint-Malo, France.\n348\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 381,
        "text": "128\n64\n32\n16\n8\n4\n2\n1\n128\n64\n32\n16\n8\n4\n2\n1\nCore i7 920\n(Nehalem)\n1024\n512\n256\n128\n64\n32\n16\n8\n1\n2\nArithmetic intensity\n4\n8\n16\n1/8\n1/4\n1/2\n1\n2\nArithmetic intensity\n4\n8\n16\n1/8\n1/4\n1/2\n1\n2\nArithmetic intensity\n4\n8\n16\n1/8\n1/4\n1/2\n1\n2\nArithmetic intensity\n4\n8\n16\n1/8\n1/4\n1/2\nCore i7 920\n(Nehalem)\nNVIDIA GTX280\n1024\n512\n256\n128\n64\n32\n16\n8\nNVIDIA GTX280\nDouble-precision GFLOP/s\nDouble-precision GFLOP/s\nSingle-precision GFLOP/s\nSingle-precision GFLOP/s\nPeak =42.66 GFLOP/s\nStream= 16.4GB/s\nStream =127 GB/s\n78 GF/s\n78 GF/s\nStream=127GB/s\n624 GF/s\nStream = 16.4GB/s\n85.33GF/s\n42.66GF/s\nFigure 4.28 Roofline model (Williams et al. 2009). These rooflines show double-precision floating-point perfor-\nmance in the top row and single-precision performance in the bottom row. (The DP FP performance ceiling is also\nin the bottom row to give perspective.) The Core i7 920 on the left has a peak DP FP performance of 42.66 GFLOP/s, a\nSP FP peak of 85.33 GFLOP/s, and a peak memory bandwidth of 16.4 GB/s. The NVIDIA GTX 280 has a DP FP peak of\n78 GFLOP/s, SP FP peak of 624 GFLOP/s, and 127 GB/s of memory bandwidth. The dashed vertical line on the left\nrepresents an arithmetic intensity of 0.5 FLOP/byte. It is limited by memory bandwidth to no more than\n8 DP GFLOP/s or 8 SP GFLOP/s on the Core i7. The dashed vertical line to the right has an arithmetic intensity of\n4 FLOP/byte. It is limited only computationally to 42.66 DP GFLOP/s and 64 SP GFLOP/s on the Core i7 and to 78\nDP GFLOP/s and 512 DP GFLOP/s on the GTX 280. To hit the highest computation rate on the Core i7, you need\nto use all 4 cores and SSE instructions with an equal number of multiplies and adds. For the GTX 280, you need\nto use fused multiply-add instructions on all multithreaded SIMD Processors.\nComparison of a GPU and a MIMD With Multimedia SIMD\n\u25a0\n349"
    },
    {
        "page": 382,
        "text": "is 0.6 for the GTX 280 versus 2.6 for the Core i7. For single-precision performance,\nthe ridge point moves far to the right, as it\u2019s considerably harder to hit the roof of\nsingle-precision performance because it is so much higher. Note that the arithmetic\nintensity of the kernel is based on the bytes that go to main memory, not the bytes\nthat go to cache memory. Thus caching can change the arithmetic intensity of a\nkernel on a particular computer, presuming that most references really go to the\ncache. The Rooflines help explain the relative performance in this case study. Note\nalso that this bandwidth is for unit-stride accesses in both architectures. Real\ngather-scatter addresses that are not coalesced are slower on the GTX 280 and\non the Core i7, as we will see.\nThe researchers said that they selected the benchmark programs by analyzing\nthe computational and memory characteristics of four recently proposed bench-\nmark suites and then \u201cformulated the set of throughput computing kernels that cap-\nture these characteristics.\u201d Figure 4.29 describes these 14 kernels, and Figure 4.30\nshows the performance results, with larger numbers meaning faster.\nGiven that the raw performance specifications of the GTX 280 vary from 2.5\u0001\nslower (clock rate) to 7.5\u0001 faster (cores per chip) while the performance varies\nfrom 2.0\u0001 slower (Solv) to 15.2\u0001 faster (GJK), the Intel researchers explored\nthe reasons for the differences:\n\u25a0\nMemory bandwidth. The GPU has 4.4\u0001 the memory bandwidth, which helps\nexplain why LBM and SAXPY run 5.0 and 5.3\u0001 faster; their working sets are\nhundreds of megabytes and thus don\u2019t fit into the Core i7 cache. (To access\nmemory intensively, they did not use cache blocking on SAXPY.) Thus the\nslope of the rooflines explains their performance. SpMV also has a large work-\ning set, but it only runs 1.9\u0001 because the double-precision floating point of the\nGTX 280 is just 1.5\u0001 faster than the Core i7.\n\u25a0\nCompute bandwidth. Five of the remaining kernels are compute bound:\nSGEMM, Conv, FFT, MC, and Bilat. The GTX is faster by 3.9, 2.8, 3.0,\n1.8, and 5.7, respectively. The first three of these use single-precision\nfloating-point arithmetic, and GTX 280 single-precision is 3\u20136\u0001 faster.\n(The 9\u0001 faster than the Core i7 as shown in Figure 4.27 occurs only in the very\nspecial case when the GTX 280 can issue a fused multiply-add and a multiply\nper clock cycle.) MC uses double-precision, which explains why it\u2019s just 1.8\u0001\nfaster since DP performance is only 1.5\u0001 faster. Bilat uses transcendental func-\ntions, which the GTX 280 supports directly (see Figure 4.17). The Core i7\nspends two-thirds of its time calculating transcendental functions, so the\nGTX 280 is 5.7\u0001 faster. This observation helps point out the value of hardware\nsupport for operations that occur in your workload: double-precision floating-\npoint and perhaps even transcendentals.\n\u25a0\nCache benefits. Ray casting (RC) is only 1.6\u0001 faster on the GTX because\ncache blocking with the Core i7 caches prevents it from becoming memory\nbandwidth bound, as it is on GPUs. Cache blocking can help Search, too. If\nthe index trees are small so that they fit into the cache, the Core i7 is twice\n350\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 383,
        "text": "as fast. Larger index trees make them memory bandwidth bound. Overall, the\nGTX 280 runs Search 1.8\u0001 faster. Cache blocking also helps Sort. While most\nprogrammers wouldn\u2019t run Sort on a SIMD Processor, it can be written with a\n1-bit Sort primitive called split. However, the split algorithm executes many\nmore instructions than a scalar sort does. As a result, the GTX 280 runs only\n0.8\u0001 as fast as the Core i7. Note that caches also help other kernels on the Core\ni7 because cache blocking allows SGEMM, FFT, and SpMV to become\nKernel\nApplication\nSIMD\nTLP\nCharacteristics\nSGEMM (SGEMM)\nLinear algebra\nRegular\nAcross 2D\ntiles\nCompute bound after tiling\nMonte Carlo (MC)\nComputational\nfinance\nRegular\nAcross\npaths\nCompute bound\nConvolution (Conv)\nImage analysis\nRegular\nAcross\npixels\nCompute bound; BW bound for small filters\nFFT (FFT)\nSignal\nprocessing\nRegular\nAcross\nsmaller\nFFTs\nCompute bound or BW bound depending\non size\nSAXPY (SAXPY)\nDot product\nRegular\nAcross\nvector\nBW bound for large vectors\nLBM (LBM)\nTime migration\nRegular\nAcross\ncells\nBW bound\nConstraint solver (Solv)\nRigid body\nphysics\nGather/\nScatter\nAcross\nconstraints\nSynchronization bound\nSpMV (SpMV)\nSparse solver\nGather\nAcross\nnonzero\nBW bound for typical large matrices\nGJK (GJK)\nCollision\ndetection\nGather/\nScatter\nAcross\nobjects\nCompute bound\nSort (Sort)\nDatabase\nGather/\nScatter\nAcross\nelements\nCompute bound\nRay casting (RC)\nVolume\nrendering\nGather\nAcross\nrays\n4\u20138 MB first level working set; over\n500 MB last level working set\nSearch (Search)\nDatabase\nGather/\nScatter\nAcross\nqueries\nCompute bound for small tree, BW bound at\nbottom of tree for large tree\nHistogram (Hist)\nImage analysis\nRequires\nconflict\ndetection\nAcross\npixels\nReduction/synchronization bound\nBilateral (Bilat)\nImage analysis\nRegular\nAcross\npixels\nCompute bound\nFigure 4.29 Throughput computing kernel characteristics. The name in parentheses identifies the benchmark\nname in this section. The authors suggest that code for both machines had equal optimization effort. From\nTable 1 in Lee, W.V., et al., 2010. Debunking the 100\u0001 GPU vs. CPU myth: an evaluation of throughput computing\non CPU and GPU. In: Proc. 37th Annual Int\u2019l. Symposium on Computer Architecture (ISCA), June 19\u201323, 2010, Saint-\nMalo, France.\nComparison of a GPU and a MIMD With Multimedia SIMD\n\u25a0\n351"
    },
    {
        "page": 384,
        "text": "compute bound. This observation reemphasizes the importance of cache block-\ning optimizations in Chapter 2.\n\u25a0\nGather-Scatter. The multimedia SIMD extensions are of little help if the data\nare scattered throughout main memory; optimal performance comes only when\ndata are aligned on 16-byte boundaries. Thus GJK gets little benefit from\nSIMD on the Core i7. As previously mentioned, GPUs offer gather-scatter\naddressing that is found in a vector architecture but omitted from SIMD exten-\nsions. The Address Coalescing Unit helps as well by combining accesses to the\nsame DRAM line, thereby reducing the number of gathers and scatters. The\nmemory controller also batches together accesses to the identical DRAM page.\nThis combination means the GTX 280 runs GJK a startling 15.2\u0001 faster than\nthe Core i7, which is larger than any single physical parameter in Figure 4.27.\nThis observation reinforces the importance of gather-scatter to vector and GPU\narchitectures that is missing from SIMD extensions.\n\u25a0\nSynchronization. The performance synchronization of Hist is limited by atomic\nupdates, which are responsible for 28% of the total runtime on the Core i7\ndespite its having a hardware fetch-and-increment instruction. Thus Hist is\nKernel\nUnits\nCore i7-960\nGTX 280\nGTX 280/\ni7-960\nSGEMM\nGFLOP/s\n94\n364\n3.9\nMC\nBillion paths/s\n0.8\n1.4\n1.8\nConv\nMillion pixels/s\n1250\n3500\n2.8\nFFT\nGFLOP/s\n71.4\n213\n3.0\nSAXPY\nGB/s\n16.8\n88.8\n5.3\nLBM\nMillion lookups/s\n85\n426\n5.0\nSolv\nFrames/s\n103\n52\n0.5\nSpMV\nGFLOP/s\n4.9\n9.1\n1.9\nGJK\nFrames/s\n67\n1020\n15.2\nSort\nMillion elements/s\n250\n198\n0.8\nRC\nFrames/s\n5\n8.1\n1.6\nSearch\nMillion queries/s\n50\n90\n1.8\nHist\nMillion pixels/s\n1517\n2583\n1.7\nBilat\nMillion pixels/s\n83\n475\n5.7\nFigure 4.30 Raw and relative performance measured for the two platforms. In this\nstudy, SAXPY is used only as a measure of memory bandwidth, so the right unit is\nGB/s and not GFLOP/s. Based on Table 3 in Lee, W.V., et al., 2010. Debunking the\n100\u0001 GPU vs. CPU myth: an evaluation of throughput computing on CPU and GPU.\nIn: Proc. 37th Annual Int\u2019l. Symposium on Computer Architecture (ISCA), June 19\u201323,\n2010, Saint-Malo, France.\n352\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 385,
        "text": "only 1.7\u0001 faster on the GTX 280. Solv solves a batch of independent con-\nstraints in a small amount of computation followed by barrier synchronization.\nThe Core i7 benefits from the atomic instructions and a memory consistency\nmodel that ensures the right results even if not all previous accesses to memory\nhierarchy have completed. Without the memory consistency model, the GTX\n280 version launches some batches from the system processor, which leads to\nthe GTX 280 running 0.5\u0001 as fast as the Core i7. This observation points out\nhow synchronization performance can be important for some data parallel\nproblems.\nIt was interesting that the gather-scatter support of vector architectures, which pre-\ndate the SIMD instructions by decades, was so important to the effective usefulness\nof these SIMD extensions, which some had predicted before the comparison (Gebis\nand Patterson, 2007). The Intel researchers noted that 6 of the 14 kernels would\nexploit SIMD better with more efficient gather-scatter support on the Core i7.\nNote that an important feature missing from this comparison was describing the\nlevel of effort to get the results for the two systems. Ideally, future comparisons\nwould release the code used on both systems so that others could re-create the same\nexperiments on different hardware platforms and possibly improve on the results.\nComparison Update\nIn the intervening years, the weaknesses of the Core i7 and Tesla GTX 280 have\nbeen addressed by their successors. Intel\u2019s ACV2 added gather instructions, and\nAVX/512 added scatter instructions, both of which are found in the Intel Skylake\nseries. Nvidia Pascal has double-precision floating-point performance that is one-\nhalf instead of one-eighth the speed of single precision, fast atomic operations, and\ncaches.\nFigure 4.31 lists the characteristics of these two successors, Figure 4.32 com-\npares performance using 3 of the 14 benchmarks in the original paper (those were\nthe ones for which we could find source code), and Figure 4.33 shows the two new\nroofline models. The new GPU chip is 15 to 50 times faster and the new CPU chips\nis 50 times faster than their predecessors, and the new GPU is 2\u20135 times faster than\nthe new CPU.\n4.8\nFallacies and Pitfalls\nWhile data-level parallelism is the easiest form of parallelism after ILP from the\nprogrammer\u2019s perspective, and plausibly the simplest from the architect\u2019s perspec-\ntive, it still has many fallacies and pitfalls.\nFallacy\nGPUs suffer from being coprocessors.\nAlthough the split between main memory and GPU memory has disadvantages,\nthere are advantages to being at a distance from the CPU.\n4.8\nFallacies and Pitfalls\n\u25a0\n353"
    },
    {
        "page": 386,
        "text": "For example, PTX exists in part because of the I/O device nature of GPUs. This\nlevel of indirection between the compiler and the hardware gives GPU architects\nmuch more flexibility than system processor architects. It\u2019s often hard to know in\nadvance whether an architecture innovation will be well supported by compilers\nand libraries and be important to applications. Sometimes a new mechanism will\neven prove useful for one or two generations and then fade in importance as the\nIT world changes. PTX allows GPU architects to try innovations speculatively\nand drop them in subsequent generations if they disappoint or fade in impor-\ntance, which encourages experimentation. The justification for inclusion is under-\nstandably considerably higher for system processors\u2014and thus much less\nXeon Platinum 8180\nP100\nRatio P100/Xeon\nNumber of processing elements (cores or SMs)\n28\n56\n2.0\nClock frequency (GHz)\n2.5\n1.3\n0.52\nDie size\nN.A.\n610 mm2\n\u2013\nTechnology\nIntel 14 nm\nTSMC 16 nm\n1.1\nPower (chip, not module)\n80 W\n300 W\n3.8\nTransistors\nN.A.\n15.3 B\n\u2013\nMemory bandwidth (GB/s)\n199\n732\n3.7\nSingle-precision SIMD width\n16\n8\n0.5\nDouble-precision SIMD width\n8\n4\n0.5\nPeak single-precision SIMD FLOPS (GFLOP/s)\n4480\n10,608\n2.4\nPeak double-precision SIMD FLOPS (GFLOP/s)\n2240\n5304\n2.4\nFigure 4.31 Intel Xeon ?? and NVIDIA P100. The rightmost column shows the ratios of P100 to the Xeon. Note that\nthese memory bandwidths are higher than in Figure 4.28 because these are DRAM pin bandwidths and those in\nFigure 4.28 are at the processors as measured by a benchmark program.\nKernel\nUnits\nXeon Platinum 8180\nP100\nP100/Xeon\nGTX 280/i7-\n960\nSGEMM\nGFLOP/s\n3494\n6827\n2.0\n3.9\nDGEMM\nGFLOP/s\n1693\n3490\n2.1\n\u2014\nFFT-S\nGFLOP/s\n410\n1820\n4.4\n3.0\nFFT-D\nGFLOP/s\n190\n811\n4.2\n\u2014\nSAXPY\nGB/s\n207\n544\n2.6\n5.3\nDAXPY\nGB/s\n212\n556\n2.6\n\u2014\nFigure 4.32 Raw and relative performance measured for modern versions of the two\nplatforms as compared to the relative performance of the original platforms. Like\nFigure 4.30, SAXPY and DAXPY are used only as a measure of memory bandwidth, so\nthe proper unit is GB/s and not GFLOP/s.\n354\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 387,
        "text": "experimentation can occur\u2014as distributing binary machine code normally implies\nthat new features must be supported by all future generations of that architecture.\nA demonstration of the value of PTX is that the different generation architec-\nture radically changed the hardware instruction set\u2014from being memory-oriented\nlike x86 to being register-oriented like RISC-V as well as doubling the address size\nto 64 bits\u2014without disrupting the NVIDIA software stack.\nPitfall\nConcentrating on peak performance in vector architectures and ignoring start-up\noverhead.\nEarly memory-memory vector processors such as the TI ASC and the CDC STAR-\n100 had long start-up times. For some vector problems, vectors had to be longer than\n100 for the vector code to be faster than the scalar code! On the CYBER 205\u2014\nderivedfromtheSTAR-100\u2014thestart-upoverheadforDAXPYis158clockcycles,\nwhich substantially increases the break-even point. If the clock rates of the Cray-1\nand the CYBER 205 were identical, the Cray-1 would be faster until the vector\nlength was greater than 64. Because the Cray-1 clock rate was also higher (even\nthough the 205 was newer), the crossover point was a vector length over 100.\n1024\n512\n256\n128\n64\n32\n16\n8\n1\n2\nArithmetic intensity\n4\n8\n16\n2\n1\n1/2\n1/4\n1/8\nArithmetic intensity\n4\n8\n16\n32\n32\n1/8\n1/4\n1/2\nCore i7 920\n(Nehalem)\n1024\n512\n256\n128\n64\n32\n16\n8\nGFLOP/s\nGFLOP/s\nStream=16.4 GB/s\n43 GF/s\n16,384\n8192\n4096\n2048\nXeon\nPlatinum 8180\n(Skylake)\nStream=199 GB/s\n4480 GF/s\n16,384\n8192\n4096\n2048\nNVIDIA P100\n(Pascal)\n3696 GF/s\n8944 GF/s\nStream=558 GB/s\nNVIDIA GTX280\n(Tesla)\n78 GF/s\n624 GF/s\n85 GF/s\nStream=127GB/s\nFigure 4.33 Roofline models of older and newer CPUs versus older and newer GPUs. The higher roofline for each\ncomputer is single-precision floating-point performance, and the lower one is double-precision performance.\n4.8\nFallacies and Pitfalls\n\u25a0\n355"
    },
    {
        "page": 388,
        "text": "Pitfall\nIncreasing\nvector\nperformance,\nwithout\ncomparable\nincreases\nin\nscalar\nperformance.\nThis imbalance was a problem on many early vector processors, and a place where\nSeymour Cray (the architect of the Cray computers) rewrote the rules. Many of the\nearly vector processors had comparatively slow scalar units (as well as large start-\nup overheads). Even today, a processor with lower vector performance but better\nscalar performance can outperform a processor with higher peak vector perfor-\nmance. Good scalar performance keeps down overhead costs (strip mining, for\nexample) and reduces the impact of Amdahl\u2019s law.\nAn excellent example of this comes from comparing a fast scalar processor and a\nvector processor with lower scalar performance. The Livermore Fortran kernels are a\ncollection of 24 scientific kernels with varying degrees of vectorization. Figure 4.34\nshows the performance of two different processors on this benchmark. Despite the\nvector processor\u2019s higher peak performance, its low scalar performance makes it\nslower than a fast scalar processor as measured by the harmonic mean.\nThe flip of this danger today is increasing vector performance\u2014say, by\nincreasing the number of lanes\u2014without increasing scalar performance. Such\nmyopia is another path to an unbalanced computer.\nThe next fallacy is closely related.\nFallacy\nYou can get good vector performance without providing memory bandwidth.\nAs we saw with the DAXPY loop and the Roofline model, memory bandwidth is\nquite important to all SIMD architectures. DAXPY requires 1.5 memory references\nper floating-point operation, and this ratio is typical of many scientific codes. Even\nif the floating-point operations took no time, a Cray-1 could not increase the\nperformance of the vector sequence used, because it is memory-limited. The\nCray-1 performance on Linpack jumped when the compiler used blocking to\nchange the computation so that values could be kept in the vector registers. This\napproach lowered the number of memory references per FLOP and improved the\nperformance by nearly a factor of two! Thus the memory bandwidth on the Cray-1\nbecame sufficient for a loop that formerly required more bandwidth.\nProcessor\nMinimum rate for\nany loop (MFLOPS)\nMaximum rate for\nany loop (MFLOPS)\nHarmonic mean of all\n24 loops (MFLOPS)\nMIPS M/\n120-5\n0.80\n3.89\n1.85\nStardent-\n1500\n0.41\n10.08\n1.72\nFigure 4.34 Performance measurements for the Livermore Fortran kernels on two\ndifferent processors. Both the MIPS M/120-5 and the Stardent-1500 (formerly the\nArdent Titan-1) use a 16.7 MHz MIPS R2000 chip for the main CPU. The Stardent-\n1500 uses its vector unit for scalar FP and has about half the scalar performance (as mea-\nsured by the minimum rate) of the MIPS M/120-5, which uses the MIPS R2010 FP chip.\nThe vector processor is more than a factor of 2.5\u0001 faster for a highly vectorizable loop\n(maximum rate). However, the lower scalar performance of the Stardent-1500 negates\nthe higher vector performance when total performance is measured by the harmonic\nmean on all 24 loops.\n356\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 389,
        "text": "Fallacy\nOn GPUs, just add more threads if you don\u2019t have enough memory performance.\nGPUs use many CUDA Threads to hide the latency to main memory. If memory\naccesses are scattered or not correlated among CUDA Threads, the memory system\nwill get progressively slower in responding to each individual request. Eventually,\neven many threads will not cover the latency. For the \u201cmore CUDA Threads\u201d strat-\negy to work, not only do you need lots of CUDA Threads, but the CUDA Threads\nthemselves also must be well behaved in terms of locality of memory accesses.\n4.9\nConcluding Remarks\nData-level parallelism is increasing in importance for personal mobile devices,\ngiven the popularity of applications showing the importance of audio, video,\nand games on these devices. When combined with a model that is easier to program\nthan task-level parallelism and with potentially better energy efficiency, it\u2019s easy to\nsee why there has been a renaissance for data-level parallelism in this decade.\nWe are seeing system processors take on more of the characteristics of GPUs,\nand vice versa. One of the biggest differences in performance between conven-\ntional processors and GPUs has been for gather-scatter addressing. Traditional vec-\ntor architectures show how to add such addressing to SIMD instructions, and we\nexpect to see more ideas added from the well-proven vector architectures to SIMD\nextensions over time.\nAs we said in the opening of Section 4.4, the GPU question is not simply which\narchitecture is best, but given the hardware investment to do graphics well, how\ncan it be enhanced to support computation that is more general? Although vector\narchitectures have many advantages on paper, it remains to be proven whether vec-\ntor architectures can be as good a foundation for graphics as GPUs. RISC-V has\nembraced vector over SIMD. Thus, like architecture debates of the past, the mar-\nketplace will help determine the importance of the strengths and weaknesses of two\nstyles of data parallel architectures.\n4.10\nHistorical Perspective and References\nSection M.6 (available online) features a discussion on the Illiac IV (a representative\nof the early SIMD architectures) and the Cray-1 (a representative of vector architec-\ntures). We also look at multimedia SIMD extensions and the history of GPUs.\nCase Study and Exercises by Jason D. Bakos\nCase Study: Implementing a Vector Kernel on a Vector\nProcessor and GPU\nConcepts illustrated by this case study\n\u25a0\nProgramming Vector Processors\n\u25a0\nProgramming GPUs\n\u25a0\nPerformance Estimation\nCase Study and Exercises by Jason D. Bakos\n\u25a0\n357"
    },
    {
        "page": 390,
        "text": "MrBayes is a popular computational biology application for inferring the evolu-\ntionary histories among a set of input species based on their prealigned DNA\nsequence data of length n. MrBayes works by performing an heuristic search over\nthe space of all binary tree topologies for which the inputs are the leaves. In order to\nevaluate a particular tree, the application must compute an n\u00014 conditional like-\nlihood table (named clP) for each interior node. The table is a function of the con-\nditional likelihood tables of the node\u2019s two descendent nodes (clL and clR, single\nprecision floating point) and the 4\u00014 transition probability table (tiPL and\ntiPR, single precision floating point). One of this application\u2019s kernels is the com-\nputation of this conditional likelihood table and is shown as follows:\nfor (k=0; k <seq_length; k++) {\nclP[h++] = (tiPL[AA]*clL[A] + tiPL[AC]*clL[C] +\ntiPL[AG]*clL[G] + tiPL[AT]*clL[T])*\n(tiPR[AA]*clR[A] + tiPR[AC]*clR[C] +\ntiPR[AG]*clR[G] + tiPR[AT]*clR[T]);\nclP[h++] = (tiPL[CA]*clL[A] + tiPL[CC]*clL[C] +\ntiPL[CG]*clL[G] + tiPL[CT]*clL[T])*\n(tiPR[CA]*clR[A] + tiPR[CC]*clR[C] +\ntiPR[CG]*clR[G] + tiPR[CT]*clR[T]);\nclP[h++] = (tiPL[GA]*clL[A] + tiPL[GC]*clL[C] +\ntiPL[GG]*clL[G] + tiPL[GT]*clL[T])*\n(tiPR[GA]*clR[A] + tiPR[GC]*clR[C] +\ntiPR[GG]*clR[G] + tiPR[GT]*clR[T]);\nclP[h++] = (tiPL[TA]*clL[A] + tiPL[TC]*clL[C] +\ntiPL[TG]*clL[G] + tiPL[TT]*clL[T])*\n(tiPR[TA]*clR[A] + tiPR[TC]*clR[C] +\ntiPR[TG]*clR[G] + tiPR[TT]*clR[T]);\nclL += 4;\nclR += 4;\n}\n4.1\n[25] <4.1, 4.2> Assume the constants shown as follows.\nConstants\nValues\nAA,AC,AG,AT\n0,1,2,3\nCA,CC,CG,CT\n4,5,6,7\nGA,GC,GG,GT\n8,9,10,11\nTA,TC,TG,TT\n12,13,14,15\nA,C,G,T\n0,1,2,3\n358\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 391,
        "text": "Write code for RISC-V and RV64V. Assume the starting addresses of tiPL,\ntiPR, clL, clR, and clP are in RtiPL, RtiPR, RclL, RclR, and RclP,\nrespectively. Do not unroll the loop. To facilitate vector addition reductions,\nassume that we add the following instructions to RV64V:\nVector Summation Reduction Single Precision:\nvsum Fd, Vs\nThis instruction performs a summation reduction on a vector register Vs, writ-\ning to the sum into scalar register Fd.\n4.2\n[5] <4.1, 4.2> Assuming seq_length == 500, what is the dynamic instruction\ncount for both implementations?\n4.3\n[25] <4.1, 4.2> Assume that the vector reduction instruction is executed on the\nvector functional unit, similar to a vector add instruction. Show how the code\nsequence lays out in convoys assuming a single instance of each vector functional\nunit. How many chimes will the code require? How many cycles per FLOP are\nneeded, ignoring vector instruction issue overhead?\n4.4\n[15] <4.1, 4.2> Consider the possibility of unrolling the loop and mapping mul-\ntiple iterations to vector operations. Assume that you can use scatter-gather loads\nand stores (vldi and vsti). How does this affect the way you can write the\nRV64Vcode for this kernel?\n4.5\n[25] <4.4> Now assume we want to implement the MrBayes kernel on a GPU\nusing a single thread block. Rewrite the C code of the kernel using CUDA.\nAssume that pointers to the conditional likelihood and transition probability\ntables are specified as parameters to the kernel. Invoke one thread for each iter-\nation of the loop. Load any reused values into shared memory before performing\noperations on it.\n4.6\n[15] <4.4> With CUDA we can use coarse-grain parallelism at the block level\nto compute the conditional likelihood of multiple nodes in parallel. Assume\nthat we want to compute the conditional likelihood from the bottom of the tree\nup. Assume seq_length \u00bc\u00bc 500 for all notes and that the group of tables for\neach of the 12 leaf nodes is stored in consecutive memory locations in the\norder of node number (e.g., the mth element of clP on node n is at clP\n[n*4*seq_length+m*4]). Assume that we want to compute the conditional\nlikelihood for nodes 12\u201317, as shown in Figure 4.35. Change the method\nby which you compute the array indices in your answer from Exercise 4.5\nto include the block number.\n4.7\n[15] <4.4> Convert your code from Exercise 4.6 into PTX code. How many\ninstructions are needed for the kernel?\n4.8\n[10] <4.4> How well do you expect this code to perform on a GPU? Explain\nyour answer.\nCase Study and Exercises by Jason D. Bakos\n\u25a0\n359"
    },
    {
        "page": 392,
        "text": "Exercises\n4.9\n[10/20/20/15/15] <4.2> Consider the following code, which multiplies two vec-\ntors that contain single-precision complex values:\nfor (i=0;i <300;i++) {\nc_re[i] = a_re[i] * b_re[i] \u0003 a_im[i] * b_im[i];\nc_im[i] = a_re[i] * b_im[i] + a_im[i] * b_re[i];\n}\nAssume that the processor runs at 700 MHz and has a maximum vector length\nof 64. The load/store unit has a start-up overhead of 15 cycles; the multiply unit,\n8 cycles; and the add/subtract unit, 5 cycles.\na. [10] <4.3> What is the arithmetic intensity of this kernel? Justify your answer.\nb. [20] <4.2> Convert this loop into RV64V assembly code using strip mining.\nc. [20] <4.2> Assuming chaining and a single memory pipeline, how many\nchimes are required? How many clock cycles are required per complex result\nvalue, including start-up overhead?\nd. [15] <4.2> If the vector sequence is chained, how many clock cycles are\nrequired per complex result value, including overhead?\ne. [15] <4.2> Now assume that the processor has three memory pipelines and\nchaining. If there are no bank conflicts in the loop\u2019s accesses, how many clock\ncycles are required per result?\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n18\n19\n21\n22\n20\n14\n15\n16\n17\nFigure 4.35 Sample tree.\n360\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 393,
        "text": "4.10\n[30] <4.2,4.3,4.4> In this problem, we will compare the performance of a vector\nprocessor with a hybrid system that contains a scalar processor and a GPU-based\ncoprocessor. In the hybrid system, the host processor has superior scalar perfor-\nmance to the GPU, so in this case all scalar code is executed on the host processor\nwhile all vector code is executed on the GPU. We will refer to the first system as the\nvector computer and the second system as the hybrid computer. Assume that your\ntarget application contains a vector kernel with an arithmetic intensity of 0.5\nFLOPs per DRAM byte accessed; however, the application also has a scalar com-\nponent that must be performed before and after the kernel in order to prepare the\ninput vectors and output vectors, respectively. For a sample dataset, the scalar por-\ntion of the code requires 400 ms of execution time on both the vector processor and\nthe host processor in the hybrid system. The kernel reads input vectors consisting\nof 200 MB of data and has output data consisting of 100 MB of data. The vector\nprocessor has a peak memory bandwidth of 30 GB/s and the GPU has a peak mem-\nory bandwidth of 150 GB/s. The hybrid system has an additional overhead that\nrequires all input vectors to be transferred between the host memory and GPU local\nmemory before and after the kernel is invoked. The hybrid system has a direct\nmemory access (DMA) bandwidth of 10 GB/s and an average latency of 10 ms.\nAssume that both the vector processor and GPU are performance bound by mem-\nory bandwidth. Compute the execution time required by both computers for this\napplication.\n4.11\n[15/25/25] <4.4, 4.5> Section 4.5 discussed the reduction operation that reduces a\nvector down to a scalar by repeated application of an operation. A reduction is a\nspecial type of a loop recurrence. An example is shown as follows:\ndot=0.0;\nfor (i=0;i <64;i++)\ndot\n=\ndot + a[i] * b[i];\nA vectorizing compiler might apply a transformation called scalar expansion,\nwhich expands dot into a vector and splits the loop such that the multiply can be\nperformed with a vector operation, leaving the reduction as a separate scalar\noperation:\nfor (i=0;i <64;i++) dot[i]\n=\na[i] * b[i];\nfor (i=1;i <64;i++) dot[0]\n=\ndot[0]\n+\ndot[i];\nAs mentioned in Section 4.5, if we allow the floating-point addition to be asso-\nciative, there are several techniques available for parallelizing the reduction.\na. [15] <4.4, 4.5> One technique is called recurrence doubling, which adds\nsequences of progressively shorter vectors (ie, two 32-element vectors, then\ntwo 16-element vectors, and so on). Show how the C code would look for exe-\ncuting the second loop in this way.\nb. [25]<4.4,4.5> Insomevectorprocessors,theindividualelementswithinthevec-\ntor registersare addressable.Inthiscase, the operandstoa vectoroperation may be\ntwo different parts of the same vector register. This allows another solution for the\nreductioncalledpartialsums.Theideaistoreducethevectortomsumswheremis\nCase Study and Exercises by Jason D. Bakos\n\u25a0\n361"
    },
    {
        "page": 394,
        "text": "the total latency through the vector functional unit, including the operand read and\nwrite times.Assume thattheVMIPS vectorregistersareaddressable (e.g.,you can\ninitiateavectoroperationwiththeoperandV1(16),indicatingthattheinputoper-\nandbeginswithelement16).Also,assumethatthetotallatencyforadds,including\nthe operand read and result write, is eight cycles. Write a VMIPS code sequence\nthat reduces the contents of V1 to eight partial sums.\nc. [25] <4.4, 4.5> When performing a reduction on a GPU, one thread is associ-\nated with each element in the input vector. The first step is for each thread to\nwrite its corresponding value into shared memory. Next, each thread enters a\nloop that adds each pair of input values. This reduces the number of elements\nby half after each iteration, meaning that the number of active threads also\nreduces by half after each iteration. In order to maximize the performance of\nthe reduction, the number of fully populated warps should be maximized\nthroughout the course of the loop. In other words, the active threads should\nbe contiguous. Also, each thread should index the shared array in such a\nway as to avoid bank conflicts in the shared memory. The following loop vio-\nlates only the first of these guidelines and also uses the modulo operator, which\nis very expensive for GPUs:\nunsigned int tid\n=\nthreadIdx.x;\nfor(unsigned int s=1; s <blockDim.x; s * \u00bc 2) {\nif ((tid % (2*s))\n==\n0) {\nsdata[tid]\n+= sdata[tid + s];\n}\n__syncthreads();\n}\nRewrite the loop to meet these guidelines and eliminate the use of the modulo\noperator. Assume that there are 32 threads per warp and a bank conflict occurs\nwhenever two or more threads from the same warp reference an index whose\nmodulo by 32 are equal.\n4.12\n[10/10/10/10] <4.3> The following kernel performs a portion of the finite-\ndifference time-domain (FDTD) method for computing Maxwell\u2019s equations in\na three-dimensional space, part of one of the SPEC06fp benchmarks:\nfor (int x=0; x <NX \u00031; x++) {\nfor (int y=0; y <NY \u00031; y++) {\nfor (int z=0; z <NZ \u00031; z++) {\nint index = x*NY*NZ + y*NZ + z;\nif (y >0 && x >0) {\nmaterial = IDx[index];\ndH1 = (Hz[index] \u0003Hz[index-incrementY])/dy[y];\ndH2 = (Hy[index] \u0003Hy[index-incrementZ])/dz[z];\nEx[index] = Ca[material]*Ex[index]+Cb[material]*\n(dH2 \u0003dH1);\n}}}}\n362\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 395,
        "text": "Assume that dH1, dH2, Hy, Hz, dy, dz, Ca, Cb, and Ex are all single-\nprecision floating-point arrays. Assume IDx is an array of unsigned int.\na. [10] <4.3> What is the arithmetic intensity of this kernel?\nb. [10] <4.3> Is this kernel amenable to vector or SIMD execution? Why or\nwhy not?\nc. [10] <4.3> Assume this kernel is to be executed on a processor that has\n30 GB/s of memory bandwidth. Will this kernel be memory bound or\ncompute bound?\nd. [10] <4.3> Develop a roofline model for this processor, assuming it has a peak\ncomputational throughput of 85 GFLOP/s.\n4.13\n[10/15] <4.4> Assume a GPU architecture that contains 10 SIMD processors.\nEach SIMD instruction has a width of 32 and each SIMD processor contains 8 lanes\nfor single-precision arithmetic and load/store instructions, meaning that each nondi-\nverged SIMD instruction can produce 32 results every 4 cycles. Assume a kernel that\nhas divergent branches that causes, on average, 80% of threads to be active. Assume\nthat 70% of all SIMD instructions executed are single-precision arithmetic and 20%\nare load/store. Because not all memory latencies are covered, assume an average\nSIMD instruction issue rate of 0.85. Assume that the GPU has a clock speed of\n1.5 GHz.\na. [10] <4.4> Compute the throughput, in GFLOP/s, for this kernel on this GPU.\nb. [15] <4.4> Assume that you have the following choices:\n(1) Increasing the number of single-precision lanes to 16\n(2) Increasing the number of SIMD processors to 15 (assume this change\ndoesn\u2019t affect any other performance metrics and that the code scales to\nthe additional processors)\n(3) Adding a cache that will effectively reduce memory latency by 40%, which\nwill increase instruction issue rate to 0.95\nWhat is speedup in throughput for each of these improvements?\n4.14\n[10/15/15] <4.5> In this exercise, we will examine several loops and analyze their\npotential for parallelization.\na. [10] <4.5> Does the following loop have a loop-carried dependency?\nfor (i=0;i <100;i++) {\nA[i] = B[2*i+4];\nB[4*i+5] = A[i];\n}\nb. [15] <4.5> In the following loop, find all the true dependences, output depen-\ndences, and antidependences. Eliminate the output dependences and antidepen-\ndences by renaming.\nfor (i=0;i <100;i++) {\nA[i] =\nA[i] * B[i]; /* S1 */\nCase Study and Exercises by Jason D. Bakos\n\u25a0\n363"
    },
    {
        "page": 396,
        "text": "B[i]\n=\nA[i] + c; /* S2 */\nA[i]\n=\nC[i] * c; /* S3 */\nC[i]\n=\nD[i] * A[i]; /* S4 */\nc. [15] <4.5> Consider the following loop:\nfor (i=0;i <100;i++) {\nA[i]\n=\nA[i]\n+\nB[i]; /* S1 */\nB[i+1]\n=\nC[i]\n+\nD[i]; /* S2 */\n}\nAre there dependences between S1 and S2? Is this loop parallel? If not, show\nhow to make it parallel.\n4.15\n[10] <4.4> List and describe at least four factors that influence the performance of\nGPU kernels. In other words, which runtime behaviors that are caused by the ker-\nnel code cause a reduction in resource utilization during kernel execution?\n4.16\n[10] <4.4> Assume a hypothetical GPU with the following characteristics:\n\u25a0\nClock rate 1.5 GHz\n\u25a0\nContains 16 SIMD processors, each containing 16 single-precision floating-\npoint units\n\u25a0\nHas 100 GB/s off-chip memory bandwidth\nWithout considering memory bandwidth, what is the peak single-precision\nfloating-point throughput for this GPU in GLFOP/s, assuming that all memory\nlatencies can be hidden? Is this throughput sustainable given the memory band-\nwidth limitation?\n4.17\n[60] <4.4> For this programming exercise, you will write and characterize the\nbehavior of a CUDA kernel that contains a high amount of data-level parallelism\nbut also contains conditional execution behavior. Use the NVIDIA CUDA Toolkit\nalong with GPU-SIM from the University of British Columbia (http://www.gpgpu-\nsim.org/) or the CUDA Profiler to write and compile a CUDA kernel that performs\n100 iterations of Conway\u2019s Game of Life for a 256\u0001256 game board and returns\nthe final state of the game board to the host. Assume that the board is initialized by\nthe host. Associate one thread with each cell. Make sure you add a barrier after each\ngame iteration. Use the following game rules:\n\u25a0\nAny live cell with fewer than two live neighbors dies.\n\u25a0\nAny live cell with two or three live neighbors lives on to the next generation.\n\u25a0\nAny live cell with more than three live neighbors dies.\n\u25a0\nAny dead cell with exactly three live neighbors becomes a live cell.\nAfter finishing the kernel answer the following questions:\na. [60] <4.4> Compile your code using the \u2013ptx option and inspect the PTX rep-\nresentation of your kernel. How many PTX instructions make up the PTX\n364\n\u25a0\nChapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures"
    },
    {
        "page": 397,
        "text": "implementation of your kernel? Did the conditional sections of your kernel\ninclude branch instructions or only predicated nonbranch instructions?\nb. [60] <4.4> After executing your code in the simulator, what is the dynamic\ninstruction count? What is the achieved instructions per cycle (IPC) or instruc-\ntion issue rate? What is the dynamic instruction breakdown in terms of control\ninstructions, arithmetic-logical unit (ALU) instructions, and memory instruc-\ntions? Are there any shared memory bank conflicts? What is the effective\noff-chip memory bandwidth?\nc. [60] <4.4> Implement an improved version of your kernel where off-chip\nmemory references are coalesced and observe the differences in runtime\nperformance.\nCase Study and Exercises by Jason D. Bakos\n\u25a0\n365"
    },
    {
        "page": 398,
        "text": "5.1\nIntroduction\n368\n5.2\nCentralized Shared-Memory Architectures\n377\n5.3\nPerformance of Symmetric Shared-Memory Multiprocessors\n393\n5.4\nDistributed Shared-Memory and Directory-Based Coherence\n404\n5.5\nSynchronization: The Basics\n412\n5.6\nModels of Memory Consistency: An Introduction\n417\n5.7\nCross-Cutting Issues\n422\n5.8\nPutting It All Together: Multicore Processors and Their Performance\n426\n5.9\nFallacies and Pitfalls\n438\n5.10\nThe Future of Multicore Scaling\n442\n5.11\nConcluding Remarks\n444\n5.12\nHistorical Perspectives and References\n445\nCase Studies and Exercises by Amr Zaky and David A. Wood\n446"
    },
    {
        "page": 399,
        "text": "5\nThread-Level Parallelism\nThe turning away from the conventional organization came in the\nmiddle 1960s, when the law of diminishing returns began to take\neffect in the effort to increase the operational speed of a computer.\n. . . Electronic circuits are ultimately limited in their speed of\noperation by the speed of light . . . and many of the circuits were\nalready operating in the nanosecond range.\nW. Jack Bouknight et al.,\nThe Illiac IV System (1972)\nWe are dedicating all of our future product development to\nmulticore designs. We believe this is a key inflection point for the\nindustry.\nIntel President Paul Otellini,\ndescribing Intel\u2019s future direction at the\nIntel Developer Forum in 2005\nSince 2004 processor designers have increased core counts to\nexploit Moore\u2019s Law scaling, rather than focusing on single-core\nperformance. The failure of Dennard scaling, to which the shift to\nmulticore parts is partially a response, may soon limit multicore\nscaling just as single-core scaling has been curtailed.\nHadi Esmaeilzadeh, et al.,\nPower Limitations and Dark Silicon\nChallenge the Future of Multicore (2012)\nComputer Architecture. https://doi.org/10.1016/B978-0-12-811905-1.00005-5\n\u00a9 2019 Elsevier Inc. All rights reserved."
    },
    {
        "page": 400,
        "text": "5.1\nIntroduction\nAsthequotationsthatopenthischaptershow,theviewthatadvancesinuniprocessor\narchitecture were nearing an end has been held by some researchers for many years.\nClearly, these views were premature; in fact, during the period of 1986\u20132003,\nuniprocessor performance growth, driven by the microprocessor, was at its highest\nrate since the first transistorized computers in the late 1950s and early 1960s.\nNonetheless, the importance of multiprocessors was growing throughout the\n1990s as designers sought a way to build servers and supercomputers that achieved\nhigher performance than a single microprocessor, while exploiting the tremendous\ncost-performance advantages of commodity microprocessors. As we discussed\nin Chapters 1 and 3, the slowdown in uniprocessor performance arising from\ndiminishing returns in exploiting instruction-level parallelism (ILP) combined\nwith growing concern over power has led to a new era in computer architec-\nture\u2014an era where multiprocessors play a major role from the low end to the high\nend. The second quotation captures this clear inflection point.\nThis increased importance of multiprocessing reflects several major factors:\n\u25a0\nThe dramatically lower efficiencies in silicon and energy use that were encoun-\ntered between 2000 and 2005 as designers attempted to find and exploit more\nILP, which turned out to be inefficient, since power and silicon costs grew\nfaster than performance. Other than ILP, the only scalable and general-purpose\nway we know to increase performance faster than the basic technology allows\n(from a switching perspective) is through multiprocessing.\n\u25a0\nA growing interest in high-end servers as cloud computing and software-as-a-\nservice become more important.\n\u25a0\nA growth in data-intensive applications driven by the availability of massive\namounts of data on the Internet.\n\u25a0\nThe insight that increasing performance on the desktop is less important\n(outside of graphics, at least), either because current performance is acceptable\nor because highly compute- and data-intensive applications are being done on\nthe cloud.\n\u25a0\nAn improved understanding of how to use multiprocessors effectively,\nespecially in server environments where there is significant inherent parallel-\nism, arising from large datasets (usually in the form of data parallelism),\n\u201cnatural-world\u201d parallelism (which occurs in scientific and engineering codes),\nor parallelism among large numbers of independent requests (request-level\nparallelism).\n\u25a0\nThe advantages of leveraging a design investment by replication rather than\nunique design; all multiprocessor designs provide such leverage.\nThe third quotation reminds us that multicore may provide only limited possi-\nbilities for scaling performance. The combination of Amdahl\u2019s Law effects and the\n368\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 401,
        "text": "end of Dennard scaling mean that the future of multicore may be limited, at least as\na method of scaling up the performance of single applications. We return to this\ntopic late in the chapter.\nIn this chapter, we focus on exploiting thread-level parallelism (TLP). TLP\nimplies the existence of multiple program counters and thus is exploited primarily\nthroughMIMDs.AlthoughMIMDs havebeenaroundfordecades,themovementof\nthread-level parallelism to the forefront across the range of computing from embed-\nded applications to high-end severs is relatively recent. Likewise, the extensive use\nof thread-level parallelism for a wide-range of general-purpose applications, versus\neither transaction processing or scientific applications, is relatively new.\nOur focus in this chapter is on multiprocessors, which we define as computers\nconsisting of tightly coupled processors whose coordination and usage are typi-\ncally controlled by a single operating system and that share memory through a\nshared address space. Such systems exploit thread-level parallelism through two\ndifferent software models. The first is the execution of a tightly coupled set of\nthreads collaborating on a single task, which is typically called parallel processing.\nThe second is the execution of multiple, relatively independent processes that may\noriginate from one or more users, which is a form of request-level parallelism,\nalthough at a much smaller scale than what we explore in the next chapter.\nRequest-level parallelism may be exploited by a single application running on\nmultiple processors, such as a database responding to queries, or multiple applica-\ntions running independently, often called multiprogramming.\nThe multiprocessors we examine in this chapter typically range in size from a\ndual processor to dozens and sometimes hundreds of processors and communicate\nand coordinate through the sharing of memory. Although sharing through memory\nimplies a shared address space, it does not necessarily mean there is a single\nphysical memory. Such multiprocessors include both single-chip systems with\nmultiple cores, known as multicore, and computers consisting of multiple chips,\neach of which is typically a multicore. Many companies make such multiproces-\nsors, including HP, Dell, Cisco, IBM, SGI, Lenovo, Oracle, Fujitsu, and many\nothers.\nIn addition to true multiprocessors, we will return to the topic of multithread-\ning, a technique that supports multiple threads executing in an interleaved fashion\non a single multiple-issue processor. Many multicore processors also include\nsupport for multithreading.\nIn the next chapter, we consider ultrascale computers built from very large\nnumbers of processors, connected with networking technology (not necessarily\nthe same networking technology used to connect computers to the Internet) and\noften called clusters; these large-scale systems are used for cloud computing\nprimarily with massive numbers of independent tasks being executed in parallel.\nMore recently, computationally intensive tasks that can be easily made parallel,\nsuch as Search and certain machine learning algorithms have also made use of\nclusters. When these clusters grow to tens of thousands of servers and beyond,\nwe call them warehouse-scale computers. Amazon, Google, Microsoft, and\nFacebook all make warehouse-scale computers.\n5.1\nIntroduction\n\u25a0\n369"
    },
    {
        "page": 402,
        "text": "In addition to the multiprocessors we study here and the warehouse-scaled\nsystems of the next chapter, there are a range of special large-scale multiprocessor\nsystems, sometimes called multicomputers, which are less tightly coupled than the\nmultiprocessors examined in this chapter but usually more tightly coupled than the\nwarehouse-scale systems of the next chapter. The primary use for such multicom-\nputers is in high-end scientific computation, although they are sometimes used for\ncommercial applications filling the niche between multiprocessors and warehouse-\nscale computers. The Cray X series and IBM BlueGene are typical examples of\nthese multicomputers.\nMany other books, such as Culler et al. (1999), cover such systems in detail.\nBecause of the large and changing nature of the field of multiprocessing (the just-\nmentioned Culler et al. reference is over 1000 pages and discusses only multipro-\ncessing!), we have chosen to focus our attention on what we believe is the\nmost important and general-purpose portions of the computing space. Appendix\nI discusses some of the issues that arise in building such computers in the context\nof large-scale scientific applications.\nOur focus will be on multiprocessors with roughly 4\u2013256 processor cores,\nwhich might occupy anywhere from 4 to 16 separate chips. Such designs vastly\ndominate in terms of both units and dollars. In large-scale multiprocessors, the\ninterconnection networks are a critical part of the design; Appendix F focuses\non that topic.\nMultiprocessor Architecture: Issues and Approach\nTo take advantage of an MIMD multiprocessor with n processors, we must usually\nhave at least n threads or processes to execute; with multithreading, which is\npresent in most multicore chips today, that number is 2\u20134 times higher. The inde-\npendent threads within a single process are typically identified by the programmer\nor created by the operating system (from multiple independent requests). At the\nother extreme, a thread may consist of a few tens of iterations of a loop, generated\nby a parallel compiler exploiting data parallelism in the loop. Although the amount\nof computation assigned to a thread, called the grain size, is important in consid-\nering how to exploit thread-level parallelism efficiently, the important qualitative\ndistinction from instruction-level parallelism is that thread-level parallelism is\nidentified at a high level by the software system or programmer and that the threads\nconsist of hundreds to millions of instructions that may be executed in parallel.\nThreads can also be used to exploit data-level parallelism, although the over-\nhead is usually higher than would be seen with an SIMD processor or with a GPU\n(see Chapter 4). This overhead means that grain size must be sufficiently large to\nexploit the parallelism efficiently. For example, although a vector processor or\nGPU may be able to efficiently parallelize operations on short vectors, the resulting\ngrain size when the parallelism is split among many threads may be so small that\nthe overhead makes the exploitation of the parallelism prohibitively expensive in\nan MIMD.\n370\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 403,
        "text": "Existing shared-memory multiprocessors fall into two classes, depending on\nthe number of processors involved, which in turn dictates a memory organization\nand interconnect strategy. We refer to the multiprocessors by their memory orga-\nnization because what constitutes a small or large number of processors continues\nto change over time.\nThe first group, which we call symmetric (shared-memory) multiprocessors\n(SMPs), or centralized shared-memory multiprocessors, features small to moder-\nate numbers of cores, typically 32 or fewer. For multiprocessors with such small\nprocessor counts, it is possible for the processors to share a single centralized mem-\nory that all processors have equal access to, thus the term symmetric. In multicore\nchips, the memory is often shared in a centralized fashion among the cores; most\nexisting multicores are SMPs, but not all. (Note that some literature mistakenly\nappears to use SMP to stand for Shared Memory Processor, but this usage is\nerroneous.)\nSome multicores have nonuniform access to the outermost cache, a structure\ncalled NUCA for Nonuniform Cache Access, and are thus are not truly SMPs, even\nif they have a single main memory. The IBM Power8 has distributed L3 caches\nwith nonuniform access time to different addresses in L3.\nIn multiprocessors consisting of multiple multicore chips, there are often sep-\narate memories for each multicore chip. Thus the memory is distributed rather\nthan centralized. As we will see later in the chapter, many designs with distrib-\nuted memory have fast access to a local memory and much slower access to\nremote memory; often the differences in access time to various remote memories\nare small in comparison to the difference between the access times to the local\nmemory and to a remote memory. In such designs, the programmer and software\nsystem need to be aware of whether accesses are to local or remote memory, but\nmay be able to ignore the distribution of accesses among remote memories.\nBecause an SMP approach becomes less attractive with a growing number of pro-\ncessors, most of the very largest multiprocessors use some form of distributed\nmemory.\nSMP architectures are also sometimes called uniform memory access\n(UMA) multiprocessors, arising from the fact that all processors have a uniform\nlatency from memory, even if the memory is organized into multiple banks.\nFigure 5.1 shows what these multiprocessors look like. The architecture of\nSMPs is the topic of Section 5.2, and we explain the approach in the context\nof a multicore.\nThe alternative design approach consists of multiprocessors with physically\ndistributed memory, called distributed shared memory (DSM). Figure 5.2\nshows what these multiprocessors look like. To support larger processor counts,\nmemory must be distributed among the processors rather than centralized; oth-\nerwise, the memory system would not be able to support the bandwidth\ndemands of a larger number of processors without incurring excessively long\naccess latency.\nWith the rapid increase in processor performance and the associated increase in\na processor\u2019s memory bandwidth requirements, the size of a multiprocessor for\n5.1\nIntroduction\n\u25a0\n371"
    },
    {
        "page": 404,
        "text": "which distributed memory is preferred continues to shrink. The introduction of\nmulticore processors has meant that even some 2-chip multiprocessors, which\nmight have 16\u201364 processor cores, use distributed memory. The larger number\nof processors also raises the need for a high-bandwidth interconnect, of which\nwe will see examples in Appendix F. Both directed networks (i.e., switches)\nand indirect networks (typically multidimensional meshes) are used.\nDistributing the memory among the nodes both increases the bandwidth and\nreduces the latency to local memory. A DSM multiprocessor is also called a NUMA\n(nonuniform memory access) because the access time depends on the location of a\ndata word in memory. The key disadvantages for a DSM are that communicating\ndata among processors becomes somewhat more complex and a DSM requires\nmore effort in the software to take advantage of the increased memory bandwidth\nProcessor\nProcessor\nProcessor\nProcessor\nMain memory\nI/O system\nOne or\nmore levels\nof cache\nOne or\nmore levels\nof cache\nOne or\nmore levels\nof cache\nOne or\nmore levels\nof cache\nShared cache\nPrivate\ncaches\nFigure 5.1 Basic structure of a centralized shared-memory multiprocessor based on\na multicore chip. Multiple processor-cache subsystems share the same physical mem-\nory, typically with one level of shared cache on the multicore, and one or more levels of\nprivate per-core cache. The key architectural property is the uniform access time to all of\nthe memory from all of the processors. In a multichip design, an interconnection net-\nwork links the processors and the memory, which may be one or more banks. In a single-\nchip multicore, the interconnection network is simply the memory bus.\n372\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 405,
        "text": "afforded by distributed memories. Because most multicore-based multiprocessors\nwith more than a few processor chips use distributed memory, we will explain the\noperation of distributed memory multiprocessors from this viewpoint.\nIn both SMP and DSM architectures, communication among threads occurs\nthrough a shared address space, meaning that a memory reference can be made\nby any processor to any memory location, assuming it has the correct access rights.\nThe term shared memory associated with both SMP and DSM refers to the fact that\nthe address space is shared.\nIn contrast, the clusters and warehouse-scale computers in the next chapter\nlook like individual computers connected by a network, and the memory of one pro-\ncessor cannot be accessed by another processor without the assistance of software\nprotocols running on both processors. In such designs, message-passing protocols\nare used to communicate data among processors.\nChallenges of Parallel Processing\nThe application of multiprocessors ranges from running independent tasks with\nessentially no communication to running parallel programs where threads must\ncommunicate to complete the task. Two important hurdles, both explainable with\nAmdahl\u2019s Law, make parallel processing challenging. To overcome these hurdles\ntypically requires a comprehensive approach that addresses the choice of\nMemory\nI/O\nInterconnection network\nMemory\nI/O\nMemory\nI/O\nMulticore\nMP\nMulticore\nMP\nMulticore\nMP\nMulticore\nMP\nMemory\nI/O\nI/O\nMemory\nMemory\nI/O\nMemory\nI/O\nMemory\nI/O\nMulticore\nMP\nMulticore\nMP\nMulticore\nMP\nMulticore\nMP\nFigure 5.2 The basic architecture of a distributed-memory multiprocessor in 2017 typically consists of a multi-\ncore multiprocessor chip with memory and possibly I/O attached and an interface to an interconnection network\nthat connects all the nodes. Each processor core shares the entire memory, although the access time to the local\nmemory attached to the core\u2019s chip will be much faster than the access time to remote memories.\n5.1\nIntroduction\n\u25a0\n373"
    },
    {
        "page": 406,
        "text": "algorithm and its implementation, the underlying programming language and sys-\ntem, the operating system and its support functions, and the architecture and hard-\nware implementation. Although in many instances, one of these is a key\nbottleneck, when scaling to a larger processor counts (approaching 100 or more),\noften all aspects of the software and hardware need attention.\nThe first hurdle has to do with the limited parallelism available in programs,\nand the second arises from the relatively high cost of communications. Limitations\nin available parallelism make it difficult to achieve good speedups in any parallel\nprocessor, as our first example shows.\nExample\nSuppose you want to achieve a speedup of 80 with 100 processors. What fraction\nof the original computation can be sequential?\nAnswer\nRecall from Chapter 1 that Amdahl\u2019s Law is\nSpeedup \u00bc\n1\nFractionenhanced\nSpeedupenhanced\n+ 1Fractionenhanced\n\u00f0\n\u00de\nFor simplicity in this example, assume that the program operates in only two\nmodes: parallel with all processors fully used, which is the enhanced mode, or\nserial with only one processor in use. With this simplification, the speedup in\nenhanced mode is simply the number of processors, whereas the fraction of\nenhanced mode is the time spent in parallel mode. Substituting into the previous\nequation:\n80 \u00bc\n1\nFractionparallel\n100\n+ 1Fractionparallel\n\u0001\n\u0003\nSimplifying this equation yields:\n0:8\u0003Fractionparallel + 80\u0003 1Fractionparallel\n\u0001\n\u0003\n\u00bc 1\n8079:2\u0003Fractionparallel \u00bc 1\nFractionparallel \u00bc 801\n79:2\nFractionparallel \u00bc 0:9975\nThus, to achieve a speedup of 80 with 100 processors, only 0.25% of the\noriginal computation can be sequential! Of course, to achieve linear speedup\n(speedup of n with n processors), the entire program must usually be parallel\nwith no serial portions. In practice, programs do not just operate in fully\nparallel or sequential mode, but often use less than the full complement of\nthe processors when running in parallel mode. Amdahl\u2019s Law can be used\nto analyze applications with varying amounts of speedup, as the next\nexample shows.\n374\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 407,
        "text": "Example\nSuppose we have an application running on a 100-processor multiprocessor,\nand assume that application can use 1, 50, or 100 processors. If we assume\nthat 95% of the time we can use all 100 processors, how much of the remain-\ning 5% of the execution time must employ 50 processors if we want a speedup\nof 80?\nAnswer\nWe use Amdahl\u2019s Law with more terms:\nSpeedup \u00bc\n1\nFraction100\nSpeedup100\n+ Fraction50\nSpeedup50\n+ 1Fraction100 Fraction50\n\u00f0\n\u00de\nSubstituting in:\n80 \u00bc\n1\n0:95\n100 + Fraction50\n50\n+ 10:95Fraction80\n\u00f0\n\u00de\nSimplifying:\n0:76 + 1:6\u0003Fraction50 + 4:080\u0003Fraction50 \u00bc 1\n4:7678:4\u0003Fraction50 \u00bc 1\nFraction50 \u00bc 0:048\nIf 95% of an application can use 100 processors perfectly, to get a speedup of 80,\n4.8% of the remaining time must be spent using 50 processors and only 0.2% can\nbe serial!\nThe second major challenge in parallel processing involves the large latency\nof remote access in a parallel processor. In existing shared-memory multipro-\ncessors, communication of data between separate cores may cost 35\u201350 clock\ncycles and among cores on separate chips anywhere from 100 clock cycles to as\nmuch as 300 or more clock cycles (for large-scale multiprocessors), depending\non the communication mechanism, the type of interconnection network, and the\nscale of the multiprocessor. The effect of long communication delays is clearly\nsubstantial. Let\u2019s consider a simple example.\nExample\nSuppose we have an application running on a 32-processor multiprocessor that has\na 100 ns delay to handle a reference to a remote memory. For this application,\nassume that all the references except those involving communication hit in the\nlocal memory hierarchy, which is obviously optimistic. Processors are stalled\non a remote request, and the processor clock rate is 4 GHz. If the base CPI (assum-\ning that all references hit in the cache) is 0.5, how much faster is the multiprocessor\nif there is no communication versus if 0.2% of the instructions involve a remote\ncommunication reference?\n5.1\nIntroduction\n\u25a0\n375"
    },
    {
        "page": 408,
        "text": "Answer\nIt is simpler to first calculate the clock cycles per instruction. The effective CPI for\nthe multiprocessor with 0.2% remote references is\nCPI \u00bc BaseCPI + Remote request rate\u0003Remote request cost\n\u00bc 0:5 + 0:2%\u0003Remote request cost\nThe remote request cost is\nRemote access cost\nCycletime\n\u00bc 100ns\n0:25ns \u00bc 400 cycles\nTherefore we can compute the CPI:\nCPI \u00bc 0:5 + 0:20%\u0003400\n\u00bc 1:3\nThe multiprocessor with all local references is 1.3/0.5\u00bc2.6 times faster. In prac-\ntice, the performance analysis is much more complex because some fraction of the\nnoncommunication references will miss in the local hierarchy and the remote\naccess time does not have a single constant value. For example, the cost of a remote\nreference could be worse because contention caused by many references trying to\nuse the global interconnect can lead to increased delays, or the access time might be\nbetter if memory were distributed and the access was to the local memory.\nThis problem could have also been analyzed using Amdahl\u2019s Law, an exercise\nwe leave to the reader.\nThese problems\u2014insufficient parallelism and long-latency remote communi-\ncation\u2014are the two biggest performance challenges in using multiprocessors.\nThe problem of inadequate application parallelism must be attacked primarily in\nsoftware with new algorithms that offer better parallel performance, as well as\nby software systems that maximize the amount of time spent executing with the\nfull complement of processors. Reducing the impact of long remote latency can\nbe attacked both by the architecture and by the programmer. For example, we\ncan reduce the frequency of remote accesses with either hardware mechanisms,\nsuch as caching shared data, or software mechanisms, such as restructuring the data\nto make more accesses local. We can try to tolerate the latency by using multi-\nthreading (discussed later in this chapter) or by using prefetching (a topic we cover\nextensively in Chapter 2).\nMuch of this chapter focuses on techniques for reducing the impact of long\nremote communication latency. For example, Sections 5.2 through 5.4 discuss\nhow caching can be used to reduce remote access frequency, while maintaining\na coherent view of memory. Section 5.5 discusses synchronization, which, because\nit inherently involves interprocessor communication and also can limit parallelism,\nis a major potential bottleneck. Section 5.6 covers latency-hiding techniques and\nmemory consistency models for shared memory. In Appendix I, we focus primar-\nily on larger-scale multiprocessors that are used predominantly for scientific work.\n376\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 409,
        "text": "In that appendix, we examine the nature of such applications and the challenges of\nachieving speedup with dozens to hundreds of processors.\n5.2\nCentralized Shared-Memory Architectures\nThe observation that the use of large, multilevel caches can substantially reduce the\nmemory bandwidth demands of a processor is the key insight that motivates central-\nized memory multiprocessors. Originally, these processors were all single-core and\noften took an entire board, and memory was located on a shared bus. With more\nrecent, higher-performance processors, the memory demands have outstripped\nthe capability of reasonable buses, and recent microprocessors directly connect\nmemory to a single chip, which is sometimes called a backside or memory bus to\ndistinguish it from the bus used to connect to I/O. Accessing a chip\u2019s local memory\nwhether for an I/O operation or for an access from another chip requires going\nthrough the chip that \u201cowns\u201d that memory. Thus access to memory is asymmetric:\nfaster to the local memory and slower to the remote memory. In a multicore that\nmemory is shared among all the cores on a single chip, but the asymmetric access\nto the memory of one multicore from the memory of another usually remains.\nSymmetric shared-memory machines usually support the caching of both\nshared and private data. Private data are used by a single processor, while shared\ndata are used by multiple processors, essentially providing communication among\nthe processors through reads and writes of the shared data. When a private item is\ncached, its location is migrated to the cache, reducing the average access time as\nwell as the memory bandwidth required. Because no other processor uses the data,\nthe program behavior is identical to that in a uniprocessor. When shared data are\ncached, the shared value may be replicated in multiple caches. In addition to the\nreduction in access latency and required memory bandwidth, this replication also\nprovides a reduction in contention that may exist for shared data items that are\nbeing read by multiple processors simultaneously. Caching of shared data, how-\never, introduces a new problem: cache coherence.\nWhat Is Multiprocessor Cache Coherence?\nUnfortunately, caching shared data introduces a new problem. Because the view of\nmemory held by two different processors is through their individual caches, the\nprocessors could end up seeing different values for the same memory location,\nas Figure 5.3 illustrates. This difficulty is generally referred to as the cache coher-\nence problem. Notice that the coherence problem exists because we have both a\nglobal state, defined primarily by the main memory, and a local state, defined\nby the individual caches, which are private to each processor core. Thus, in a multi-\ncore where some level of caching may be shared (e.g., an L3), although some levels\nare private (e.g., L1 and L2), the coherence problem still exists and must be solved.\nInformally, we could say that a memory system is coherent if any read of a data\nitem returns the most recently written value of that data item. This definition,\n5.2\nCentralized Shared-Memory Architectures\n\u25a0\n377"
    },
    {
        "page": 410,
        "text": "although intuitively appealing, is vague and simplistic; the reality is much more\ncomplex. This simple definition contains two different aspects of memory system\nbehavior, both of which are critical to writing correct shared-memory programs.\nThe first aspect, called coherence, defines what values can be returned by a read.\nThe second aspect, called consistency, determines when a written value will be\nreturned by a read. Let\u2019s look at coherence first.\nA memory system is coherent if\n1. A read by processor P to location X that follows a write by P to X, with no writes\nof X by another processor occurring between the write and the read by P, always\nreturns the value written by P.\n2. A read by a processor to location X that follows a write by another processor to\nX returns the written value if the read and write are sufficiently separated in time\nand no other writes to X occur between the two accesses.\n3. Writes to the same location are serialized; that is, two writes to the same\nlocation by any two processors are seen in the same order by all processors.\nFor example, if the values 1 and then 2 are written to a location, processors\ncan never read the value of the location as 2 and then later read it as 1.\nThe first property simply preserves program order\u2014we expect this property to\nbe true even in uniprocessors. The second property defines the notion of what it\nmeans to have a coherent view of memory: if a processor could continuously read\nan old data value, we would clearly say that memory was incoherent.\nThe need for write serialization is more subtle, but equally important. Suppose\nwe did not serialize writes, and processor P1 writes location X followed by P2\nwriting location X. Serializing the writes ensures that every processor will see\nthe write done by P2 at some point. If we did not serialize the writes, it might\nbe the case that some processors could see the write of P2 first and then see the\nwrite of P1, maintaining the value written by P1 indefinitely. The simplest way\nTime\nEvent\nCache contents for\nprocessor A\nCache contents for\nprocessor B\nMemory contents for\nlocation X\n0\n1\n1\nProcessor A reads X\n1\n1\n2\nProcessor B reads X\n1\n1\n1\n3\nProcessor A stores\n0 into X\n0\n1\n0\nFigure 5.3 The cache coherence problem for a single memory location (X), read and written by two processors (A\nand B). We initially assume that neither cache contains the variable and that X has the value 1. We also assume a write-\nthrough cache; a write-back cache adds some additional but similar complications. After the value of X has been\nwritten by A, A\u2019s cache and the memory both contain the new value, but B\u2019s cache does not, and if B reads the value\nof X it will receive 1!\n378\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 411,
        "text": "to avoid such difficulties is to ensure that all writes to the same location are seen in\nthe same order; this property is called write serialization.\nAlthough the three properties just described are sufficient to ensure coherence,\nthe question of when a written value will be seen is also important. To see why,\nobserve that we cannot require that a read of X instantaneously see the value\nwritten for X by some other processor. If, for example, a write of X on one\nprocessor precedes a read of X on another processor by a very small time, it\nmay be impossible to ensure that the read returns the value of the data written, since\nthe written data may not even have left the processor at that point. The issue of\nexactly when a written value must be seen by a reader is defined by a memory\nconsistency model\u2014a topic discussed in Section 5.6.\nCoherence and consistency are complementary: Coherence defines the behav-\nior of reads and writes to the same memory location, while consistency defines the\nbehavior of reads and writes with respect to accesses to other memory locations.\nFor now, make the following two assumptions. First, a write does not complete\n(and allow the next write to occur) until all processors have seen the effect of that\nwrite. Second, the processor does not change the order of any write with respect to\nany other memory access. These two conditions mean that, if a processor writes\nlocation A followed by location B, any processor that sees the new value of B must\nalso see the new value of A. These restrictions allow the processor to reorder reads,\nbut forces the processor to finish a write in program order. We will rely on this\nassumption until we reach Section 5.6, where we will see exactly the implications\nof this definition, as well as the alternatives.\nBasic Schemes for Enforcing Coherence\nThe coherence problem for multiprocessors and I/O, although similar in origin, has\ndifferent characteristics that affect the appropriate solution. Unlike I/O, where\nmultiple data copies are a rare event\u2014one to be avoided whenever possible\u2014a\nprogram running on multiple processors will normally have copies of the same data\nin several caches. In a coherent multiprocessor, the caches provide both migration\nand replication of shared data items.\nCoherent caches provide migration because a data item can be moved to a local\ncache and used there in a transparent fashion. This migration reduces both the\nlatency to access a shared data item that is allocated remotely and the bandwidth\ndemand on the shared memory.\nBecause the caches make a copy of the data item in the local cache, coherent\ncaches also provide replication for shared data that are being read simultaneously.\nReplication reduces both latency of access and contention for a read shared data\nitem. Supporting this migration and replication is critical to performance in acces-\nsing shared data. Thus, rather than trying to solve the problem by avoiding it in\nsoftware, multiprocessors adopt a hardware solution by introducing a protocol\nto maintain coherent caches.\nThe protocols to maintain coherence for multiple processors are called cache\ncoherence protocols. Key to implementing a cache coherence protocol is tracking\n5.2\nCentralized Shared-Memory Architectures\n\u25a0\n379"
    },
    {
        "page": 412,
        "text": "the state of any sharing of a data block. The state of any cache block is kept using\nstatus bits associated with the block, similar to the valid and dirty bits kept in a\nuniprocessor cache. There are two classes of protocols in use, each of which uses\ndifferent techniques to track the sharing status:\n\u25a0\nDirectory based\u2014The sharing status of a particular block of physical memory\nis kept in one location, called the directory. There are two very different types\nof directory-based cache coherence. In an SMP, we can use one centralized\ndirectory, associated with the memory or some other single serialization point,\nsuch as the outermost cache in a multicore. In a DSM, it makes no sense to have\na single directory because that would create a single point of contention and\nmake it difficult to scale to many multicore chips given the memory demands\nof multicores with eight or more cores. Distributed directories are more com-\nplex than a single directory, and such designs are the subject of Section 5.4.\n\u25a0\nSnooping\u2014Rather than keeping the state of sharing in a single directory, every\ncache that has a copy of the data from a block of physical memory could track\nthe sharing status of the block. In an SMP, the caches are typically all acces-\nsible via some broadcast medium (e.g., a bus connects the per-core caches to\nthe shared cache or memory), and all cache controllers monitor or snoop on the\nmedium to determine whether they have a copy of a block that is requested on a\nbus or switch access. Snooping can also be used as the coherence protocol for a\nmultichip multiprocessor, and some designs support a snooping protocol on\ntop of a directory protocol within each multicore.\nSnooping protocols became popular with multiprocessors using microproces-\nsors (single-core) and caches attached to a single shared memory by a bus. The bus\nprovided a convenient broadcast medium to implement the snooping protocols.\nMulticore architectures changed the picture significantly because all multicores\nshare some level of cache on the chip. Thus some designs switched to using direc-\ntory protocols, since the overhead was small. To allow the reader to become fami-\nliar with both types of protocols, we focus on a snooping protocol here and discuss\na directory protocol when we come to DSM architectures.\nSnooping Coherence Protocols\nThere are two ways to maintain the coherence requirement described in the prior\nsection. One method is to ensure that a processor has exclusive access to a data item\nbefore writing that item. This style of protocol is called a write invalidate protocol\nbecause it invalidates other copies on a write. It is by far the most common pro-\ntocol. Exclusive access ensures that no other readable or writable copies of an item\nexist when the write occurs: all other cached copies of the item are invalidated.\nFigure 5.4 shows an example of an invalidation protocol with write-back\ncaches in action. To see how this protocol ensures coherence, consider a write\nfollowed by a read by another processor: because the write requires exclusive\n380\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 413,
        "text": "access, any copy held by the reading processor must be invalidated (thus the\nprotocol name). Therefore when the read occurs, it misses in the cache and is\nforced to fetch a new copy of the data. For a write, we require that the writing\nprocessor has exclusive access, preventing any other processor from being able\nto write simultaneously. If two processors do attempt to write the same data simul-\ntaneously, one of them wins the race (we\u2019ll see how we decide who wins shortly),\ncausing the other processor\u2019s copy to be invalidated. For the other processor to\ncomplete its write, it must obtain a new copy of the data, which must now contain\nthe updated value. Therefore this protocol enforces write serialization.\nThe alternative to an invalidate protocol is to update all the cached copies of a\ndata item when that item is written. This type of protocol is called a write update or\nwrite broadcast protocol. Because a write update protocol must broadcast all\nwrites to shared cache lines, it consumes considerably more bandwidth. For this\nreason, virtually all recent multiprocessors have opted to implement a write\ninvalidate protocol, and we will focus only on invalidate protocols for the rest\nof the chapter.\nProcessor activity\nBus activity\nContents of processor\nA\u2019s cache\nContents of processor\nB\u2019s cache\nContents of memory\nlocation X\n0\nProcessor A reads X\nCache miss\nfor X\n0\n0\nProcessor B reads X\nCache miss\nfor X\n0\n0\n0\nProcessor A writes a\n1 to X\nInvalidation\nfor X\n1\n0\nProcessor B reads X\nCache miss\nfor X\n1\n1\n1\nFigure 5.4 An example of an invalidation protocol working on a snooping bus for a single cache block (X) with\nwrite-back caches. We assume that neither cache initially holds X and that the value of X in memory is 0. The pro-\ncessor and memory contents show the value after the processor and bus activity have both completed. A blank indi-\ncates no activity or no copy cached. When the second miss by B occurs, processor A responds with the value\ncanceling the response from memory. In addition, both the contents of B\u2019s cache and the memory contents of X\nare updated. This update of memory, which occurs when a block becomes shared, simplifies the protocol, but it\nis possible to track the ownership and force the write-back only if the block is replaced. This requires the introduction\nof an additional status bit indicating ownership of a block. The ownership bit indicates that a block may be shared for\nreads, but only the owning processor can write the block, and that processor is responsible for updating any other\nprocessors and memory when it changes the block or replaces it. If a multicore uses a shared cache (e.g., L3), then all\nmemory is seen through the shared cache; L3 acts like the memory in this example, and coherency must be handled\nfor the private L1 and L2 caches for each core. It is this observation that led some designers to opt for a directory\nprotocol within the multicore. To make this work, the L3 cache must be inclusive; recall from Chapter 2, that a cache is\ninclusive if any location in a higher level cache (L1 and L2 in this case) is also in L3. We return to the topic of inclusion\non page 423.\n5.2\nCentralized Shared-Memory Architectures\n\u25a0\n381"
    },
    {
        "page": 414,
        "text": "Basic Implementation Techniques\nThe key to implementing an invalidate protocol in a multicore is the use of the bus,\nor another broadcast medium, to perform invalidates. In older multiple-chip mul-\ntiprocessors, the bus used for coherence is the shared-memory access bus. In a\nsingle-chip multicore, the bus can be the connection between the private caches\n(L1 and L2 in the Intel i7) and the shared outer cache (L3 in the i7). To perform\nan invalidate, the processor simply acquires bus access and broadcasts the address\nto be invalidated on the bus. All processors continuously snoop on the bus, watch-\ning the addresses. The processors check whether the address on the bus is in their\ncache. If so, the corresponding data in the cache are invalidated.\nWhen a write to a block that is shared occurs, the writing processor must acquire\nbus access to broadcast its invalidation. If two processors attempt to write shared\nblocks at the same time, their attempts to broadcast an invalidate operation will\nbe serialized when they arbitrate for the bus. The first processor to obtain bus access\nwill cause any other copies of the block it is writing to be invalidated. If the proces-\nsors were attempting to write the same block, the serialization enforced by the bus\nwould also serialize their writes. One implication of this scheme is that a write to a\nshared data item cannot actually complete until it obtains bus access. All coherence\nschemes require some method of serializing accesses to the same cache block, either\nby serializing access to the communication medium or to another shared structure.\nIn addition to invalidating outstanding copies of a cache block that is being writ-\nten into, we also need to locate a data item when a cache miss occurs. In a write-\nthrough cache, it is easy to find the recent value of a data item because all written\ndata are always sent to the memory, from which the most recent value of a data item\ncan always be fetched. (Write buffers can lead to some additional complexities and\nmust effectively be treated as additional cache entries.)\nFor a write-back cache, the problem of finding the most recent data value is\nharder because the most recent value of a data item can be in a private cache rather\nthanintheshared cacheormemory.Fortunately,write-backcachescanusethesame\nsnooping scheme both for cache misses and for writes: each processor snoops every\naddress placed on the shared bus. If a processor finds that it has a dirty copy of the\nrequested cache block, it provides that cache block in response to the read request\nand causes the memory (or L3) access to be aborted. The additional complexity\ncomes from having to retrieve the cache block from another processor\u2019s private\ncache (L1 or L2), which can often take longer than retrieving it from L3. Because\nwrite-back caches generate lower requirements for memory bandwidth, they can\nsupport larger numbers of faster processors. As a result, all multicore processors\nuse write-back at the outermost levels of the cache, and we will examine the imple-\nmentation of coherence with write-back caches.\nThe normal cache tags can be used to implement the process of snooping, and\nthe valid bit for each block makes invalidation easy to implement. Read misses,\nwhether generated by an invalidation or by some other event, are also straightfor-\nward because they simply rely on the snooping capability. For writes, we want to\nknow whether any other copies of the block are cached because, if there are no\nother cached copies, then the write does not need to be placed on the bus in a\n382\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 415,
        "text": "write-back cache. Not sending the write reduces both the time to write and the\nrequired bandwidth.\nTo track whether or not a cache block is shared, we can add an extra state bit\nassociated with each cache block, just as we have a valid bit and a dirty bit. By\nadding a bit indicating whether the block is shared, we can decide whether a write\nmust generate an invalidate. When a write to a block in the shared state occurs, the\ncache generates an invalidation on the bus and marks the block as exclusive. No\nfurther invalidations will be sent by that core for that block. The core with the sole\ncopy of a cache block is normally called the owner of the cache block.\nWhen an invalidation is sent, the state of the owner\u2019s cache block is changed\nfrom shared to unshared (or exclusive). If another processor later requests this\ncache block, the state must be made shared again. Because our snooping cache also\nsees any misses, it knows when the exclusive cache block has been requested by\nanother processor and the state should be made shared.\nEvery bus transaction must check the cache-address tags, which could poten-\ntially interfere with processor cache accesses. One way to reduce this interference\nis to duplicate the tags and have snoop accesses directed to the duplicate tags.\nAnother approach is to use a directory at the shared L3 cache; the directory indicates\nwhether a given block is shared and possibly which cores have copies. With the\ndirectory information, invalidates can be directed only to those caches with copies\nof the cache block. This requires that L3 must always have a copy of any data item in\nL1 or L2, a property called inclusion, which we will return to in Section 5.7.\nAn Example Protocol\nA snooping coherence protocol is usually implemented by incorporating a finite-\nstate controller in each core. This controller responds to requests from the proces-\nsor in the core and from the bus (or other broadcast medium), changing the state of\nthe selected cache block, as well as using the bus to access data or to invalidate it.\nLogically, you can think of a separate controller as being associated with each\nblock; that is, snooping operations or cache requests for different blocks can pro-\nceed independently. In actual implementations, a single controller allows multiple\noperations to distinct blocks to proceed in interleaved fashion (i.e., one operation\nmay be initiated before another is completed, even though only one cache access or\none bus access is allowed at a time). Also, remember that, although we refer to a\nbus in the following description, any interconnection network that supports a\nbroadcast to all the coherence controllers and their associated private caches can\nbe used to implement snooping.\nThe simple protocol we consider has three states: invalid, shared, and modified.\nThe shared state indicates that the block in the private cache is potentially shared,\nwhereas the modified state indicates that the block has been updated in the private\ncache; note that the modified state implies that the block is exclusive. Figure 5.5\nshows the requests generated by a core (in the top half of the table) as well as\nthose coming from the bus (in the bottom half of the table). This protocol is for\na write-back cache but is easily changed to work for a write-through cache by rein-\nterpreting the modified state as an exclusive state and updating the cache on writes\n5.2\nCentralized Shared-Memory Architectures\n\u25a0\n383"
    },
    {
        "page": 416,
        "text": "in the normal fashion for a write-through cache. The most common extension\nof this basic protocol is the addition of an exclusive state, which describes a block\nthat is unmodified but held in only one private cache. We describe this and other\nextensions on page 388.\nRequest\nSource\nState of\naddressed\ncache block\nType of\ncache\naction\nFunction and explanation\nRead hit\nProcessor\nShared or\nmodified\nNormal hit\nRead data in local cache.\nRead\nmiss\nProcessor\nInvalid\nNormal miss\nPlace read miss on bus.\nRead\nmiss\nProcessor\nShared\nReplacement\nAddress conflict miss: place read miss on bus.\nRead\nmiss\nProcessor\nModified\nReplacement\nAddress conflict miss: write-back block; then place read\nmiss on bus.\nWrite hit\nProcessor\nModified\nNormal hit\nWrite data in local cache.\nWrite hit\nProcessor\nShared\nCoherence\nPlace invalidate on bus. These operations are often called\nupgrade or ownership misses, because they do not fetch the\ndata but only change the state.\nWrite\nmiss\nProcessor\nInvalid\nNormal miss\nPlace write miss on bus.\nWrite\nmiss\nProcessor\nShared\nReplacement\nAddress conflict miss: place write miss on bus.\nWrite\nmiss\nProcessor\nModified\nReplacement\nAddress conflict miss: write-back block; then place write\nmiss on bus.\nRead\nmiss\nBus\nShared\nNo action\nAllow shared cache or memory to service read miss.\nRead\nmiss\nBus\nModified\nCoherence\nAttempt to read shared data: place cache block on bus,\nwrite-back block, and change state to shared.\nInvalidate\nBus\nShared\nCoherence\nAttempt to write shared block; invalidate the block.\nWrite\nmiss\nBus\nShared\nCoherence\nAttempt to write shared block; invalidate the cache block.\nWrite\nmiss\nBus\nModified\nCoherence\nAttempt to write block that is exclusive elsewhere; write-\nback the cache block and make its state invalid in the local\ncache.\nFigure 5.5 The cache coherence mechanism receives requests from both the core\u2019s processor and the shared bus\nand responds to these based on the type of request, whether it hits or misses in the local cache, and the state of\nthe local cache block specified in the request. The fourth column describes the type of cache action as normal hit or\nmiss (the same as a uniprocessor cache would see), replacement (a uniprocessor cache replacement miss), or coher-\nence (required to maintain cache coherence); a normal or replacement action may cause a coherence action depend-\ning on the state of the block in other caches. For read, misses, write misses, or invalidates snooped from the bus, an\naction is required only if the read or write addresses match a block in the local cache and the block is valid.\n384\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 417,
        "text": "When an invalidate or a write miss is placed on the bus, any cores whose pri-\nvate caches have copies of the cache block invalidate it. For a write miss in a write-\nback cache, if the block is exclusive in just one private cache, that cache also writes\nback the block; otherwise, the data can be read from the shared cache or memory.\nFigure 5.6 shows a finite-state transition diagram for a single private cache\nblock using a write invalidation protocol and a write-back cache. For simplicity,\nthe three states of the protocol are duplicated to represent transitions based on\nCPU read hit\nShared\n(read only)\nCPU write miss\nWrite-back cache block\nPlace write miss on bus\nCPU write hit\nCPU read hit\nExclusive\n(read/write)\nExclusive\n(read/write)\nCache state transitions\nbased on requests from CPU\nPlace write\nmiss on bus\nInvalid\nCPU read\nPlace read miss on bus\nCPU write\nCPU read miss   Write-back block\nPlace read miss on bus\nPlace invalidate on bus   CPU write\nCPU write miss\nPlace write miss on bus\nPlace read\nmiss on bus\nCPU\nread\nmiss\nCache state transitions based\non requests from the bus\nWrite miss\nfor this block \nWrite-back block; abort\nmemory access\nWrite-back block;\nabort memory\naccess\nRead miss\nfor this block \nCPU\nread\nmiss\nShared\n(read only)\nInvalid\nInvalidate for\nthis block\nWrite miss for this block\nFigure 5.6 A write invalidate, cache coherence protocol for a private write-back cache showing the states and\nstate transitions for each block in the cache. The cache states are shown in circles, with any access permitted by the\nlocal processor without a state transition shown in parentheses under the name of the state. The stimulus causing a\nstate change is shown on the transition arcs in regular type, and any bus actions generated as part of the state tran-\nsition are shown on the transition arc in bold. The stimulus actions apply to a block in the private cache, not to a\nspecific address in the cache. Thus a read miss to a block in the shared state is a miss for that cache block but\nfor a different address. The left side of the diagram shows state transitions based on actions of the processor asso-\nciated with this cache; the right side shows transitions based on operations on the bus. A read miss in the exclusive or\nshared state and a write miss in the exclusive state occur when the address requested by the processor does not\nmatch the address in the local cache block. Such a miss is a standard cache replacement miss. An attempt to write\na block in the shared state generates an invalidate. Whenever a bus transaction occurs, all private caches that contain\nthe cache block specified in the bus transaction take the action dictated by the right half of the diagram. The protocol\nassumes that memory (or a shared cache) provides data on a read miss for a block that is clean in all local caches. In\nactual implementations, these two sets of state diagrams are combined. In practice, there are many subtle variations\non invalidate protocols, including the introduction of the exclusive unmodified state, as to whether a processor or\nmemory provides data on a miss. In a multicore chip, the shared cache (usually L3, but sometimes L2) acts as the\nequivalent of memory, and the bus is the bus between the private caches of each core and the shared cache, which\nin turn interfaces to the memory.\n5.2\nCentralized Shared-Memory Architectures\n\u25a0\n385"
    },
    {
        "page": 418,
        "text": "processor requests (on the left, which corresponds to the top half of the table in\nFigure 5.5), as opposed to transitions based on bus requests (on the right, which\ncorresponds to the bottom half of the table in Figure 5.5). Boldface type is used\nto distinguish the bus actions, as opposed to the conditions on which a state tran-\nsition depends. The state in each node represents the state of the selected private\ncache block specified by the processor or bus request.\nAll of the states in this cache protocol would be needed in a uniprocessor cache,\nwhere they would correspond to the invalid, valid (and clean), and dirty states.\nMost of the state changes indicated by arcs in the left half of Figure 5.6 would\nbe needed in a write-back uniprocessor cache, with the exception being the inval-\nidate on a write hit to a shared block. The state changes represented by the arcs in\nthe right half of Figure 5.6 are needed only for coherence and would not appear at\nall in a uniprocessor cache controller.\nAs mentioned earlier, there is only one finite-state machine per cache, with\nstimuli coming either from the attached processor or from the bus. Figure 5.7\nshows how the state transitions in the right half of Figure 5.6 are combined\nwith those in the left half of the figure to form a single state diagram for each\ncache block.\nTo understand why this protocol works, observe that any valid cache block is\neither in the shared state in one or more private caches or in the exclusive state in\nexactly one cache. Any transition to the exclusive state (which is required for a\nprocessor to write to the block) requires an invalidate or write miss to be placed\non the bus, causing all local caches to make the block invalid. In addition, if some\nother local cache had the block in exclusive state, that local cache generates a write-\nback, which supplies the block containing the desired address. Finally, if a read\nmiss occurs on the bus to a block in the exclusive state, the local cache with the\nexclusive copy changes its state to shared.\nThe actions in gray in Figure 5.7, which handle read and write misses on the\nbus, are essentially the snooping component of the protocol. One other property\nthat is preserved in this protocol, and in most other protocols, is that any memory\nblock in the shared state is always up to date in the outer shared cache (L2 or L3, or\nmemory if there is no shared cache), which simplifies the implementation. In fact, it\ndoes not matter whether the level out from the private caches is a shared cache or\nmemory; the key is that all accesses from the cores go through that level.\nAlthough our simple cache protocol is correct, it omits a number of complica-\ntions that make the implementation much trickier. The most important of these is\nthat the protocol assumes that operations are atomic\u2014that is, an operation can be\ndone in such a way that no intervening operation can occur. For example, the\nprotocol described assumes that write misses can be detected, acquire the\nbus, and receive a response as a single atomic action. In reality this is not true.\nIn fact, even a read miss might not be atomic; after detecting a miss in the L2\nof a multicore, the core must arbitrate for access to the bus connecting to the shared\nL3. Nonatomic actions introduce the possibility that the protocol can deadlock,\nmeaning that it reaches a state where it cannot continue. We will explore these\ncomplications later in this section and when we examine DSM designs.\n386\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 419,
        "text": "With multicore processors, the coherence among the processor cores is all\nimplemented on chip, using either a snooping or simple central directory protocol.\nMany multiprocessor chips, including the Intel Xeon and AMD Opteron, support\nmultichip multiprocessors that could be built by connecting a high-speed interface\nalready incorporated in the chip. These next-level interconnects are not just exten-\nsions of the shared bus, but use a different approach for interconnecting multicores.\nA multiprocessor built with multiple multicore chips will usually have a\ndistributed memory architecture and will need an interchip coherency mechanism\nabove and beyond the one within the chip. In most cases, some form of directory\nscheme is used.\nCPU write hit\nCPU read hit\nWrite miss\nfor block\nCPU write\nPlace write miss on bus\nRead miss for block\nCPU read miss\nWrite-back block\nPlace invalidate on bus\nCPU write\nPlace read miss on bus\nWrite miss for this block\nCPU read \nCPU write miss\nInvalid\nInvalidate for this block\nWrite-back data; place read miss on bus\nShared\n(read only)\nWrite-back block\nCPU write miss\nPlace write miss on bus\nCPU\nread\nhit\nWrite-back data\nPlace write miss on bus\nCPU\nread\nmiss\nPlace read\nmiss on bus\nExclusive\n(read/write)\nFigure 5.7 Cache coherence state diagram with the state transitions induced by the\nlocal processor shown in black and by the bus activities shown in gray. As in\nFigure 5.6, the activities on a transition are shown in bold.\n5.2\nCentralized Shared-Memory Architectures\n\u25a0\n387"
    },
    {
        "page": 420,
        "text": "Extensions to the Basic Coherence Protocol\nThe coherence protocol we have just described is a simple three-state protocol and\nis often referred to by the first letter of the states, making it a MSI (Modified,\nShared, Invalid) protocol. There are many extensions of this basic protocol, which\nwe mention in the captions of figures in this section. These extensions are created\nby adding additional states and transactions that optimize certain behaviors, pos-\nsibly resulting in improved performance. Two of the most common extensions are\n1. MESI adds the state Exclusive to the basic MSI protocol, yielding four states\n(Modified, Exclusive, Shared, and Invalid). The exclusive state indicates that\na cache block is resident in only a single cache but is clean. If a block is in\nthe E state, it can be written without generating any invalidates, which optimizes\nthe case where a block is read by a single cache before being written by that\nsame cache. Of course, when a read miss to a block in the E state occurs,\nthe block must be changed to the S state to maintain coherence. Because all\nsubsequent accesses are snooped, it is possible to maintain the accuracy of this\nstate. In particular, if another processor issues a read miss, the state is changed\nfrom exclusive to shared. The advantage of adding this state is that a subsequent\nwrite to a block in the exclusive state by the same core need not acquire bus\naccess or generate an invalidate, since the block is known to be exclusively\nin this local cache; the processor merely changes the state to modified. This state\nis easily added by using the bit that encodes the coherent state as an exclusive\nstate and using the dirty bit to indicate that a bock is modified. The Intel i7 uses\na variant of a MESI protocol, called MESIF, which adds a state (Forward) to\ndesignate which sharing processor should respond to a request. It is designed\nto enhance performance in distributed memory organizations.\n2. MOESI adds the state Owned to the MESI protocol to indicate that the associ-\nated block is owned by that cache and out-of-date in memory. In MSI and MESI\nprotocols, when there is an attempt to share a block in the Modified state, the\nstate is changed to Shared (in both the original and newly sharing cache), and\nthe block must be written back to memory. In a MOESI protocol, the block can\nbe changed from the Modified to Owned state in the original cache without\nwriting it to memory. Other caches, which are newly sharing the block, keep\nthe block in the Shared state; the O state, which only the original cache holds,\nindicates that the main memory copy is out of date and that the designated cache\nis the owner. The owner of the block must supply it on a miss, since memory\nis not up to date and must write the block back to memory if it is replaced.\nThe AMD Opteron processor family uses the MOESI protocol.\nThe next section examines the performance of these protocols for our parallel\nand multiprogrammed workloads; the value of these extensions to a basic protocol\nwill be clear when we examine the performance. But, before we do that, let\u2019s take\na brief look at the limitations on the use of a symmetric memory structure and a\nsnooping coherence scheme.\n388\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 421,
        "text": "Limitations in Symmetric Shared-Memory Multiprocessors and\nSnooping Protocols\nAs the number of processors in a multiprocessor grows, or as the memory demands\nof each processor grow, any centralized resource in the system can become a\nbottleneck. For multicores, a single shared bus became a bottleneck with only a\nfew cores. As a result, multicore designs have gone to higher bandwidth intercon-\nnection schemes, as well as multiple, independent memories to allow larger num-\nbers of cores. The multicore chips we examine in Section 5.8 use three different\napproaches:\n1. The IBM Power8, which has up to 12 processors in a single multicore, uses\n8 parallel buses that connect the distributed L3 caches and up to 8 separate\nmemory channels.\n2. The Xeon E7 uses three rings to connect up to 32 processors, a distributed\nL3 cache, and two or four memory channels (depending on the configu-\nration).\n3. The Fujitsu SPARC64 X+ uses a crossbar to connect a shared L2 to up to 16\ncores and multiple memory channels.\nThe SPARC64 X+ is a symmetric organization with uniform access time. The\nPower8 has nonuniform access time for both L3 and memory. Although the\nuncontended access time differences among memory addresses within a single\nPower8 multicore are not large, with contention for memory, the access time\ndifferences can become significant even within one chip. The Xeon E7 can\noperate as if access times were uniform; in practice, software systems usually\norganize memory so that the memory channels are associated with a subset of\nthe cores.\nSnooping bandwidth at the caches can also become a problem because every\ncache must examine every miss, and having additional interconnection bandwidth\nonly pushes the problem to the cache. To understand this problem, consider the\nfollowing example.\nExample\nConsider an 8-processor multicore where each processor has its own L1 and L2\ncaches, and snooping is performed on a shared bus among the L2 caches. Assume\nthe average L2 request, whether for a coherence miss or other miss, is 15 cycles.\nAssume a clock rate of 3.0 GHz, a CPI of 0.7, and a load/store frequency of 40%.\nIf our goal is that no more than 50% of the L2 bandwidth is consumed by coherence\ntraffic, what is the maximum coherence miss rate per processor?\nAnswer\nStart with an equation for the number of cache cycles that can be used (where CMR\nis the coherence miss rate):\n5.2\nCentralized Shared-Memory Architectures\n\u25a0\n389"
    },
    {
        "page": 422,
        "text": "Cache cycles available \u00bc\nClockrate\nCycles per request\u00032 \u00bc 3:0Ghz\n30\n\u00bc 0:1\u0003109\nCache cycles available \u00bc Memory references=clock=processor \u0003Clock rate\n\u0003processor count\u0003CMR\n\u00bc 0:4\n0:7\u00033:0GHz\u00038\u0003CMR \u00bc 13:7\u0003109 \u0003CMR\nCMR \u00bc 0:1\n13:7 \u00bc 0:0073 \u00bc 0:73%\nThis means that the coherence miss rate must be 0.73% or less. In the next section,\nwe will see several applications with coherence miss rates in excess of 1%. Alter-\nnatively, if we assume that CMR can be 1%, then we could support just under 6\nprocessors. Clearly, even small multicores will require a method for scaling snoop\nbandwidth.\nThere are several techniques for increasing the snoop bandwidth:\n1. As mentioned earlier, the tags can be duplicated. This doubles the effective\ncache-level snoop bandwidth. If we assume that half the coherence requests\ndo not hit on a snoop request and the cost of the snoop request is only 10 cycles\n(versus 15), then we can cut the average cost of a CMR to 12.5 cycles. This\nreduction allows the coherence miss rate to be 0.88, or alternatively to support\none additional processor (7 versus 6).\n2. If the outermost cache on a multicore (typically L3) is shared, we can distribute\nthat cache so that each processor has a portion of the memory and handles\nsnoops for that portion of the address space. This approach, used by the\nIBM 12-core Power8, leads to a NUCA design, but effectively scales the snoop\nbandwidth at L3 by the number of processors. If there is a snoop hit in L3, then\nwe must still broadcast to all L2 caches, which must in turn snoop their contents.\nSince L3 is acting as a filter on the snoop requests, L3 must be inclusive.\n3. We can place a directory at the level of the outermost shared cache (say, L3).\nL3 acts as a filter on snoop requests and must be inclusive. The use of a directory\nat L3 means that we need not snoop or broadcast to all the L2 caches, but\nonly those that the directory indicates may have a copy of the block. Just as L3\nmay be distributed, the associated directory entries may also be distributed. This\napproach is used in the Intel Xeon E7 series, which supports from 8 to 32 cores.\nFigure 5.8 shows how a multicore with a distributed cache system, such as that\nused in schemes 2 or 3, might look. If additional multicore chips were added to\nform a larger multiprocessor, an off-chip network would needed, as well as a\nmethod to extend the coherence mechanisms (as we will see in Section 5.8).\n390\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 423,
        "text": "The AMD Opteron represents another intermediate point in the spectrum\nbetween a snooping and a directory protocol. Memory is directly connected to each\nmulticore chip, and up to four multicore chips can be connected. The system is a\nNUMA because local memory is somewhat faster. The Opteron implements its\ncoherence protocol using the point-to-point links to broadcast up to three other\nchips. Because the interprocessor links are not shared, the only way a processor\ncan know when an invalid operation has completed is by an explicit acknowledg-\nment. Thus the coherence protocol uses a broadcast to find potentially shared cop-\nies, like a snooping protocol, but uses the acknowledgments to order operations,\nlike a directory protocol. Because local memory is only somewhat faster than\nremote memory in the Opteron implementation, some software treats the Opteron\nmultiprocessor as having uniform memory access.\nIn Section 5.4, we examine directory-based protocols, which eliminate the\nneed for broadcast to all caches on a miss. Some multicore designs use directories\nProcessor\nInterconnection network\nProcessor\nProcessor\nProcessor\nMemory\nBank 3\nshared\ncache\nBank 2\nshared\ncache\nBank 1\nshared\ncache\nBank 0\nshared\ncache\nOne or\nmore levels\nof private\ncache\nOne or\nmore levels\nof private\ncache\nOne or\nmore levels\nof private\ncache\nOne or\nmore levels\nof private\ncache\nI/O system\nFigure 5.8 A single-chip multicore with a distributed cache. In current designs, the\ndistributed shared cache is usually L3, and levels L1 and L2 are private. There are typ-\nically multiple memory channels (2\u20138 in today\u2019s designs). This design is NUCA, since the\naccess time to L3 portions varies with faster access time for the directly attached core.\nBecause it is NUCA, it is also NUMA.\n5.2\nCentralized Shared-Memory Architectures\n\u25a0\n391"
    },
    {
        "page": 424,
        "text": "within the multicore (Intel Xeon E7), while others add directories when scaling\nbeyond a multicore. Distributed directories eliminate the need for a single point\nto serialize all accesses (typically a single shared bus in a snooping scheme),\nand any scheme that removes the single point of serialization must deal with many\nof the same challenges as a distributed directory scheme.\nImplementing Snooping Cache Coherence\nThe devil is in the details.\nClassic proverb\nWhen we wrote the first edition of this book in 1990, our final \u201cPutting It All\nTogether\u201d was a 30-processor, single-bus multiprocessor using snoop-based cohe-\nrence; the bus had a capacity of just over 50 MiB/s, which would not be enough bus\nbandwidth to support even one core of an Intel i7 in 2017! When we wrote the\nsecond edition of this book in 1995, the first cache coherence multiprocessors with\nmore than a single bus had recently appeared, and we added an appendix descri-\nbing the implementation of snooping in a system with multiple buses. In 2017,\nevery multicore multiprocessor system that supports 8 or more cores uses an inter-\nconnect other than a single bus, and designers must face the challenge of imple-\nmenting snooping (or a directory scheme) without the simplification of a bus to\nserialize events.\nAs we observed on page 386, the major complication in actually implement-\ning the snooping coherence protocol we have described is that write and\nupgrade misses are not atomic in any recent multiprocessor. The steps of detect-\ning a write or upgrade miss, communicating with the other processors and\nmemory, getting the most recent value for a write miss and ensuring that any\ninvalidates are processed, and updating the cache cannot be done as though they\ntook a single cycle.\nIn a multicore with a single bus, these steps can be made effectively atomic by\narbitrating for the bus to the shared cache or memory first (before changing the\ncache state) and not releasing the bus until all actions are complete. How can\nthe processor know when all the invalidates are complete? In early designs, a single\nline was used to signal when all necessary invalidates had been received and were\nbeing processed. Following that signal, the processor that generated the miss could\nrelease the bus, knowing that any required actions would be completed before any\nactivity related to the next miss. By holding the bus exclusively during these steps,\nthe processor effectively made the individual steps atomic.\nIn a system without a single, central bus, we must find some other method of\nmaking the steps in a miss atomic. In particular, we must ensure that two processors\nthat attempt to write the same block at the same time, a situation which is called a\nrace, are strictly ordered: one write is processed and precedes before the next is\nbegun. It does not matter which of two writes in a race wins the race, just that there\nbe only a single winner whose coherence actions are completed first. In a multicore\n392\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 425,
        "text": "using multiple buses, races can be eliminated if each block of memory is associated\nwith only a single bus, ensuring that two attempts to access the same block must be\nserialized by that common bus. This property, together with the ability to restart the\nmiss handling of the loser in a race, are the keys to implementing snooping cache\ncoherence without a bus. We explain the details in Appendix I.\nIt is possible to combine snooping and directories, and several designs use\nsnooping within a multicore and directories among multiple chips or a combination\nof directories at one cache level and snooping at another level.\n5.3\nPerformance of Symmetric Shared-Memory\nMultiprocessors\nIn a multicore using a snooping coherence protocol, several different phenomena\ncombine to determine performance. In particular, the overall cache performance is\na combination of the behavior of uniprocessor cache miss traffic and the traffic\ncaused by communication, which results in invalidations and subsequent cache\nmisses. Changing t he processor count, cache size, and block size can affect these\ntwo components of the miss rate in different ways, leading to overall system behav-\nior that is a combination of the two effects.\nAppendix B breaks the uniprocessor miss rate into the three C\u2019s classification\n(capacity, compulsory, and conflict) and provides insight into both application\nbehavior and potential improvements to the cache design. Similarly, the misses that\narise from interprocessor communication, which are often called coherence misses,\ncan be broken into two separate sources.\nThe first source is the true sharing misses that arise from the communication of\ndata through the cache coherence mechanism. In an invalidation-based protocol,\nthe first write by a processor to a shared cache block causes an invalidation to\nestablish ownership of that block. Additionally, when another processor attempts\nto read a modified word in that cache block, a miss occurs and the resultant block is\ntransferred. Both these misses are classified as true sharing misses because they\ndirectly arise from the sharing of data among processors.\nThe second effect, called false sharing, arises from the use of an invalidation-\nbased coherence algorithm with a single valid bit per cache block. False sharing\noccurs when a block is invalidated (and a subsequent reference causes a miss)\nbecause some word in the block, other than the one being read, is written into.\nIf the word written into is actually used by the processor that received the inval-\nidate, then the reference was a true sharing reference and would have caused a miss\nindependent of the block size. If, however, the word being written and the word\nread are different and the invalidation does not cause a new value to be commu-\nnicated, but only causes an extra cache miss, then it is a false sharing miss. In a false\nsharing miss, the block is shared, but no word in the cache is actually shared, and\nthe miss would not occur if the block size were a single word. The following exam-\nple makes the sharing patterns clear.\n5.3\nPerformance of Symmetric Shared-Memory Multiprocessors\n\u25a0\n393"
    },
    {
        "page": 426,
        "text": "Example\nAssume that words z1 and z2 are in the same cache block, which is in the shared\nstate in the caches of both P1 and P2. Assuming the following sequence of events,\nidentify each miss as a true sharing miss, a false sharing miss, or a hit. Any miss\nthat would occur if the block size were one word is designated a true sharing miss.\nTime\nP1\nP2\n1\nWrite z1\n2\nRead z2\n3\nWrite z1\n4\nWrite z2\n5\nRead z2\nAnswer\nHere are the classifications by time step:\n1. This event is a true sharing miss, since z1 is in the shared state in P2 and needs to\nbe invalidated from P2.\n2. This event is a false sharing miss, since z2 was invalidated by the write of z1 in\nP1, but that value of z1 is not used in P2.\n3. This event is a false sharing miss, since the block containing z1 is marked\nshared due to the read in P2, but P2 did not read z1. The cache block containing\nz1 will be in the shared state after the read by P2; a write miss is required to\nobtain exclusive access to the block. In some protocols, this will be handled\nas an upgrade request, which generates a bus invalidate, but does not transfer\nthe cache block.\n4. This event is a false sharing miss for the same reason as step 3.\n5. This event is a true sharing miss, since the value being read was written by P2.\nAlthough we will see the effects of true and false sharing misses in commercial\nworkloads, the role of coherence misses is more significant for tightly coupled\napplications that share significant amounts of user data. We examine their effects\nin detail in Appendix I when we consider the performance of a parallel scientific\nworkload.\nA Commercial Workload\nIn this section, we examine the memory system behavior of a 4-processor shared-\nmemory multiprocessor when running an online transaction processing workload.\nThe study we examine was done with a 4-processor Alpha system in 1998, but it\nremains the most comprehensive and insightful study of the performance of a mul-\ntiprocessor for such workloads. We will focus on understanding the multiprocessor\ncache activity, and particularly the behavior in L3, where much of the traffic is\ncoherence-related.\n394\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 427,
        "text": "The results were collected either on an AlphaServer 4100 or using a configur-\nable simulator modeled after the AlphaServer 4100. Each processor in the Alpha-\nServer 4100 is an Alpha 21164, which issues up to four instructions per clock and\nruns at 300 MHz. Although the clock rate of the Alpha processor in this system is\nconsiderably slower than processors in systems designed in 2017, the basic struc-\nture of the system, consisting of a four-issue processor and a three-level cache hier-\narchy, is very similar to the multicore Intel i7 and other processors, as shown in\nFigure 5.9. Rather than focus on the performance details, we consider data that\nlooks at the simulated L3 behavior for L3 caches varying from 2 to 8 MiB per\nprocessor.\nAlthough the original study considered three different workloads, we focus our\nattention on the online transaction-processing (OLTP) workload modeled after\nTPC-B (which has memory behavior similar to its newer cousin TPC-C, described\nin Chapter 1) and using Oracle 7.3.2 as the underlying database. The workload con-\nsists of a set of client processes that generate requests and a set of servers that han-\ndle them. The server processes consume 85% of the user time, with the remaining\ngoing to the clients. Although the I/O latency is hidden by careful tuning and\nenough requests to keep the processor busy, the server processes typically block\nfor I/O after about 25,000 instructions. Overall, 71% of the execution time is spent\nin user mode, 18% in the operating system, and 11% idle, primarily waiting for I/O.\nOf the commercial applications studied, the OLTP application stresses the memory\nsystem the hardest and shows significant challenges even when evaluated with\nmuch larger L3 caches. For example, on the AlphaServer, the processors are stalled\nCache level\nCharacteristic\nAlpha 21164\nIntel i7\nL1\nSize\n8 KB I/8 KB D\n32 KB I/32 KB D\nAssociativity\nDirect-mapped\n8-way I/8-way D\nBlock size\n32 B\n64 B\nMiss penalty\n7\n10\nL2\nSize\n96 KB\n256 KB\nAssociativity\n3-way\n8-way\nBlock size\n32 B\n64 B\nMiss penalty\n21\n35\nL3\nSize\n2 MiB (total 8 MiB unshared)\n2 MiB per core (8 MiB total shared)\nAssociativity\nDirect-mapped\n16-way\nBlock size\n64 B\n64 B\nMiss penalty\n80\n\u0004100\nFigure 5.9 The characteristics of the cache hierarchy of the Alpha 21164 used in this study and the Intel i7.\nAlthough the sizes are larger and the associativity is higher on the i7, the miss penalties are also higher, so the behav-\nior may differ only slightly. Both systems have a high penalty (125 cycles or more) for a transfer required from a private\ncache. A key difference is that L3 is shared in the i7 versus four separate, unshared caches in the Alpha server.\n5.3\nPerformance of Symmetric Shared-Memory Multiprocessors\n\u25a0\n395"
    },
    {
        "page": 428,
        "text": "for approximately 90% of the cycles with memory accesses occupying almost half\nthe stall time and L2 misses 25% of the stall time.\nWe start by examining the effect of varying the size of the L3 cache. In these\nstudies, the L3 cache is varied from 1 to 8 MiB per processor; at 2 MiB per pro-\ncessor, the total size of L3 is equal to that of the Intel i7 6700. In the case of the i7,\nhowever, the cache is shared, which provides both some advantages and disadvan-\ntages. It is unlikely that the shared 8 MiB cache will outperform separate L3 caches\nwith a total size of 16 MiB. Figure 5.10 shows the effect of increasing the cache\nsize, using two-way set associative caches, which reduces the large number of con-\nflict misses. The execution time is improved as the L3 cache grows because of the\nreduction in L3 misses. Surprisingly, almost all of the gain occurs in going from 1\nto 2 MiB (or 4 to 8 MiB of total cache for the four processors). There is little addi-\ntional gain beyond that, despite the fact that cache misses are still a cause of sig-\nnificant performance loss with 2 MiB and 4 MiB caches. The question is, Why?\nTo better understand the answer to this question, we need to determine what\nfactors contribute to the L3 miss rate and how they change as the L3 cache grows.\nFigure 5.11 shows these data, displaying the number of memory access cycles con-\ntributed per instruction from five sources. The two largest sources of L3 memory\naccess cycles with a 1 MiB L3 are instruction and capacity/conflict misses. With a\nlarger L3, these two sources shrink to be minor contributors. Unfortunately, the\n100\n90\n80\n70\n60\n50\nNormalized execution time  \n40\n30\n20\n10\n0\n1\n2\n4\n8\nL3 cache size (MB)\nPAL code\nMemory access\nL2/L3 cache access\nIdle\nInstruction execution\nFigure 5.10 The relative performance of the OLTP workload as the size of the L3\ncache, which is set as two-way set associative, grows from 1 to 8 MiB. The idle time\nalso grows as cache size is increased, reducing some of the performance gains. This\ngrowth occurs because, with fewer memory system stalls, more server processes are\nneeded to cover the I/O latency. The workload could be retuned to increase the com-\nputation/communication balance, holding the idle time in check. The PAL code is a set\nof sequences of specialized OS-level instructions executed in privileged mode; an exam-\nple is the TLB miss handler.\n396\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 429,
        "text": "compulsory, false sharing, and true sharing misses are unaffected by a larger L3.\nThus, at 4 and 8 MiB, the true sharing misses generate the dominant fraction of the\nmisses; the lack of change in true sharing misses leads to the limited reductions in\nthe overall miss rate when increasing the L3 cache size beyond 2 MiB.\nIncreasing the cache size eliminates most of the uniprocessor misses while\nleaving the multiprocessor misses untouched. How does increasing the processor\ncount affect different types of misses? Figure 5.12 shows these data assuming a\nbase configuration with a 2 MiB, two-way set associative L3 cache (the same\neffective per processor cache size as the i7 but with less associativity). As we might\nexpect, the increase in the true sharing miss rate, which is not compensated for by\nany decrease in the uniprocessor misses, leads to an overall increase in the memory\naccess cycles per instruction.\nThe final question we examine is whether increasing the block size\u2014which\nshould decrease the instruction and cold miss rate and, within limits, also reduce\nthe capacity/conflict miss rate and possibly the true sharing miss rate\u2014is helpful\nfor this workload. Figure 5.13 shows the number of misses per 1000 instructions as\nthe block size is increased from 32 to 256 bytes. Increasing the block size from 32\nto 256 bytes affects four of the miss rate components:\n\u25a0\nThe true sharing miss rate decreases by more than a factor of 2, indicating some\nlocality in the true sharing patterns.\n3.25\n3\n2.75\n2.5\n2.25\n2\n1.75\n1.5\n1.25\n1\n0.75\n0.5\n0.25\nMemory cycles per instruction\n0\n1\n2\n4\n8\nCache size (MB)\nCompulsory\nCapacity/conflict\nFalse sharing\nInstruction\nTrue sharing\nFigure 5.11 The contributing causes of memory access cycle shift as the cache size is\nincreased. The L3 cache is simulated as two-way set associative.\n5.3\nPerformance of Symmetric Shared-Memory Multiprocessors\n\u25a0\n397"
    },
    {
        "page": 430,
        "text": "3\n2.5\n2\n1.5\n1\n0.5\n0\nMemory cycles per instruction\n1\n2\n4\n6\n8\nProcessor count\nCompulsory\nCapacity/conflict\nFalse sharing\nInstruction\nTrue sharing\nFigure 5.12 The contribution to memory access cycles increases as processor count\nincreases primarily because of increased true sharing. The compulsory misses slightly\nincrease because each processor must now handle more compulsory misses.\n16\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nMisses per 1000 instructions\n32\n64\n128\n256\nBlock size (bytes)\nCompulsory\nCapacity/conflict\nFalse sharing\nInstruction\nTrue sharing\nFigure 5.13 The number of misses per 1000 instructions drops steadily as the block\nsize of the L3 cache is increased, making a good case for an L3 block size of at least\n128 bytes. The L3 cache is 2 MiB, two-way set associative.\n398\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 431,
        "text": "\u25a0\nThe compulsory miss rate significantly decreases, as we would expect.\n\u25a0\nThe conflict/capacity misses show a small decrease (a factor of 1.26 compared\nto a factor of 8 increase in block size), indicating that the spatial locality is not\nhigh in the uniprocessor misses that occur with L3 caches larger than 2 MiB.\n\u25a0\nThe false sharing miss rate, although small in absolute terms, nearly doubles.\nThe lack of a significant effect on the instruction miss rate is startling. If there\nwere an instruction-only cache with this behavior, we would conclude that the spa-\ntial locality is very poor. In the case of mixed L2 and L3 caches, other effects such\nas instruction-data conflicts may also contribute to the high instruction cache miss\nrate for larger blocks. Other studies have documented the low spatial locality in the\ninstruction stream of large database and OLTP workloads, which have lots of short\nbasic blocks and special-purpose code sequences. Based on these data, the miss\npenalty for a larger block size L3 to perform as well as the 32-byte block size\nL3 can be expressed as a multiplier on the 32-byte block size penalty.\nBlock size\nMiss penalty relative to 32-byte\nblock miss penalty\n64 bytes\n1.19\n128 bytes\n1.36\n256 bytes\n1.52\nWith modern DDR SDRAMs that make block access fast, these numbers are\nattainable, especially at the 64 byte (the i7 block size) and the 128 byte block size.\nOf course, we must also worry about the effects of the increased traffic to memory\nand possible contention for the memory with other cores. This latter effect may\neasily negate the gains obtained from improving the performance of a single\nprocessor.\nA Multiprogramming and OS Workload\nOur next study is a multiprogrammed workload consisting of both user activity and\nOS activity. The workload used is two independent copies of the compile phases of\nthe Andrew benchmark, a benchmark that emulates a software development envi-\nronment. The compile phase consists of a parallel version of the UNIX \u201cmake\u201d\ncommand executed using eight processors. The workload runs for 5.24 seconds\non eight processors, creating 203 processes and performing 787 disk requests\non three different file systems. The workload is run with 128 MiB of memory,\nand no paging activity takes place.\nThe workload has three distinct phases: compiling the benchmarks, which\ninvolves substantial compute activity; installing the object files in a library; and\nremoving the object files. The last phase is completely dominated by I/O, and only\ntwo processes are active (one for each of the runs). In the middle phase, I/O also\n5.3\nPerformance of Symmetric Shared-Memory Multiprocessors\n\u25a0\n399"
    },
    {
        "page": 432,
        "text": "plays a major role, and the processor is largely idle. The overall workload is much\nmore system- and I/O-intensive than the OLTP workload.\nFor the workload measurements, we assume the following memory and I/O\nsystems:\n\u25a0\nLevel 1 instruction cache\u201432 KB, two-way set associative with a 64-byte\nblock, 1 clock cycle hit time.\n\u25a0\nLevel 1 data cache\u201432 KB, two-way set associative with a 32-byte block, 1\nclock cycle hit time. Our focus is on examining the behavior in the Level 1 data\ncache, in contrast to the OLTP study, which focused on the L3 cache.\n\u25a0\nLevel 2 cache\u20141 MiB unified, two-way set associative with a 128-byte block,\n10 clock cycle hit time.\n\u25a0\nMain memory\u2014Single memory on a bus with an access time of 100 clock\ncycles.\n\u25a0\nDisk system\u2014Fixed-access latency of 3 ms (less than normal to reduce\nidle time).\nFigure 5.14 shows how the execution time breaks down for the eight processors\nusing the parameters just listed. Execution time is broken down into four\ncomponents:\n1. Idle\u2014Execution in the kernel mode idle loop\n2. User\u2014Execution in user code\n3. Synchronization\u2014Execution or waiting for synchronization variables\n4. Kernel\u2014Execution in the OS that is neither idle nor in synchronization access\nThis multiprogramming workload has a significant instruction cache perfor-\nmance loss, at least for the OS. The instruction cache miss rate in the OS for a\n64-byte block size, two-way set associative cache varies from 1.7% for a 32 KB\nUser\nexecution\nKernel\nexecution\nSynchronization\nwait\nProcessor idle\n(waiting for I/O)\nInstructions\nexecuted\n27%\n3%\n1%\n69%\nExecution\ntime\n27%\n7%\n2%\n64%\nFigure 5.14 The distribution of execution time in the multiprogrammed parallel\n\u201cmake\u201d workload. The high fraction of idle time is due to disk latency when only\none of the eight processors is active. These data and the subsequent measurements\nfor this workload were collected with the SimOS system (Rosenblum et al., 1995).\nThe actual runs and data collection were done by M. Rosenblum, S. Herrod, and\nE. Bugnion of Stanford University.\n400\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 433,
        "text": "cache to 0.2% for a 256 KB cache. User-level instruction cache misses are roughly\none-sixth of the OS rate, across the variety of cache sizes. This partially accounts\nfor the fact that, although the user code executes nine times as many instructions as\nthe kernel, those instructions take only about four times as long as the smaller num-\nber of instructions executed by the kernel.\nPerformance of the Multiprogramming and OS Workload\nIn this section, we examine the cache performance of the multiprogrammed work-\nload as the cache size and block size are changed. Because of differences between\nthe behavior of the kernel and that of the user processes, we keep these two com-\nponents separate. Remember, though, that the user processes execute more than\neight times as many instructions, so the overall miss rate is determined primarily\nby the miss rate in user code, which as we will see, is often one-fifth of the kernel\nmiss rate.\nAlthough the user code executes more instructions, the behavior of the oper-\nating system can cause more cache misses than the user processes for two reasons\nbeyond larger code size and lack of locality. First, the kernel initializes all pages\nbefore allocating them to a user, which significantly increases the compulsory\ncomponent of the kernel\u2019s miss rate. Second, the kernel actually shares data and\nthus has a nontrivial coherence miss rate. In contrast, user processes cause coher-\nence misses only when the process is scheduled on a different processor, and this\ncomponent of the miss rate is small. This is a major difference between a multi-\nprogrammed workload and one like the OLTP workload.\nFigure 5.15 shows the data miss rate versus data cache size and versus block\nsize for the kernel and user components. Increasing the data cache size affects the\nuser miss rate more than it affects the kernel miss rate. Increasing the block size has\nbeneficial effects for both miss rates because a larger fraction of the misses arise\nfrom compulsory and capacity, both of which can be potentially improved with\nlarger block sizes. Because coherence misses are relatively rarer, the negative\neffects of increasing block size are small. To understand why the kernel and user\nprocesses behave differently, we can look at how the kernel misses behave.\nFigure 5.16 shows the variation in the kernel misses versus increases in cache\nsize and in block size. The misses are broken into three classes: compulsory misses,\ncoherence misses (from both true and false sharing), and capacity/conflict misses\n(which include misses caused by interference between the OS and the user process\nand between multiple user processes). Figure 5.16 confirms that, for the kernel ref-\nerences, increasing the cache size reduces only the uniprocessor capacity/conflict\nmiss rate. In contrast, increasing the block size causes a reduction in the compulsory\nmiss rate. The absence of large increases in the coherence miss rate as block size is\nincreased means that false sharing effects are probably insignificant, although such\nmisses may be offsetting some of the gains from reducing the true sharing misses.\nIf we examine the number of bytes needed per data reference, as in Figure 5.17,\nwe see that the kernel has a higher traffic ratio that grows with block size. It is easy\nto see why this occurs: when going from a 16-byte block to a 128-byte block, the\n5.3\nPerformance of Symmetric Shared-Memory Multiprocessors\n\u25a0\n401"
    },
    {
        "page": 434,
        "text": "4%\n5%\n6%\n7%\n3%\n2%\n1%\nMiss rate\nMiss rate\n0%\nCache size (KB)\n32\n64\n128\n256\nKernel miss rate\nUser miss rate\n6%\n7%\n8%\n9%\n10%\n4%\n5%\n3%\n1%\n2%\n0%\nBlock size (bytes)\n16\n32\n64\n128\nKernel miss rate\nUser miss rate\nFigure 5.15 The data miss rates for the user and kernel components behave differently for increases in the L1\ndata cache size (on the left) versus increases in the L1 data cache block size (on the right). Increasing the L1 data\ncache from 32 to 256 KB (with a 32-byte block) causes the user miss rate to decrease proportionately more than the\nkernel miss rate: the user-level miss rate drops by almost a factor of 3, whereas the kernel-level miss rate drops by a\nfactor of only 1.3. At the largest size, the L1 is closer to the size of L2 in a modern multicore processors. Thus the data\nindicates that the kernel miss rate will still be significant in an L2 cache. The miss rate for both user and kernel com-\nponents drops steadily as the L1 block size is increased (while keeping the L1 cache at 32 KB). In contrast to the effects\nof increasing the cache size, increasing the block size improves the kernel miss rate more significantly (just under a\nfactor of 4 for the kernel references when going from 16-byte to 128-byte blocks versus just under a factor of 3 for the\nuser references).\nMiss rate\nMiss rate\n0%\n2%\n4%\n6%\n5%\n3%\n1%\n32\n64\n128\nCache size (KB)\n256\n7%\n0%\n2%\n4%\n9%\n8%\n7%\n6%\n5%\n3%\n1%\n16\n32\n64\nBlock size (bytes)\n128\n10%\nCompulsory\nCoherence\nCapacity/conflict\nFigure 5.16 The components of the kernel data miss rate change as the L1 data cache size is increased from 32 to\n256 KB, when the multiprogramming workload is run on eight processors. The compulsory miss rate component\nstays constant because it is unaffected by cache size. The capacity component drops by more than a factor of 2,\nwhereas the coherence component nearly doubles. The increase in coherence misses occurs because the probability\nof a miss being caused by an invalidation increases with cache size, since fewer entries are bumped due to capacity.\nAs we would expect, the increasing block size of the L1 data cache substantially reduces the compulsory miss rate in\nthe kernel references. It also has a significant impact on the capacity miss rate, decreasing it by a factor of 2.4 over the\nrange of block sizes. The increased block size has a small reduction in coherence traffic, which appears to stabilize at\n64 bytes, with no change in the coherence miss rate in going to 128-byte lines. Because there are no significant reduc-\ntions in the coherence miss rate as the block size increases, the fraction of the miss rate caused by coherence grows\nfrom about 7% to about 15%."
    },
    {
        "page": 435,
        "text": "miss rate drops by about 3.7, but the number of bytes transferred per miss increases\nby 8, so the total miss traffic increases by just over a factor of 2. The user program\nalso more than doubles as the block size goes from 16 to 128 bytes, but it starts out\nat a much lower level.\nFor the multiprogrammed workload, the OS is a much more demanding user of\nthe memory system. If more OS or OS-like activity is included in the workload, and\nthe behavior is similar to what was measured for this workload, it will become very\ndifficult to build a sufficiently capable memory system. One possible route to\nimproving performance is to make the OS more cache-aware through either better\nprogramming environments or through programmer assistance. For example, the\nOS reuses memory for requests that arise from different system calls. Despite the\nfact that the reused memory will be completely overwritten, the hardware, not rec-\nognizing this, will attempt to preserve coherency and the possibility that some por-\ntion of a cache block may be read, even if it is not. This behavior is analogous to the\nreuse of stack locations on procedure invocations. The IBM Power series has sup-\nport to allow the compiler to indicate this type of behavior on procedure invoca-\ntions, and the newest AMD processors have similar support. It is harder to detect\nsuch behavior by the OS, and doing so may require programmer assistance, but the\npayoff is potentially even greater.\nOS and commercial workloads pose tough challenges for multiprocessor mem-\nory systems, and unlike scientific applications, which we examine in Appendix I,\nthey are less amenable to algorithmic or compiler restructuring. As the number of\ncores increases, predicting the behavior of such applications is likely to get more\ndifficult. Emulation or simulation methodologies that allow the simulation of tens\n3.5\n2.0\n2.5\n3.0\n1.5\n1.0\n0.5\nMemory traffic measured as bytes \nper data reference\n0.0\nBlock size (bytes)\n16\n32\n64\n128\nKernel traffic\nUser traffic\nFigure 5.17 The number of bytes needed per data reference grows as block size is\nincreased for both the kernel and user components. It is interesting to compare this\nchart with the data on scientific programs shown in Appendix I.\n5.3\nPerformance of Symmetric Shared-Memory Multiprocessors\n\u25a0\n403"
    },
    {
        "page": 436,
        "text": "to hundreds of cores with large applications (including operating systems) will be\ncrucial to maintaining an analytical and quantitative approach to design.\n5.4\nDistributed Shared-Memory and Directory-Based\nCoherence\nAs we saw in Section 5.2, a snooping protocol requires communication with all\ncaches on every cache miss, including writes of potentially shared data. The\nabsence of any centralized data structure that tracks the state of the caches is both\nthe fundamental advantage of a snooping-based scheme, since it allows it to be\ninexpensive, as well as its Achilles\u2019 heel when it comes to scalability.\nFor example, consider a multiprocessor consisting of four 4-core multicores\ncapable of sustaining one data reference per clock and a 4 GHz clock. From the\ndata in Section I.5 in Appendix I, we can see that the applications may require\n4\u2013170 GiB/s of memory bus bandwidth. The maximum memory bandwidth sup-\nported by the i7 with two DDR4 memory channels is 34 GiB/s. If several i7 multi-\ncore processors shared the same memory system, they would easily swamp it. In\nthe last few years, the development of multicore processors forced all designers to\nshift to some form of distributed memory to support the bandwidth demands of the\nindividual processors.\nWe can increase the memory bandwidth and interconnection bandwidth by dis-\ntributing the memory, as shown in Figure 5.2 on page 373; this immediately sep-\narates local memory traffic from remote memory traffic, reducing the bandwidth\ndemands on the memory system and on the interconnection network. Unless we\neliminate the need for the coherence protocol to broadcast on every cache miss,\ndistributing the memory will gain us little.\nAs we mentioned earlier, the alternative to a snooping-based coherence proto-\ncol is a directory protocol. A directory keeps the state of every block that may be\ncached. Information in the directory includes which caches (or collections of\ncaches) have copies of the block, whether it is dirty, and so on. Within a multicore\nwith a shared outermost cache (say, L3), it is easy to implement a directory scheme:\nsimply keep a bit vector of the size equal to the number of cores for each L3 block.\nThe bit vector indicates which private L2 caches may have copies of a block in L3,\nand invalidations are sent only to those caches. This works perfectly for a single\nmulticore if L3 is inclusive, and this scheme is the one used in the Intel i7.\nThe solution of a single directory used in a multicore is not scalable, even\nthough it avoids broadcast. The directory must be distributed, but the distribution\nmust be done in a way that the coherence protocol knows where to find the direc-\ntory information for any cached block of memory. The obvious solution is to dis-\ntribute the directory along with the memory so that different coherence requests\ncan go to different directories, just as different memory requests go to different\nmemories. If the information is maintained at an outer cache, like L3, which is mul-\ntibanked, the directory information can be distributed with the different cache\nbanks, effectively increasing the bandwidth.\n404\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 437,
        "text": "A distributed directory retains the characteristic that the sharing status of a\nblock is always in a single known location. This property, together with the main-\ntenance of information that says what other nodes may be caching the block, is\nwhat allows the coherence protocol to avoid broadcast. Figure 5.18 shows how\nour distributed-memory multiprocessor looks with the directories added to\neach node.\nThe simplest directory implementations associate an entry in the directory with\neach memory block. In such implementations, the amount of information is pro-\nportional to the product of the number of memory blocks (where each block is\nthe same size as the L2 or L3 cache block) times the number of nodes, where a\nnode is a single multicore processor or a small collection of processors that imple-\nments coherence internally. This overhead is not a problem for multiprocessors\nwith less than a few hundred processors (each of which might be a multicore)\nbecause the directory overhead with a reasonable block size will be tolerable.\nFor larger multiprocessors, we need methods to allow the directory structure to\nbe efficiently scaled, but only supercomputer-sized systems need to worry\nabout this.\nInterconnection network\nMulticore\nprocessor\n+ caches\nMemory\nI/O\nDirectory\nMemory\nI/O\nDirectory\nMemory\nI/O\nDirectory\nMemory\nI/O\nMemory\nDirectory\nMemory\nI/O\nMemory\nMemory\nI/O\nI/O\nMemory\nI/O\nDirectory\nDirectory\nDirectory\nDirectory\nMulticore\nprocessor\n+ caches\nMulticore\nprocessor\n+ caches\nMulticore\nprocessor\n+ caches\nMulticore\nprocessor\n+ caches\nMulticore\nprocessor\n+ caches\nMulticore\nprocessor\n+ caches\nMulticore\nprocessor\n+ caches\nFigure 5.18 A directory is added to each node to implement cache coherence in a distributed-memory multi-\nprocessor. In this case, a node is shown as a single multicore chip, and the directory information for the associated\nmemory may reside either on or off the multicore. Each directory is responsible for tracking the caches that share the\nmemory addresses of the portion of memory in the node. The coherence mechanism will handle both the mainte-\nnance of the directory information and any coherence actions needed within the multicore node.\n5.4\nDistributed Shared-Memory and Directory-Based Coherence\n\u25a0\n405"
    },
    {
        "page": 438,
        "text": "Directory-Based Cache Coherence Protocols: The Basics\nJust as with a snooping protocol, there are two primary operations that a directory\nprotocol must implement: handling a read miss and handling a write to a shared,\nclean cache block. (Handling a write miss to a block that is currently shared is a\nsimple combination of these two.) To implement these operations, a directory must\ntrack the state of each cache block. In a simple protocol, these states could be the\nfollowing:\n\u25a0\nShared\u2014One or more nodes have the block cached, and the value in memory is\nup to date (as well as in all the caches).\n\u25a0\nUncached\u2014No node has a copy of the cache block.\n\u25a0\nModified\u2014Exactly one node has a copy of the cache block, and it has written\nthe block, so the memory copy is out of date. The processor is called the owner\nof the block.\nIn addition to tracking the state of each potentially shared memory block, we\nmust track which nodes have copies of that block because those copies will need to\nbe invalidated on a write. The simplest way to do this is to keep a bit vector for each\nmemory block. When the block is shared, each bit of the vector indicates whether\nthe corresponding processor chip (which is likely a multicore) has a copy of that\nblock. We can also use the bit vector to keep track of the owner of the block when\nthe block is in the exclusive state. For efficiency reasons, we also track the state of\neach cache block at the individual caches.\nThe states and transitions for the state machine at each cache are identical to\nwhat we used for the snooping cache, although the actions on a transition are\nslightly different. The processes of invalidating and locating an exclusive copy\nof a data item are different because they both involve communication between\nthe requesting node and the directory and between the directory and one or more\nremote nodes. In a snooping protocol, these two steps are combined through the\nuse of a broadcast to all the nodes.\nBefore we see the protocol state diagrams, it is useful to examine a catalog of\nthe message types that may be sent between the processors and the directories for\nthe purpose of handling misses and maintaining coherence. Figure 5.19 shows the\ntypes of messages sent among nodes. The local node is the node where a request\noriginates. The home node is the node where the memory location and the directory\nentry of an address reside. The physical address space is statically distributed, so\nthe node that contains the memory and directory for a given physical address is\nknown. For example, the high-order bits may provide the node number, whereas\nthe low-order bits provide the offset within the memory on that node. The local\nnode may also be the home node. The directory must be accessed when the home\nnode is the local node because copies may exist in yet a third node, called a remote\nnode.\n406\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 439,
        "text": "A remote node is the node that has a copy of a cache block, whether exclusive\n(in which case it is the only copy) or shared. A remote node may be the same as\neither the local node or the home node. In such cases, the basic protocol does not\nchange, but interprocessor messages may be replaced with intraprocessor\nmessages.\nIn this section, we assume a simple model of memory consistency. To mini-\nmize the type of messages and the complexity of the protocol, we make an assump-\ntion that messages will be received and acted upon in the same order they are sent.\nThis assumption may not be true in practice and can result in additional compli-\ncations, some of which we address in Section 5.6 when we discuss memory con-\nsistency models. In this section, we use this assumption to ensure that invalidates\nsent by a node are honored before new messages are transmitted, just as we\nassumed in the discussion of implementing snooping protocols. As we did in\nthe snooping case, we omit some details necessary to implement the coherence\nMessage\ntype\nSource\nDestination\nMessage\ncontents\nFunction of this message\nRead miss\nLocal cache\nHome\ndirectory\nP, A\nNode P has a read miss at address A; request data and make P a\nread sharer.\nWrite\nmiss\nLocal cache\nHome\ndirectory\nP, A\nNode P has a write miss at address A; request data and make P\nthe exclusive owner.\nInvalidate\nLocal cache\nHome\ndirectory\nA\nRequest to send invalidates to all remote caches that are\ncaching the block at address A.\nInvalidate\nHome\ndirectory\nRemote\ncache\nA\nInvalidate a shared copy of data at address A.\nFetch\nHome\ndirectory\nRemote\ncache\nA\nFetch the block at address A and send it to its home directory;\nchange the state of A in the remote cache to shared.\nFetch/\ninvalidate\nHome\ndirectory\nRemote\ncache\nA\nFetch the block at address A and send it to its home directory;\ninvalidate the block in the cache.\nData\nvalue\nreply\nHome\ndirectory\nLocal cache\nD\nReturn a data value from the home memory.\nData\nwrite-\nback\nRemote\ncache\nHome\ndirectory\nA, D\nWrite back a data value for address A.\nFigure 5.19 The possible messages sent among nodes to maintain coherence, along with the source and des-\ntination node, the contents (where P5requesting node number, A5requested address, and D5data contents),\nand the function of the message. The first three messages are requests sent by the local node to the home. The\nfourth through sixth messages are messages sent to a remote node by the home when the home needs the data\nto satisfy a read or write miss request. Data value replies are used to send a value from the home node back to the\nrequesting node. Data value write-backs occur for two reasons: when a block is replaced in a cache and must be\nwritten back to its home memory, and also in reply to fetch or fetch/invalidate messages from the home. Writing\nback the data value whenever the block becomes shared simplifies the number of states in the protocol because\nany dirty block must be exclusive and any shared block is always available in the home memory.\n5.4\nDistributed Shared-Memory and Directory-Based Coherence\n\u25a0\n407"
    },
    {
        "page": 440,
        "text": "protocol. In particular, the serialization of writes and knowing that the invalidates\nfor a write have completed are not as simple as in the broadcast-based snooping\nmechanism. Instead, explicit acknowledgments are required in response to write\nmisses and invalidate requests. We discuss these issues in more detail in\nAppendix I.\nAn Example Directory Protocol\nThe basic states of a cache block in a directory-based protocol are exactly like those\nin a snooping protocol, and the states in the directory are also analogous to those we\nshowed earlier. Thus we can start with simple state diagrams that show the state tran-\nsitions for an individual cache block and then examine the state diagram for the direc-\ntory entry corresponding to each block in memory. As in the snooping case, these\nstate transition diagrams do not represent all the details of a coherence protocol; how-\never, the actual controller is highly dependent on a number of details of the multi-\nprocessor (message delivery properties, buffering structures, and so on). In this\nsection, we present the basic protocol state diagrams. The knotty issues involved\nin implementing these state transition diagrams are examined in Appendix I.\nFigure 5.20 shows the protocol actions to which an individual cache responds.\nWe use the same notation as in the last section, with requests coming from outside\nthe node in gray and actions in bold. The state transitions for an individual cache\nare caused by read misses, write misses, invalidates, and data fetch requests;\nFigure 5.20 shows these operations. An individual cache also generates read miss,\nwrite miss, and invalidate messages that are sent to the home directory. Read and\nwrite misses require data value replies, and these events wait for replies before\nchanging state. Knowing when invalidates complete is a separate problem and\nis handled separately.\nThe operation of the state transition diagram for a cache block in Figure 5.20 is\nessentially the same as it is for the snooping case: the states are identical, and the\nstimulus is almost identical. The write miss operation, which was broadcast on the\nbus (or other network) in the snooping scheme, is replaced by the data fetch and\ninvalidate operations that are selectively sent by the directory controller. Like the\nsnooping protocol, any cache block must be in the exclusive state when it is writ-\nten, and any shared block must be up to date in memory. In many multicore pro-\ncessors, the outermost level in the processor cache is shared among the cores (as is\nthe L3 in the Intel i7, the AMD Opteron, and the IBM Power7), and hardware at\nthat level maintains coherence among the private caches of each core on the same\nchip, using either an internal directory or snooping. Thus the on-chip multicore\ncoherence mechanism can be used to extend coherence among a larger set of pro-\ncessors simply by interfacing to the outermost shared cache. Because this interface\nis at L3, contention between the processor and coherence requests is less of an\nissue, and duplicating the tags could be avoided.\nIn a directory-based protocol, the directory implements the other half of the\ncoherence protocol. A message sent to a directory causes two different types of\n408\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 441,
        "text": "actions: updating the directory state and sending additional messages to satisfy the\nrequest. The states in the directory represent the three standard states for a block;\nunlike in a snooping scheme, however, the directory state indicates the state of all\nthe cached copies of a memory block, rather than for a single cache block.\nThe memory block may be uncached by any node, cached in multiple nodes\nand readable (shared), or cached exclusively and writable in exactly one node.\nIn addition to the state of each block, the directory must track the set of nodes that\nhave a copy of a block; we use a set called Sharers to perform this function. In\nmultiprocessors with fewer than 64 nodes (each of which may represent four to\neight times as many processors), this set is typically kept as a bit vector. Directory\nModified\n(read/write)\nCPU write hit\nCPU read hit\nFetch\ninvalidate\nCPU write\nSend write miss message\nFetch\nCPU read miss\nData write-back\nSend invalidate message\nCPU write hit\nSend read miss message\nRead miss\nCPU read \nCPU read hit\nCPU write miss\nData write-back\nWrite miss\nCPU\nread\nmiss\nInvalidate\nData write-back; read miss\nData write-back\nSend write miss message\nCPU write miss\nInvalid\nShared\n(read only)\nFigure 5.20 State transition diagram for an individual cache block in a directory-based system. Requests by the\nlocal processor are shown in black, and those from the home directory are shown in gray. The states are identical to\nthose in the snooping case, and the transactions are very similar, with explicit invalidate and write-back requests\nreplacing the write misses that were formerly broadcast on the bus. As we did for the snooping controller, we assume\nthat an attempt to write a shared cache block is treated as a miss; in practice, such a transaction can be treated as an\nownership request or upgrade request and can deliver ownership without requiring that the cache block be fetched.\n5.4\nDistributed Shared-Memory and Directory-Based Coherence\n\u25a0\n409"
    },
    {
        "page": 442,
        "text": "requests need to update the set Sharers and also read the set to perform\ninvalidations.\nFigure 5.21 shows the actions taken at the directory in response to messages\nreceived. The directory receives three different requests: read miss, write miss,\nand data write-back. The messages sent in response by the directory are shown\nin bold, and the updating of the set Sharers is shown in bold italics. Because all\nthe stimulus messages are external, all actions are shown in gray. Our simplified\nprotocol assumes that some actions are atomic, such as requesting a value and\nsending it to another node; a realistic implementation cannot use this assumption.\nTo understand these directory operations, let\u2019s examine the requests received\nand actions taken state by state. When a block is in the uncached state, the copy in\nmemory is the current value, so the only possible requests for that block are\n\u25a0\nRead miss\u2014The requesting node is sent the requested data from memory, and\nthe requester is made the only sharing node. The state of the block is\nmade shared.\nSharers = {}\nInvalidate; Sharers = {P}; data value reply\nData value reply\nSharers = Sharers + {P}  \nData value reply;\nSharers = {P}  \nData value reply;\nSharers = {P}  \nWrite\nmiss\nFetch; data value reply; Sharers = Sharers + {P} \nRead miss\nWrite miss\nUncached\nFetch/invalidate\nData value reply\nSharers = {P}\nRead\nmiss\nData\nwrite-back\nWrite miss\nRead miss\nShared\n(read only)\nExclusive\n(read/write)\nFigure 5.21 The state transition diagram for the directory has the same states and structure as the transition\ndiagram for an individual cache. All actions are in gray because they are all externally caused. Bold indicates the\naction taken by the directory in response to the request.\n410\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 443,
        "text": "\u25a0\nWrite miss\u2014The requesting node is sent the value and becomes the sharing\nnode. The block is made exclusive to indicate that the only valid copy is\ncached. Sharers indicates the identity of the owner.\nWhen the block is in the shared state, the memory value is up to date, so the same\ntwo requests can occur:\n\u25a0\nRead miss\u2014The requesting node is sent the requested data from memory, and\nthe requesting node is added to the sharing set.\n\u25a0\nWrite miss\u2014The requesting node is sent the value. All nodes in the set Sharers\nare sent invalidate messages, and the Sharers set is to contain the identity of the\nrequesting node. The state of the block is made exclusive.\nWhen the block is in the exclusive state, the current value of the block is held in a\ncache on the node identified by the set Sharers (the owner), so there are three pos-\nsible directory requests:\n\u25a0\nRead miss\u2014The owner is sent a data fetch message, which causes the state of\nthe block in the owner\u2019s cache to transition to shared and causes the owner to\nsend the data to the directory, where it is written to memory and sent back to the\nrequesting processor. The identity of the requesting node is added to the set\nSharers, which still contains the identity of the processor that was the owner\n(since it still has a readable copy).\n\u25a0\nData write-back\u2014The owner is replacing the block and therefore must write it\nback. This write-back makes the memory copy up to date (the home directory\nessentially becomes the owner), the block is now uncached, and the Sharers set\nis empty.\n\u25a0\nWrite miss\u2014The block has a new owner. A message is sent to the old owner,\ncausing the cache to invalidate the block and send the value to the directory,\nfrom which it is sent to the requesting node, which becomes the new owner.\nSharers is set to the identity of the new owner, and the state of the block remains\nexclusive.\nThis state transition diagram in Figure 5.21 is a simplification, just as it was in\nthe snooping cache case. In the case of a directory, as well as a snooping scheme\nimplemented with a network other than a bus, our protocols will need to deal with\nnonatomic memory transactions. Appendix I explores these issues in depth.\nThe directory protocols used in real multiprocessors contain additional optimi-\nzations. In particular, in this protocol when a read or write miss occurs for a block\nthat is exclusive, the block is first sent to the directory at the home node. From there\nit is stored into the home memory and also sent to the original requesting node.\nMany of the protocols in use in commercial multiprocessors forward the data from\nthe owner node to the requesting node directly (as well as performing the write-\nback to the home). Such optimizations often add complexity by increasing the pos-\nsibility of deadlock and by increasing the types of messages that must be handled.\n5.4\nDistributed Shared-Memory and Directory-Based Coherence\n\u25a0\n411"
    },
    {
        "page": 444,
        "text": "Implementing a directory scheme requires solving most of the same challenges\nwe discussed for snooping protocols. There are, however, new and additional prob-\nlems, which we describe in Appendix I. In Section 5.8, we briefly describe how mod-\nern multicores extend coherence beyond a single chip. The combinations of\nmultichip coherence and multicore coherence include all four possibilities of snoop-\ning/snooping (AMD Opteron), snooping/directory, directory/snooping, and direc-\ntory/directory! Many multiprocessors have chosen some form of snooping within\na single chip, which is attractive if the outermost cache is shared and inclusive,\nand directories across multiple chips. Such an approach simplifies implementation\nbecause only the processor chip, rather than an individual core, need be tracked.\n5.5\nSynchronization: The Basics\nSynchronization mechanisms are typically built with user-level software routines\nthat rely on hardware-supplied synchronization instructions. For smaller multipro-\ncessors or low-contention situations, the key hardware capability is an uninterrup-\ntible instruction or instruction sequence capable of atomically retrieving and\nchanging a value. Software synchronization mechanisms are then constructed\nusing this capability. In this section, we focus on the implementation of lock\nand unlock synchronization operations. Lock and unlock can be used straightfor-\nwardly to create mutual exclusion, as well as to implement more complex synchro-\nnization mechanisms.\nIn high-contention situations, synchronization can become a performance bot-\ntleneck because contention introduces additional delays and because latency is\npotentially greater in such a multiprocessor. We discuss how the basic synchroni-\nzation mechanisms of this section can be extended for large processor counts in\nAppendix I.\nBasic Hardware Primitives\nThe key ability we require to implement synchronization in a multiprocessor is a\nset of hardware primitives with the ability to atomically read and modify a memory\nlocation. Without such a capability, the cost of building basic synchronization\nprimitives will be too high and will increase as the processor count increases. There\nare a number of alternative formulations of the basic hardware primitives, all of\nwhich provide the ability to atomically read and modify a location, together with\nsome way to tell whether the read and write were performed atomically. These\nhardware primitives are the basic building blocks that are used to build a wide vari-\nety of user-level synchronization operations, including things such as locks and\nbarriers. In general, architects do not expect users to employ the basic hardware\nprimitives, but instead expect that the primitives will be used by system program-\nmers to build a synchronization library, a process that is often complex and tricky.\nLet\u2019s start with one such hardware primitive and show how it can be used to build\nsome basic synchronization operations.\n412\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 445,
        "text": "One typical operation for building synchronization operations is the atomic\nexchange, which interchanges a value in a register for a value in memory. To see\nhow to use this to build a basic synchronization operation, assume that we want to\nbuild a simple lock where the value 0 is used to indicate that the lock is free and 1\nisusedtoindicatethatthelockisunavailable.Aprocessortriestosetthelockbydoing\nan exchange of 1, which is in a register, with the memory address corresponding to\nthe lock. The value returned from the exchange instruction is 1 if some other\nprocessor had otherwise already claimed access and 0. In the latter case, the value\nis also changed to 1, preventing any competing exchange from also retrieving a 0.\nFor example, consider two processors that each try to do the exchange simul-\ntaneously: This race is broken because exactly one of the processors will perform\nthe exchange first, returning 0, and the second processor will return 1 when it does\nthe exchange. The key to using the exchange (or swap) primitive to implement syn-\nchronization is that the operation is atomic: the exchange is indivisible, and two\nsimultaneous exchanges will be ordered by the write serialization mechanisms.\nIt is impossible for two processors trying to set the synchronization variable in this\nmanner to both determine they have simultaneously set the variable.\nThere are a number of other atomic primitives that can be used to implement\nsynchronization. They all have the key property that they read and update a memory\nvalue in such a manner that we can tell whether the two operations executed atom-\nically. One operation, present in many older multiprocessors, is test-and-set, which\ntests a value and sets it if the value passes the test. For example, we could define an\noperation that tested for 0 and set the value to 1, which can be used in a fashion sim-\nilar to how we used atomic exchange. Another atomic synchronization primitive is\nfetch-and-increment: it returns the value of a memory location and atomically incre-\nments it. By using the value 0 to indicate that the synchronization variable is\nunclaimed, we can use fetch-and-increment, just as we used exchange. There are\nother uses of operations like fetch-and-increment, which we will see shortly.\nImplementing a single atomic memory operation introduces some challenges\nbecause it requires both a memory read and a write in a single, uninterruptible\ninstruction. This requirement complicates the implementation of coherence\nbecause the hardware cannot allow any other operations between the read and\nthe write, and yet must not deadlock.\nAn alternative is to have a pair of instructions where the second instruction\nreturns a value from which it can be deduced whether the pair of instructions\nwas executed as though the instructions were atomic. The pair of instructions is\neffectively atomic if it appears as though all other operations executed by any pro-\ncessor occurred before or after the pair. Thus, when an instruction pair is effec-\ntively atomic, no other processor can change the value between the instruction\npair. This is the approach used in the MIPS processors and in RISC V.\nIn RISC V, the pair of instructions includes a special load called a load\nreserved (also called load linked or load locked) and a special store called a store\nconditional. Load reserved loads the contents of memory given by rs1 into rd and\ncreates a reservation on that memory address. Store conditional stores the value in\nrs2 into the memory address given by rs1. If the reservation of the load is broken by\n5.5\nSynchronization: The Basics\n\u25a0\n413"
    },
    {
        "page": 446,
        "text": "a write to the same memory location, the store conditional fails and writes a non-\nzero to rd; if it succeeds, the store conditional writes 0. If the processor does a con-\ntext switch between the two instructions, then the store conditional always fails.\nThese instructions are used in sequence, and because the load reserved returns\nthe initial value and the store conditional returns 0 only if it succeeds, the following\nsequence implements an atomic exchange on the memory location specified by the\ncontents of x1 with the value in x4:\ntry:\nmov\nx3,x4\n;mov exchange value\nlr\nx2,x1\n;load reserved from\nsc\nx3,0(x1)\n;store conditional\nbnez\nx3,try\n;branch store fails\nmov\nx4,x2\n;put load value in x4\nAt the end of this sequence, the contents of x4 and the memory location specified\nby x1 have been atomically exchanged. Anytime a processor intervenes and\nmodifies the value in memory between the lr and sc instructions, the sc returns\n0 in x3, causing the code sequence to try again.\nAn advantage of the load reserved/store conditional mechanism is that it can be\nused to build other synchronization primitives. For example, here is an atomic\nfetch-and-increment:\ntry:\nlr\nx2,x1\n;load reserved 0(x1)\naddi\nx3,x2,1\n;increment\nsc\nx3,0(x1)\n;store conditional\nbnez\nx3,try\n;branch store fails\nThese instructions are typically implemented by keeping track of the address\nspecified in the lr instruction in a register, often called the reserved register. If\nan interrupt occurs, or if the cache block matching the address in the link register\nis invalidated (e.g., by another sc), the link register is cleared. The sc instruction\nsimply checks that its address matches that in the reserved register. If so, the sc suc-\nceeds; otherwise, it fails. Because the store conditional will fail after either another\nattempted store to the load reserved address or any exception, care must be taken in\nchoosing what instructions are inserted between the two instructions. In particular,\nonly register-register instructions can safely be permitted; otherwise, it is possible to\ncreate deadlock situations where the processor can never complete the sc. In addi-\ntion, the number of instructions between the load reserved and the store conditional\nshould be small to minimize the probability that either an unrelated event or a com-\npeting processor causes the store conditional to fail frequently.\nImplementing Locks Using Coherence\nOnce we have an atomic operation, we can use the coherence mechanisms of a\nmultiprocessor to implement spin locks\u2014locks that a processor continuously tries\nto acquire, spinning around a loop until it succeeds. Spin locks are used when pro-\ngrammers expect the lock to be held for a very short amount of time and when they\nwant the process of locking to be low latency when the lock is available. Because\n414\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 447,
        "text": "spin locks tie up the processor waiting in a loop for the lock to become free, they are\ninappropriate in some circumstances.\nThe simplest implementation, which we would use if there were no cache\ncoherence, would be to keep the lock variables in memory. A processor could con-\ntinually try to acquire the lock using an atomic operation, say, atomic exchange,\nand test whether the exchange returned the lock as free. To release the lock, the\nprocessor simply stores the value 0 to the lock. Here is the code sequence to lock\na spin lock whose address is in x1. It uses EXCH as a macro for the atomic\nexchange sequence from page 414:\naddi\nx2,R0,#1\nlockit:\nEXCH\nx2,0(x1)\n;atomic exchange\nbnez\nx2,lockit\n;already locked?\nIf our multiprocessor supports cache coherence, we can cache the locks using\nthe coherence mechanism to maintain the lock value coherently. Caching locks has\ntwo advantages. First, it allows an implementation where the process of \u201cspinning\u201d\n(trying to test and acquire the lock in a tight loop) could be done on a local cached\ncopy rather than requiring a global memory access on each attempt to acquire the\nlock. The second advantage comes from the observation that there is often locality\nin lock accesses; that is, the processor that used the lock last will use it again in the\nnear future. In such cases, the lock value may reside in the cache of that processor,\ngreatly reducing the time to acquire the lock.\nObtaining the first advantage\u2014being able to spin on a local cached copy rather\nthan generating a memory request for each attempt to acquire the lock\u2014requires a\nchange in our simple spin procedure. Each attempt to exchange in the preceding\nloop requires a write operation. If multiple processors are attempting to get the\nlock, each will generate the write. Most of these writes will lead to write misses\nbecause each processor is trying to obtain the lock variable in an exclusive state.\nThus we should modify our spin lock procedure so that it spins by doing reads\non a local copy of the lock until it successfully sees that the lock is available. Then\nit attempts to acquire the lock by doing a swap operation. A processor first reads the\nlock variable to test its state. A processor keeps reading and testing until the value\nof the read indicates that the lock is unlocked. The processor then races against all\nother processes that were similarly \u201cspin waiting\u201d to see which can lock the var-\niable first. All processes use a swap instruction that reads the old value and stores a\n1 into the lock variable. The single winner will see the 0, and the losers will see a 1\nthat was placed there by the winner. (The losers will continue to set the variable to\nthe locked value, but that doesn\u2019t matter.) The winning processor executes the code\nafter the lock and, when finished, stores a 0 into the lock variable to release the\nlock, which starts the race all over again. Here is the code to perform this spin lock\n(remember that 0 is unlocked and 1 is locked):\nlockit:\nld\nx2,0(x1)\n;load of lock\nbnez\nx2,lockit\n;not available-spin\naddi\nx2,R0,#1\n;load locked value\nEXCH\nx2,0(x1)\n;swap\nbnez\nx2,lockit\n;branch if lock wasn\u2019t 0\n5.5\nSynchronization: The Basics\n\u25a0\n415"
    },
    {
        "page": 448,
        "text": "Let\u2019s examine how this \u201cspin lock\u201d scheme uses the cache coherence mecha-\nnisms. Figure 5.22 shows the processor and bus or directory operations for multiple\nprocesses trying to lock a variable using an atomic swap. Once the processor with\nthe lock stores a 0 into the lock, all other caches are invalidated and must fetch the\nnew value to update their copy of the lock. One such cache gets the copy of the\nunlocked value (0) first and performs the swap. When the cache miss of other pro-\ncessors is satisfied, they find that the variable is already locked, so they must return\nto testing and spinning.\nThis example shows another advantage of the load reserved/store conditional\nprimitives: the read and write operations are explicitly separated. The load reserved\nneed not cause any bus traffic. This fact allows the following simple code\nsequence, which has the same characteristics as the optimized version using\nexchange (x1 has the address of the lock, the lr has replaced the LD, and the\nsc has replaced the EXCH):\nStep\nP0\nP1\nP2\nCoherence\nstate of lock at\nend of step\nBus/directory activity\n1\nHas lock\nBegins spin, testing\nif lock\u00bc0\nBegins spin,\ntesting if lock\u00bc0\nShared\nCache misses for P1 and P2 satisfied\nin either order. Lock state becomes\nshared.\n2\nSet lock\nto 0\n(Invalidate\nreceived)\n(Invalidate\nreceived)\nExclusive (P0)\nWrite invalidate of lock variable\nfrom P0.\n3\nCache miss\nCache miss\nShared\nBus/directory services P2 cache\nmiss; write-back from P0; state\nshared.\n4\n(Waits while bus/\ndirectory busy)\nLock\u00bc0 test\nsucceeds\nShared\nCache miss for P2 satisfied.\n5\nLock\u00bc0\nExecutes swap,\ngets cache miss\nShared\nCache miss for P1 satisfied.\n6\nExecutes swap,\ngets cache miss\nCompletes swap:\nreturns 0 and sets\nlock\u00bc1\nExclusive (P2)\nBus/directory services P2 cache\nmiss; generates invalidate; lock is\nexclusive.\n7\nSwap completes\nand returns 1, and\nsets lock\u00bc1\nEnter critical\nsection\nExclusive (P1)\nBus/directory services P1 cache\nmiss; sends invalidate and generates\nwrite-back from P2.\n8\nSpins, testing if\nlock\u00bc0\nNone\nFigure 5.22 Cache coherence steps and bus traffic for three processors, P0, P1, and P2. This figure assumes write\ninvalidate coherence. P0 starts with the lock (step 1), and the value of the lock is 1 (i.e., locked); it is initially exclu-\nsive and owned by P0 before step 1 begins. P0 exits and unlocks the lock (step 2). P1 and P2 race to see which reads\nthe unlocked value during the swap (steps 3\u20135). P2 wins and enters the critical section (steps 6 and 7), while P1\u2019s\nattempt fails, so it starts spin waiting (steps 7 and 8). In a real system, these events will take many more than 8 clock\nticks because acquiring the bus and replying to misses take much longer. Once step 8 is reached, the process can\nrepeat with P2, eventually getting exclusive access and setting the lock to 0.\n416\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 449,
        "text": "lockit:\nlr\nx2,0(x1)\n;load reserved\nbnez\nx2,lockit\n;not available-spin\naddi\nx2,R0,#1\n;locked value\nsc\nx2,0(x1)\n;store\nbnez\nx2,lockit\n;branch if store fails\nThe first branch forms the spinning loop; the second branch resolves races when\ntwo processors see the lock available simultaneously.\n5.6\nModels of Memory Consistency: An Introduction\nCache coherence ensures that multiple processors see a consistent view of mem-\nory. It does not answer the question of how consistent the view of memory must be.\nBy \u201chow consistent,\u201d we are really asking when a processor must see a value that\nhas been updated by another processor. Because processors communicate through\nshared variables (used both for data values and for synchronization), the question\nboils down to this: In what order must a processor observe the data writes of\nanother processor? Because the only way to \u201cobserve the writes of another proces-\nsor\u201d is through reads, the question becomes what properties must be enforced\namong reads and writes to different locations by different processors?\nAlthough the question of how consistent memory must be seems simple, it is\nremarkably complicated, as we can see with a simple example. Here are two code\nsegments from processes P1 and P2, shown side by side:\nP1:\nA = 0;\nP2:\nB = 0;\n.....\n.....\nA = 1;\nB = 1;\nL1:\nif (B == 0)...\nL2:\nif (A == 0)...\nAssume that the processes are running on different processors, and that locations A\nand B are originally cached by both processors with the initial value of 0. If writes\nalways take immediate effect and are immediately seen by other processors, it will\nbe impossible for both IF statements (labeled L1 and L2) to evaluate their condi-\ntions as true, since reaching the IF statement means that either A or B must have\nbeen assigned the value 1. But suppose the write invalidate is delayed, and the\nprocessor is allowed to continue during this delay. Then it is possible that both\nP1 and P2 have not seen the invalidations for B and A (respectively) before they\nattempt to read the values. The question now is should this behavior be allowed,\nand, if so, under what conditions?\nThe most straightforward model for memory consistency is called sequential\nconsistency. Sequential consistency requires that the result of any execution be the\nsame as though the memory accesses executed by each processor were kept in\norder and the accesses among different processors were arbitrarily interleaved.\nSequential consistency eliminates the possibility of some nonobvious execution\nin the previous example because the assignments must be completed before the\nIF statements are initiated.\n5.6\nModels of Memory Consistency: An Introduction\n\u25a0\n417"
    },
    {
        "page": 450,
        "text": "The simplest way to implement sequential consistency is to require a processor\nto delay the completion of any memory access until all the invalidations caused by\nthat access are completed. Of course, it is equally effective to delay the next mem-\nory access until the previous one is completed. Remember that memory consis-\ntency involves operations among different variables: the two accesses that must\nbe ordered are actually to different memory locations. In our example, we must\ndelay the read of A or B (A == 0 or B == 0) until the previous write has completed\n(B = 1 or A = 1). Under sequential consistency, we cannot, for example, simply\nplace the write in a write buffer and continue with the read.\nAlthough sequential consistency presents a simple programming paradigm, it\nreduces potential performance, especially in a multiprocessor with a large number\nof processors or long interconnect delays, as we can see in the following example.\nExample\nSuppose we have a processor where a write miss takes 50 cycles to establish own-\nership, 10 cycles to issue each invalidate after ownership is established, and 80\ncycles for an invalidate to complete and be acknowledged once it is issued. Assum-\ning that four other processors share a cache block, how long does a write miss stall\nthe writing processor if the processor is sequentially consistent? Assume that the\ninvalidates must be explicitly acknowledged before the coherence controller\nknows they are completed. Suppose we could continue executing after obtaining\nownership for the write miss without waiting for the invalidates; how long would\nthe write take?\nAnswer\nWhen we wait for invalidates, each write takes the sum of the ownership time plus\nthe time to complete the invalidates. Because the invalidates can overlap, we need\nonly worry about the last one, which starts 10+10+10+10\u00bc40 cycles after own-\nership is established. Therefore the total time for the write is 50+40+80\u00bc170\ncycles. In comparison, the ownership time is only 50 cycles. With appropriate write\nbuffer implementations, it is even possible to continue before ownership is\nestablished.\nTo provide better performance, researchers and architects have explored two dif-\nferent routes. First, they developed ambitious implementations that preserve\nsequential consistency but use latency-hiding techniques to reduce the penalty;\nwe discuss these in Section 5.7. Second, they developed less restrictive memory\nconsistency models that allow for faster hardware. Such models can affect how\nthe programmer sees the multiprocessor, so before we discuss these less restrictive\nmodels, let\u2019s look at what the programmer expects.\nThe Programmer\u2019s View\nAlthough the sequential consistency model has a performance disadvantage, from\nthe viewpoint of the programmer, it has the advantage of simplicity. The challenge\n418\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 451,
        "text": "is to develop a programming model that is simple to explain and yet allows a high-\nperformance implementation.\nOne such programming model that allows us to have a more efficient imple-\nmentation is to assume that programs are synchronized. A program is synchronized\nif all accesses to shared data are ordered by synchronization operations. A data ref-\nerence is ordered by a synchronization operation if, in every possible execution, a\nwrite of a variable by one processor and an access (either a read or a write) of that\nvariable by another processor are separated by a pair of synchronization opera-\ntions, one executed after the write by the writing processor and one executed before\nthe access by the second processor. Cases where variables may be updated without\nordering by synchronization are called data races because the execution outcome\ndepends on the relative speed of the processors, and like races in hardware design,\nthe outcome is unpredictable, which leads to another name for synchronized pro-\ngrams: data-race-free.\nAs a simple example, consider a variable being read and updated by two dif-\nferent processors. Each processor surrounds the read and update with a lock and an\nunlock, both to ensure mutual exclusion for the update and to ensure that the read is\nconsistent. Clearly, every write is now separated from a read by the other processor\nby a pair of synchronization operations: one unlock (after the write) and one lock\n(before the read). Of course, if two processors are writing a variable with no inter-\nvening reads, then the writes must also be separated by synchronization operations.\nIt is a broadly accepted observation that most programs are synchronized. This\nobservation is true primarily because, if the accesses were unsynchronized, the\nbehavior of the program would likely be unpredictable because the speed of exe-\ncution would determine which processor won a data race and thus affect the results\nof the program. Even with sequential consistency, reasoning about such programs\nis very difficult.\nProgrammers could attempt to guarantee ordering by constructing their own\nsynchronization mechanisms, but this is extremely tricky, can lead to buggy pro-\ngrams, and may not be supported architecturally, meaning that they may not work\nin future generations of the multiprocessor. Instead, almost all programmers will\nchoose to use synchronization libraries that are correct and optimized for the mul-\ntiprocessor and the type of synchronization.\nFinally, the use of standard synchronization primitives ensures that even if the\narchitecture implements a more relaxed consistency model than sequential consis-\ntency, a synchronized program will behave as though the hardware implemented\nsequential consistency.\nRelaxed Consistency Models: The Basics and\nRelease Consistency\nThe key idea in relaxed consistency models is to allow reads and writes to complete\nout of order, but to use synchronization operations to enforce ordering so that a\nsynchronized program behaves as though the processor were sequentially\n5.6\nModels of Memory Consistency: An Introduction\n\u25a0\n419"
    },
    {
        "page": 452,
        "text": "consistent. There are a variety of relaxed models that are classified according to\nwhat read and write orderings they relax. We specify the orderings by a set of rules\nof the form X!Y, meaning that operation X must complete before operation Y is\ndone. Sequential consistency requires maintaining all four possible orderings:\nR!W, R!R, W!R, and W!W. The relaxed models are defined by the subset\nof four orderings they relax:\n1. Relaxing only the W!R ordering yields a model known as total store ordering\nor processor consistency. Because this model retains ordering among writes,\nmany programs that operate under sequential consistency operate under this\nmodel, without additional synchronization.\n2. Relaxing both the W!R ordering and the W!W ordering yields a model\nknown as partial store order.\n3. Relaxing all four orderings yields a variety of models including weak ordering,\nthe PowerPC consistency model, and release consistency, the RISC V\nconsistency model.\nBy relaxing these orderings, the processor may obtain significant performance\nadvantages, which is the reason that RISC V, ARMv8, as well as the C++ and\nC language standards chose release consistency as the model.\nRelease consistency distinguishes between synchronization operations that\nare used to acquire access to a shared variable (denoted SA) and those that\nrelease an object to allow another processor to acquire access (denoted SR).\nRelease consistency is based on the observation that in synchronized programs,\nan acquire operation must precede a use of shared data, and a release operation\nmust follow any updates to shared data and also precede the time of the next\nacquire. This property allows us to slightly relax the ordering by observing that\na read or write that precedes an acquire need not complete before the acquire,\nand also that a read or write that follows a release need not wait for the release.\nThus the orderings that are preserved involve only SA and SR, as shown in\nFigure 5.23; as the example in Figure 5.24 shows, this model imposes the few-\nest orders of the five models.\nRelease consistency provides one of the least restrictive models that is easily\ncheckable and ensures that synchronized programs will see a sequentially consis-\ntent execution. Although most synchronization operations are either an acquire or a\nrelease (an acquire normally reads a synchronization variable and atomically\nupdates it, and a release usually just writes it), some operations, such as a barrier,\nact as both an acquire and a release and cause the ordering to be equivalent to weak\nordering. Although synchronization operations always ensure that previous writes\nhave completed, we may want to guarantee that writes are completed without an\nidentified synchronization operation. In such cases, an explicit instruction, called\nFENCE in RISC V, is used to ensure that all previous instructions in that thread\nhave completed, including completion of all memory writes and associated inval-\nidates. For more information about the complexities, implementation issues, and\n420\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 453,
        "text": "= A\nB =\nacquire (S);\nC =\n    = D\nrelease (S);\nE =\nF =\nSequential\nconsistency\n    = A\nB =\nacquire (S);\nC =\n    = D\nrelease (S);\nE =\nF =\nTSO (total store\norder) or\nprocessor\nconsistency\n    = A\nB =\nacquire (S);\nC =\n    = D\nrelease (S);\nE =\nF =\nPSO (partial\nstore order)\n    = A\nB =\nacquire (S);\nC =\n    = D\nrelease (S);\nE =\nF =\nWeak ordering\nRelease\nconsistency\n    = A\nB =\nacquire (S);\nC =\n    = D\nrelease (S);\nE =\nF =\nFigure 5.24 These examples of the five consistency models discussed in this section show the reduction in the\nnumber of orders imposed as the models become more relaxed. Only the minimum orders are shown with arrows.\nOrders implied by transitivity, such as the write of C before the release of S in the sequential consistency model or the\nacquire before the release in weak ordering or release consistency, are not shown.\nModel\nUsed in\nOrdinary\norderings\nSynchronization orderings\nSequential consistency\nMost machines as an\noptional mode\nR!R, R!W,\nW!R, W!W\nS!W, S!R, R!S, W!S, S!S\nTotal store order or\nprocessor consistency\nIBMS/370, DEC VAX,\nSPARC\nR!R, R!W,\nW!W\nS!W, S!R, R!S, W!S, S!S\nPartial store order\nSPARC\nR!R, R!W\nS!W, S!R, R!S, W!S, S!S\nWeak ordering\nPowerPC\nS!W, S!R, R!S, W!S, S!S\nRelease consistency\nMIPS, RISC V, Armv8, C,\nand C++ specifications\nSA!W, SA!R, R!SR, W!SR,\nSA!SA, SA!SR, SR!SA, SR!SR\nFigure 5.23 The orderings imposed by various consistency models are shown for both ordinary accesses and\nsynchronization accesses. The models grow from most restrictive (sequential consistency) to least restrictive (release\nconsistency), allowing increased flexibility in the implementation. The weaker models rely on fences created by syn-\nchronization operations, as opposed to an implicit fence at every memory operation. SA and SR stand for acquire and\nrelease operations, respectively, and are needed to define release consistency. If we were to use the notation SA and\nSR for each S consistently, each ordering with one S would become two orderings (e.g., S!W becomes SA!W,\nSR!W), and each S!S would become the four orderings shown in the last line of the bottom-right table entry.\n5.6\nModels of Memory Consistency: An Introduction\n\u25a0\n421"
    },
    {
        "page": 454,
        "text": "performance potential from relaxed models, we highly recommend the excellent\ntutorial by Adve and Gharachorloo (1996).\n5.7\nCross-Cutting Issues\nBecause multiprocessors redefine many system characteristics (e.g., performance\nassessment, memory latency, and the importance of scalability), they introduce\ninteresting design problems that cut across the spectrum, affecting both hardware\nand software. In this section, we give several examples related to the issue of mem-\nory consistency. We then examine the performance gained when multithreading is\nadded to multiprocessing.\nCompiler Optimization and the Consistency Model\nAnother reason for defining a model for memory consistency is to specify the range\nof legal compiler optimizations that can be performed on shared data. In explicitly\nparallel programs, unless the synchronization points are clearly defined and the pro-\ngrams are synchronized, the compiler cannot interchange a read and a write of two\ndifferent shared data items because such transformations might affect the semantics\nof the program. This restriction prevents even relatively simple optimizations, such\nas register allocation of shared data, because such a process usually interchanges\nreads and writes. In implicitly parallelized programs\u2014for example, those written\nin High Performance Fortran (HPF)\u2014programs must be synchronized and the\nsynchronization points are known, so this issue does not arise. Whether compilers\ncan get significant advantage from more relaxed consistency models remains an\nopen question, both from a research viewpoint and from a practical viewpoint,\nwherethelackofuniformmodelsislikelytoretardprogressondeployingcompilers.\nUsing Speculation to Hide Latency in Strict\nConsistency Models\nAs we saw in Chapter 3, speculation can be used to hide memory latency. It can\nalso be used to hide latency arising from a strict consistency model, giving much of\nthe benefit of a relaxed memory model. The key idea is for the processor to use\ndynamic scheduling to reorder memory references, letting them possibly execute\nout of order. Executing the memory references out of order may generate violations\nof sequential consistency, which might affect the execution of the program. This\npossibility is avoided by using the delayed commit feature of a speculative proces-\nsor. Assume the coherency protocol is based on invalidation. If the processor\nreceives an invalidation for a memory reference before the memory reference is\ncommitted, the processor uses speculation recovery to back out of the computation\nand restart with the memory reference whose address was invalidated.\nIf the reordering of memory requests by the processor yields an execution order\nthat could result in an outcome that differs from what would have been seen under\n422\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 455,
        "text": "sequential consistency, the processor will redo the execution. The key to using this\napproach is that the processor need only guarantee that the result would be the same\nas if all accesses were completed in order, and it can achieve this by detecting when\nthe results might differ. The approach is attractive because the speculative restart\nwill rarely be triggered. It will be triggered only when there are unsynchronized\naccesses that actually cause a race (Gharachorloo et al., 1992).\nHill (1998) advocated the combination of sequential or processor consistency\ntogether with speculative execution as the consistency model of choice. His argu-\nment has three parts. First, an aggressive implementation of either sequential con-\nsistency or processor consistency will gain most of the advantage of a more relaxed\nmodel. Second, such an implementation adds very little to the implementation cost\nof a speculative processor. Third, such an approach allows the programmer to rea-\nson using the simpler programming models of either sequential or processor con-\nsistency. The MIPS R10000 design team had this insight in the mid-1990s and\nused the R10000\u2019s out-of-order capability to support this type of aggressive imple-\nmentation of sequential consistency.\nOne open question is how successful compiler technology will be in optimizing\nmemory references to shared variables. The state of optimization technology and\nthe fact that shared data are often accessed via pointers or array indexing have lim-\nited the use of such optimizations. If this technology were to become available and\nlead to significant performance advantages, compiler writers would want to be able\nto take advantage of a more relaxed programming model. This possibility and the\ndesire to keep the future as flexible as possible led the RISC V designers to opt for\nrelease consistency, after a long series of debates.\nInclusion and Its Implementation\nAll multiprocessors use multilevel cache hierarchies to reduce both the demand on\nthe global interconnect and the latency of cache misses. If the cache also provides\nmultilevel inclusion\u2014every level of cache hierarchy is a subset of the level farther\naway from the processor\u2014then we can use the multilevel structure to reduce the\ncontention between coherence traffic and processor traffic that occurs when snoops\nand processor cache accesses must contend for the cache. Many multiprocessors\nwith multilevel caches enforce the inclusion property, although recent multiproces-\nsors with smaller L1 caches and different block sizes have sometimes chosen not to\nenforce inclusion. This restriction is also called the subset property because each\ncache is a subset of the cache below it in the hierarchy.\nAt first glance, preserving the multilevel inclusion property seems trivial. Con-\nsider a two-level example: Any miss in L1 either hits in L2 or generates a miss in\nL2, causing it to be brought into both L1 and L2. Likewise, any invalidate that hits\nin L2 must be sent to L1, where it will cause the block to be invalidated if it exists.\nThe catch is what happens when the block sizes of L1 and L2 are different.\nChoosing different block sizes is quite reasonable, since L2 will be much larger\nand have a much longer latency component in its miss penalty, and thus will want\n5.7\nCross-Cutting Issues\n\u25a0\n423"
    },
    {
        "page": 456,
        "text": "to use a larger block size. What happens to our \u201cautomatic\u201d enforcement of inclu-\nsion when the block sizes differ? A block in L2 represents multiple blocks in L1,\nand a miss in L2 causes the replacement of data that is equivalent to multiple L1\nblocks. For example, if the block size of L2 is four times that of L1, then a miss in\nL2 will replace the equivalent of four L1 blocks. Let\u2019s consider a detailed example.\nExample\nAssume that L2 has a block size four times that of L1. Show how a miss for an\naddress that causes a replacement in L1 and L2 can lead to violation of the inclu-\nsion property.\nAnswer\nAssume that L1 and L2 are direct-mapped and that the block size of L1 is b bytes\nand the block size of L2 is 4b bytes. Suppose L1 contains two blocks with starting\naddresses x and x+b and that x mod 4b\u00bc0, meaning that x also is the starting\naddress of a block in L2; then that single block in L2 contains the L1 blocks x,\nx+b, x+2b, and x+3b. Suppose the processor generates a reference to block y that\nmaps to the block containing x in both caches and thus misses. Because L2 missed,\nit fetches 4b bytes and replaces the block containing x, x+b, x+2b, and x+3b,\nwhile L1 takes b bytes and replaces the block containing x. Because L1 still con-\ntains x+b, but L2 does not, the inclusion property no longer holds.\nTo maintain inclusion with multiple block sizes, we must probe the higher\nlevels of the hierarchy when a replacement is done at the lower level to ensure that\nany words replaced in the lower level are invalidated in the higher-level caches;\ndifferent levels of associativity create the same sort of problems. Baer and\nWang (1988) described the advantages and challenges of inclusion in detail,\nand in 2017 most designers have opted to implement inclusion, often by settling\non one block size for all levels in the cache. For example, the Intel i7 uses inclusion\nfor L3, meaning that L3 always includes the contents of all of L2 and L1. This\ndecision allows the i7 to implement a straightforward directory scheme at L3\nand to minimize the interference from snooping on L1 and L2 to those circum-\nstances where the directory indicates that L1 or L2 have a cached copy. The\nAMD Opteron, in contrast, makes L2 inclusive of L1 but has no such restriction\nfor L3. It uses a snooping protocol, but only needs to snoop at L2 unless there is a\nhit, in which case a snoop is sent to L1.\nPerformance Gains From Multiprocessing and Multithreading\nIn this section, we briefly look at a study of the effectiveness of using multithread-\ning on a multicore processor, the IBM Power5; we will return to this topic in the\nnext section, when we examine the performance of the Intel i7. The IBM Power5 is\na dual-core that supports simultaneous multithreading (SMT); its basic architecture\nis very similar to the more recent Power8 (which we examine in the next section),\nbut it has only two cores per processor.\n424\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 457,
        "text": "To examine the performance of multithreading in a multiprocessor, measure-\nments were made on an IBM system with eight Power5 processors, using only one\ncore on each processor. Figure 5.25 shows the speedup for an 8-processor Power5\nmultiprocessor, with and without SMT, for the SPECRate2000 benchmarks, as\ndescribed in the caption. On average, the SPECintRate is 1.23 times faster, and\nthe SPECfpRate is 1.16 times faster. Note that a few floating-point benchmarks\nexperience a slight decrease in performance in SMT mode, with the maximum\nwupwise\nswim\nmgrid\napplu\nmesa\ngalgel\nSPECfpRate\nSPECintRate\nart\nequake\nfacerec\nammp\nlucas\nfma3d\nsixtrack\napptu\ngzip\nvpr\ngcc\nmcf\ncrafty\nparser\neon\nperlbmk\ngap\nvortex\nbzip2\ntwolf\n0.9\n1.0\n1.1\n1.2\nSpeedup\n1.3\n1.4\n1.5\nFigure 5.25 A comparison of SMT and single-thread (ST) performance on the 8-processor IBM eServer p5 575\nusing SPECfpRate (top half) and SPECintRate (bottom half) as benchmarks. Note that the x-axis starts at a speedup\nof 0.9, a performance loss. Only one processor in each Power5 core is active, which should slightly improve the results\nfrom SMT by decreasing destructive interference in the memory system. The SMT results are obtained by creating 16\nuser threads, whereas the ST results use only eight threads; with only one thread per processor, the Power5 is\nswitched to single-threaded mode by the OS. These results were collected by John McCalpin at IBM. As we can\nsee from the data, the standard deviation of the results for the SPECfpRate is higher than for SPECintRate (0.13 versus\n0.07), indicating that the SMT improvement for FP programs is likely to vary widely.\n5.7\nCross-Cutting Issues\n\u25a0\n425"
    },
    {
        "page": 458,
        "text": "reduction in speedup being 0.93. Although one might expect that SMT would do a\nbetter job of hiding the higher miss rates of the SPECFP benchmarks, it appears\nthat limits in the memory system are encountered when running in SMT mode\non such benchmarks.\n5.8\nPutting It All Together: Multicore Processors\nand Their Performance\nFor roughly 10 years, multicore has been the primary focus for scaling perfor-\nmance, although the implementations vary widely, as does their support for larger\nmultichip multiprocessors. In this section, we examine the design of three different\nmulticores, the support they provide for larger multiprocessors, and some perfor-\nmance characteristics, before doing a broader evaluation of small to large multi-\nprocessor Xeon systems, and concluding with a detailed evaluation of the\nmulticore i7 920, a predecessor of the i7 6700.\nPerformance of Multicore-Based Multiprocessors\non a Multiprogrammed Workload\nFigure 5.26 shows the key characteristics of three multicore processors designed\nfor server applications and available in 2015 through 2017. The Intel Xeon E7 is\nbased on the same basic design as the i7, but it has more cores, a slightly slower\nclock rate (power is the limitation), and a larger L3 cache. The Power8 is the new-\nest in the IBM Power series and features more cores and bigger caches. The Fujitsu\nSPARC64 X+ is the newest SPARC server chip; unlike the T-series mentioned in\nChapter 3, it uses SMT. Because these processors are configured for multicore and\nmultiprocessor servers, they are available as a family, varying processor count,\ncache size, and so on, as the figure shows.\nThese three systems show a range of techniques both for connecting the on-\nchip cores and for connecting multiple processor chips. First, let\u2019s look at how\nthe cores are connected within a chip. The SPARC64 X+ is the simplest: it shares\na single L2 cache, which is 24-way set associative, among the 16 cores. There are\nfour separate DIMM channels to attach memory accessible with a 16\u00034 switch\nbetween the cores and the channels.\nFigure 5.27 shows how the Power8 and Xeon E7 chips are organized. Each\ncore in the Power8 has an 8 MiB bank of L3 directly connected; other banks\nare accessed via the interconnection network, which has 8 separate buses. Thus\nthe Power8 is a true NUCA (Nonuniform Cache Architecture), because the access\ntime to the attached bank of L3 will be much faster than accessing another L3. Each\nPower8 chip has a set of links that can be used to build a large multiprocessor using\nan organization we will see shortly. The memory links are connected to a special\nmemory controller that includes an L4 and interfaces directly with DIMMs.\nPart B of Figure 5.27, shows how the Xeon E7 processor chip is organized\nwhen there are 18 or more cores (20 cores are shown in this figure). Three rings\n426\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 459,
        "text": "connect the cores and the L3 cache banks, and each core and each bank of L3 is\nconnected to two rings. Thus any cache bank or any core is accessible from any\nother core by choosing the right ring. Therefore, within the chip, the E7 has uni-\nform access time. In practice, however, the E7 is normally operated as a NUMA\narchitecture by logically associating half the cores with each memory channel; this\nFeature\nIBM Power8\nIntel Xeon E7\nFujitsu SPARC64 X+\nCores/chip\n4, 6, 8, 10, 12\n4, 8, 10, 12, 22, 24\n16\nMultithreading\nSMT\nSMT\nSMT\nThreads/core\n8\n2\n2\nClock rate\n3.1\u20133.8 GHz\n2.1\u20133.2 GHz\n3.5 GHz\nL1 I cache\n32 KB per core\n32 KB per core\n64 KB per core\nL1 D cache\n64 KB per core\n32 KB per core\n64 KB per core\nL2 cache\n512 KB per core\n256 KB per core\n24 MiB shared\nL3 cache\nL3: 32\u201396 MiB: 8 MiB per\ncore (using eDRAM); shared\nwith nonuniform access time\n10\u201360 MiB @ 2.5 MiB per core;\nshared, with larger core counts\nNone\nInclusion\nYes, L3 superset\nYes, L3 superset\nYes\nMulticore\ncoherence\nprotocol\nExtended MESI with\nbehavioral and locality hints\n(13-states)\nMESIF: an extended form of MESI\nallowing direct transfers of clean\nblocks\nMOESI\nMultichip\ncoherence\nimplementation\nHybrid strategy with\nsnooping and directory\nHybrid strategy with snooping and\ndirectory\nHybrid strategy with\nsnooping and directory\nMultiprocessor\ninterconnect\nsupport\nCan connect up to 16\nprocessor chips with 1 or 2\nhops to reach any processor\nUp to 8 processor chips directly via\nQuickpath; larger system and\ndirectory support with additional\nlogic\nCrossbar interconnect chip,\nsupports up to 64 processors;\nincludes directory support\nProcessor chip\nrange\n1\u201316\n2\u201332\n1\u201364\nCore count\nrange\n4\u2013192\n12\u2013576\n8\u20131024\nFigure 5.26 Summary of the characteristics of three recent high-end multicore processors (2015\u20132017 releases)\ndesigned for servers. The table shows the range of processor counts, clock rates, and cache sizes within each pro-\ncessor family. The Power8 L3 is a NUCA (Nonuniform Cache Access) design, and it also supports off-chip L4 of up to\n128 MiB using EDRAM. A 32-core Xeon has recently been announced, but no system shipments have occurred. The\nFujitsu SPARC64 is also available as an 8-core design, which is normally configured as a single processor system. The\nlast row shows the range of configured systems with published performance data (such as SPECintRate) with both\nprocessor chip counts and total core counts. The Xeon systems include multiprocessors that extend the basic inter-\nconnect with additional logic; for example, using the standard Quickpath interconnect limits the processor count to\n8 and the largest system to 8\u000324\u00bc192 cores, but SGI extends the interconnect (and coherence mechanisms) with\nextra logic to offer a 32 processor system using 18-core processor chips for a total size of 576 cores. Newer releases of\nthese processors increased clock rates (significantly in the Power8 case, less so in others) and core counts (signifi-\ncantly in the case of Xeon).\n5.8\nPutting It All Together: Multicore Processors and Their Performance\n\u25a0\n427"
    },
    {
        "page": 460,
        "text": "(A)\nThe Power8 chip organization\nCore\n512 KB\nL2\n8 MB\nL3\nRegion\nCore\n512 KB\nL2\n8 MB\nL3\nRegion\nCore\n512 KB\nL2\n8 MB\nL3\nRegion\nCore\n512 KB\nL2\n8 MB\nL3\nRegion\nUp to 12 Cores\n. . .\n. . .\nOn-chip coherence and data interconnect\nOff-chip\nIntra-group\nSMP Ctrl\nOff-chip\nIntergroup\nSMP Ctrl\nInterrupt\nControl\nPHB\nMemory\nControl\nx8\nMemory\nControl\nx8\nMemory\nControl\nx8\nMemory\nControl\nx8\nMemory\nControl\nx8\nMemory\nControl\nx8\nMemory\nControl\nx8\nMemory\nControl\nPHB\nPHB\nMux\nAccelerator\nInterface\nRNG\nAccel\nCrypto\nAccel\nCmp\nAccel\nCAPP\nIntragroup links\n8 B @ 4.8 GHz /\ndirection\nx 3 single-ended links\nIntergroup links\n2 B @ 6.4 GHz /\ndirection\nx 3 differential links\nPCI gen 3 I/O\n8 GHz / direction differential\n(16x+16x) or (16x+8x+8x)\n2 B @ 9.6 GHz read, 1B+cmd\n@ 9.6 GHz write\nx 8 differential channels\n(B)\nCore\n>=2.5 GHz\nCore\nBo\nCore\n>=2.5 GHz\nCore\nBo\nCore\n>=2.5 GHz\nCore\nBo\nCore\n>=2.5 GHz\nCore\nBo\nCore\n>=2.5 GHz\nCore\nBo\nCore\n>=2.5 GHz\nCore\nBo\nCore\n>=2.5 GHz\nCore\nBo\nCore\n>=2.5 GHz\nCore\nBo\nCore\n>=2.5 GHz\nCore\nBo\nCore\n>=2.5 GHz\nCore\nBo\nCore\n>=2.5 GHz\nCore\nBo\nCacheBo\nL3\n2.5 MB\nCacheBo\nL3\n2.5 MB\nCacheBo\nL3\n2.5 MB\nCacheBo\nL3\n2.5 MB\nCacheBo\nL3\n2.5 MB\nCacheBo\nL3\n2.5 MB\nCacheBo\nL3\n2.5 MB\nCacheBo\nL3\n2.5 MB\nCacheBo\nL3\n2.5 MB\nCacheBo\nL3\n2.5 MB\nCacheBo\nThe Xeon E7 organization\nL3\n2.5 MB\nCacheBo\nL3\n2.5 MB\nCacheBo\nL3\n2.5 MB\nCacheBo\nL3\n2.5 MB\nCacheBo\nL3\n2.5 MB\nCore\n>=2.5 GHz\nCore\nBo\nHome Agent\nDD (4 channels)\nCore\n>=2.5 GHz\nCore\nBo\nCore\n>=2.5 GHz\nCore\nBo\nCore\n>=2.5 GHz\nCore\nBo\nDD\nDD\nDD\nDD\nHome Agent\nDDR (2 channels)\nDD\nDD\nLink0\nLink1\nQPI Agent\n2x20, 9.6 GT/s\nLink2\nQPI Agent\n1x20, 9.6 GT/s\nPCle\nx10\nPCle\nx8\nPCle\nx16\nx4\nDMI\nPort0\nPort1\nPort2\nDMI\nDMA\nIOAPIC\nIIO\nUBox\nPCU\nGlobally routed power\nmanagement links\nMsg Ch\nFigure 5.27 The on-chip organizations of the Power8 and Xeon E7 are shown. The Power8 uses 8 separate buses\nbetween L3 and the CPU cores. Each Power8 also has two sets of links for connecting larger multiprocessors. The\nXeon uses three rings to connect processors and L3 cache banks, as well QPI for interchip links. Software is used\nto logically associate half the cores with each memory channel."
    },
    {
        "page": 461,
        "text": "increases the probability that a desired memory page is open on a given access. The\nE7 provides 3 QuickPath Interconnect (QPI) links for connecting multiple E7s.\nMultiprocessors consisting of these multicores use a variety of different inter-\nconnection strategies, as Figure 5.28 shows. The Power8 design provides support\nfor connecting 16 Power8 chips for a total of 192 cores. The intragroup links pro-\nvide higher bandwidth interconnect among a completely connected module of 4\nprocessor chips. The intergroup links are used to connect each processor chip to\nthe 3 other modules. Thus each processor is two hops from any other, and the mem-\nory access time is determined by whether an address resides in local memory, clus-\nter memory, or intercluster memory (actually the latter can have two different\nvalues, but the difference is swamped by the intercluster time).\nThe Xeon E7 uses QPI to interconnect multiple multicore chips. In a 4-chip,\nmultiprocessor, which with the latest announced Xeon could have 128 cores,\nthe three QPI links on each processor are connected to three neighbors, yielding\na 4-chip fully connected multiprocessor. Because memory is directly connected\nto each E7 multicore, even this 4-chip arrangement has nonuniform memory access\ntime (local versus remote). Figure 5.28 shows how 8 E7 processors can be con-\nnected; like the Power8, this leads to a situation where every processor is one\nor two hops from every other processor. There are a number of Xeon-based mul-\ntiprocessor servers that have more than 8 processor chips. In such designs, the typ-\nical organization is to connect 4 processor chips together in a square, as a module,\nwith each processor connecting to two neighbors. The third QPI in each chip is\nconnected to a crossbar switch. Very large systems can be created in this fashion.\nMemory accesses can then occur at four locations with different timings: local to\nthe processor, an immediate neighbor, the neighbor in the cluster that is two hops\naway, and through the crossbar. Other organizations are possible and require less\nthan a full crossbar in return for more hops to get to remote memory.\nThe SPARC64 X+ also uses a 4-processor module, but each processor has\nthree connections to its immediate neighbors plus two (or three in the largest con-\nfiguration) connections to a crossbar. In the largest configuration, 64 processor\nchips can be connected to two crossbar switches, for a total of 1024 cores. Memory\naccess is NUMA (local, within a module, and through the crossbar), and coherency\nis directory-based.\nPerformance of Multicore-Based Multiprocessors\non a Multiprogrammed Workload\nFirst, we compare the performance scalability of these three multicore processors\nusing SPECintRate, considering configurations up to 64 cores. Figure 5.29 shows\nhow the performance scales relative to the performance of the smallest configura-\ntion, which varies between 4 and 16 cores. In the plot, the smallest configuration is\nassumed to have perfect speedup (i.e., 8 for 8 cores, 12 for 12 cores, etc.). This\nfigure does not show performance among these different processors. Indeed such\nperformance varies significantly: in the 4-core configuration, the IBM Power8 is\n5.8\nPutting It All Together: Multicore Processors and Their Performance\n\u25a0\n429"
    },
    {
        "page": 462,
        "text": "4 chip\ngroup\n12-core\nPower 8\n12-core\nPower 8\n12-core\nPower 8\n12-core\nPower 8\n12-core\nPower 8\n12-core\nPower 8\n12-core\nPower 8\n12-core\nPower 8\n12-core\nPower 8\n12-core\nPower 8\n12-core\nPower 8\n12-core\nPower 8\n12-core\nPower 8\n12-core\nPower 8\n12-core\nPower 8\n12-core\nPower 8\n25.6 GB/s\nInter-group\nCable\n78.4 GB/s\nIntra-group\nBus\nPower8 system with up to 16 chips.\n(A)\n(B)  Xeon E7 system showing with up to 8 chips.\nE7\nE7\nE7\nE7\nE7\nE7\nE7\nE7\n(C)  SPARC64 X+ with the 4-chip building block.\nCPU\nCPU\nXB\nXB\nCPU\nCPU\nFigure 5.28 The system architecture for three multiprocessors built from multicore chips.\n430\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 463,
        "text": "1.5 times as fast as the SPARC64 X+ on a per core basis! Instead Figure 5.29\nshows how the performance scales for each processor family as additional cores\nare added.\nTwo of the three processors show diminishing returns as they scale to 64 cores.\nThe Xeon systems appear to show the most degradation at 56 and 64 cores. This\nmay be largely due to having more cores share a smaller L3. For example, the 40-\ncore system uses 4 chips, each with 60 MiB of L3, yielding 6 MiB of L3 per core.\nThe 56-core and 64-core systems also use 4 chips but have 35 or 45 MiB of L3 per\nchip, or 2.5\u20132.8 MiB per core. It is likely that the resulting larger L3 miss rates lead\nto the reduction in speedup for the 56-core and 64-core systems.\nThe IBM Power8 results are also unusual, appearing to show significant super-\nlinear speedup. This effect, however, is due largely to differences in the clock rates,\nwhich are much larger across the Power8 processors than for the other processors\n4\n4\n8\n12\n16\n20\n24\n28\n32\nNumber of cores\nSpeedup relative to the smallest configuration\n36\n40\n44\n48\n52\n56\n60\n64\n8\n12\n16\n20\n24\n28\n32\n36\n40\n44\n48\n52\n56\n60\n64\n68\n72\n76\n80\n84\nIntel Xeon E7\nIBM Power8\nFujitsu SPARC64 X+\nFigure 5.29 The performance scaling on the SPECintRate benchmarks for four multicore processors as the\nnumber of cores is increased to 64. Performance for each processor is plotted relative to the smallest configuration\nand assuming that configuration had perfect speedup. Although this chart shows how a given multiprocessor\nscales with additional cores, it does not supply any data about performance among processors. There are differences\nin the clock rates, even within a given processor family. These are generally swamped by the core scaling effects,\nexcept for the Power8 that shows a clock range spread of 1.5\u0003 from the smallest configuration to the 64 core\nconfiguration.\n5.8\nPutting It All Together: Multicore Processors and Their Performance\n\u25a0\n431"
    },
    {
        "page": 464,
        "text": "in this figure. In particular, the 64-core configuration has the highest clock rate\n(4.4 GHz), whereas the 4-core configuration has a 3.0 GHz clock. If we normalize\nthe relative speedup for the 64-core system based on the clock rate differential with\nthe 4-core system, the effective speedup is 57 rather than 84. Therefore, while the\nPower8 system scales well, and perhaps the best among these processors, it is not\nmiraculous.\nFigure 5.30 shows scaling for these three systems at configurations above 64\nprocessors. Once again, the clock rate differential explains the Power8 results; the\nclock-rate equivalent scaled speedup with 192 processors is 167, versus 223, when\nnot accounting for clock rate differences. Even at 167, the Power8 scaling is some-\nwhat better than that on the SPARC64 X+ or Xeon systems. Surprisingly, although\nthere are some effects on speedup in going from the smallest system to 64 cores,\nthey do not seem to get dramatically worse at these larger configurations. The\nnature of the workload, which is highly parallel and user-CPU-intensive, and\nthe overheads paid in going to 64 cores probably lead to this result.\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n220\n240\n260\n280\n300\n320\n340\n360\n380\n400\n420\n440\n460\n480\n500\n520\n540\n560\n580\n600\nSpeedup relative to the smallest configuration\nNumber of cores\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n220\n240\n260\n280\n300\n320\n340\n360\n380\n400\n420\n440\n460\n480\n500\n520\n540\n560\n580\n600\nIntel Xeon E7\nIBM Power8\nFujitu SPARC64 X+\nFigure 5.30 The scaling of relative performance for multiprocessor multicore. As before, performance is shown\nrelative to the smallest available system. The Xeon result at 80 cores is the same L3 effect that showed up for smaller\nconfigurations. All systems larger than 80 cores have between 2.5 and 3.8 MiB of L3 per core, and the 80-core, or\nsmaller, systems have 6 MiB per core.\n432\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 465,
        "text": "Scalability in an Xeon MP With Different Workloads\nIn this section, we focus on the scalability of the Xeon E7 multiprocessors on three\ndifferent workloads: a Java-based commercially oriented workload, a virtual\nmachine workload, and a scientific parallel processing workload, all from the\nSPEC benchmarking organization, as described next.\n\u25a0\nSPECjbb2015: Models a supermarket IT system that handles a mix of\npoint-of-sale requests, online purchases, and data-mining operations. The\nperformance metric is throughput-oriented, and we use the maximum per-\nformance measurement on the server side running multiple Java virtual\nmachines.\n\u25a0\nSPECVirt2013: Models a collection of virtual machines running independent\nmixes of other SPEC benchmarks, including CPU benchmarks, web servers,\nand mail servers. The system must meet a quality of service guarantee for each\nvirtual machine.\n\u25a0\nSPECOMP2012: A collection of 14 scientific and engineering programs writ-\nten with the OpenMP standard for shared-memory parallel processing. The\ncodes are written in Fortran, C, and C++ and range from fluid dynamics to\nmolecular modeling to image manipulation.\nAs with the previous results, Figure 5.31 shows performance assuming linear\nspeedup on the smallest configuration, which for these benchmarks varies from 48\ncores to 72 cores, and plotting performance relative to the that smallest configu-\nration. The SPECjbb2015 and SPECVirt2013 include significant systems soft-\nware, including the Java VM software and the VM hypervisor. Other than the\nsystem software, the interaction among the processes is very small. In contrast,\nSPECOMP2012 is a true parallel code with multiple user processes sharing data\nand collaborating in the computation.\nLet\u2019s begin by examining SPECjbb2015. It obtains speedup efficiency\n(speedup/processor ratio) of between 78% and 95%, showing good speedup, even\nin the largest configuration. SPECVirt2013 does even better (for the range of sys-\ntem measured), obtaining almost linear speedup at 192 cores. Both SPECjbb2015\nand SPECVirt2013 are benchmarks that scale up the application size (as in the TPC\nbenchmarks discussed in Chapter 1) with larger systems so that the effects of\nAmdahl\u2019s Law and interprocess communication are minor.\nFinally, let\u2019s turn to SPECOMP2012, the most compute-intensive of these\nbenchmarks and the one that truly involves parallel processing. The major trend\nvisible here is a steady loss of efficiency as we scale from 30 to 576 cores so that\nby 576 cores, the system exhibits only half the efficiency it showed at 30 cores.\nThis reduction leads to a relative speedup of 284, assuming that the 30-core\nspeedup is 30. These are probably Amdahl\u2019s Law effects resulting from limited\nparallelism as well as synchronization and communication overheads. Unlike\n5.8\nPutting It All Together: Multicore Processors and Their Performance\n\u25a0\n433"
    },
    {
        "page": 466,
        "text": "the SPECjbb2015 and SPECVirt2013, these benchmarks are not scaled for larger\nsystems.\nPerformance and Energy Efficiency of the Intel i7 920\nMulticore\nIn this section, we closely examine the performance of the i7 920, a predecessor of\nthe 6700, on the same two groups of benchmarks we considered in Chapter 3: the\nparallel Java benchmarks and the parallel PARSEC benchmarks (described in detail\nin Figure 3.32 on page 247). Although this study uses the older i7 920, it remains, by\nfar, the most comprehensive study of energy efficiency in multicore processors and\nthe effects of multicore combined with SMT. The fact that the i7 920 and 6700 are\nsimilar indicates that the basic insights should also apply to the 6700.\n40\n60\n80\n100\n120\n140\n160\n180\n200\n220\n240\n260\n280\n300\n320\n340\nPerformance relative to the smallest configuration\n40\n60\n80\n100\n120\n140\n160\n180\n200\n220\n240\n260\n280\n300\n320\n340\n360\n380\n400\n420\n440\n460\n480\n500\n520\n540\n560\n580\nSPECVirtSC2013\nSPECOMP2012\nSPECJBB2015\nFigure 5.31 Scaling of performance on a range of Xeon E7 systems showing performance relative to the smallest\nbenchmark configuration, and assuming that configuration gets perfect speedup (e.g., the smallest SPEWCOMP\nconfiguration is 30 cores and we assume a performance of 30 for that system). Only relative performance can be\nassessed from this data, and comparisons across the benchmarks have no relevance. Note the difference in the scale\nof the vertical and horizontal axes.\n434\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 467,
        "text": "First, we look at the multicore performance and scaling versus a single-core\nwithout the use of SMT. Then we combine both the multicore and SMT capabil-\nity. All the data in this section, like that in the earlier i7 SMT evaluation\n(Chapter 3) come from Esmaeilzadeh et al. (2011). The dataset is the same as\nthat used earlier (see Figure 3.32 on page 247), except that the Java benchmarks\ntradebeans and pjbb2005 are removed (leaving only the five scalable Java bench-\nmarks); tradebeans and pjbb2005 never achieve speedup above 1.55 even with\nfour cores and a total of eight threads, and thus are not appropriate for evaluating\nmore cores.\nFigure 5.32 plots both the speedup and energy efficiency of the Java and\nPARSEC benchmarks without the use of SMT. Energy efficiency is computed\nby the ratio: energy consumed by the single-core run divided by the energy con-\nsumed by the two- or four-core run (i.e., efficiency is the inverse of energy con-\nsumed). Higher energy efficiency means that the processor consumes less\nenergy for the same computation, with a value of 1.0 being the break-even\n1\n3.5\n1.06\n4\n1.10\n1.04\n1.02\n1.00\n0.98\ni7 2P and 4P energy efficiency\ni7 2P and 4P speedup\n0.96\n0.94\n0.92\n0.90\n0.88\n0.86\n3\n2.5\n2\n1.5\n1.08\nJava speedup\nPARSEC speedup\nJava energy efficiency\nPARSEC energy efficiency\n2P\n4P\nCores\nFigure 5.32 This chart shows the speedup and energy efficiency for two- and four-core executions of the par-\nallel Java and PARSEC workloads without SMT. These data were collected by Esmaeilzadeh et al. (2011) using the\nsame setup as described in Chapter 3. Turbo Boost is turned off. The speedup and energy efficiency are summarized\nusing harmonic mean, implying a workload where the total time spent running each benchmark on 2 cores is\nequivalent.\n5.8\nPutting It All Together: Multicore Processors and Their Performance\n\u25a0\n435"
    },
    {
        "page": 468,
        "text": "point. The unused cores in all cases were in deep sleep mode, which minimized\ntheir power consumption by essentially turning them off. In comparing the data\nfor the single-core and multicore benchmarks, it is important to remember that\nthe full energy cost of the L3 cache and memory interface is paid in the single-\ncore (as well as the multicore) case. This fact increases the likelihood that\nenergy consumption will improve for applications that scale reasonably well.\nHarmonic mean is used to summarize results with the implication described\nin the caption.\nAs the figure shows, the PARSEC benchmarks get better speedup than the\nJava benchmarks, achieving 76% speedup efficiency (i.e., actual speedup\ndivided by processor count) on four cores, whereas the Java benchmarks\nachieve 67% speedup efficiency on four cores. Although this observation is\nclear from the data, analyzing why this difference exists is difficult. It is quite\npossible that Amdahl\u2019s Law effects have reduced the speedup for the Java\nworkload, which includes some typically serial parts, such as the garbage\ncollector. In addition, interaction between the processor architecture and the\napplication, which affects issues such as the cost of synchronization or commu-\nnication, may also play a role. In particular, well-parallelized applications, such\nas those in PARSEC, sometimes benefit from an advantageous ratio between\ncomputation and communication, which reduces the dependence on communi-\ncations costs (see Appendix I).\nThese differences in speedup translate to differences in energy efficiency.\nFor example, the PARSEC benchmarks actually slightly improve energy effi-\nciency over the single-core version; this result may be significantly affected by\nthe fact that the L3 cache is more effectively used in the multicore runs than in\nthe single-core case and the energy cost is identical in both cases. Thus, for the\nPARSEC benchmarks, the multicore approach achieves what designers hoped\nfor when they switched from an ILP-focused design to a multicore design; namely,\nit scales performance as fast or faster than scaling power, resulting in constant or\neven improved energy efficiency. In the Java case, we see that neither the two- nor\nfour-core runs break even in energy efficiency because of the lower speedup levels\nof the Java workload (although Java energy efficiency for the 2p run is the same\nas for PARSEC). The energy efficiency in the four-core Java case is reasonably\nhigh (0.94). It is likely that an ILP-centric processor would need even more power\nto achieve a comparable speedup on either the PARSEC or Java workload. Thus\nthe TLP-centric approach is also certainly better than the ILP-centric approach for\nimproving performance for these applications. As we will see in Section 5.10, there\nare reasons to be pessimistic about simple, efficient, long-term scaling of\nmulticore.\nPutting Multicore and SMT Together\nFinally, we consider the combination of multicore and multithreading by measur-\ning the two benchmark sets for two to four processors and one to two threads\n(a total of four data points and up to eight threads). Figure 5.33 shows the speedup\n436\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 469,
        "text": "and energy efficiency obtained on the Intel i7 when the processor count is two or\nfour and SMT is or is not employed, using harmonic mean to summarize the two\nbenchmarks sets. Clearly, SMT can add to performance when there is sufficient\nthread-level parallelism available even in the multicore situation. For example,\nin the four-core, no-SMT case, the speedup efficiencies were 67% and 76% for\nJava and PARSEC, respectively. With SMT on four cores, those ratios are an\nastonishing 83% and 97%.\nEnergy efficiency presents a slightly different picture. In the case of PARSEC,\nspeedup is essentially linear for the four-core SMT case (eight threads), and power\nscales more slowly, resulting in an energy efficiency of 1.1 for that case. The Java\nsituation is more complex; energy efficiency peaks for the two-core SMT (four-\nthread) run at 0.97 and drops to 0.89 in the four-core SMT (eight-thread) run. It\nseems highly likely that the Java benchmarks are encountering Amdahl\u2019s Law\neffects when more than four threads are deployed. As some architects have\nobserved, multicore does shift more responsibility for performance (and thus\nenergy efficiency) to the programmer, and the results for the Java workload\ncertainly bear this out.\n1\n2Px1T\n2Px2T\n4Px1T\n4Px2T\n1.5\n4\n1.20\n1.15\n1.10\n1.05\ni7 2Px1T, 2Px2T, 4Px1T, and 4Px2T speedup\ni7 2Px1T, 2Px2T, 4Px1T, and 4Px2T energy efficiency\n1.00\n0.95\n0.90\n0.85\n0.80\n3.5\n3\n2.5\n2\nJava speedup\nPARSEC speedup\nJava energy efficiency\nPARSEC energy efficiency\nFigure 5.33 This chart shows the speedup for two- and four-core executions of the\nparallel Java and PARSEC workloads both with and without SMT. Remember that the\npreceding results vary in the number of threads from two to eight and reflect both archi-\ntectural effects and application characteristics. Harmonic mean is used to summarize\nresults, as discussed in the Figure 5.32 caption.\n5.8\nPutting It All Together: Multicore Processors and Their Performance\n\u25a0\n437"
    },
    {
        "page": 470,
        "text": "5.9\nFallacies and Pitfalls\nGiven the lack of maturity in our understanding of parallel computing, there are\nmany hidden pitfalls that will be uncovered either by careful designers or by unfor-\ntunate ones. Given the large amount of hype that has surrounded multiprocessors\nover the years, common fallacies abound. We have included a selection of them.\nPitfall\nMeasuring performance of multiprocessors by linear speedup versus execution\ntime.\nGraphs like those in Figures 5.32 and 5.33, which plot performance versus number\nof processors, showing linear speedup, a plateau, and then a falling off, have long\nbeen used to judge the success of parallel processors. Although speedup is one\nfacet of a parallel program, it is not a direct measure of performance. The first issue\nis the power of the processors being scaled: a program that linearly improves per-\nformance to equal 100 Intel Atom processors (the low-end processor used for net-\nbooks) may be slower than the version run on an 8-core Xeon. Be especially careful\nof floating-point-intensive programs; processing elements without hardware assist\nmay scale wonderfully but have poor collective performance.\nComparing execution times is fair only if you are comparing the best algo-\nrithms on each computer. Comparing the identical code on two computers may\nseem fair, but it is not; the parallel program may be slower on a uniprocessor than\non a sequential version. Developing a parallel program will sometimes lead to\nalgorithmic improvements, so comparing the previously best-known sequential\nprogram with the parallel code\u2014which seems fair\u2014will not compare equivalent\nalgorithms. To reflect this issue, the terms relative speedup (same program) and\ntrue speedup (best program) are sometimes used.\nResults that suggest superlinear performance, when a program on n processors\nis more than n times faster than the equivalent uniprocessor, may indicate that the\ncomparison is unfair, although there are instances where \u201creal\u201d superlinear\nspeedups have been encountered. For example, some scientific applications regu-\nlarly achieve superlinear speedup for small increases in processor count (2 or 4 to\n8 or 16). These results usually arise because critical data structures that do not fit\ninto the aggregate caches of a multiprocessor with 2 or 4 processors fit into the\naggregate cache of a multiprocessor with 8 or 16 processors. As we saw in the pre-\nvious section, other differences (such as high clock rate) may appear to yield super-\nlinear speedups when comparing slightly different systems.\nIn summary, comparing performance by comparing speedups is at best tricky\nand at worst misleading. Comparing the speedups for two different multiproces-\nsors does not necessarily tell us anything about the relative performance of the mul-\ntiprocessors, as we also saw in the previous section. Even comparing two different\nalgorithms on the same multiprocessor is tricky because we must use true speedup,\nrather than relative speedup, to obtain a valid comparison.\nFallacy\nAmdahl\u2019s Law doesn\u2019t apply to parallel computers.\nIn 1987 the head of a research organization claimed that Amdahl\u2019s Law (see\nSection 1.9) had been broken by an MIMD multiprocessor. This statement hardly\n438\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 471,
        "text": "meant, however, that the law has been overturned for parallel computers; the\nneglected portion of the program will still limit performance. To understand the\nbasis of the media reports, let\u2019s see what Amdahl (1967) originally said:\nA fairly obvious conclusion which can be drawn at this point is that the effort\nexpended on achieving high parallel processing rates is wasted unless it is\naccompanied by achievements in sequential processing rates of very nearly the\nsame magnitude. [p. 483]\nOne interpretation of the law was that, because portions of every program must be\nsequential, there is a limit to the useful economic number of processors\u2014say, 100.\nBy showing linear speedup with 1000 processors, this interpretation of Amdahl\u2019s\nLaw was disproved.\nThe basis for the statement that Amdahl\u2019s Law had been \u201covercome\u201d was the\nuse of scaled speedup, also called weak scaling. The researchers scaled the bench-\nmark to have a dataset size that was 1000 times larger and compared the unipro-\ncessor and parallel execution times of the scaled benchmark. For this particular\nalgorithm, the sequential portion of the program was constant independent of\nthe size of the input, and the rest was fully parallel\u2014thus, linear speedup with\n1000 processors. Because the running time grew faster than linear, the program\nactually ran longer after scaling, even with 1000 processors.\nSpeedup that assumes scaling of the input is not the same as true speedup, and\nreporting it as if it were is misleading. Because parallel benchmarks are often run\non different-sized multiprocessors, it is important to specify what type of applica-\ntion scaling is permissible and how that scaling should be done. Although simply\nscaling the data size with processor count is rarely appropriate, assuming a fixed\nproblem size for a much larger processor count (called strong scaling) is often\ninappropriate, as well, because it is likely that users given a much larger multipro-\ncessor would opt to run a larger or more detailed version of an application. See\nAppendix I for more discussion on this important topic.\nFallacy\nLinear speedups are needed to make multiprocessors cost-effective.\nIt is widely recognized that one of the major benefits of parallel computing is to offer\na \u201cshorter time to solution\u201d than the fastest uniprocessor. Many people, however,\nalso hold the view that parallel processors cannot be as cost-effective as uniproces-\nsorsunlesstheycanachieveperfectlinearspeedup.Thisargumentsaysthat,because\nthe cost of the multiprocessor is a linear function of the number of processors, any-\nthingless than linear speedup means that the performance/cost ratio decreases, mak-\ning a parallel processor less cost-effective than using a uniprocessor.\nThe problem with this argument is that cost is not only a function of processor\ncount but also depends on memory, I/O, and the overhead of the system (box,\npower supply, interconnect, etc.). It also makes less sense in the multicore era,\nwhen there are multiple processors per chip.\nThe effect of including memory in the system cost was pointed out by Wood\nand Hill (1995). We use an example based on more recent data using TPC-C and\nSPECRate benchmarks, but the argument could also be made with a parallel sci-\nentific application workload, which would likely make the case even stronger.\n5.9\nFallacies and Pitfalls\n\u25a0\n439"
    },
    {
        "page": 472,
        "text": "Figure 5.34 shows the speedup for TPC-C, SPECintRate, and SPECfpRate on\nan IBM eServer p5 multiprocessor configured with 4\u201364 processors. The figure\nshows that only TPC-C achieves better than linear speedup. For SPECintRate\nand SPECfpRate, speedup is less than linear, but so is the cost, because unlike\nTPC-C, the amount of main memory and disk required both scale less than linearly.\nAs Figure 5.35 shows, larger processor counts can actually be more cost-\neffective than the 4-processor configuration. In comparing the cost-performance\nof two computers, we must be sure to include accurate assessments of both total\nsystem cost and what performance is achievable. For many applications with larger\nmemory demands, such a comparison can dramatically increase the attractiveness\nof using a multiprocessor.\nPitfall\nNot developing the software to take advantage of, or optimize for, a multiproces-\nsor architecture.\nThere is a long history of software lagging behind on multiprocessors, probably\nbecause the software problems are much harder. We give one example to show\nthe subtlety of the issues, but there are many examples we could choose from.\nOne frequently encountered problem occurs when software designed for a uni-\nprocessor is adapted to a multiprocessor environment. For example, the SGI\nSpeedup\n72\n64\n56\n48\n40\n32\n24\n16\n8\n0\nLinear speedup\nSpeedup TPM\nSpeedup SPECintRate\nSpeedup SPECfpRate\n0\nProcessor count\n8\n16\n24\n32\n40\n48\n56\n64\nFigure 5.34 Speedup for three benchmarks on an IBM eServer p5 multiprocessor when configured with 4, 8, 16,\n32, and 64 processors. The dashed line shows linear speedup.\n440\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 473,
        "text": "operating system in 2000 originally protected the page table data structure with a\nsingle lock, assuming that page allocation was infrequent. In a uniprocessor, this\ndoes not represent a performance problem. In a multiprocessor, it can become a\nmajor performance bottleneck for some programs.\nConsider a program that uses a large number of pages that are initialized at\nstartup, which UNIX does for statically allocated pages. Suppose the program is\nparallelized so that multiple processes allocate the pages. Because page allocation\nrequires the use of the page table data structure, which is locked whenever it is in\nuse, even an OS kernel that allows multiple threads in the OS will be serialized if\nthe processes all try to allocate their pages at once (which is exactly what we might\nexpect at initialization time).\nPerformance/cost relative to four-processor system\n1.15\n1.10\n1.05\n0\n1.00\n0.95\n0.90\n0.85\nProcessor count\n8\n16\n24\n32\n40\n48\n56\n64\nTPM performance/cost\nSPECint performance/cost\nSPECfp performance/cost\nFigure 5.35 The performance/cost for IBM eServer p5 multiprocessors with 4\u201364 processors is shown relative to\nthe 4-processor configuration. Anymeasurementabove1.0indicatesthattheconfigurationismorecost-effectivethan\nthe 4-processor system. The 8-processor configurations show an advantage for all three benchmarks, whereas two of the\nthree benchmarks show a cost-performance advantage in the 16- and 32-processor configurations. For TPC-C, the con-\nfigurations are those used in the official runs, which means that disk and memory scale nearly linearly with processor\ncount, and a 64-processor machine is approximately twice as expensive as a 32-processor version. In contrast, the disk\nandmemoryarescaledmoreslowly(althoughstillfasterthannecessarytoachievethebestSPECRateat64processors).In\nparticular, the disk configurations go from one drive for the 4-processor version to four drives (140 GB) for the 64-\nprocessor version. Memory is scaled from 8 GiB for the 4-processor system to 20 GiB for the 64-processor system.\n5.9\nFallacies and Pitfalls\n\u25a0\n441"
    },
    {
        "page": 474,
        "text": "This page table serialization eliminates parallelism in initialization and has sig-\nnificant impact on overall parallel performance. This performance bottleneck per-\nsists even under multiprogramming. For example, suppose we split the parallel\nprogram apart into separate processes and run them, one process per processor,\nso that there is no sharing between the processes. (This is exactly what one user\ndid, because he reasonably believed that the performance problem was due to unin-\ntended sharing or interference in his application.) Unfortunately, the lock still seri-\nalizes all the processes, so even the multiprogramming performance is poor. This\npitfall indicates the kind of subtle but significant performance bugs that can arise\nwhen software runs on multiprocessors. Like many other key software compo-\nnents, the OS algorithms and data structures must be rethought in a multiprocessor\ncontext. Placing locks on smaller portions of the page table effectively eliminates\nthe problem. Similar problems exist in memory structures, which increases the\ncoherence traffic in cases where no sharing is actually occurring.\nAs multicore became the dominant theme in everything from desktops to\nservers, the lack of an adequate investment in parallel software became apparent.\nGiven the lack of focus, it will likely be many years before the software systems we\nuse adequately exploit the growing numbers of cores.\n5.10\nThe Future of Multicore Scaling\nFor more than 30 years, researchers and designers have predicted the end of uni-\nprocessors and their dominance by multiprocessors. Until the early years of this\ncentury, this prediction was constantly proven wrong. As we saw in Chapter 3,\nthe costs of trying to find and exploit more ILP became prohibitive in efficiency\n(both in silicon area and in power). Of course, multicore does not magically solve\nthe power problem because it clearly increases both the transistor count and the\nactive number of transistors switching, which are the two dominant contributions\nto power. As we will see in this section, energy issues are likely to limit multicore\nscaling more severely than previously thought.\nILP scaling failed because of both limitations in the ILP available and the effi-\nciency of exploiting that ILP. Similarly, a combination of two factors means that\nsimply scaling performance by adding cores is unlikely to be broadly successful.\nThis combination arises from the challenges posed by Amdahl\u2019s Law, which\nassesses the efficiency of exploiting parallelism, and the end of Dennard\u2019s Scaling,\nwhich dictates the energy required for a multicore processor.\nTo understand these factors, we take a simple model of both technology scaling\n(based on an extensive and highly detailed analysis in Esmaeilzadeh et al. (2012)).\nLet\u2019s start by reviewing energy consumption and power in CMOS. Recall from\nChapter 1 that the energy to switch a transistor is given as\nEnergy / Capacitive load\u0003Voltage2\nCMOS scaling is limited primarily bythermal power, which is a combination of static\nleakage power and dynamic power, which tends to dominate. Power is given by\n442\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 475,
        "text": "Power \u00bc Energyper Transistor \u0003Frequency\u0003Transistors switched\n\u00bc Capacitive load\u0003Voltage2 \u0003Frequency\u0003Transistors switched\nTo understand the implications of how energy and power scale, let\u2019s compare\ntoday\u2019s 22 nm technology with a technology projected to be available in 2021\u2013\n24 (depending on the rate at which Moore\u2019s Law continues to slow down).\nFigure 5.36 shows this comparison based on technology projections and resulting\neffects on energy and power scaling. Notice that power scaling>1.0 means that the\nfuture device consumes more power; in this case, 1.79\u0003 as much.\nConsider the implications of this for one of the latest Intel Xeon processors, the\nE7-8890, which has 24 cores, 7.2 billion transistors (including almost 70 MiB of\ncache), operates at 2.2 GHz, has a thermal power rating of 165 watts, and a die size\nof 456 mm2. The clock frequency is already limited by power dissipation: a 4-core\nversion has a clock of 3.2 GHz, and a 10-core version has a 2.8 GHz clock. With\nthe 11 nm technology, the same size die would accommodate 96 cores with almost\n280 MiB of cache and operate at a clock rate (assuming perfect frequency scaling)\nof 4.9 GHz. Unfortunately, with all cores operating and no efficiency improve-\nments, it would consume 165\u00031.79\u00bc295 watts. If we assume the 165-W heat\ndissipation limit remains, then only 54 cores can be active. This limit yields a max-\nimum performance speedup of 54/24\u00bc2.25 over a 5\u20136 year period, less than one-\nhalf the performance scaling seen in the late 1990s. Furthermore, we may have\nAmdahl\u2019s Law effects, as the next example shows.\nExample\nSuppose we have a 96-core future generation processor, but on average only 54\ncores can be busy. Suppose that 90% of the time, we can use all available cores;\n9% of the time, we can use 50 cores; and 1% of the time is strictly serial. How much\nspeedup might we expect? Assume that cores can be turned off when not in use and\ndraw no power and assume that the use of a different number of cores is distributed\nso that we need to worry only about average power consumption. How would the\nDevice count scaling (since a transistor is 1/4 the size)\n4\nFrequency scaling (based on projections of device speed)\n1.75\nVoltage scaling projected\n0.81\nCapacitance scaling projected\n0.39\nEnergy per switched transistor scaling (CV2)\n0.26\nPower scaling assuming fraction of transistors switching is the same and chip exhibits full frequency\nscaling\n1.79\nFigure 5.36 A comparison of the 22 nm technology of 2016 with a future 11 nm technology, likely to be available\nsometime between 2022 and 2024. The characteristics of the 11 nm technology are based on the International Tech-\nnology Roadmap for Semiconductors, which has been recently discontinued because of uncertainty about the con-\ntinuation of Moore\u2019s Law and what scaling characteristics will be seen.\n5.10\nThe Future of Multicore Scaling\n\u25a0\n443"
    },
    {
        "page": 476,
        "text": "multicore speedup compare to the 24-processor count version that can use all its\nprocessor 99% of the time?\nAnswer\nWe can find how many cores can be used for the 90% of the time when more than\n54 are usable, as follows:\nAverage Processor Usage \u00bc 0:09\u000350 + 0:01\u00031 + 0:90\u0003Max processor\n54 \u00bc 4:51 + 0:90\u0003Max processor\nMax processor \u00bc 55\nNow, we can find the speedup:\nSpeedup \u00bc\n1\nFraction55\n55\n+ Fraction50\n50\n+ 1Fraction55 Fraction50\n\u00f0\n\u00de\nSpeedup \u00bc\n1\n0:90\n55 + 0:09\n50 + 0:01\n\u00bc 35:5\nNow compute the speedup on 24 processors:\nSpeedup \u00bc\n1\nFraction24\n24\n+ 1Fraction24\n\u00f0\n\u00de\nSpeedup \u00bc\n1\n0:99\n24 + 0:01\n\u00bc 19:5\nWhen considering both power constraints and Amdahl\u2019s Law effects, the 96-\nprocessor version achieves less than a factor of 2 speedup over the 24-processor\nversion. In fact, the speedup from clock rate increase nearly matches the speedup\nfrom the 4\u0003 processor count increase. We comment on these issues further in the\nconcluding remarks.\n5.11\nConcluding Remarks\nAs we saw in the previous section, multicore does not magically solve the power\nproblem because it clearly increases both the transistor count and the active number\nof transistors switching, which are the two dominant contributions to power. The\nfailure of Dennard scaling merely makes it more extreme.\nBut multicore does alter the game. By allowing idle cores to be placed in\npower-saving mode, some improvement in power efficiency can be achieved, as\nthe results in this chapter have shown. For example, shutting down cores in the\nIntel i7 allows other cores to operate in Turbo mode. This capability allows a\ntrade-off between higher clock rates with fewer processors and more processors\nwith lower clock rates.\nMore importantly, multicore shifts the burden for keeping the processor busy\nby relying more on TLP, which the application and programmer are responsible for\n444\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 477,
        "text": "identifying, rather than on ILP, for which the hardware is responsible. Multipro-\ngrammed and highly parallel workloads that avoid Amdahl\u2019s Law effects will ben-\nefit more easily.\nAlthough multicore provides some help with the energy efficiency challenge\nand shifts much of the burden to the software system, there remain difficult chal-\nlenges and unresolved questions. For example, attempts to exploit thread-level ver-\nsions of aggressive speculation have so far met the same fate as their ILP\ncounterparts. That is, the performance gains have been modest and are likely less\nthan the increase in energy consumption, so ideas such as speculative threads or\nhardware run-ahead have not been successfully incorporated in processors. As\nin speculation for ILP, unless the speculation is almost always right, the costs\nexceed the benefits.\nThus, at the present, it seems unlikely that some form of simple multicore scal-\ning will provide a cost-effective path to growing performance. A fundamental\nproblem must be overcome: finding and exploiting significant amounts of paral-\nlelism in an energy- and silicon-efficient manner. In the previous chapter, we\nexamined the exploitation of data parallelism via a SIMD approach. In many appli-\ncations, data parallelism occurs in large amounts, and SIMD is a more energy-\nefficient method for exploiting data parallelism. In the next chapter, we explore\nlarge-scale cloud computing. In such environments, massive amounts of parallel-\nism are available from millions of independent tasks generated by individual users.\nAmdahl\u2019s Law plays little role in limiting the scale of such systems because the\ntasks (e.g., millions of Google search requests) are independent. Finally, in\nChapter 7, we explore the rise of domain-specific architectures (DSAs). Most\ndomain-specific architectures exploit the parallelism of the targeted domain, which\nis often data parallelism, and as with GPUs, DSAs can achieve much higher effi-\nciency as measured by energy consumption or silicon utilization.\nIn the last edition, published in 2012, we raised the question of whether it\nwould be worthwhile to consider heterogeneous processors. At that time, no such\nmulticore was delivered or announced, and heterogeneous multiprocessors had\nseen only limited success in special-purpose computers or embedded systems.\nWhile the programming models and software systems remain challenging, it\nappears inevitable that multiprocessors with heterogeneous processors will play\nan important role. Combining domain-specific processors, like those discussed\nin Chapters 4 and 7, with general-purpose processors is perhaps the best road for-\nward to achieve increased performance and energy efficiency while maintaining\nsome of the flexibility that general-purpose processors offer.\n5.12\nHistorical Perspectives and References\nSection M.7 (available online) looks at the history of multiprocessors and parallel\nprocessing. Divided by both time period and architecture, the section features dis-\ncussions on early experimental multiprocessors and some of the great debates in\nparallel processing. Recent advances are also covered. References for further read-\ning are included.\n5.12\nHistorical Perspectives and References\n\u25a0\n445"
    },
    {
        "page": 478,
        "text": "Case Studies and Exercises by Amr Zaky and\nDavid A. Wood\nCase Study 1: Single Chip Multicore Multiprocessor\nConcepts illustrated by this case study\n\u25a0\nSnooping Coherence Protocol Transitions\n\u25a0\nCoherence Protocol Performance\n\u25a0\nCoherence Protocol Optimizations\n\u25a0\nSynchronization\nA multicore SMT multiprocessor is illustrated in Figure 5.37. Only the cache con-\ntents are shown. Each core has a single, private cache with coherence maintained\nusing the snooping coherence protocol of Figure 5.7. Each cache is direct-mapped,\nwith four lines, each holding 2 bytes (to simplify diagram). For further simplifica-\ntion, the whole line addresses in memory are shown in the address fields in the\ncaches, where the tag would normally exist. The coherence states are denoted\nM, S, and I for Modified, Shared, and Invalid.\n5.1.\n[10/10/10/10/10/10/10] <5.2> For each part of this exercise, the initial cache and\nmemory state are assumed to initially have the contents shown in Figure 5.37. Each\npart of this exercise specifies a sequence of one or more CPU operations of\nthe form\nLine \nnumber\nCoherency\nstate\nAddress\nData\n0\nI\nAC00\n0010\n1\nS\nAC08\n0008\n2\nM\nAC10\n0030\n3\nI\nAC18\n0010\nCache\nline\nCoherency\nstate\nAddress\nData\n0\nI\nAC00\n0010\n1\nM\nAC28\n0068\n2\nI\nAC10\n0010\n3\nS\nAC18\n0018\nCache\nline\nCoherency\nstate\nAddress\nData\n0\nS\nAC20\n20\n1\nS\nAC08\n0008\n2\nI\nAC10\n0010\n3\nI\nAC18\n0010\nAddress\nData\n\u2026\n\u2026\nAC00\n0010\nAC08\n0008\nAC10\n0010\nAC18\n0018\nAC20\n0020\nAC28\n0028\nAC30\n0030\n\u2026.\n\u2026..\nCore 0\nCore 1\nCore3\nMemory\nFigure 5.37 Multicore (point-to-point) multiprocessor.\n446\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 479,
        "text": "Ccore#: R, <address> for reads\nand\nCcore#: W, <address> <-- <value written> for writes.\nFor example,\nC3: R, AC10 & C0: W, AC18 <-- 0018\nRead and write operations are for 1 byte at a time. Show the resulting state\n(i.e.,coherencestate,tags,anddata)ofthecachesandmemoryaftertheactionsgiven\nbelow. Show only the cache lines that experience some state change; for example:\nC0.L0: (I, AC20, 0001) indicates that line 0 in core 0 assumes an\n\u201cinvalid\u201d coherence state (I), stores AC20 from the memory, and has data con-\ntents 0001. Furthermore, represent any changes to the memory state as M:\n<address> <\u2013 value.\nDifferent parts (a) through (g) do not depend on one another: assume the\nactions in all parts are applied to the initial cache and memory states.\na. [10] <5.2> C0: R, AC20\nb. [10] <5.2> C0: W, AC20 <-- 80\nc. [10] <5.2> C3: W, AC20 <-- 80\nd. [10] <5.2> C1: R, AC10\ne. [10] <5.2> C0: W, AC08 <-- 48\nf. [10] <5.2> C0: W, AC30 <-- 78\ng. [10] <5.2> C3: W, AC30 <-- 78\n5.2.\n[20/20/20/20] <5.3> The performance of a snooping cache-coherent multiprocessor\ndepends on many detailed implementation issues that determine how quickly a cache\nresponds with data in an exclusive or M state block. In some implementations, a pro-\ncessor read miss to a cache block that is exclusive in another processor\u2019s cache is\nfaster than a miss to a block in memory. This is because caches are smaller, and thus\nfaster, than main memory. Conversely, in some implementations, misses satisfied by\nmemory are faster than those satisfied by caches. This is because caches are generally\noptimized for \u201cfront side\u201d or CPU references, rather than \u201cback side\u201d or snooping\naccesses. For the multiprocessor illustrated in Figure 5.37, consider the execution\nof a sequence of operations on a single processor core where\n\u25a0\nread and write hits generate no stall cycles;\n\u25a0\nread and write misses generate Nmemory and Ncache stall cycles if satisfied by\nmemory and cache, respectively;\n\u25a0\nwrite hits that generate an invalidate incur Ninvalidate stall cycles; and\n\u25a0\na write-back of a block, either due to a conflict or another processor\u2019s request to\nan exclusive block, incurs an additional Nwriteback stall cycles.\nConsider two implementations with different performance characteristics summa-\nrized in Figure 5.38.\nCase Studies and Exercises by Amr Zaky and David A. Wood\n\u25a0\n447"
    },
    {
        "page": 480,
        "text": "To observe how these cycle values are used, we illustrate how the following\nsequence of operations, assuming the initial caches\u2019 states in Figure 5.37, behave\nunder implementation 1.\nC1: R, AC10\nC3: R, AC10\nFor simplicity, assume that the second operation begins after the first com-\npletes, even though they are on different processor cores.\nFor Implementation 1,\n\u25a0\nthe first read generates 50 stall cycles because the read is satisfied by C0\u2019s\ncache: C1 stalls for 40 cycles while it waits for the block, and C0 stalls for 10\ncycles while it writes the block back to memory in response to C1\u2019s request; and\n\u25a0\nthe second read by C3 generates 100 stall cycles because its miss is satisfied by\nmemory.\nTherefore this sequence generates a total of 150 stall cycles.\nFor the following sequences of operations, how many stall cycles are generated\nby each implementation?\na. [20] <5.3> C0: R, AC20\nC0: R, AC28\nC0: R, AC30\nb. [20] <5.3> C0: R, AC00\nC0: W, AC08 <-- 48\nC0: W, AC30 <-- 78\nc. [20] <5.3> C1: R, AC20\nC1: R, AC28\nC1: R, AC30\nd. [20] <5.3> C1: R, AC00\nC1: W, AC08 <-- 48\nC1: W, AC30 <-- 78\n5.3.\n[20]<5.2>Someapplicationsreadalargedatasetfirstandthenmodifymostorallofit.\nThe base MSI coherence protocol will first fetch all of the cache blocks in the Shared\nstate and then be forced to perform an invalidate operation to upgrade them to the\nParameter\nImplementation 1\nCycles\nImplementation 2\nCycles\nNmemory\n100\n100\nNcache\n40\n130\nNinvalidate\n15\n15\nNwriteback\n10\n10\nFigure 5.38 Snooping coherence latencies.\n448\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 481,
        "text": "Modified state. The additional delay has a significant impact on some workloads. The\nMESI addition to the standard protocol (see Section 5.2) provides some relief in these\ncases. Draw new protocol diagrams for a MESI protocol that adds the Exclusive state\nand transitions to the base MSI protocol\u2019s Modified, Shared, and Invalidate states.\n5.4.\n[20/20/20/20/20] <5.2> Assume the cache contents of Figure 5.37 and the timing\nof Implementation 1 in Figure 5.38. What are the total stall cycles for the following\ncode sequences with both the base protocol and the new MESI protocol in Exercise\n5.3? Assume state transitions that require zero interconnect transactions incur no\nadditional stall cycles.\na. [20] <5.2> C0: R, AC00\nC0: W, AC00 <-- 40\nb. [20] <5.2> C0: R, AC20\nC0: W, AC20 <-- 60\nc. [20] <5.2> C0: R, AC00\nC0: R, AC20\nd. [20] <5.2> C0: R, AC00\nC1: W, AC00 <-- 60\ne. [20] <5.2> C0: R, AC00\nC0: W, AC00 <-- 60\nC1: W, AC00 <-- 40\n5.5.\nCode running on a single core and not sharing any variables with other cores can\nsuffer some performance degradation because of the snooping coherence protocol.\nConsider the two following iterative loops are NOT functionally equivalent but they\nseem similar in complexity. One could be led to conclude that they would spend a\ncomparably close number of cycles when executed on the same processor core.\nLoop 1\nLoop 2\nRepeat i: 1 .. n\nRepeat i:1 .. n\nA[i] <-- A[i-1] +B[i];\nA[i] <-- A[i] +B[i];\nAssume that\n\u25a0\nevery cache line can hold exactly one element of A or B;\n\u25a0\narrays A and B do not interfere in the cache; and\n\u25a0\nall the elements of A or B are in the cache before either loop is executed.\nCompare their performance when run on a core whose cache uses the MESI\ncoherence protocol. Use the stall cycles data for Implementation 1 in Figure 5.38.\nAssume that a cache line can hold multiple elements of A and B (A and B go to\nseparate cache lines). How will this affect the relative performances of Loop1\nand Loop2?\nCase Studies and Exercises by Amr Zaky and David A. Wood\n\u25a0\n449"
    },
    {
        "page": 482,
        "text": "Suggest hardware and/or software mechanisms that would improve the perfor-\nmance of Loop1 on a single core.\n5.6.\n[20] <5.2> Many snooping coherence protocols have additional states, state tran-\nsitions, or bus transactions to reduce the overhead of maintaining cache coherency.\nIn Implementation 1 of Exercise 5.2, misses are incurring fewer stall cycles when\nthey are supplied by cache than when they are supplied by memory. The MOESI\nprotocol extension (see Section 5.2) addresses this need.\nDraw new protocol diagrams with the additional state and transitions.\n5.7.\n[20/20/20/20] <5.2> For the following code sequences and the timing parameters\nfor the two implementations in Figure 5.36, compute the total stall cycles for the\nbase MSI protocol and the optimized MESI protocol in Exercise 5.3. Assume state\ntransitions that do not require bus transactions incur no additional stall cycles.\na. [20] <5.2> C1: R, AC10\nC3: R, AC10\nC0: R, AC10\nb. [20] <5.2> C1: R, AC20\nC3: R, AC20\nC0: R, AC20\nc. [20] <5.2> C0: W, AC20 <-- 80\nC3: R, AC20\nC0: R, AC20\nd. [20] <5.2> C0: W, AC08 <--88\nC3: R, AC08\nC0: W, AC08 <-- 98\n5.8.\n[20/20/20/20] <5.5> The spin lock is the simplest synchronization mechanism\npossible on most commercial shared-memory machines. This spin lock relies on\nthe exchange primitive to atomically load the old value and store a new value.\nThe lock routine performs the exchange operation repeatedly until it finds the lock\nunlocked (i.e., the returned value is 0).\naddi x2, x0, #1\nlockit:\nEXCH x2, 0(x1)\nbnez x2, lockit\nThe lock is released simply by storing a 0 into x2.\nAs discussed in Section 5.5, the more optimized spin lock employs cache\ncoherence and uses a load to check the lock, allowing it to spin with a shared var-\niable in the cache.\nlockit:\nld\nx2, 0(x1)\nbnez\nx2, lockit\naddi\nx2, x0, #1\nEXCH\nx2,0(x1)\nbnez\nx2, lockit\n450\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 483,
        "text": "Assume that processor cores C0, C1, and C3 are all trying to acquire a lock at\naddress 0xAC00 (i.e., register R1 holds the value 0xAC00). Assume the cache con-\ntents from Figure 5.37 and the timing parameters from Implementation 1 in\nFigure 5.38. For simplicity, assume the critical sections are 1000 cycles long.\na. [20] <5.5> Using the simple spin lock, determine approximately how many\nmemory stall cycles each processor incurs before acquiring the lock.\nb. [20] <5.5> Using the optimized spin lock, determine approximately how many\nmemory stall cycles each processor incurs before acquiring the lock.\nc. [20] <5.5> Using the simple spin lock, approximately how many memory\naccesses occur?\nd. [20] <5.5> Using the optimized spin lock, approximately how many memory\naccesses occur?\nCase Study 2: Simple Directory-Based Coherence\nConcepts illustrated by this case study\n\u25a0\nDirectory Coherence Protocol Transitions\n\u25a0\nCoherence Protocol Performance\n\u25a0\nCoherence Protocol Optimizations\nConsider the distributed shared-memory system illustrated in Figure 5.39. It con-\nsists of 8 nodes of processor cores organized as three-dimensional hypercube with\npoint-to-point interconnections, as shown in the figure. For simplification, we\nassume the following scaled-down configuration:\n\u25a0\nEvery node has a single processor core with a direct-mapped L1 data cache\nwith its dedicated cache controller.\n\u25a0\nThe L1 data cache has a capacity of two cache lines with a line size of B bytes.\n\u25a0\nThe L1 cache states are denoted M, S, and I for Modified, Shared, and Invalid.\nAn example cache entry in some would like\n1: S, M3, 0xabcd -->\nCache line 1 is in the \u201cShared\u201d state; it contains memory block M3 and the data\nvalue of the block is 0xabcd.\n\u25a0\nThe system memory comprises 8 memory blocks (i.e., one memory block per\nnode) and is distributed among the eight nodes, with every node owning a\nmemory block. Node Ci owns memory block Mi.\n\u25a0\nEach memory block is B-bytes wide and is tracked by a coherency directory\nentry stored with the memory block.\nCase Studies and Exercises by Amr Zaky and David A. Wood\n\u25a0\n451"
    },
    {
        "page": 484,
        "text": "\u25a0\nThe state of each memory directory entry is denoted DM, DS, and DI for Direc-\ntory Modified, Directory Shared, and Directory Invalid. Additionally, the\ndirectory entry lists the block sharers using a bit vector with 1 bit for every\nnode. Here is an example memory block and associated directory entry:\nM3: 0XABCD, DS, 00000011 -->\nMemory block M3 (in node C3) contains the value 0xABCD and is shared by\nnodes 0 and 1 (corresponding to 1s in the bit vector).\nRead/Write Notation\nTo describe read/write transactions, we will use the notation\nCi#: R, <Mi> for reads\nand\nCi#: W, <Mi> <-- <value written> for writes.\nNode 0\n(000)\nNode 2\n(010)\nNode 6\n(110)\nNode 4\n(100)\nNode 1\n(001)\nNode 5\n(101)\nNode 7\n(111)\nNode 3\n(011)\nFigure 5.39 Multicore multiprocessor with DSM.\n452\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 485,
        "text": "For example,\nC3: R, M2 describes the core in node 3 issuing a read transaction from an address\nat memory block M2 (the address may possibly be cached in C3 already).\nC0: W, M3 <-- 0018 describes the core in node 0 issuing a write transaction\n(data is 0X0018) to an address at memory block M3 (the address may possibly\nbe cached in C0 already).\nMessages\nThe directory coherency schemes depend on exchange of command and/or data\nmessages as described by the directory protocol described in Figure 5.20. An\nexample of a command message is a read request. An example of a data message\nis a read response (with data included).\n\u25a0\nMessages originating an ending in the same node do not cross any inter-\nnode links.\n\u25a0\nMessage with distinct source/destination nodes travel through inter-node links.\nThese messages may be destined from one cache controller to another, from a\ncache controller to a directory controller, or from a directory controller to a\ncache controller.\n\u25a0\nMessages traveling from a source node to a distinct destination node are stat-\nically routed.\n\u0005\nThe static routing algorithm selects a shorted path between the source and\ndestination nodes.\n\u0005\nThe short path is determined by considering the binary representations of\nthe source and destination indices (e.g., 001 for node C1 and 100 for node\nC4), then by moving from one node to a neighboring node that was not\nalready crossed by the message.\n\u2013\nFor example, to go from node 6 to node 0 (110 --> 000), the path is\n110--> 100--> 000.\n\u2013\nBecausemorethanoneshortedpathmay exist (110--> 010--> 000is\nanother path for the preceding example), we assume that the path is\nselected by inverting first the least significant bit that is different from\nthe corresponding bit in destination index. For example, to travel from\nnode 1 to node 6 (001--> 110), the path is 001--> 000-->\n010--> 110.\n\u0005\nThe longest possible path traveled by any message has 3 links (equal to the\nnumber of bits in the binary representation of a node index).\n\u25a0\nA node can simultaneously process up to three messages from/to distinct\nneighboring nodes\u2019 links if no two of them are competing for the same link\nresource as clarified by the following examples of messages sent/received\nto/from/through node 000.\nCase Studies and Exercises by Amr Zaky and David A. Wood\n\u25a0\n453"
    },
    {
        "page": 486,
        "text": "Messages: from 001 --> 010; 010 --> 000 (to cache/directory controller);\n100 --> 001. OK (distinct destinations).\nMessage: from 001 --> 010; 000 --> 001 (from cache/directory control-\nler); 100 --> 001.\nNot OK as two messages are destined to node 001\nIn case of destination contention, ties are broken assigning priority to\na. message destined to the node (000 in example) cache or directory\ncontroller; then\nb. messages forwarded from one to another (through 000 in example); then\nc. messages originating from the node (000 in example) cache or directory\ncontroller.\n\u25a0\nAssume the transmission and service delays in the following table.\nMessage type\nCache controller\nDirectory controller\nLink\nNo data\n2 cycles\n5 cycles\n10 cycles\nWith data\n(3 +\nB=4\nd\ne) cycles\n(6 + 10\u0006B) cycles\n(4 + B)\n\u25a0\nIf a message is forwarded through a node, it is first completely received by the\nnode before being sent to the next node on the path.\n\u25a0\nAssume any cache controller; directory controller has unlimited capacity to\nenqueue messages and service them in FCFS order.\n5.9.\n[10/10/10] <5.4> For each part of this exercise, assume that initially all caches\nlines are invalid, and the data in memory Mi is the byte i (0X00 <= i <=\n0x07) repeated as many times as the block size. Assume that successive requests\nare completely serialized. That is, no core will issue a coherency request until the\nprevious request (by same or different core) is completed.\nFor each of the following parts,\n\u25a0\nshow the final state (i.e., coherence state, sharers/owners, tags, and data) of the\ncaches and directory controller (including data values) after the given transac-\ntion sequence has completed; and\n\u25a0\nshow the messages transferred (choose a suitable format for message types).\na. [10] <5.4> C3: R, M4\nC3: R, M2\nC7: W, M4 <--0xaaaa\nC1: W, M4 <--0xbbbb\nb. [10] <5.4> C3: R, M0\nC3: R, M2\nC6: W, M4 <--0xaaaa\nC3: W, M4 <--0xbbbb\n454\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 487,
        "text": "c. [10] <5.4> C0: R, M7\nC3: R, M4\nC6: W, M2 <--0xaaaa\nC2: W, M2 <--0xbbbb\n5.10.\n[10/10/10] <5.4> The directory protocol used in 5.9 (based on Figure 5.20)\nassumes that the directory controller receives requests, sends invalidates, receives\nmodified data, sends modified data to requester if block was dirty, and so on.\nAssume now that the directory controller will delegate some work to the cores.\nFor example, it will notify the exclusive owner of a modified block when some\nother core needs the block and will have the owner send the block to the new\nsharer. Specifically, consider the following optimizations and indicate what their\nbenefits (if any) are. Also, specify how the messages will be modified (in compar-\nison with Figure 5.20 protocol) to support the new change.\nHint: Benefits might be reduction in number of messages, faster response time, and\nso on.\na. [10] <5.4> On a write miss to a shared memory block, the directory controller\nsends the data to the requester and instructs the sharers to send their invalidate\nacknowledgements directly to the requester.\nb. [10] <5.4> On a read miss to a block modified in some other core, the directory\ncontroller instructs the owner of the modified copy to directly forward the data\nto the requester.\nc. [10] <5.4> On a read miss to a block in shared (S) state in some other cores, the\ndirectory controller instructs one of the sharers (say, the one closest to the\nrequester) to directly forward the data to the requester.\n5.11.\n[15/15/15] <5.4> In problem 5.9, it was assumed that all transactions on the sys-\ntem were serially executed, which is both unrealistic and inefficient in a DSM mul-\nticore. We now relax this condition. We will require only that all transactions\noriginating in one core are serialized. However, different cores can independently\nissue their read/write transactions and even compete for the same memory block.\nThe transactions of problem 5.9 are represented next to reflect the new, relaxed\nconstraints. Redo problem 5.9 with the new, relaxed constraints.\na. [15] <5.4>\nC1: W, M4 <--0xbbbb\nC3: R, M4\nC7: R, M2\nC3: W, M4 <--0xaaaa\nb. [15] <5.4>\nC3: R, M0\nC6: W, M4 <--0xaaaa\nC3: R, M2\nC3: W, M4 <--0xbbbb\nc. [15] <5.4>\nC0:R, M7 C2:W, M2 <--0xbbbb C3:R, M4 C6: W, M2 <--0xaaaa\n5.12.\n[10/10] <5.4> Use the routing and delay information described earlier and trace\nhow the following groups of transactions will progress in the system (assume that\nall accesses are misses).\nCase Studies and Exercises by Amr Zaky and David A. Wood\n\u25a0\n455"
    },
    {
        "page": 488,
        "text": "a. C0:R, M7 C2: W, M2 <--0xbbbb C3: R, M4 C6: W, M2 <--0xaaaa\nb. C0: R, M7\nC3: R, M7\nC2: W, M7 <--0xbbbb\n5.13.\n[20] <5.4> What extra complexities may arise if the messages can be adaptively\nrerouted on the links? For example, a coherency message from core M1 directory\ncontroller to C2 (expressed in binary as M001 --> C010) will be routed either through\nthe inter-node path C001--> C000--> C010 or the inter-node path C001--> C011--\n> C010, depending on link availability.\n5.14.\n[20] <5.4> In a read miss, a cache might overwrite a line in the shared (S) state\nwithout notifying the directory that owns the corresponding memory block. Alter-\nnatively, it will notify the directory so that it deletes this cache from the list of\nsharers.\nShow how the following transaction groups (performed one at a time in series)\nwill proceed under both approaches.\nC3:\nR, M4\nC3:\nR, M2\nC2:\nW, M4 <--0xabcd\nCase Study 3: Memory Consistency\nConcepts Illustrated by This Case Study\n\u25a0\nLegitimate Program Behavior Under Sequential Consistency (SC) Models\n\u25a0\nHardware Optimization Allowed for SC Models\n\u25a0\nUsing Synchronization Primitives to Make a Consistency Model Emulate a\nMore Restrictive Model\n5.15.\n[10/10] <5.6> Consider the following code segments running on two processors\nP1 and P2. Assume A and B are initially 0.\nP1:\nWhile (B == 0);\nA=1;\nP2:\nWhile (A==0);\nB = 1;\na. If the processors adhere to sequential consistency (SC) consistency model.\nWhat are the possible values of A and B at the end of the segments? Show\nthe statement interleaving supporting your answer(s).\nb. Repeat (a) if the processors adhere to the total store order (TSO)\nconsistency model.\n5.16.\n[5] <5.6> Consider the following code segments running on two processors P1\nand P2. Assume A, and B, are initially 0. Explain how an optimizing compiler\nmight make it impossible for B to be ever set to 2 in a sequentially consistent\nexecution model.\n456\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 489,
        "text": "P1:\nA=1;\nA=2;\nWhile (B == 0);\nP2:\nB=1;\nWhile (A <> 1);\nB= 2;\n5.17.\n[10] <5.4>. In a processor implementing a SC consistency model, the data cache\nis augmented with a data prefetch unit. Will that alter the SC implementation exe-\ncution results? Why or why not?\n5.18.\n[10/10] <5.6> Assume that the following code segment is executed on a processor\nthat implements partial store order (PSO),\nA=1;\nB=2;\nIf (C== 3)\nD=B;\na. Augment the code with synchronization primitives to make it emulate the\nbehavior of a total store order (TSO) implementation.\nb. Augment the code with synchronization primitives to make it emulate the\nbehavior of a sequential consistency (SC) implementation.\n5.19.\n[20/20/20] <5.6> Sequential consistency (SC) requires that all reads and writes\nappear to have executed in some total order. This may require the processor to stall\nin certain cases before committing a read or write instruction. Consider the code\nsequence\nwrite A\nread B\nwhere the write A results in a cache miss and the read B results in a cache hit.\nUnder SC, the processor must stall read B until after it can order (and thus per-\nform) write A. Simple implementations of SC will stall the processor until the cache\nreceives the data and can perform the write.\nRelease consistency (RC) consistency mode (see Section 5.6) relaxes these\nconstraints: ordering\u2014when desired\u2014is enforced by judicious use of synchroni-\nzation operations. This allows, among other optimizations, processors to imple-\nment write buffers, which hold committed writes that have not yet been ordered\nwith respect to other processors\u2019 writes. Reads can pass (and potentially bypass)\nthe write buffer in RC (which they could not do in SC).\nAssume that one memory operation can be performed per cycle and that oper-\nations that hit in the cache or that can be satisfied by the write buffer introduce no\nstall cycles. Operations that miss incur the latencies listed in Figure 5.38.\nHow many stall cycles occur prior to each operation for both the SC and RC\nconsistency models? (Write buffer can hold at most one write.)\na. [20] <5.6> P0: write 110 <-- 80\n//assume miss (no other\ncache has the line)\nP0: read 108\n//assume miss (no other\ncache has the line)\nCase Studies and Exercises by Amr Zaky and David A. Wood\n\u25a0\n457"
    },
    {
        "page": 490,
        "text": "b. [20] <5.6> P0: read 110\n//assume miss (no other\ncache has the line)\nP0: write 100 <-- 90\n//assume hit\nc. [20] <5.6> P0: write 100 <-- 80\n//assume miss\nP0: write 110 <-- 90\n//assume hit\n5.20.\n[20] <5.6> Repeat part (a) of problem 5.19 under an SC model on a processor that\nhas a read prefetch unit. Assume a read prefetch was triggered 20 cycles in advance\nof the write operation.\nExercises\n5.21.\n[15] <5.1> Assume that we have a function for an application of the form F(i, p),\nwhich gives the fraction of time that exactly i processors are usable given that a\ntotal of p processors are available. This means that\nXp\ni\u00bc1F i, p\n\u00f0\n\u00de \u00bc 1\nAssume that when i processors are in use, the applications run i times faster.\na. Rewrite Amdahl\u2019s Law so that it gives the speedup as a function of p for some\napplication.\nb. An application A runs on single processor for a time T seconds. Different por-\ntions of its running time can improve if a larger number of processors is used.\nFigure 5.40 provides the details.\nHow much speedup will A achieve when on 8 processors?\nc. Repeat for 32 processors and an infinite number of processors.\n5.22.\n[15/20/10] <5.1> In this exercise, we examine the effect of the interconnection\nnetwork topology on the CPI of programs running on a 64-processor\ndistributed-memory multiprocessor. The processor clock rate is 2.0 GHz, and\nthe base CPI of an application with all references hitting in the cache is 0.75.\nAssume that 0.2% of the instructions involve a remote communication reference.\nThe cost of a remote communication reference is (100+10 h) ns, h being the num-\nber of communication network hops that a remote reference has to make to the\nremote processor memory and back. Assume all communication links are\nbidirectional.\na. [15] <5.1> Calculate the worst-case remote communication cost when the 64\nprocessors are arranged as a ring, as an 8\u00038 processor grid, or as a hypercube\n(hint: longest communication path on a 2n hypercube has n links).\nFraction of T\n20%\n20%\n10%\n5%\n15%\n20%\n10%\nProcessors (P)\n1\n2\n4\n6\n8\n16\n128\nFigure 5.40 Percentage of application\u2019s A time that can use up to P processors.\n458\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 491,
        "text": "b. [20] <5.1> Compare the base CPI of the application with no remote commu-\nnication to the CPI achieved with each of the three topologies in part (a).\n5.23.\n[15] <5.2> Show how the basic snooping protocol of Figure 5.6 can be changed\nfor a write-through cache. What is the major hardware functionality that is not\nneeded with a write-through cache compared with a write-back cache?\n5.24.\n[20/20] <5.2> Please answer the following problems:\na. [20] <5.2> Add a clean exclusive state to the basic snooping cache coherence\nprotocol (Figure 5.6). Show the protocol in the finite state machine format used\nin the figure.\nb. [20] <5.2> Add an \u201cowned\u201d state to the protocol of part (a) and describe using\nthe same finite state machine format used in Figure 5.6.\n5.25.\n[15] <5.2> One proposed solution for the problem of false sharing is to add a valid\nbit per word. This would allow the protocol to invalidate a word without removing\nthe entire block, letting a processor keep a portion of a block in its cache while\nanother processor writes a different portion of the block. What extra complications\nare introduced into the basic snooping cache coherence protocol (Figure 5.6) by\nthis addition? Consider all possible protocol actions.\n5.26.\n[15/20] <5.3> This exercise studies the impact of aggressive techniques to exploit\ninstruction-level parallelism in the processor when used in the design of shared-\nmemory multiprocessor systems. Consider two systems identical except for the\nprocessor. System A uses a processor with a simple single-issue, in-order pipeline,\nand system B uses a processor with four-way issue, out-of-order execution and a\nreorder buffer with 64 entries.\na. [15] <5.3> Following the convention of Figure 5.11, let us divide the execution\ntime into instruction execution, cache access, memory access, and other stalls.\nHow would you expect each of these components to differ between system A\nand system B?\nb. [10] <5.3> Based on the discussion of the behavior of OLTP workload in\nSection 5.3, what is the important difference between the OLTP workload\nand other benchmarks that limit benefit from a more aggressive processor\ndesign?\n5.27.\n[15] <5.3> How would you change the code of an application to avoid false shar-\ning? What might be done by a compiler and what might require programmer\ndirectives?\n5.28.\n[15] <5.3> An application is calculating the number of occurrences of a certain\nword in a very large number of documents. A very large number of processors\ndivided the work, searching the different documents. They created a huge\narray\u2014word_count\u2014of 32-bit integers, every element of which is the number\nof times the word occurred in some document. In a second phase, the computation\nis moved to a small SMP server with four processors. Each processor sums up\napproximately \u00bc of the array elements. Later, one processor calculates the\ntotal sum.\nCase Studies and Exercises by Amr Zaky and David A. Wood\n\u25a0\n459"
    },
    {
        "page": 492,
        "text": "for (int p= 0; p<=3; p++) // Each iteration of is executed on a\nseparate processor.\n{\nsum [p] = 0;\nfor (int i= 0; i< n/4; i++) // n is size of word_count and\nis divisible by 4\nsum[p] = sum[p] + word_count[p+4*i];\n}\ntotal_sum = sum[0] +sum[1]+sum[2]+sum[3] //executed only\non processor.\na. Assuming each processor has a 32-byte L1 data cache. Identify the cache line\nsharing (true or false) that the code exhibits.\nb. Rewrite the code to reduce the number of misses to elements of the array\nword_count.\nc. Identify a manual fix you can make to the code to rid it of any false sharing.\n5.29.\n[15] <5.4> Assume a directory-based cache coherence protocol. The directory\ncurrently has information that indicates that processor P1 has the data in \u201cexclu-\nsive\u201d mode. If the directory now gets a request for the same cache block from pro-\ncessor P1, what could this mean? What should the directory controller do? (Such\ncases are called \u201crace conditions\u201d and are the reason why coherence protocols are\nso hard to design and verify.)\n5.30.\n[20] <5.4> A directory controller can send invalidates for lines that have been\nreplaced by the local cache controller. To avoid such messages, and to keep the\ndirectory consistent, replacement hints are used. Such messages tell the controller\nthat a block has been replaced. Modify the directory coherence protocol of\nSection 5.4 to use such replacement hints.\n5.31.\n[20/15/20/15] <5.4> One downside of a straightforward implementation of direc-\ntories using fully populated bit vectors is that the total size of the directory infor-\nmation scales as the product: processor count\u0003memory blocks. If memory grows\nlinearly with processor count, the total size of the directory grows quadratically in\nthe processor count. In practice, because the directory needs only 1 bit per memory\nblock (which is typically 32\u2013128 bytes), this problem is not serious for small-to-\nmoderate processor counts. For example, assuming a 128-byte block, and P pro-\ncessors, the amount of directory storage compared to main memory is P/(128*8)\u00bc\nP/1024, which is 12.5% overhead for P\u00bc128 processors. We can avoid this prob-\nlem by observing that we need to keep only an amount of information that is pro-\nportional to the cache size of each processor. We explore some solutions in these\nexercises.\na. [20] <5.4> One method to obtain a scalable directory protocol is to organize\nthe multiprocessor as a logical hierarchy with the processors as leaves of the\nhierarchy and directories positioned at the root of each subtree. The directory\nat each subtree records which descendants cache which memory blocks. It also\n460\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 493,
        "text": "records the memory blocks\u2014with a home in that subtree\u2014that are cached out-\nside the subtree. Compute the amount of storage needed to record the proces-\nsor information for the directories, assuming that each directory is fully\nassociative. Your answer should incorporate both the number of nodes at each\nlevel of the hierarchy as well as the total number of nodes.\nb. [15] <5.4> Another approach to reducing the directory size is to allow only a\nlimited number of the directory\u2019s memory blocks to be shared at any given time.\nImplement the directory as a four-way set-associative cache storing full bit vec-\ntors. If a directory cache miss occurs, choose a directory entry and invalidate the\nentry. Describe how this organization will work elaborating what will happen as\na is block read, written replaced and written back to memory. Modify the pro-\ntocol in Figure 5.20 to reflect the new transitions required by this directory\norganization.\nc. [20] <5.4> Rather than reducing the number of directory entries, we can imple-\nment bit vectors that are not dense. For example, we can set every directory\nentry to 9 bits. If a block is cached in only one node outside its home, this field\ncontains the node number. If the block is cached in more than one node outside\nits home, this field is a bit vector with each bit indicating a group of eight pro-\ncessors, at least one of which caches the block. Illustrate how this scheme would\nwork for a 64-processor DSM machine that consists of eight 8-processors\ngroups.\nd. [15] An extreme approach to reducing the directory size is to implement an\n\u201cempty\u201d directory; that is, the directory in every processor does not store\nany memory states. It receives requests and forwards them as appropriate.\nWhat is the benefit of having such a directory over having no directory at all\nfor a DSM system?\n5.32.\n[10] <5.5> Implement the classical compare-and-swap instruction using the load\nlinked/store conditional instruction pair.\n5.33.\n[15] <5.5> One performance optimization commonly used is to pad synchroniza-\ntion variables so as not to have any other useful data in the same cache line. Con-\nstruct an example demonstrating that this optimization can be extremely useful in\nsome situations. Assume a snoopy write invalidate protocol.\n5.34.\n[30] <5.5> One possible implementation of the load linked/store conditional pair\nfor multicore processors is to constrain these instructions to using uncached mem-\nory operations. A monitor unit intercepts all reads and writes from any core to the\nmemory. It keeps track of the source of the load linked instructions and whether\nany intervening stores occur between the load linked and its corresponding store\nconditional instruction. The monitor can prevent any failing store conditional from\nwriting any data and can use the interconnect signals to inform the processor that\nthis store failed.\nDesign such a monitor for a memory system supporting a four-core SMP. Take\ninto account that, generally, read and write requests can have different data sizes (4/\n8/16/32 bytes). Any memory location can be the target of a load linked/store\nCase Studies and Exercises by Amr Zaky and David A. Wood\n\u25a0\n461"
    },
    {
        "page": 494,
        "text": "conditional pair, and the memory monitor should assume that load linked/store\nconditional references to any location can, possibly, be interleaved with regular\naccesses to the same location. The monitor complexity should be independent\nof the memory size.\n5.35.\n[25] <5.5> Prove that, in a two-level cache hierarchy where L1 is closer to the\nprocessor, inclusion is maintained with no extra action if L2 has at least as much\nassociativity as L1, both caches use LRU replacement, and both caches have the\nsame block sizes.\n5.36.\n[Discussion] <5> When trying to perform detailed performance evaluation of a\nmultiprocessor system, system designers use one of three tools: analytical models,\ntrace-driven simulation, and execution-driven simulation. Analytical models use\nmathematical expressions to model the behavior of programs. Trace-driven simu-\nlations run the applications on a real machine and generate a trace, typically of\nmemory operations. These traces can be replayed through a cache simulator or\na simulator with a simple processor model to predict the performance of the system\nwhen various parameters are changed. Execution-driven simulators simulate the\nentire execution maintaining an equivalent structure for the processor state and\nso on.\na. What are the accuracy/speed trade-offs between these approaches?\nb. CPU traces, if not carefully collected, can exhibit artifacts of the system they are\ncollected on. Discuss this issue while using branch-prediction and spin-wait\nsynchronization as examples. (Hint: The program itself is not available to a pure\nCPU trace; just the trace is available.)\n5.37.\n[40] <5.7, 5.9> Multiprocessors and clusters usually show performance increases\nas you increase the number of the processors, with the ideal being n times speedup\nfor n processors. The goal of this biased benchmark is to make a program that gets\nworse performance as you add processors. For example, this means that one pro-\ncessor on the multiprocessor or cluster runs the program fastest, two are slower,\nfour are slower than two, and so on. What are the key performance characteristics\nfor each organization that give inverse linear speedup?\n462\n\u25a0\nChapter Five Thread-Level Parallelism"
    },
    {
        "page": 495,
        "text": "This page intentionally left blank"
    },
    {
        "page": 496,
        "text": "6.1\nIntroduction\n466\n6.2\nProgramming Models and Workloads for Warehouse-Scale Computers 471\n6.3\nComputer Architecture of Warehouse-Scale Computers\n477\n6.4\nThe Efficiency and Cost of Warehouse-Scale Computers\n482\n6.5\nCloud Computing: The Return of Utility Computing\n490\n6.6\nCross-Cutting Issues\n501\n6.7\nPutting It All Together: A Google Warehouse-Scale Computer\n503\n6.8\nFallacies and Pitfalls\n514\n6.9\nConcluding Remarks\n518\n6.10\nHistorical Perspectives and References\n519\nCase Studies and Exercises by Parthasarathy Ranganathan\n519"
    },
    {
        "page": 497,
        "text": "6\nWarehouse-Scale Computers\nto Exploit Request-Level and\nData-Level Parallelism\nThe datacenter is the computer.\nLuiz Andr\u0001e Barroso,\nGoogle (2007)\nA hundred years ago, companies stopped generating their own\npower with steam engines and dynamos and plugged into the\nnewly built electric grid. The cheap power pumped out by electric\nutilities didn\u2019t just change how businesses operate. It set off a chain\nreaction of economic and social transformations that brought the\nmodern world into existence. Today, a similar revolution is under\nway. Hooked up to the Internet\u2019s global computing grid, massive\ninformation-processing plants have begun pumping data and\nsoftware code into our homes and businesses. This time, it\u2019s\ncomputing that\u2019s turning into a utility.\nNicholas Carr,\nThe Big Switch: Rewiring the World, from\nEdison to Google (2008)\nComputer Architecture. https://doi.org/10.1016/B978-0-12-811905-1.00006-7\n\u00a9 2019 Elsevier Inc. All rights reserved."
    },
    {
        "page": 498,
        "text": "6.1\nIntroduction\nAnyone can build a fast CPU. The trick is to build a fast system.\nSeymour Cray,\nConsidered the father of the supercomputer\nThe warehouse-scale computer (WSC)1 is the foundation of Internet services\nbillions of people use every day: search, social networking, online maps, video\nsharing, online shopping, email services, and so on. The tremendous popularity\nof such Internet services necessitated the creation of WSCs that could keep up with\nthe rapid demands of the public. Although WSCs may appear to be just large data\ncenters, their architecture and operation are quite different, as we will see. Today\u2019s\nWSCs act as one giant machine that costs hundreds of million dollars for the build-\ning, the electrical and cooling infrastructure, the servers, and the networking equip-\nment that connects and houses 50,000\u2013100,000servers. Moreover, the rapid growth\nofcommercialcloudcomputing(seeSection6.5)makesWSCsaccessibletoanyone\nwith a credit card.\nComputer architecture extends naturally to designing WSCs. For example,\nLuiz Barroso of Google (quoted earlier) did his dissertation research in computer\narchitecture. He believes that an architect\u2019s skills of designing for scale, designing\nfor dependability, and a knack for debugging hardware are very helpful in the cre-\nation and operation of WSCs.\nAt this leading-edge scale, which requires innovation in power distribution,\ncooling, monitoring, and operations, the WSC is the modern descendant of the\nsupercomputer\u2014making Seymour Cray the godfather of today\u2019s WSC architects.\nHis extreme computers handled computations that could be done nowhere else,\nbut were so expensive that only a few companies could afford them. This time\nthe target is providing information technology for the world instead of high-\nperformance computing (HPC) for scientists and engineers; thus WSCs arguably\nplay a more important role for society today than Cray\u2019s supercomputers did in\nthe past.\nUnquestionably, WSCs have many orders of magnitude more users than high-\nperformance computing, and they represent a much greater share of the IT market.\nWhether measured by the number of users or revenue, Google is 1000 times larger\nthan Cray Research ever was.\n1This chapter is based on material from the book The Datacenter as a Computer: An Introduction to the Design of\nWarehouse-Scale Machines, Second Edition, by Luiz Andr\u0001e Barroso, Jimmy Clidaras, and Urs H\u20acolzle of Google\n(2013); the blog Perspectives at mvdirona.com and the talks \u201cCloud-Computing Economies of Scale\u201d and \u201cData Center\nNetworks Are in My Way,\u201d by James Hamilton of Amazon Web Services (2009, 2010); and the paper Above the Clouds:\nA View of Cloud Computing, by Michael Armbrust et al. (2010).\n466\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 499,
        "text": "WSC architects share many goals and requirements with server architects:\n\u25a0\nCost-performance\u2014Work done per dollar is critical in part because of the\nscale. Reducing the costs of a collection of WSCs by few percent could save\nmillions of dollars.\n\u25a0\nEnergy efficiency\u2014Except for the photons that leave WSCs, they are essen-\ntially closed systems, with almost all the energy consumed turned into heat that\nmust be removed. Thus, peak power and consumed power drive both the cost\nof power distribution and the cost of cooling systems. The majority of the\ninfrastructure costs of building a WSC goes toward power and cooling.\nMoreover, energy efficiency is an important part of environmental steward-\nship. Therefore, work done per joule is critical for both WSCs and its servers\nbecause of the high cost of building the power and mechanical infrastructure\nfor a warehouse of computers and for the resulting monthly utility bills.\n\u25a0\nDependability via redundancy\u2014The long-running nature of Internet services\nmeans that the hardware and software in a WSC must collectively provide at\nleast 99.99% (called \u201cfour nines\u201d) of availability; that is, services must be down\nless than 1 h per year. Redundancy is the key to dependability for both WSCs\nand servers. Although server architects often utilize more hardware at higher\ncosts to reach high availability, WSC architects rely instead on numerous cost-\neffective servers connected by a network and redundancy managed by soft-\nware. In addition to local redundancy inside a WSC, an organization needs\nredundant WSCs to mask events that can take out whole WSCs. Indeed,\nalthough every cloud service needs to be available at least 99.99% of the\ntime, the dependability of a full Internet company like Amazon, Google, or\nMicrosoft needs to be even higher. If one of these companies was completely\noffline for 1 h per year\u2014that is, 99.99% availability\u2014that would be front page\nnews. Multiple WSCs have the added benefit of reducing latency for services\nthat are widely deployed (Figures 6.18\u20136.20).\n\u25a0\nNetwork I/O\u2014Server architects must provide a good network interface to the\nexternal world, and WSC architects must also. Networking is needed to keep\ndata consistent between multiple WSCs as well as to interface with the public.\n\u25a0\nBoth interactive and batch processing workloads\u2014Although one expects\nhighly interactive workloads for services like search and social networking\nwith billions of users, WSCs, like servers, also run massively parallel batch\nprograms to calculate metadata useful to such services. For example, MapRe-\nduce jobs are run to convert the pages returned from crawling the web into\nsearch indices (see Section 6.2).\nNot surprisingly, there are also characteristics not shared with server architecture:\n\u25a0\nAmpleparallelism\u2014Aconcernforaserverarchitectiswhethertheapplicationsin\nthe targeted marketplace have enough concurrency to justify the amount of par-\nallel hardware and whether the cost is too high for sufficient communication\n6.1\nIntroduction\n\u25a0\n467"
    },
    {
        "page": 500,
        "text": "hardware to exploit this parallelism. A WSC architect has no such concern. First,\nbatch applications benefit from the large number of distinct datasets that require\nindependentprocessing,suchasbillionsofwebpagesfromawebcrawl.Thispro-\ncessing is data-level parallelism, which we saw in Chapter 4, this time applied to\ndata in storage instead of data in memory. Second, interactive Internet service\napplications, also known as software as a service (SaaS), can benefit from mil-\nlions of independent users of interactive Internet services. Reads and writes are\nseldom dependent in SaaS, so SaaS rarely needs to synchronize. For example,\nsearch uses a read-only index and email normally reads and writes independent\ninformation. We call this type of easy parallelism request-level parallelism, as\nmany independent efforts can proceed in parallel naturally with little need for\ncommunication or synchronization; an example is that journal-based updating\ncan reduce throughput demands. Even read-/write-dependent features are some-\ntimes dropped to offer storage that can scale to the size of modern WSCs. In any\ncase,WSCapplicationshavenochoicebuttofindalgorithmsthatcanscaleacross\nhundredstothousandsofservers,asthatiswhatcustomersexpectandthatiswhat\nthe WSC technology provides.\n\u25a0\nOperational costscount\u2014Serverarchitectstypicallyignoreoperational costsofa\nserver, assuming that they pale in comparison to purchase costs. WSCs have lon-\nger lifetimes\u2014the building and electrical and cooling infrastructure are often\namortized 10\u201315 years\u2014so the operational costs add up: energy, power distribu-\ntion, and cooling represent more than 30% of the costs of a WSC over 10 years.\n\u25a0\nLocation counts\u2014To build a WSC, the first step is building a warehouse. One\nquestion is where? Real estate agents emphasize location, but the location for a\nWSC means access to water, inexpensive electricity, proximity to Internet back-\nbone optical fibers, people nearby to work in the WSC, and low risk from envi-\nronmental disasters, such as earthquakes, floods, and hurricanes. A more obvious\nconcern is just the cost of the land, including enough space to grow the WSC. For\ncompanies with many WSCs, another concern is finding a place geographically\nnear a current or future population of Internet users, to reduce latency over the\nInternet. Other factors include taxes, property costs, social issues (people some-\ntimes want a facility in their country), political issues (some jurisdictions require\nlocal hosting),costofnetworking,reliability ofnetworking,costofpower,source\nof power (e.g., hydroelectric versus coal), weather (cooler is cheaper, as\nSection 6.4 shows), and overall Internet connectivity (Australia is close to Singa-\npore geographically, but the network link bandwidth between them is not great).\n\u25a0\nComputing efficiently at low utilization\u2014Server architects usually design their\nsystems for peak performance within a cost budget and worry about power only\nto make sure they don\u2019t exceed the cooling capacity of their enclosure. As we will\nsee (Figure 6.3), WSC servers are rarely fully utilized, in part to ensure low\nresponse time and in part to offer the redundancy needed to deliver dependable\ncomputing.Given that operational costs count, suchservers needtocompute effi-\nciently at all utilization levels.\n\u25a0\nScale and the opportunities/problems associated with scale\u2014Often extreme\ncomputers are extremely expensive because they require custom hardware,\n468\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 501,
        "text": "and yet the cost of customization cannot be effectively amortized since few\nextreme computers are made. However, when purchasing thousands of\nservers at a time, there are great volume discounts. WSCs are so massive\ninternally that there is economy of scale even if there are not many WSCs.\nAs we will see in Sections 6.5 and 6.10, these economies of scale led to com-\nmercial cloud computing because the lower per-unit costs of a WSC meant\nthat companies could rent servers at a profit below what it costs outsiders\nto do so themselves. The flip side of 100,000 servers is failures.\nFigure 6.1 shows outages and anomalies for 2400 servers. Even if a server\nhad a mean time to failure (MTTF) of an amazing 25 years (200,000 h),\nthe WSC architect would need to design for five server failures a day.\nFigure 6.1 lists the annualized disk failure rate as 2%\u201310%. Given two disks\nper server and an annual failure rate of 4%, with 100,000 servers the WSC\narchitect should expect to see one disk fail per hour. However, software\nfailures vastly outnumber hardware failures, as Figure 6.1 shows, so the\nsystem design must be resilient to server crashes caused by software bugs,\nwhich would happen even more frequently than disk failures. With the thou-\nsands of servers in these very large facilities, WSC operators become very\ngood at changing disks, so the cost of disk failure is much lower for a\nWSC than a small data center. The same applies to DRAMs. Plausibly, WSCs\ncould use even less reliable components if cheaper ones were available.\nApprox. number\nevents in 1st year\nCause\nConsequence\n1 or 2\nPower utility\nfailures\nLose power to whole WSC; doesn\u2019t bring down WSC if UPS and\ngenerators work (generators work about 99% of time).\n4\nCluster\nupgrades\nPlanned outage to upgrade infrastructure, many times for evolving\nnetworking needs such as recabling, to switch firmware upgrades, and so\non. There are about nine planned cluster outages for every unplanned\noutage.\n1000s\nHard-drive\nfailures\n2%\u201310% annual disk failure rate (Pinheiro et al., 2007)\nSlow disks\nStill operate, but run 10\u0001 to 20\u0001 more slowly\nBad memories\nOne uncorrectable DRAM error per year (Schroeder et al., 2009)\nMisconfigured\nmachines\nConfiguration led to \u000330% of service disruptions (Barroso and H\u20acOlzle,\n2009)\nFlaky machines\n1% of servers reboot more than once a week (Barroso and H\u20acOlzle, 2009)\n5000\nIndividual\nserver crashes\nMachine reboot; typically takes about 5 min (caused by problems in\nsoftware or hardware).\nFigure 6.1 List of outages and anomalies with the approximate frequencies of occurrences in the first year\nof a new cluster of 2400 servers. We label what Google calls a cluster an array; see Figure 6.5. Based on Barroso,\nL.A., 2010. Warehouse Scale Computing [keynote address]. In: Proceedings of ACM SIGMOD, June 8\u201310, 2010,\nIndianapolis, IN.\n6.1\nIntroduction\n\u25a0\n469"
    },
    {
        "page": 502,
        "text": "Example\nCalculate the availability of a service running on the 2400 servers in Figure 6.1.\nUnlike a service in a real WSC, in this example the service cannot tolerate\nhardware or software failures. Assume that the time to reboot software is 5 min\nand the time to repair hardware is 1 h.\nAnswer\nWe can estimate service availability by calculating the time of outages because\nof failures of each component. We\u2019ll conservatively take the lowest number in each\ncategory in Figure 6.1 and split the 1000 outages evenly between four components.\nWe ignore slow disks\u2014the fifth component of the 1000 outages\u2014because\nthey hurt performance but not availability, and power utility failures, because\nthe uninterruptible power supply (UPS) system hides 99% of them.\nHours Outageservice \u00bc 4 + 250 + 250 + 250\n\u00f0\n\u00de\u00011 h + 250 + 5000\n\u00f0\n\u00de\u00015 min\n\u00bc 754 + 438 \u00bc 1192h\nSince there are 365\u000124 or 8760 h in a year, availability is\nAvailabilitysystem \u00bc\n8760\u00041192\n8760\n\u0001\n\u0003\n\u00bc 7568\n8760 \u00bc 86%\nWithout software redundancy to mask the many outages, a service on those 2400\nservers would be down on average one day a week\u2014zero \u201cnines\u201d\u2014which is far\nbelow the 99.99% of availability is the goal of WSCs.\nAs Section 6.10 explains, the forerunners of WSCs are computer clusters.\nClusters are collections of independent computers that are connected together using\nlocal area networks (LANs) and switches. For workloads that did not require\nintensive communication, clusters offered much more cost-effective computing\nthan shared-memory multiprocessors. (Shared-memory multiprocessors were the\nforerunners of the multicore computers discussed in Chapter 5.) Clusters became\npopular in the late 1990s for scientific computing and then later for Internet services.\nOne view of WSCs is that they are just the logical evolution from clusters of\nhundreds of servers to tens of thousands of servers.\nA natural question is whether WSCs are similar to modern clusters for high-\nperformance computing. Although some have similar scale and cost\u2014there\nare HPC designs with a million processors that cost hundreds of millions of\ndollars\u2014they historically have had more powerful processors and much lower-\nlatency networks between the nodes than are found in WSCs because the HPC\napplications are more interdependent and communicate more frequently (see\nSection 6.3). The programming environment also emphasizes thread-level paral-\nlelism or data-level parallelism (see Chapters 4 and 5), typically emphasizing\nlatency to complete a single task in contrast to bandwidth to complete many inde-\npendent tasks via request-level parallelism. The HPC clusters also tend to have\nlong-running jobs that keep the servers fully utilized, even for weeks at a time,\n470\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 503,
        "text": "whereas the utilization of servers in WSCs ranges between 10% and 50% (see\nFigure 6.3 on page 441) and varies every day. Unlike supercomputer environ-\nments, thousands of developers work on the WSC code base and deploy significant\nsoftware releases every week (Barroso et al., 2017).\nHow do WSCs compare to conventional data centers? The operators of a\ntraditional data center generally collect machines and third-party software from\nmany parts of an organization and run them centrally for others. Their main focus\ntends to be consolidation of the many services onto fewer machines, which are\nisolated from each other to protect sensitive information. Thus, virtual machines\nare increasingly important in data centers. Virtual machines are important for\nWSCs as well, but they play a different role. They are used to offer isolation\nbetween different customers and to slice hardware resources into different-sized\nshares to rent at several price points (see Section 6.5). Unlike WSCs, conventional\ndata centers tend to have a great deal of hardware and software heterogeneity to\nserve their varied customers inside an organization. WSC programmers customize\nthird-party software or build their own, and WSCs have much more homogeneous\nhardware; the WSC goal is to make the hardware/software in the warehouse act like\na single computer that typically runs a variety of applications. Often the biggest\ncost in a conventional data center is the people to maintain it, whereas, as we will\nsee in Section 6.4, in a well-designed WSC, the server hardware is the greatest cost,\nand people costs shift from the topmost to the bottommost. Conventional data\ncenters also don\u2019t have the scale of a WSC, so they don\u2019t get the economic benefits\nof the scale previously mentioned.\nThus, although a WSC might be considered as an extreme data center in that\ncomputers are housed separately in a space with special electrical and cooling\ninfrastructure, traditional data centers share little with the challenges and opportu-\nnities of a WSC, either architecturally or operationally.\nWe start the introduction to WSCs with their workload and a programming\nmodel.\n6.2\nProgramming Models and Workloads for\nWarehouse-Scale Computers\nIf a problem has no solution, it may not be a problem, but a fact\u2014not to be\nsolved, but to be coped with over time.\nShimon Peres\nIn addition to the public-facing Internet services such as search, video sharing,\nand social networking that make them famous, WSCs also run batch applications,\nsuch as converting videos into new formats or creating search indexes from web\ncrawls.\nA popular framework for batch processing in a WSC is MapReduce (Dean and\nGhemawat, 2008) and its open-source twin Hadoop. Figure 6.2 shows the increas-\ning popularity of MapReduce at Google over time. Inspired by the Lisp functions\n6.2\nProgramming Models and Workloads for Warehouse-Scale Computers\n\u25a0\n471"
    },
    {
        "page": 504,
        "text": "of the same name, Map first applies a programmer-supplied function to each\nlogical input record. Map runs on hundreds of computers to produce an interme-\ndiate result of key-value pairs. Reduce collects the output of those distributed tasks\nand collapses them using another programmer-defined function. Assuming the\nReduce function is commutative and associative, it can run in log N time. With\nappropriate software support, both functions are fast yet easy to understand\nand use. Within 30 min, a novice programmer can run a MapReduce task on thou-\nsands of computers.\nFigure 6.2 shows the average job uses hundreds of servers. Other than a few\nhighly tuned applications from high-performance computing, such MapReduce\njobs are the most parallel applications today, whether measured in total CPU time\nor number of servers utilized.\nHere is a MapReduce program that calculates the number of occurrences of\nevery English word in a large collection of documents. Following is a simplified\nversion of that program, which shows just the inner loop and that assumes\nonly one occurrence of all English words found in a document (Dean and\nGhemawat, 2008):\nMonth\nNumber of\nMapReduce\nJobs\nAverage\ncompletion\ntime (s)\nAverage no.\nservers per\njob\nAvg. no.\ncores per\nserver\nCPU\ncore\nyears\nInput\ndata\n(PB)\nIntermediate\ndata (PB)\nOutput\ndata\n(PB)\nSep-16\n95,775,891\n331\n130\n2.4\n311,691\n11,553\n4095\n6982\nSep-15\n115,375,750\n231\n120\n2.7\n272,322\n8307\n3980\n5801\nSep-14\n55,913,646\n412\n142\n1.9\n200,778\n5989\n2530\n3951\nSep-13\n28,328,775\n469\n137\n1.4\n81,992\n2579\n1193\n1684\nSep-12\n15,662,118\n480\n142\n1.8\n60,987\n2171\n818\n874\nSep-11\n7,961,481\n499\n147\n2.2\n40,993\n1162\n276\n333\nSep-10\n5,207,069\n714\n164\n1.6\n30,262\n573\n139\n37\nSep-09\n4,114,919\n515\n156\n3.2\n33,582\n548\n118\n99\nSep-07\n2,217,000\n395\n394\n1.0\n11,081\n394\n34\n14\nMar-06\n171,000\n874\n268\n1.6\n2002\n51\n7\n3\nAug-04\n29,000\n634\n157\n1.9\n217\n3.2\n0.7\n0.2\nFigure 6.2 Monthly MapReduce usage at Google from 2004 to 2016. Over 12 years the number of MapReduce jobs\nincreased by a factor of 3300. Figure 6.17 on page 461 estimates that running the September 2016 workload on Ama-\nzon\u2019s cloud computing service EC2 would cost $114 million. Updated from Dean, J., 2009. Designs, lessons and\nadvice from building large distributed systems [keynote address]. In: Proceedings of 3rd ACM SIGOPS International\nWorkshop on Large-Scale Distributed Systems and Middleware, Co-located with the 22nd ACM Symposium on Oper-\nating Systems Principles, October 11\u201314, 2009, Big Sky, Mont.\n472\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 505,
        "text": "map(String key, String value):\n// key: document name\n// value: document contents\nfor each word w in value:\nEmitIntermediate(w, \u201c1\u201d); // Produce list of\nall words\nreduce(String key, Iterator values):\n// key: a word\n// values: a list of counts\nint result = 0;\nfor each v in values:\nresult += ParseInt(v); // get integer from key-\nvalue pair\nEmit(AsString(result));\nThe function EmitIntermediate used in the Map function emits each word in\nthe document and the value one. Then the Reduce function sums all the values per\nword for each document using ParseInt() to get the number of occurrences per\nword in all documents. The MapReduce runtime environment schedules map tasks\nand reduce tasks to the nodes of a WSC. (The complete version of the program is\nfound in Dean and Ghemawat (2008).)\nMapReduce can be thought of as a generalization of the single instruction\nstream, multiple data streams (SIMD) operation (Chapter 4)\u2014except that a func-\ntion to be applied is passed to the data\u2014that is followed by a function that is used in\na reduction of the output from the Map task. Because reductions are commonplace\neven in SIMD programs, SIMD hardware often offers special operations for the\nreductions. For example, Intel\u2019s AVX SIMD instructions include \u201chorizontal\u201d\ninstructions that add pairs of operands that are adjacent in registers.\nTo accommodate variability in performance from hundreds of computers, the\nMapReduce scheduler assigns new tasks based on how quickly nodes complete\nprior tasks. Obviously, a single slow task can hold up completion of a large\nMapReduce job. Dean and Barroso (2013) label such a situation tail latency. In\na WSC, the solution to slow tasks is to provide software mechanisms to cope with\nsuch variability that is inherent at this scale. This approach is in sharp contrast to\nthe solution for a server in a conventional data center, where traditionally slow\ntasks mean hardware is broken and needs to be replaced or that server software\nneeds tuning and rewriting. Performance heterogeneity is the norm for 50,000\u2013\n100,000 servers in a WSC. For example, toward the end of a MapReduce program,\nthe system will start backup executions on other nodes of the tasks that haven\u2019t\ncompleted yet and take the result from whichever finishes first. In return for\nincreasing resource usage a few percentage points, Dean and Ghemawat (2008)\nfound that some large tasks completed 30% faster.\nDependability was built into MapReduce from the start. For example, each\nnode in a MapReduce job is required to report back to the master node periodically\n6.2\nProgramming Models and Workloads for Warehouse-Scale Computers\n\u25a0\n473"
    },
    {
        "page": 506,
        "text": "with a list of completed tasks and with updated status. If a node does not report\nback by the deadline, the master node deems the node dead and reassigns the\nnode\u2019s work to other nodes. Given the amount of equipment in a WSC, it\u2019s not\nsurprising that failures are commonplace, as the prior example attests. To deliver\non 99.99% availability, systems software must cope with this reality in a WSC. To\nreduce operational costs, all WSCs use automated monitoring software allowing\none operator to be responsible for more than 1000 servers.\nProgramming frameworks such as MapReduce for batch processing and exter-\nnally facing SaaS such as Search rely upon internal software services for their\nsuccess. For example, MapReduce relies on the Google File System (GFS)\n(Ghemawat et al., 2003) or on Colossus (Fikes, 2010) to supply files to any com-\nputer, so that MapReduce tasks can be scheduled anywhere.\nIn addition to GFS and Colossus, examples of these scalable storage systems\ninclude Amazon\u2019s key value storage system Dynamo (DeCandia et al., 2007) and\nthe Google record storage system BigTable (Chang et al., 2006). Note that such\nsystems often build upon each other. For example, BigTable stores its logs and\ndata on GFS or Colossus, much as a relational database may use the file system\nprovided by the kernel operating system.\nThese internal services usually make different decisions than similar software\nrunning on single servers. For example, rather than assuming storage is reliable, such\nas by using RAID storage servers, these systems often make complete replicas of the\ndata. Replicas can help with read performance as well as with availability; with\nproper placement, replicas can overcome many other system failures, like those\nin Figure 6.1. Systems like Colossus use error-correcting codes rather than full rep-\nlicas to reduce storage costs, but the constant is cross-server redundancy rather than\nwithin-a-server or within-a-storage array redundancy. Thus, failure of the entire\nserver or storage device doesn\u2019t negatively affect availability of the data.\nAnother example of the different approach is that WSC storage software often\nuses relaxed consistencyrather thanfollowingallthe ACID(atomicity, consistency,\nisolation, and durability) requirements of conventional database systems. The\ninsight is that it\u2019s important for multiple replicas of data to agree some time, but,\nfor most applications, they do not need to be in agreement at all times. For example,\neventual consistency is fine for video sharing. Eventual consistency makes storage\nsystems much easier to scale, which is an absolute requirement for WSCs.\nThe workload demands of these public interactive services all vary consider-\nably; even a prominent global service such as Google Search varies by a factor of\ntwo depending on the time of day. When factoring in weekends, holidays, and\npopular times of year for some applications\u2014such as photograph-sharing services\nafter New Year\u2019s Day or online shopping before Christmas\u2014a much greater\nvariation in server utilization becomes apparent. Figure 6.3 shows average\nutilization of 5000 Google servers over a 6-month period. Note that less than\n0.5% of servers averaged 100% utilization, and most servers operated between\n10% and 50% utilization. Stated alternatively, just 10% of all servers were utilized\nmore than 50%. Thus, it\u2019s much more important for servers in a WSC to perform\n474\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 507,
        "text": "well while doing little than to perform efficiently only at their peak, as they rarely\noperate at their peak.\nIn summary, WSC hardware and software must cope with variability in\nload based on user demand and in performance and dependability because of\nthe vagaries of hardware at this scale.\nExample\nAs a result of measurements like those in Figure 6.3, the SPECpower benchmark\nmeasures power and performance from 0% load to 100% in 10% increments (see\nChapter 1). The overall single metric that summarizes this benchmark is the sum of\nall the performance measures (server-side Java operations per second) divided by\nthe sum of all power measurements in watts. Thus, each level is assumed to be\nequally likely. How would the numbers summary metric change if the levels were\nweighted by the utilization frequencies in Figure 6.3?\n0\n0\n0.005\n0.01\n0.015\n0.02\n0.025\n0.03\n0.1\n0.2\n0.3\n0.4\n0.5\nCPU utilization\nFraction of time\n0.6\n0.7\n0.8\n0.9\n1.0\nFigure 6.3 Average CPU utilization of more than 5000 servers during a 6-month\nperiod at Google. Servers are rarely completely idle or fully utilized, instead operat-\ning most of the time at between 10% and 50% of their maximum utilization. The third\ncolumn from the right in Figure 6.4 calculates percentages plus or minus 5% to come up\nwith the weightings; thus 1.2% for the 90% row means that 1.2% of servers were\nbetween 85% and 95% utilized. From Figure 1 in Barroso, L.A., H\u20acolzle, U., 2007. The case\nfor energy-proportional computing. IEEE Comput. 40 (12), 33\u201337.\n6.2\nProgramming Models and Workloads for Warehouse-Scale Computers\n\u25a0\n475"
    },
    {
        "page": 508,
        "text": "Answer\nFigure 6.4 shows the original weightings and the new weighting that match\nFigure 6.3. These weightings reduce the performance summary by 30% from\n3210 ssj_ops/watt to 2454.\nGiven the scale, software must handle failures, which means there is little rea-\nson to buy \u201cgold-plated\u201d hardware that reduces the frequency of failures. The pri-\nmary impact would be to increase cost. Barroso and H\u20acolzle (2009) found a factor of\n20 difference in price-performance between a high-end Hewlett Packard shared-\nmemory multiprocessor and a commodity Hewlett Packard server when running\nthe TPC-C database benchmark. Not surprisingly, Google and all other companies\nwith WSCs use low-end commodity servers. In fact, the Open Compute Project\n(http://opencompute.org) is an organization where such companies collaborate\non open designs of servers and racks for data centers.\nSuch WSC services also tend to develop their own software rather than buy\nthird-party commercial software, in part to cope with the huge scale and in part\nto save money. For example, even on the best price-performance platform for\nTPC-C in 2017, adding the cost of the SAP SQL Anywhere database and the\nWindows operating system increases the cost of the Dell PowerEdge T620 server\nby 40%. In contrast, Google runs BigTable and the Linux operating system on its\nservers, for which it pays no licensing fees.\nGiven this review of the applications and systems software of a WSC, we are\nready to look at the computer architecture of a WSC.\nLoad\nPerformance Watts\nSPEC\nweightings\nWeighted\nperformance\nWeighted\nwatts\nFigure 6.3\nweightings\nWeighted\nperformance\nWeighted\nwatts\n100%\n2,889,020\n662\n9.09%\n262,638\n60\n0.80%\n22,206\n5\n90%\n2,611,130\n617\n9.09%\n237,375\n56\n1.20%\n31,756\n8\n80%\n2,319,900\n576\n9.09%\n210,900\n52\n1.50%\n35,889\n9\n70%\n2,031,260\n533\n9.09%\n184,660\n48\n2.10%\n42,491\n11\n60%\n1,740,980\n490\n9.09%\n158,271\n45\n5.10%\n88,082\n25\n50%\n1,448,810\n451\n9.09%\n131,710\n41\n11.50%\n166,335\n52\n40%\n1,159,760\n416\n9.09%\n105,433\n38\n19.10%\n221,165\n79\n30%\n869,077\n382\n9.09%\n79,007\n35\n24.60%\n213,929\n94\n20%\n581,126\n351\n9.09%\n52,830\n32\n15.30%\n88,769\n54\n10%\n290,762\n308\n9.09%\n26,433\n28\n8.00%\n23,198\n25\n0%\n0\n181\n9.09%\n0\n16\n10.90%\n0\n20\nTotal\n15,941,825\n4967\n1,449,257\n452\n933,820\n380\nssj_ops/W\n3210\nssj_ops/W\n2454\nFigure 6.4 SPECpower result using the weightings from Figure 6.3 instead of even weightings.\n476\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 509,
        "text": "6.3\nComputer Architecture of Warehouse-Scale Computers\nNetworks are the connective tissue that binds 50,000\u2013100,000 servers together.\nAnalogous to the memory hierarchy of Chapter 2, WSCs use a hierarchy of\nnetworks. Figure 6.5 shows one example. Ideally, the combined network would\nprovide nearly the performance of a custom high-end switch for 100,000 servers\nat about the cost per port of a commodity switch designed for 50 servers. As we will\nsee in Section 6.6, networks for WSCs are an area of active innovation.\nThe structure that holds the servers is a rack. Although the width of racks varies\nper WSC\u2014some are the classic 19-in. wide rack; others are two or three times\nwider\u2014the height tends to be no higher than 6\u20137 ft since people must service them.\nSuch a rack has roughly 40\u201380 servers. Because it is often convenient to connect\nthe network cables at the top of the rack, this switch is commonly called a Top\nof Rack (ToR) switch. (Some WSCs have racks with multiple ToR switches.)\nTypically, the bandwidth within the rack is much higher than between racks,\nRack\nswitch\nRack\nArray\nswitch\nFigure 6.5 Hierarchy of switches in a WSC. Based on Figure 1.1 in Barroso, L.A.,\nClidaras, J., H\u20acolzle, U., 2013. The datacenter as a computer: an introduction to the design\nof warehouse-scale machines. Synth. Lect. Comput. Architect. 8 (3), 1\u2013154.\n6.3\nComputer Architecture of Warehouse-Scale Computers\n\u25a0\n477"
    },
    {
        "page": 510,
        "text": "so it matters less where the software places the sender and the receiver if they are\nwithin the same rack. This flexibility is ideal from a software perspective.\nThese switches often offer 4\u201316 uplinks, which leave the rack to go to the\nnext higher switch in the network hierarchy. Thus, the bandwidth leaving the rack\nis 6\u201324 times smaller than the bandwidth within the rack. This ratio is called over-\nsubscription. However, large oversubscription means programmers must be aware\nof the performance consequences when placing senders and receivers in different\nracks. This increased software-scheduling burden is another argument for network\nswitches designed specifically for the data center.\nThe switch that connects an array of racks is considerably more expensive than\nthe ToR switch. This cost is due in part because of the higher connectivity and in\npart because the bandwidth through the switch must be much greater to reduce the\noversubscription problem. Barroso et al. (2013) reported that a switch having 10\ntimes the bisection bandwidth\u2014basically, the worst-case internal bandwidth\u2014of a\nrack switch costs about 100 times as much. One reason is that the cost of switch\nbandwidth for n ports can grow as n2. Sections 6.6 and 6.7 describe the networking\nabove the ToR switch in great detail.\nStorage\nA natural design is to fill a rack with servers, minus whatever space needed for the\nswitches.Thisdesignleaves openthequestionofwherethestorageisplaced.Froma\nhardware construction perspective, the simplest solution would be to include disks\ninside the rack and rely on Ethernet connectivity for access to information on the\ndisks of remote servers. An expensive alternative would be to use network-attached\nstorage (NAS), perhaps over a storage network like InfiniBand. In the past, WSCs\ngenerally relied on local disks and provided storage software that handled connec-\ntivity and dependability. For example, GFS used local disks and maintained replicas\nto overcome dependability problems. This redundancy covered not only local disk\nfailures but also power failures to racks and to whole clusters. The flexibility of\nGFS\u2019s eventual consistency lowers the cost of keeping replicas consistent, which\nalso reduces the network bandwidth requirements of the storage system.\nToday the storage options are considerably more varied. Although some racks\nare balanced in terms of servers and disks, as in the past, there may also be racks\ndeployed without local disks and some racks loaded with disks. System software\ntoday often uses RAID-like error correction codes to lower the storage cost of\ndependability.\nBe aware that there is confusion about the term cluster when talking about the\narchitecture of a WSC. Using the definition in Section 6.1, a WSC is just an\nextremely large cluster. In contrast, Barroso et al. (2013) used the term cluster\nto mean the next-sized grouping of computers, containing many racks. In this chap-\nter, to avoid confusion, we will use the term array to mean a large collection\nof racks organized in rows, preserving the original definition of the word cluster\nto represent anything from a collection of networked computers within a rack to an\nentire warehouse full of networked computers.\n478\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 511,
        "text": "WSC Memory Hierarchy\nFigure 6.6 shows the latency, bandwidth, and capacity of memory hierarchy inside\na WSC, and Figure 6.7 shows the same data visually. These figures are based on\nthe following assumptions (Barroso et al., 2013):\nLocal\nRack\nArray\nDRAM latency (\u03bcs)\n0.1\n300\n500\nFlash latency (\u03bcs)\n100\n400\n600\nDisk latency (\u03bcs)\n10,000\n11,000\n12,000\nDRAM bandwidth (MB/s)\n20,000\n100\n10\nFlash bandwidth (MB/s)\n1000\n100\n10\nDisk bandwidth (MB/s)\n200\n100\n10\nDRAM capacity (GB)\n16\n1024\n31,200\nFlash capacity (GB)\n128\n20,000\n600,000\nDisk capacity (GB)\n2000\n160,000\n4,800,000\nFigure 6.6 Latency, bandwidth, and capacity of the memory hierarchy of a WSC\n(Barroso et al., 2013). Figure 6.7 plots this same information.\n0.1\nLocal DRAM\nLocal Disk\nRack DRAM\nRack Disk\nDatacenter\nDRAM\nDatacenter\nDisk\n1.0\n10.0\n100.0\n1,000.0\n10,000.0\n100,000.0\n1,000,000.0\n10,000,000.0\nLatency (us)\nBandwidth (MB/s)\nCapacity (GB)\nFigure 6.7 Graph of latency, bandwidth, and capacity of the memory hierarchy of a WSC for data in Figure 6.6\n(Barroso et al., 2013).\n6.3\nComputer Architecture of Warehouse-Scale Computers\n\u25a0\n479"
    },
    {
        "page": 512,
        "text": "\u25a0\nEach server contains 16 GiB of memory with a 100-ns access time and trans-\nfers at 20 GB/s, 128 GiB of Flash with 100-\u03bcs latency and transfers at 1 GB/s,\nand 2 TB of disk that offer a 10-ms access time and transfer at 200 MB/s. There\nare two sockets per board, and they share one 1 Gbit/s Ethernet port.\n\u25a0\nIn this example, every pair of racks includes one rack switch and holds 80\nservers. Networking software plus switch overhead increases the latency to\nDRAM to 100 \u03bcs and the disk access latency to 11 ms. Thus, the total storage\ncapacity of a rack is roughly 1 TB of DRAM, 20 TB of Flash, and 160 TB of\ndisk storage. The 1 Gbit/s Ethernet limits the remote bandwidth to DRAM,\nFlash, or disk within the rack to 100 MB/s.\n\u25a0\nThe array is 30 racks, so storage capacity of an array goes up by a factor of 30:\n30 TB of DRAM, 600 TB of Flash, and 4.8 PB of disk. The array switch hard-\nware and software increases latency to DRAM within an array to 500 \u03bcs, to\n600 \u03bcs for Flash, and disk latency to 12 ms. The bandwidth of the array switch\nlimits the remote bandwidth to either array DRAM, array Flash, or array disk to\n10 MB/s.\nFigures 6.6 and 6.7 show that network overhead dramatically increases latency\nbetween local DRAM and Flash, rack DRAM and Flash, or array DRAM and\nFlash, but all still have more than 10 times better latency than accessing the local\ndisk. The network collapses the difference in bandwidth between rack DRAM,\nFlash, and disk and between array DRAM, Flash, and disk.\nThe WSC needs 40 arrays to reach 100,000 servers, so there is one more level\nin the networking hierarchy. Figure 6.8 shows the conventional Layer 3 routers to\nconnect the arrays together and to the Internet.\nMost applications fit into a single array within a WSC. Those that need more\nthan one array use sharding or partitioning, meaning that the dataset is split into\nindependent pieces and then distributed to different arrays. As an analogy, it\u2019s like\npicking up registration packets for a conference with one person handling names A\nto M and another doing N to Z. Operations on the whole dataset are sent to the\nservers hosting the pieces, and the results are coalesced by the client computer.\nExample\nWhat is the average memory latency assuming that 90% of accesses are local to the\nserver, 9% are outside the server but within the rack, and 1% are outside the rack\nbut within the array?\nAnswer\nThe average memory access time is\n90%\u00010:1\n\u00f0\n\u00de + 9%\u0001100\n\u00f0\n\u00de + 1%\u0001300\n\u00f0\n\u00de \u00bc 0:09 + 27 + 5 \u00bc 32:09 \u03bcs\nor a factor of more than 300 slowdown versus 100% local accesses. Clearly,\nlocality of access within a server is vital for WSC performance.\n480\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 513,
        "text": "Example\nHow long does it take to transfer 1000 MB between disks within the server,\nbetween servers in the rack, and between servers in different racks in the array?\nHow much faster is it to transfer 1000 MB between DRAM in the three cases?\nAnswer\nA 1000 MB transfer between disks takes\nWithin server \u00bc 1000=200 \u00bc 5 s\nWithin rack \u00bc 1000=100 \u00bc 10 s\nWithin array \u00bc 1000=10 \u00bc 100 s\nA memory-to-memory block transfer takes\nWithin server \u00bc 1000=20000 \u00bc 0:05 s\nWithin rack \u00bc 1000=100 \u00bc 10 s\nWithin array \u00bc 1000=10 \u00bc 100 s\nThus, for block transfers outside a single server, it doesn\u2019t even matter whether\nthe data are in memory or on disk because the rack switch and array switch\nare the bottlenecks. These performance limits affect the design of WSC software\nand inspire the need for higher-performance switches (see Section 6.6).\nInternet\nLB\nLB\nCR\nCR\nInternet\nS\nS\nS\nS\nS\nS\nAR\nAR\nAR\nAR\nKey:\n \u2022 CR = L3 core router\n \u2022 AR = L3 access router\n \u2022 S = Array switch\n \u2022 LB = Load balancer\n \u2022 R = Rack of 80 servers\n         with top of rack switch\nR\nR\nR\nR\n..\n..\n...\n...\nR\nR\nDatacenter\nLayer 3\nLayer 2\nFigure 6.8 A Layer 3 network used to link arrays together and to the Internet (Greenberg et al., 2009). A load\nbalancer monitors how busy a set of servers is and directs traffic to the less loaded ones to try to keep the servers\napproximately equally utilized. Another option is to use a separate border router to connect the Internet to the data\ncenter Layer 3 switches. As we will see in Section 6.6, many modern WSCs have abandoned the conventional layered\nnetworking stack of traditional switches.\n6.3\nComputer Architecture of Warehouse-Scale Computers\n\u25a0\n481"
    },
    {
        "page": 514,
        "text": "Although these examples are educational, note that computers and networking\nequipment can be much larger and faster than these examples from 2013 (see\nSection 6.7). Servers are being deployed in 2017 with 256\u20131024 GiB of DRAM,\nand recent switches have reduced delays to only 300 ns per hop.\nGiven the architecture of the IT equipment, we are now ready to see how to\nhouse, power, and cool it and to discuss the cost to build and operate the whole\nWSC, as compared to just the IT equipment within it.\n6.4\nThe Efficiency and Cost of Warehouse-Scale Computers\nInfrastructure costs for power distribution and cooling are the majority of the\nconstruction costs of a WSC, so we concentrate on them. (Section 6.7 describes\nthe power and cooling infrastructure of a WSC in detail.)\nA computer room air-conditioning (CRAC) unit cools the air in the server room\nusing chilled water, similar to how a refrigerator removes heat by releasing it out-\nside the refrigerator. As a liquid absorbs heat, it evaporates. Conversely, when a\nliquid releases heat, it condenses. Air conditioners pump the liquid into coils under\nlow pressure to evaporate and absorb heat, which is then sent to an external con-\ndenser where it is released. Thus, in a CRAC unit, fans push warm air past a set of\ncoils filled with cold water, and a pump moves the warmed water to the chillers to\nbe cooled down. Figure 6.9 shows the large collection of fans and water pumps that\nmove air and water throughout the system.\nIn addition to chillers, some data centers leverage colder outside air or water\ntemperature to cool the water before it is sent to the chillers. However, depending\non the location, the chillers may still be needed during the warmer times of the year.\nSurprisingly, it\u2019s not obvious how to figure out how many servers a WSC can\nsupport after subtracting the overhead for power distribution and cooling. The\nnameplate power rating from the server manufacturer is always conservative:\nit\u2019s the maximum power a server can draw. The first step then is to measure a single\nserver under a variety of workloads to be deployed in the WSC. (Networking is\ntypically about 5% of power consumption, so it can be ignored at first.)\nTo determine the number of servers for a WSC, the available power for IT\nequipment could be divided just by the measured server power; however, this\nwould again be too conservative according to Fan et al. (2007). They found that\nthere is a significant gap between what thousands of servers could theoretically\ndo, in the worst case, and what they will do in practice, since no real workloads\nwill keep thousands of servers all simultaneously at their peaks. They found that\nthey could safely oversubscribe the number of servers by as much as 40% based\non the power of a single server. They recommended that WSC architects should do\nso to increase the average utilization of power within a WSC; however, they also\nsuggested using extensive monitoring software along with a safety mechanism that\nde-schedules lower priority tasks in case the workload shifts.\nHere is the power usage inside the IT equipment for a Google WSC deployed\nin 2012 (Barroso et al., 2013):\n482\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 515,
        "text": "\u25a0\n42% of power for processors\n\u25a0\n12% for DRAM\n\u25a0\n14% for disks\n\u25a0\n5% for networking\n\u25a0\n15% for cooling overhead\n\u25a0\n8% for power overhead\n\u25a0\n4% miscellaneous\nMeasuring Efficiency of a WSC\nA widely used, simple metric to evaluate the efficiency of a data center or a WSC is\ncalled power utilization effectiveness (or PUE):\nPUE \u00bc Total facility power\n\u00f0\n\u00de= IT equipment power\n\u00f0\n\u00de\nThus, PUE must be greater than or equal to 1, and the bigger the PUE, the less\nefficient the WSC.\nComputer\nroom air \nhandler\nCooling\ntower\nCWS\npump\nHeat\nexchanger\n(Water-side economizer)\nA/C\ncondenser\nPrimary\npump\nA/C\nevaporator\nLeakage\nCold\nHot\nDiluted hot/cold mix\nCold\nFans\nAir impeller\nServer fans 6 to 9 W each\nA/C\ncompressor\nBlow down & evaporative loss at \n8 MW facility: ~200,000 gal/day\nFigure 6.9 Mechanical design for cooling systems. CWS stands for circulating water system. From Hamilton, J.,\n2010. Cloud computing economies of scale. In: Paper Presented at the AWS Workshop on Genomics and\nCloud\nComputing,\nJune\n8,\n2010,\nSeattle,\nWA.\nhttp://mvdirona.com/jrh/TalksAndPapers/JamesHamilton_\nGenomicsCloud20100608.pdf.\n6.4\nThe Efficiency and Cost of Warehouse-Scale Computers\n\u25a0\n483"
    },
    {
        "page": 516,
        "text": "Greenberg et al. (2009) reported on the PUE of 19 data centers and the portion\nof the overhead that went into the cooling infrastructure. Figure 6.10 shows what\nthey found, sorted by PUE from most to least efficient. The median PUE is 1.69,\nwith the cooling infrastructure using more than half as much power as the\nservers\u2014on average, 0.55 of the 1.69 is for cooling. Note that these are average\nPUEs, which can vary daily depending on workload and even external air temper-\nature, as we will see (Figure 6.11).\nWith attention paid to PUE in the past decade, data centers are much more\nefficient today. However, as Section 6.8 explains, there is no universally accepted\ndefinition of what is included in PUE: If the batteries to preserve operation during a\npower failure are in a separate building, are they included or not? Do you measure\nfrom the output of the power substation, or where power first enters the WSC?\nFigure 6.10 shows the improvement in the average PUE of all Google data centers\nover time, which Google measures inclusively.\nSince performance per dollar is the ultimate metric, we still need to measure\nperformance. As Figure 6.7 shows, bandwidth drops and latency increases depend-\ning on the distance to the data. In a WSC, the DRAM bandwidth within a server is\n200 times greater than within a rack, which in turn is 10 times greater than within\nan array. Thus, there is another kind of locality to consider in the placement of data\nand programs within a WSC.\nPower usage effectiveness (PUE)\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\nIT\nAC\nOther\n1.33\n1.35\n1.43\n1.47\n1.49\n1.52\n1.59\n1.67\n1.69\n1.69\n1.69\n1.82\n2.04\n2.04\n2.13\n2.33\n2.38\n2.63\n3.03\nFigure 6.10 Power utilization efficiency of 19 data centers in 2006 (Greenberg et al., 2009). The power for air\nconditioning (AC) and other uses (such as power distribution) is normalized to the power for the IT equipment in\ncalculating the PUE. Thus, power for IT equipment must be 1.0, and AC varies from about 0.30 to 1.40 times the power\nof the IT equipment. Power for \u201cother\u201d varies from about 0.05 to 0.60 of the IT equipment.\n484\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 517,
        "text": "Although designers of a WSC often focus on bandwidth, programmers devel-\noping applications on a WSC are also concerned with latency because latency is\nvisible to users. Users\u2019 satisfaction and productivity are tied to response time of a\nservice. Several studies from the timesharing days report that user productivity is\ninversely proportional to time for an interaction, which was typically broken down\ninto human entry time, system response time, and time for the person to think about\nthe response before hitting the next entry (Doherty and Thadhani, 1982). The\nresults of experiments showed that cutting system response time by 30% shaved\nthe time of an interaction by 70% (Brady, 1986). This implausible result is\nexplained by human nature: people need less time to think when given a faster\nresponse, as they are less likely to get distracted and remain \u201con a roll.\u201d\nFigure 6.12 shows the results of a more recent experiment for the Bing search\nengine, where delays of 50\u20132000 ms were inserted at the search server (Schurman\nand Brutlag, 2009). As expected from previous studies, time to next click roughly\ndoubled the delay; that is, a 200 ms delay at the server led to a 500 ms increase in\ntime to next click. Revenue dropped linearly with increasing delay, as did user sat-\nisfaction. A separate study on the Google search engine found that these effects\nlingered long after the 4-week experiment ended. Five weeks later, there were\n0.1% fewer searchers per day for users who experienced 200 ms delays, and there\nwere 0.2% fewer searches by users who experienced 400 ms delays. Given the\namount of money made in search, even such small changes are disconcerting.\nIn fact, the results were so negative that they ended the experiment prematurely.\nBecause of this extreme concern with satisfaction of all users of an Internet\nservice, performance goals are typically specified so that a high percentage of\nrequests are below a latency threshold, rather than just offer a target for the average\nlatency. Such threshold goals are called service level objectives (SLOs). An SLO\nmight be that 99% of requests must be below 100 ms. Thus, the designers of\n2013\n2008\n2009\n2010\nTrailing twelve-month (TTM) PUE\n2011\n2012\n2014\n2015\n2016\n2017\n1.10\n1.14\n1.18\nPUE\n1.22\n1.26\n1.12\n1.11\nQuarterly PUE\nContinuous PUE improvement\nAverage PUE for all data centers\nFigure 6.11 Average power utilization efficiency (PUE) of the 15 Google WSCs between 2008 and 2017. The\nspiking line is the quarterly average PUE, and the straighter line is the trailing 12-month average PUE. For Q4\n2016, the averages were 1.11 and 1.12, respectively.\n6.4\nThe Efficiency and Cost of Warehouse-Scale Computers\n\u25a0\n485"
    },
    {
        "page": 518,
        "text": "Amazon\u2019s Dynamo key-value storage system decided that for services to offer\ngood latency on top of Dynamo, their storage system had to deliver on its latency\ngoal 99.9% of the time (DeCandia et al., 2007). For example, one improvement of\nDynamo helped the 99.9th percentile much more than the average case, which\nreflects their priorities.\nDean and Barroso (2013) proposed the term tail tolerant to describe systems\ndesigned to meet such goals:\nJust as fault-tolerant computing aims to create a reliable whole out of less-\nreliable parts, large online services need to create a predictably responsive\nwhole out of less-predictable parts.\nThe causes of unpredictability include contention for shared resources (processors\nnetworks, etc.), queuing, variable microprocessor performance because of optimi-\nzations like Turbo mode or energy-saving techniques like DVFS, software garbage\ncollection, and many more. Google concluded that instead of trying to prevent such\nvariability in a WSC, it made more sense to develop tail-tolerant techniques to\nmask or work around temporary latency spikes. For example, fine-grained load\nbalancing can quickly move small amounts for work between servers to reduce\nqueuing delays.\nCost of a WSC\nAs mentioned in the introduction, unlike most architects, designers of WSCs worry\nabout the cost to operate as well as the cost to build the WSC. Accounting labels the\nformer costs as operational expenditures (OPEX) and the latter costs as capital\nexpenditures (CAPEX).\nTo put the cost of energy into perspective, Hamilton (2010) did a case study to\nestimate the costs of a WSC. He determined that the CAPEX of an 8-MW facility\nwas $88 million and that the roughly 46,000 servers and corresponding networking\nequipment added another $79 million to the CAPEX for the WSC. Figure 6.13\nshows the rest of the assumptions for the case study.\nServer\ndelay (ms)\nIncreased time to\nnext click (ms)\nQueries/\nuser\nAny\nclicks/\nuser\nUser\nsatisfaction\nRevenue/\nuser\n50\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n200\n500\n\u2013\n\u00040.3%\n\u00040.4%\n\u2013\n500\n1200\n\u2013\n\u00041.0%\n\u00040.9%\n\u00041.2%\n1000\n1900\n\u00040.7%\n\u00041.9%\n\u00041.6%\n\u00042.8%\n2000\n3100\n\u00041.8%\n\u00044.4%\n\u00043.8%\n\u00044.3%\nFigure 6.12 Negative impact of delays at the Bing search server on user behavior\n(Schurman and Brutlag, 2009).\n486\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 519,
        "text": "Hamilton\u2019s study works out to $11/watt for the building, power, and cooling.\nBarroso et al. (2013) reported consistent results for several cases, with the cost at $9\nto $13/watt. Thus, a 16-MW facility costs $144 million to $208 million, not includ-\ning the computing, storage, and networking equipment.\nWe can convert CAPEX into OPEX by a cost of capital conversion, assuming\n5% borrowing cost, which is a standard convention in US accounting rules. That is,\nwe can just amortize CAPEX as a fixed amount each month for the effective life of\nSize of facility (critical load watts)\n8,000,000\nAverage power usage (%)\n80%\nPower usage effectiveness\n1.45\nCost of power ($/kWh)\n$0.07\n% Power and cooling infrastructure (% of total facility cost)\n82%\nCAPEX for facility (not including IT equipment)\n$88,000,000\nNumber of servers\n45,978\nCost/server\n$1450\nCAPEX for servers\n$66,700,000\nNumber of rack switches\n1150\nCost/rack switch\n$4800\nNumber of array switches\n22\nCost/array switch\n$300,000\nNumber of layer 3 switches\n2\nCost/layer 3 switch\n$500,000\nNumber of border routers\n2\nCost/border router\n$144,800\nCAPEX for networking gear\n$12,810,000\nTotal CAPEX for WSC\n$167,510,000\nServer amortization time\n3 years\nNetworking amortization time\n4 years\nFacilities amortization time\n10 years\nAnnual cost of money\n5%\nFigure 6.13 Case study for a WSC, rounded to nearest $5000. Internet bandwidth\ncosts vary by application, so they are not included here. The remaining 18% of the CAPEX\nfor the facility includes buying the property and the cost of construction of the building.\nWe added people costs for security and facilities management in Figure 6.14, which were\nnot part of the case study. Note that Hamilton\u2019s estimates were done before he joined\nAmazon, and they are not based on the WSC of a particular company. Based on\nHamilton, J., 2010. Cloud computing economies of scale. In: Paper Presented at the\nAWS Workshop on Genomics and Cloud Computing, June 8, 2010, Seattle, WA. http://\nmvdirona.com/jrh/TalksAndPapers/JamesHamilton_GenomicsCloud20100608.pdf.\n6.4\nThe Efficiency and Cost of Warehouse-Scale Computers\n\u25a0\n487"
    },
    {
        "page": 520,
        "text": "the equipment. Figure 6.14 breaks down the monthly OPEX for Hamilton\u2019s case\nstudy. Note that the amortization rates differ significantly for his case study, from\n10 years for the facility to 4 years for the networking equipment and 3 years for the\nservers. Thus, the WSC facility lasts a decade, but the servers are replaced every\n3 years and the networking equipment every 4 years. By amortizing the CAPEX,\nHamilton came up with a monthly OPEX, including accounting for the cost of\nborrowing money (5% annually) to pay for the WSC. At $3.8 million, the monthly\nOPEX is about 2% of the CAPEX (or 24% annually).\nThis figure allows us to calculate a handy guideline to keep in mind when\nmaking decisions about which components to use when being concerned about\nenergy. The fully burdened cost of a watt per year in a WSC, including the cost\nof amortizing the power and cooling infrastructure, is\nMonthly cost of infrastructure + monthly cost of power\nFacility size in watts\n\u000112 \u00bc $765K + $475K\n8M\n\u000112 \u00bc $1:86\nThe cost is roughly $2 per watt-year. Thus, reducing costs by saving energy should\nnot result in spending more than $2 per watt-year (see Section 6.8).\nNote that in Figure 6.14, more than a third of OPEX is related to power, with\nthat category trending up while server costs are trending down over time. The\nnetworking equipment is significant at 8% of total OPEX and 19% of the server\nCAPEX, and networking equipment is not trending down as quickly as servers\nare, perhaps because of the continuing demand for higher network bandwidth\n(see Figure 6.22 on page 467). This difference is especially true for the switches\nin the networking hierarchy above the rack, which represent most of the network-\ning costs (see Section 6.6). People costs for security and facilities management are\njust 2% of OPEX. Dividing the OPEX in Figure 6.14 by the number of servers and\nhours per month, the cost is about $0.11 per server per hour.\nExpense (% total)\nCategory\nMonthly cost\nPercent monthly cost\nAmortized CAPEX (85%)\nServers\n$2,000,000\n53%\nNetworking equipment\n$290,000\n8%\nPower and cooling infrastructure\n$765,000\n20%\nOther infrastructure\n$170,000\n4%\nOPEX (15%)\nMonthly power use\n$475,000\n13%\nMonthly people salaries and benefits\n$85,000\n2%\nTotal OPEX\n$3,800,000\n100%\nFigure 6.14 Monthly OPEX for Figure 6.13, rounded to the nearest $5000. Note that the 3-year amortization of\nservers means purchasing new servers every 3 years, whereas the facility is amortized for 10 years. Thus, the amor-\ntized capital costs for servers are about three times more than for the facility. People costs include three security\nguard positions continuously for 24 h a day, 365 days a year, at $20 per hour per person, and one facilities person\nfor 24 h a day, 365 days a year, at $30 per hour. Benefits are 30% of salaries. This calculation does not include the cost\nof network bandwidth to the Internet because it varies by application nor vendor maintenance fees because they vary\nby equipment and by negotiations.\n488\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 521,
        "text": "Barroso et al. (2013) evaluated CAPEX and OPEX in terms of cost per watt per\nmonth. Thus, if a 12-MW WSC is depreciated over 12 years, the depreciation cost\nis $0.08 per watt per month. They assumed the company got the capital for the\nWSC by taking out a loan at 8% annually\u2014corporate loans are typically between\n7% and 12%\u2014and the interest payments added another $0.05, giving a total of\n$0.13 per watt per month. They factored in the cost of servers similarly. A 500 watt\nserver that cost $4000 was $8 per watt, and the 4-year depreciation was $0.17 per\nwatt per month. An 8% interest on a loan for the servers added $0.02. They\nestimated networking at $0.03 per watt per month. They reported that the typical\nOPEX cost for multiple MW WSCs varied from $0.02 to $0.08 per watt per month.\nThe grand total was $0.37 to $0.43 per watt per month. For an 8-MW WSC, the\nmonthly cost minus the cost of electricity is about $3.0 million to $3.5 million. If\nwe subtract the monthly power use from Hamilton\u2019s calculation, his estimate of the\nmonthly rate will be $3.3 million. Given the different approaches to predicting\ncosts, these estimates are remarkably consistent.\nExample\nThe cost of electricity varies by region in the United States from $0.03 to $0.15 per\nkilowatt-hour. What is the impact on hourly server costs of these two\nextreme rates?\nAnswer\nWe multiply the critical load of 8 MW by the average PUE from Figure 6.13 (sec-\nond row) to calculate the average power usage:\n8\u00011:45\u000180% \u00bc 9:28 Megawatts\nThe monthly cost for power then goes from $475,000 in Figure 6.14 to $205,000 at\n$0.03 per kilowatt-hour and to $1,015,000 at $0.15 per kilowatt-hour. These\nchanges in electricity cost alter the hourly server costs from $0.11 to $0.10 and\n$0.13, respectively.\nExample\nWhat would happen to monthly costs if the amortization times were all made to be\nthe same\u2014say, 5 years? How would that change the hourly cost per server?\nAnswer\nThe spreadsheet is available online at http://mvdirona.com/jrh/TalksAndPapers/\nPerspectivesDataCenterCostAndPower.xls. Changing the amortization time to\n5 years changes the first four rows of Figure 6.14 to\nServers\n$1,260,000\n37%\nNetworking equipment\n$242,000\n7%\nPower and cooling infrastructure\n$1,115,000\n33%\nOther infrastructure\n$245,000\n7%\nand the total monthly OPEX is $3,422,000. If we replaced everything every 5 years,\nthe cost would be $0.103 per server hour, with more of the amortized costs now\nbeing for the facility rather than the servers, as in Figure 6.14.\n6.4\nThe Efficiency and Cost of Warehouse-Scale Computers\n\u25a0\n489"
    },
    {
        "page": 522,
        "text": "The rate of about $0.10 per server per hour can be much less than the cost for\nmany companies that own and operate their own (smaller) conventional data cen-\nters. The cost advantage of WSCs led large Internet companies to offer computing\nas a utility where, like electricity, you pay only for what you use. Today, utility\ncomputing is better known as cloud computing.\n6.5\nCloud Computing: The Return of Utility Computing\nIf computers of the kind I have advocated become the computers of the future,\nthen computing may someday be organized as a public utility just as the tele-\nphone system is a public utility\u2026. The computer utility could become the basis\nof a new and important industry.\nJohn McCarthy,\nMIT centennial celebration (1961)\nDriven by the demand of an increasing number of users, Internet companies such\nas Amazon, Google, and Microsoft built increasingly larger warehouse-scale\ncomputers from commodity components, making McCarthy\u2019s prediction eventu-\nally come true, but not as he thought because of the popularity of timesharing.\nThis demand led to innovations in systems software to support operating at this\nscale, including BigTable, Colossus, Dynamo, GFS, and MapReduce. It also\ndemanded improvement in operational techniques to deliver a service available\nat least 99.99% of the time despite component failures and security attacks.\nExamples of these techniques include failover, firewalls, virtual machines, and\nprotection against distributed denial-of-service attacks. With the software and\nexpertise providing the ability to scale and increasing customer demand that jus-\ntified the investment, WSCs with 50,000\u2013100,000 servers have become common-\nplace in 2017.\nWith increasing scale came increasing economies of scale. Based on a study in\n2006 that compared a WSC with a data center with only 1000 servers, Hamilton\n(2010) reported the following advantages:\n\u25a0\n5.7 times reduction in storage costs\u2014It cost the WSC $4.6 per GB per year for\ndisk storage versus $26 per GB for the data center.\n\u25a0\n7.1 times reduction in administrative costs\u2014The ratio of servers per adminis-\ntrator was over 1000 for the WSC versus just 140 for the data center.\n\u25a0\n7.3 times reduction in networking costs\u2014Internet bandwidth cost the WSC\n$13 per Mbit/s/month versus $95 for the data center. Not surprisingly, one\ncan negotiate a much better price per Mbit/s by ordering 1000 Mbit/s than\nby ordering 10 Mbit/s.\nAnother economy of scale comes during purchasing. The high level of pur-\nchasing leads to volume discount prices on virtually everything in the WSC.\n490\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 523,
        "text": "Economies of scale also apply to operational costs. From the prior section, we\nsaw that many data centers operate with a PUE of 2.0. Large firms can justify hiring\nmechanical and power engineers to develop WSCs with lower PUEs, in the range\nof 1.1\u20131.2 (see Section 6.7).\nInternet services need to be distributed to multiple WSCs both for dependabil-\nity and to reduce latency, especially for international markets. All large firms use\nmultiple WSCs for that reason. It\u2019s much more expensive for individual firms to\ncreate multiple, small data centers around the world than a single data center in\ntheir corporate headquarters.\nFinally, for the reasons presented in Section 6.1, servers in data centers tend\nto be utilized only 10%\u201320% of the time. By making WSCs available to the public,\nuncorrelated peaks between different customers can raise average utilization\nabove 50%.\nThus, economies of scale for a WSC offer factors of 5\u20137 for several compo-\nnents of a WSC plus a few factors of 1.5\u20132 for the entire WSC.\nSince the last edition of this book, the concerns about security have flipped for\nthe cloud. In 2011 there was skepticism about placing critical data in the cloud\nbecause that could make it easier for hackers to break into than if the data were\nkept on premises (\u201con prem\u201d) locked down in the local data center. In 2017 data\nbreak-ins into such data centers are so routine that they barely make the news.\nFor example, this insecurity has even led to rapid growth of ransomware\u2014\nwhere criminals break in, encrypt all the data of an organization, and won\u2019t\nrelease the key until paid a ransom\u2014costing firms $1 billion in 2015. In contrast,\nWSCs are continuously under attack, their operators respond more quickly to halt\nthem and thus build better defenses. As a result, ransomware is unheard of inside\nWSCs. WSCs are clearly more secure than the vast majority of local data centers\ntoday, so many CIOs now believe that critical data is safer in the cloud than\n\u201con prem.\u201d\nAlthough there are several cloud computing providers, we feature Amazon\nWeb Services (AWS) since it is one of the oldest and currently the largest commer-\ncial cloud provider.\nAmazon Web Services\nUtility computing goes back to commercial timesharing systems and even batch\nprocessing systems of the 1960s and 1970s, where companies only paid for a\nterminal and a phone line and then were billed based on how much computing they\nused. Many efforts since the end of timesharing have tried to offer such pay-as-\nyou-go services, but they were often met with failure.\nWhen Amazon started offering utility computing via the Amazon Simple\nStorage Service (Amazon S3) and then Amazon Elastic Computer Cloud (Amazon\nEC2) in 2006, it made some novel technical and business decisions:\n\u25a0\nVirtual machines. Building the WSC using x86-commodity computers run-\nning the Linux operating system and the Xen virtual machine solved several\n6.5\nCloud Computing: The Return of Utility Computing\n\u25a0\n491"
    },
    {
        "page": 524,
        "text": "problems. First, it allowed Amazon to protect users from each other. Second, it\nsimplified software distribution within a WSC, in that customers needed to\ninstall only an image and then AWS automatically distributed it to all the\ninstances being used. Third, the ability to kill a virtual machine reliably made\nit easy for Amazon and customers to control resource usage. Fourth, virtual\nmachines could limit the rate at which they used the physical processors, disks,\nand the network as well as the amount of main memory, which gave AWS\nmultiple price points: the lowest price option by packing many virtual cores\non a single server, the highest price option of exclusive access to all the\nmachine resources, as well as several intermediary points. Fifth, virtual\nmachines hid the identity of hardware, allowing AWS to continue to sell time\non older machines that might otherwise be unattractive to customers if they\nknew the age of the machines. Finally, virtual machines allowed AWS to intro-\nduce new and faster hardware either by packing even more virtual cores per\nserver or simply by offering instances that had higher performance per virtual\ncore; virtualization meant that offered performance need not be an integer\nmultiple of the performance of the hardware.\n\u25a0\nVery low cost. When AWS announced a rate of $0.10 per hour per instance in\n2006, it was a startlingly low amount. An instance is one virtual machine, and\nat $0.10 per hour, AWS allocated two instances per core on a multicore server.\nThus, one EC2 computer unit is equivalent to a 1.0\u20131.2 GHz AMD Opteron or\nIntel Xeon of that era.\n\u25a0\n(Initial) reliance on open source software. The availability of good-quality\nsoftware that had no licensing problems or costs associated with running on\nhundreds or thousands of servers made utility computing much more econom-\nical for both Amazon and its customers. AWS later started offering instances\nincluding commercial third-party software at higher prices.\n\u25a0\nNo (initial) guarantee of service. Amazon originally promised only best effort.\nThe low cost was so attractive that many could live without a service guarantee.\nToday AWS provides availability SLOs of up to 99.95% on services such as\nAmazon EC2 and Amazon S3. Additionally, Amazon S3 was designed for\ndurability by saving multiple replicas of each object across multiple locations.\n(According to AWS, the chances of permanently losing an object are one\nin 100 billion.) AWS also provides a Service Health Dashboard that shows\nthe current operational status of each of the AWS services in real time so that\nAWS uptime and performance are fully transparent.\n\u25a0\nNo contract required. In part because the costs are so low, all that is necessary\nto start using EC2 is a credit card.\nFigures 6.15 and 6.16 show the hourly price of the many types of EC2\ninstances in 2017. Expanding from the 10 instance types in 2006, there are now\nmore than 50. The fastest instance is 100 times quicker than the slowest, and\n492\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 525,
        "text": "Instance\nPer hour\nRatio to\nm4.large\nVirtual\ncores\nCompute\nunits\nMemory\n(GiB)\nStorage (GB)\nGeneral-purpose\nt2.nano\n$0.006\n0.05\n1\nVariable\n0.5\nEBS only\nt2.micro\n$0.012\n0.11\n1\nVariable\n1.0\nEBS only\nt2.small\n$0.023\n0.21\n1\nVariable\n2.0\nEBS only\nt2.medium\n$0.047\n0.4\n2\nVariable\n4.0\nEBS only\nt2.large\n$0.094\n0.9\n2\nVariable\n8.0\nEBS only\nt2.xlarge\n$0.188\n1.7\n4\nVariable\n16.0\nEBS only\nt2.2xlarge\n$0.376\n3.5\n8\nVariable\n32.0\nEBS only\nm4.large\n$0.108\n1.0\n2\n6.5\n8.0\nEBS only\nm4.xlarge\n$0.215\n2.0\n4\n13\n16.0\nEBS only\nm4.2xlarge\n$0.431\n4.0\n8\n26\n32.0\nEBS only\nm4.4xlarge\n$0.862\n8.0\n16\n54\n64.0\nEBS only\nm4.10xlarge\n$2.155\n20.0\n40\n125\n160.0\nEBS only\nm4.16xlarge\n$3.447\n31.9\n64\n188\n256.0\nEBS only\nm3.medium\n$0.067\n0.6\n1\n3\n3.8\n1\u00014 SSD\nm3.large\n$0.133\n1.2\n2\n6.5\n7.5\n1\u000132 SSD\nm3.xlarge\n$0.266\n2.5\n4\n13\n15.0\n2\u000140 SSD\nm3.2xlarge\n$0.532\n4.9\n8\n26\n30.0\n2\u000180 SSD\nCompute-optimized\nc4.large\n$0.100\n0.9\n2\n8\n3.8\nEBS only\nc4.xlarge\n$0.199\n1.8\n4\n16\n7.5\nEBS only\nc4.2xlarge\n$0.398\n3.7\n8\n31\n15.0\nEBS only\nc4.4xlarge\n$0.796\n7.4\n16\n62\n30.0\nEBS only\nc4.8xlarge\n$1.591\n14.7\n36\n132\n60.0\nEBS only\nc3.large\n$0.105\n1.0\n2\n7\n3.8\n2\u000116 SSD\nc3.xlarge\n$0.210\n1.9\n4\n14\n7.5\n2\u000140 SSD\nc3.2xlarge\n$0.420\n3.9\n8\n28\n15.0\n2\u000180 SSD\nc3.4xlarge\n$0.840\n7.8\n16\n55\n30.0\n2\u0001160 SSD\nc3.8xlarge\n$1.680\n15.6\n32\n108\n60.0\n2\u0001320 SSD\nFigure 6.15 Price and characteristics of on-demand general-purpose and compute-optimized EC2 instances in\nthe Virginia region of the United States in February 2017. When AWS started, one EC2 computer unit was equiv-\nalent to a 1.0\u20131.2 GHz AMD Opteron or Intel Xeon of 2006. Variable instances are the newest and cheapest category.\nThey offer the full performance of a high-frequency Intel CPU core if your workload utilizes less than 5% of the core on\naverage over 24 h, such as for serving web pages. AWS also offers Spot Instances at a much lower cost (about 25%).\nWith Spot Instances, customers set the price they are willing to pay and the number of instances they are willing to\nrun, and then AWS runs the bids when the spot price drops below their level. AWS also offers Reserved Instances for\ncases where customers know they will use most of the instance for a year. They pay a yearly fee per instance and then\nan hourly rate that is about 30% of column 1 to use the service. If a Reserved Instance is used 100% for a whole year,\nthe average cost per hour including amortization of the annual fee will be about 65% of the rate in the first column.\nEBS is Elastic Block Storage, which is a raw block-level storage system found elsewhere on the network, rather than in\na local disk or local solid stage disk (SSD) within the same server as the VM.\n6.5\nCloud Computing: The Return of Utility Computing\n\u25a0\n493"
    },
    {
        "page": 526,
        "text": "the largest offers 2000 times more memory than the smallest. Rent for the cheapest\ninstance for a whole year is just $50.\nIn addition to computation, EC2 charges for long-term storage and for Internet\ntraffic. (There is no cost for network traffic inside AWS regions.) Elastic Block\nStorage (EBS) costs $0.10 per GB per month when using SSDs and $0.045 per\nGB monthly for hard disk drives. Internet traffic costs $0.01 per GB going to\nEC2 and $0.09 per GB coming from EC2.\nInstance\nPer hour\nRatio to\nm4.large\nVirtual cores\nCompute units\nMemory (GiB)\nStorage (GB)\nGPU\np2.xlarge\n$0.900\n8.3\n4\n12\n61.0\nEBS only\np2.8xlarge\n$7.200\n66.7\n32\n94\n488.0\nEBS only\np2.16xlarge\n$14.400\n133.3\n64\n188\n732.0\nEBS only\ng2.2xlarge\n$0.650\n6.0\n8\n26\n15.0\n60 SSD\ng2.8xlarge\n$2.600\n24.1\n32\n104\n60.0\n2\u0001120 SSD\nFPGA\nf1.2xlarge\n$1.650\n15.3\n8 (1 FPGA)\n26\n122.0\n1\u0001470 SSD\nf1.16xlarge\n$13.200\n122.2\n64 (8 FPGA)\n188\n976.0\n4\u0001940 SSD\nMemory-optimized\nx1.16xlarge\n$6.669\n61.8\n64\n175\n976.0\n1\u00011920 SSD\nx1.32xlarge\n$13.338\n123.5\n128\n349\n1,952.0\n2\u00011920 SSD\nr3.large\n$0.166\n1.5\n2\n6.5\n15.0\n1\u000132 SSD\nr3.xlarge\n$0.333\n3.1\n4\n13\n30.5\n1\u000180 SSD\nr3.2xlarge\n$0.665\n6.2\n8\n26\n61.0\n1\u0001160 SSD\nr3.4xlarge\n$1.330\n12.3\n16\n52\n122.0\n1\u0001320 SSD\nr3.8xlarge\n$2.660\n24.6\n32\n104\n244.0\n2\u0001320 SSD\nr4.large\n$0.133\n1.2\n2\n7\n15.3\nEBS only\nr4.xlarge\n$0.266\n2.5\n4\n14\n30.5\nEBS only\nr4.2xlarge\n$0.532\n4.9\n8\n27\n61.0\nEBS only\nr4.4xlarge\n$1.064\n9.9\n16\n53\n122.0\nEBS only\nr4.8xlarge\n$2.128\n19.7\n32\n99\n244.0\nEBS only\nr4.16xlarge\n$4.256\n39.4\n64\n195\n488.0\nEBS only\nStorage-optimized\ni2.xlarge\n$0.853\n7.9\n4\n14\n30.5\n1\u0001800 SSD\ni2.2xlarge\n$1.705\n15.8\n8\n27\n61.0\n2\u0001800 SSD\ni2.4xlarge\n$3.410\n31.6\n16\n53\n122.0\n4\u0001800 SSD\ni2.8xlarge\n$6.820\n63.1\n32\n104\n244.0\n8\u0001800 SSD\nd2.xlarge\n$0.690\n6.4\n4\n14\n30.5\n3\u00012000 HDD\nd2.2xlarge\n$1.380\n12.8\n8\n28\n61.0\n6\u00012000 HDD\nd2.4xlarge\n$2.760\n25.6\n16\n56\n122.0\n12\u00012000 HDD\nd2.8xlarge\n$5.520\n51.1\n36\n116\n244.0\n24\u00012000 HDD\nFigure 6.16 Price and characteristics of on-demand GPUs, FPGAs, memory-optimized, and storage-optimized\nEC2 instances in the Virginia region of the United States in February 2017.\n494\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 527,
        "text": "Example\nCalculate the cost of running the average MapReduce job in Figure 6.2 on page 438\non EC2 for several months over the years. Assume there are plenty of jobs, so there\nis no significant extra cost to round up so as to get an integer number of hours. Next\ncalculate the cost per month to run all the MapReduce jobs.\nAnswer\nThe first question is, what is the right size instance to match the typical server at\nGoogle? Let\u2019s assume the closest match in Figure 6.15 is a c4.large with 2 virtual\ncores and 3.6 GiB of memory, which costs $0.100 per hour. Figure 6.17 calculates\nthe average and total cost per year of running the Google MapReduce workload on\nEC2. The average September 2016 MapReduce job would cost a little over $1 on\nEC2, and the total workload for that month would cost $114 million on AWS.\nExample\nGiven the cost of MapReduce jobs, imagine that your boss wants you to investigate\nways to lower costs. How much might you save using AWS Spot Instances?\nAnswer\nThe MapReduce jobs could be disrupted by being kicked off a spot instance, but\nMapReduce is designed to tolerate and restart failed jobs. The AWS Spot price for\nc4.large was $0.0242 versus $0.100, which meant a savings of $87 million for Sep-\ntember 2016, but there were no guarantees on the response times!\nIn addition to the low-cost and a pay-for-use model of utility computing,\nanother strong attractor for cloud computing users is that the cloud-computing pro-\nviders take on the risks of over-provisioning or under-provisioning. Because either\nmistake could be fatal, risk avoidance is a godsend for startup companies. If too\nmuch of the precious investment is spent on servers before the product is ready\nfor heavy use, a company could run out of money. If the service suddenly became\npopular but there weren\u2019t enough servers to match the demand, a company could\nAug-04\nSep-09\nSep-12\nSep-16\nAverage completion time (h)\n0.15\n0.14\n0.13\n0.11\nAverage number of servers per job\n157\n156\n142\n130\nCost per hour of EC2 c4.large instance\n$0.100\n$0.100\n$0.100\n$0.100\nAverage EC2 cost per MapReduce job\n$2.76\n$2.23\n$1.89\n$1.20\nMonthly number of MapReduce jobs\n29,000\n4,114,919\n15,662,118\n95,775,891\nTotal cost of MapReduce jobs on EC2/EBS\n$80,183\n$9,183,128\n$29,653,610\n$114,478,794\nFigure 6.17 Estimated cost to run the Google MapReduce workload for select months between 2004 and 2016\n(Figure 6.2) using 2017 prices for AWS EC2. Because we are using 2017 prices, these are underestimates of actual\nAWS costs.\n6.5\nCloud Computing: The Return of Utility Computing\n\u25a0\n495"
    },
    {
        "page": 528,
        "text": "make a very bad impression with the potential new customers it desperately needs\nin order to grow.\nThe poster child for this scenario is FarmVille from Zynga, a social networking\ngame on Facebook. Before FarmVille was announced, the largest social game was\nabout five million daily players. FarmVille had one million players 4 days after\nlaunching and 10 million players after 60 days. After 270 days, it had 28 million\ndaily players and 75 million monthly players. Because FarmVille is deployed on\nAWS, it is able to grow seamlessly with the number of users. Moreover, it is able to\nshed load based on customer demand and time of day.\nFarmVille was so successful that Zynga decided to open its own data centers in\n2012. In 2015, Zynga returned to AWS, deciding it was better to let AWS run its\ndata centers (Hamilton, 2015). When FarmVille dropped from the most popular\nFacebook application to 110th in 2016, Zynga was able to downsize gracefully\nwith AWS, much as it grew with AWS in the beginning.\nIn 2014, AWS offered a new service that hearkened back to the timesharing\ndays of the 1960s that John McCarthy was referring to in the opening quote of\nthis section. Instead of managing virtual machines in the cloud, Lambda lets users\nsupply a function in source code (such as Python) and lets AWS automatically\nmanage the resources required by that code to scale with input size and to make\nit highly available. Google Cloud Compute Functions and Microsoft Azure Func-\ntions are equivalent capabilities from competing cloud providers. As Section 6.10\nexplains, Google App Engine originally offered a quite similar service in 2008.\nThis trend is referred to as Serverless Computing, in that users don\u2019t have to\nmanage servers (but these functions are in fact run on servers). The tasks provided\ninclude operating system maintenance, capacity provisioning and automatic scal-\ning, code and security patch deployment, and code monitoring and logging. It runs\ncode in response to events, such as an http request or database update. One way to\nthink of Serverless Computing is as a set of processes running in parallel across the\nentire WSC that share data through a disaggregated storage service such as\nAWS S3.\nThere is no cost for Serverless Computing when a program is idle. The AWS\naccounting is six orders of magnitude finer than EC2, recording usage per 100 ms\ninstead of per hour. Cost varies depending on the amount of memory needed, but if\nyour program used 1 GiB of memory, the cost is $0.000001667 per 100 ms or\nabout $6 per hour.\nServerless Computing can be thought of as the next evolutionary step toward\nrealizing the cloud computing ideals of the data center as a computer, as pay-as-\nyou-go pricing, and as a means for automatic dynamic scaling.\nCloud computing has made the benefits of WSC available to everyone. Cloud\ncomputing offers cost associativity with the illusion of infinite scalability at no\nextra cost to the user: 1000 servers for 1 h cost no more than 1 server for\n1000 h. It is up to the cloud computing provider to ensure that there are enough\nservers, storage, and Internet bandwidth available to meet the demand. The previ-\nously mentioned optimized supply chain, which drops time-to-delivery to a week\nfor new computers, is a considerable aid in providing that illusion without\n496\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 529,
        "text": "bankrupting the provider. This transfer of risks, cost associativity, pay-as-you-go\npricing, and greater security is a powerful argument for companies of varying sizes\nto use cloud computing.\nHow Big Is the AWS Cloud?\nAWS started in 2006 and grew so large that Amazon.com, rather than use a\nseparate computing infrastructure, became one of AWS\u2019s customers in 2010.\nFigure 6.18 shows that AWS had facilities in 16 locations around the world in\n2017, with two more on the way. As a point of interest, Figures 6.19 and 6.20 show\nsimilar maps for Google and Microsoft.\nEach AWS location consists of two to three nearby facilities (one or two\nkilometers apart) called availability zones. They are so named because it should\nbe safe to have your software running on two of them to ensure dependability\nas it is unlikely that both would fail simultaneously because of power outages\nor a natural disaster (Hamilton, 2014). These 16 locations contain 42 availability\nzones, and each of those zones has one or more WSCs. In 2014 each WSC had at\nleast 50,000 servers, and some had more than 80,000.\n3\n3\n5\n3\n3\n2\n2\n2\n2 2\n2\n2\n3\n3\n3\n2\nFigure 6.18 In 2017 AWS had 16 sites (\u201cregions\u201d), with two more opening soon. Most sites have two to three\navailability zones, which are located nearby but are unlikely to be affected by the same natural disaster or power\noutage, if one were to occur. (The number of availability zones are listed inside each circle on the map.) These 16\nsites or regions collectively have 42 availability zones. Each availability zone has one or more WSCs. https://aws.ama\nzon.com/about-aws/global-infrastructure/.\n6.5\nCloud Computing: The Return of Utility Computing\n\u25a0\n497"
    },
    {
        "page": 530,
        "text": "Hamilton (2017) says its best to have at least three WSCs per region. The reason\nis simply that when one WSC fails, the other in the region needs to take on the load of\nthe failed WSC. If there were only one other WSC, each would have to reserve half\nof its capacity for failover. With three, they could be used at two-thirds of capacity\nand still handle a quick failover. The more data centers you have, the less reserved\nexcess capacity; AWS has regions with more than 10 WSCs.\nWe have found two published estimates of the total number of servers in AWS\nin 2014. One estimate was 2 million servers, when AWS had just 11 regions and 28\navailability zones (Clark, 2014). Another estimate was between 2.8 and 5.6 million\nservers (Morgan 2014). If we extrapolate from 2014 to 2017 based on the increased\nnumber of availability zones, the estimates will grow to 3.0 million servers on the\nlow end and 8.4 million on the high end. The total number of WSCs (data centers)\nis 84\u2013126. Figure 6.21 shows the growth over time, using extrapolations from\nthese two projections to offer high and low estimates of the number of severs\nand WSCs over time.\nAWS is understandably mum on the actual number. They said that AWS had\nmore than 1 million customers in 2014 and that \u201cevery day AWS adds enough\nphysical server capacity equivalent to that needed to support Amazon.com in\n2004\u201d when it was a $7 billion annual revenue company (Hamilton, 2014).\nOne way to check the validity of these estimates is to look at investments.\nAmazon spent $24 billion in capital investments in property and equipment\nFigure 6.19 In 2017 Google had 15 sites. In the Americas: Berkeley County, South Carolina; Council Bluffs, Iowa;\nDouglas County, Georgia; Jackson County, Alabama; Lenoir, North Carolina; Mayes County, Oklahoma; Montgomery\nCounty, Tennessee; Quilicura, Chile; and The Dalles, Oregon. In Asia: Changhua County, Taiwan; Singapore. In Europe:\nDublin, Ireland; Eemshaven, Netherlands; Hamina, Finland; St. Ghislain, Belgium. https://www.google.com/about/\ndatacenters/inside/locations/.\n498\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 531,
        "text": "Australia East\nSoutheast Asia\nEast Asia\nGermany Central\nGermany Northeast\nNorth Central US\nUS DoD Central\nUS DoD East\nBrazil South\nEast US\nEast US 2\nUS Gov Virginia\nWest Europe\nJapan West\nJapan East\nChina North\nNorth Europe\nUK West\nUS Gov Arizona\nWest Central US\nUS Gov Texas\nSouth Central US\nWest US\nWest US 2\nFrance Central\nFrance South\nUK South\nCanada East\nUS Gov lowa\nCentral US\nCanada Central\nChina East\nWest India\nCentral India\nSouth India\nKorea South\nKorea Central\nAustralia Southeast\nGenerally available\nComing soon\nFigure 6.20 In 2017 Microsoft had 34 sites, with four more opening soon. https://azure.microsoft.com/en-us/regions/.\n6.5\nCloud Computing: The Return of Utility Computing\n\u25a0\n499"
    },
    {
        "page": 532,
        "text": "between 2013 and 2015, and one estimate is that two-thirds of that investment was\nfor AWS (Gonzalez and Day 2016; Morgan 2016). Assume that it takes a year to\nconstruct a new WSC. The estimate in Figure 6.21 for 2014 to 2016 is from 34 to\n51 WSCs. The cost per AWS WSC will then be $310 million to $470 million.\nHamilton states that \u201ceven a medium sized datacenter (WSC) will likely exceed\n$200M.\u201d (Hamilton, 2017). He goes on to say that today cloud providers currently\nhave \u201cO(102)\u201d WSCs; Figure 6.21 estimate is 84\u2013126 AWS WSCs. Despite the\nfuzziness of these estimates, they appear to be surprisingly consistent. He goes\non to predict that to meet the future demands, the largest cloud providers will even-\ntually rise to \u201cO(105)\u201d WSCs, or 1000 times more WSCs than today!\n0\n2006\n2008\n2010\n2012\n2014\n2016\n2500\n5000\nServers (1000s)\nRegions, availability zones, and WSCs\n7500\n10000\nRegions\nAvailability Zones\nWSCs (low estimate)\nWSCs (high estimate)\nServers (low estimate)\nServers (high estimate)\nFigure 6.21 Growth of AWS regions and availability zones (right vertical axis) over time. Most regions have two or\nthree availability zones. Each availability zone can have one or more WSCs, with the largest having more than 10\nWSCs. Each WSC has at least 50,000 servers, with the biggest having more than 80,000 servers (Hamilton, 2014).\nBased on two published estimates for the number of AWS servers in 2014 (Clark, 2014; Morgan 2014), we project\nthe number of servers per year (left vertical axis) and WSCs (right vertical access) as a function of the actual number\nof availability zones.\n500\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 533,
        "text": "No matter how many servers and WSCs are in the cloud, two cross-cutting\nissues that shape the cost-performance of WSCs and thus cloud computing are\nthe WSC network and the efficiency of the server hardware and software.\n6.6\nCross-Cutting Issues\nNet gear is the SUV of the datacenter.\nJames Hamilton (2009)\nPreventing the WSC Network From Being a Bottleneck\nFigure 6.22 shows the network demands doubling every 12\u201315 months for Google,\nresulting in a 50\u0001 growth in traffic from the servers in Google\u2019s fleet of WSCs in\njust 7 years. Clearly, without great care, the WSC network could easily become a\nperformance or cost bottleneck.\nIn the previous edition, we pointed out that a data center switch could cost\nalmost $1 million, or more than 50 times as much as a Top of Rack switch. Not\nonly was such a switch expensive, the resulting oversubscription affected the\ndesign of software and the placement of services and data within the WSC. The\nWSC network bottlenecks constrained data placement, which in turn complicated\nWSC software. Because this software is one of the most valuable assets of a WSC\ncompany, the cost of this added complexity was significant.\nThe ideal WSC network would be a black box whose topology and band-\nwidth are uninteresting because there are no restrictions: any workload could\nJul \u201808\nJun \u201809\nMay \u201810\nApr \u201811\nMar \u201812\nFeb \u201813\nDec \u201813\nTime\nAggregate traffic\nNov \u201814\n50x\n1x\nTraffic generated by servers in our datacenters\nTime\nAggregate traffic\ne\ng\n50x\n1x\ng\ny\nFigure 6.22 Network traffic from all the servers in Google\u2019s WSCs over 7 years (Singh\net al., 2015).\n6.6\nCross-Cutting Issues\n\u25a0\n501"
    },
    {
        "page": 534,
        "text": "be placed anywhere and optimized for server utilization rather than network\ntraffic locality. Vahdat et al. (2010) proposed borrowing networking technology\nfrom supercomputing to overcome the price and performance problems. The lat-\nter proposed a networking infrastructure that could scale to 100,000 ports and\n1 Pbit/s of bisection bandwidth. A major benefit of these novel data center\nswitches is to simplify the software challenges because of oversubscription.\nSince that time, many companies with WSC have designed their own switches\nto overcome these challenges (Hamilton, 2014). Singh et al. (2015) reported on\nseveral generations of custom networks used inside Google WSCs, which\nFigure 6.23 lists.\nTo keep costs down, they built their switches from standard commodity switch\nchips. They found that the features of traditional data center switches that were used\nin part to justify their high costs\u2014such as decentralized network routing and pro-\ntocols to manage support of arbitrary deployment scenarios\u2014were unnecessary\nin a WSC because the network topology could be planned in advance of deployment\nData center\ngeneration\nswitch\nFirst\ndeployed\nMerchant\nsilicon\nTop of rack\n(ToR) switch\nconfig\nEdge\naggregation\nblock\nSpine\nblock\nFabric\nspeed\nHost speed\nBisection\nBW\nFour-Post\nCRs\n2004\nVendor\n48\u00011 Gbps\n\u2013\n\u2013\n10 Gbps\n1 Gbps\n2 Tbps\nFirehose 1.0\n2005\n8\u000110 Gbps\n4\u000110 Gbps\n(ToR)\n2\u000110 Gbps\nup\n24\u00011 Gbps\ndown\n2\u000132\u000110\nGbps\n32\u000110\nGbps\n10 Gbps\n1 Gbps\n10 Tbps\nFirehose 1.1\n2006\n8\u000110 Gbps\n4\u000110 Gbps\nup\n48\u00011 Gbps\ndown\n64\u000110 Gbps\n32\u000110\nGbps\n10 Gbps\n1 Gbps\n10 Tbps\nWatchtower\n2008\n16\u000110 Gbps\n4\u000110 Gbps\nup\n48\u00011 Gbps\ndown\n4\u0001128\u000110\nGbps\n128\u000110\nGbps\n10 Gbps\nn\u00011 Gbps\n82 Tbps\nSaturn\n2009\n24\u000110 Gbps 24\u000110 Gbps\n4\u0001288\u000110\nGbps\n288\u000110\nGbps\n10 Gbps n\u000110 Gbps\n207 Tbps\nJupiter\n2012\n16\u000140 Gbps 16\u000140 Gbps\n8\u0001128\u000140\nGbps\n128\u000140\nGbps\n10/40\nGbps\nn\u000110 Gbps/\nn\u000140 Gbps\n1300 Tbps\nFigure 6.23 Six generations of network switches deployed at Google WSCs (Singh et al., 2015). The Four-Post CRs\nused commercial 512 port, 1 Gbit/s Ethernet switches, and 48-port, 1 Gbit/s Ethernet Top of Rack (ToR) switches,\nwhich allowed 20,000 servers in the array. The goal of Firehose 1.0 was to deliver 1 Gbps of nonblocking bisection\nbandwidth to each of 10,000 servers, but it ran into problems with the low connectivity of the ToR switch that caused\nproblems when links failed. Firehose 1.1 was the first custom-designed switch with better connectivity in the ToR\nswitch. Watchtower and Saturn followed in the same footsteps, but used new, faster merchant switch chips. Jupiter\nuses 40 Gbps links and switches to deliver more than 1 Pbit/s of bisection bandwidth. Section 6.7 describes the Jupi-\nter switch and the Edge Aggregation and Spine Blocks of Clos networks in more detail.\n502\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 535,
        "text": "and the network had only a single operator. Google instead used centralized\ncontrol that relied on a common configuration that was copied to all data center\nswitches. The modular hardware design and robust software control allowed these\nswitches to be used both for inside the WSC and for wide area networks between\nWSCs. Google scaled the bandwidth of its WSCs networks by 100X in a decade,\nand offered more than 1 Pbit/s of bisection bandwidth in 2015.\nUsing Energy Efficiently Inside the Server\nAlthough PUE measures the efficiency of a WSC, it has nothing to say about what\ngoes on inside the IT equipment. Thus, another source of electrical inefficiency is\nthe power supply inside the server, which converts input of high voltage to the\nvoltages that chips and disks use. In 2007 many power supplies were 60%\u2013\n80% efficient, which meant there were greater losses inside the server than there\nwere going through the many steps and voltage changes from the high-voltage\nlines at the utility tower to supply the low-voltage lines at the server. One reason\nwas that the power supply was often oversized in watts for what was on the\nmotherboard. Moreover, such power supplies were typically at their worst effi-\nciency at 25% load or less, even though, as Figure 6.3 on page 441 shows, many\nWSC servers operate in that range. Computer motherboards also have voltage reg-\nulator modules (VRMs), and they can have relatively low efficiency as well.\nBarroso and H\u20acolzle (2007) said the goal for the whole server should be energy\nproportionality; that is, servers should consume energy in proportion to the amount\nof work performed. A decade later, we\u2019ve gotten close but have not hit that ideal\ngoal. For example, the best-rated SPECpower servers in Chapter 1 still use about\n20% of the full power when idle and almost 50% of full power at just 20% load.\nThat represents huge progress since 2007 when an idle computer used 60% of full\npower and 70% at a 20% load, but there is still room to improve.\nSystems software is designed to use all of an available resource if it potentially\nimproves performance, without concern for the energy implications. For example,\noperating systems use all of memory for program data or for file caches, although\nmuch of the data will likely never be used. Software architects need to consider\nenergy as well as performance in future designs (Carter and Rajamani, 2010).\nGiven the background from these six sections, we are now ready to appreciate\nthe work of the Google WSC architects.\n6.7\nPutting It All Together: A Google\nWarehouse-Scale Computer\nBecause many companies with WSCs are competing vigorously in the market-\nplace, most have been reluctant to share their latest innovations with the public\n(and each other). Fortunately, Google has continued its tradition of providing\ndetails on recent WSCs for new editions of this book, once again making this\n6.7\nPutting It All Together: A Google Warehouse-Scale Computer\n\u25a0\n503"
    },
    {
        "page": 536,
        "text": "edition likely the most up-to-date public description of a Google WSC, which is\nrepresentative of the current state-of-the-art.\nPower Distribution in a Google WSC\nWe start with power distribution. Although there are many variations deployed, in\nNorth America electric power typically goes through multiple voltage changes on\nthe way to the server, starting with the high-voltage lines at the utility tower of over\n110,000 V.\nFor large-scale sites with multiple WSCs, power is delivered to on-site substa-\ntions (Figure 6.24). The substations are sized for hundreds of megawatts of power.\nThe voltage is reduced to between 10,000 and 35,000 V for distribution to WSCs\non the site.\nNear the buildings of the WSC, the voltage is further reduced to around 400 V\n(Figure 6.25) for distribution to the rows of servers on the data center floor. (480 V\nis common in North America, but 400 V in the rest of the world; Google uses\n415 V.) To prevent the whole WSC from going offline if power is lost, WSCs have\ntheir version of an uninterruptible power supply (UPS), just as most servers do in\nconventional data centers. Diesel generators are connected to the power distribu-\ntion system at this level to provide power in the event of an issue with the utility\npower. Although most outages are less than a few minutes, WSCs store thousands\nof gallons of diesel on site for an extended event. The operators even make pro-\nvisions with local fuel companies for continuous delivery of diesel should a site\nneed to operate from generators for days or weeks.\nInside the WSC, power is delivered to the racks via copper bus ducts that run\nabove each row of racks, as Figure 6.26 shows. The last step splits the three-phase\npower into three separate single-phase powers of 240\u2013277 V delivered by power\nFigure 6.24 An on-site substation.\n504\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 537,
        "text": "cables to the rack. Near the top of the rack, power converters turn the 240 V AC\ncurrent into 48 V DC to bring the voltage down to what boards can use.\nIn summary, power is distributed in a hierarchy in a WSC, with each level of\nthe hierarchy corresponding to a distinct failure and maintenance unit: the whole\nWSC, arrays, rows, and racks. Software is aware of the hierarchy, and it spreads\nwork and storage topographically to increase dependability.\nWSCs around the world have different distribution voltages and frequencies,\nbut the overall design is similar. The primary places for improvement in power\nFigure 6.25 This image shows transformers, switch gear, and generators in close\nproximity to a WSC.\nFigure 6.26 Row of servers with the copper bus ducts above that distribute 400 V to\nthe servers. Although hard to see, they are above the shelf on the right side of the\nphoto. It also shows a cold aisle that operators use to service the equipment.\n6.7\nPutting It All Together: A Google Warehouse-Scale Computer\n\u25a0\n505"
    },
    {
        "page": 538,
        "text": "efficiency are in the voltage transformers at each step, but these are highly\noptimized components, so there is little opportunity left.\nCooling in a Google WSC\nNow that we can deliver power from the utility poles to the floor of the WSC, we\nneed to remove the heat generated from using it. There are considerably more\nopportunities for improvement in the cooling infrastructure.\nOne of the easiest ways to improve energy efficiency is simply to run the IT\nequipment at higher temperatures so that the air does not need to be cooled as\nmuch. Google runs its equipment at 80+\u00b0F (27+\u00b0C), which is considerably higher\nthan traditional data centers that are so cold that you need to wear a jacket.\nAirflow is carefully planned for the IT equipment, even using Computational\nFluid Dynamics simulation to design the facility. Efficient designs preserve the\ntemperature of the cool air by reducing the chances of it mixing with hot air.\nFor example, most WSCs today have alternating aisles of hot air and cold air by\norienting servers in opposite directions in alternating rows of racks so that hot\nexhaust blows in alternating directions. They are referred to as hot aisles and cold\naisles. Figure 6.26 shows a cold aisle that people use to service the servers, and\nFigure 6.27 shows the hot aisle. The hot air from the hot aisle rises through ducts\ninto the ceiling.\nFigure 6.27 Hot aisle in a Google data center, which is clearly not designed to accommodate people.\n506\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 539,
        "text": "In conventional data centers, each server relies on internal fans to ensure a suf-\nficient flow of cool air over the hot chips to maintain their temperature. These\nmechanical fans are one of the weakest components in servers; for example, the\nMTBF of fans is 150,000 h versus 1,200,000 h for disks. In a Google WSC, the\nserver fans work synergistically with dozens of giant fans in the room to ensure\nairflow for the whole room (Figure 6.28). This division of labor means the small\nserver fans use as little power as possible while delivering maximum performance\nat the worst-case power and ambient conditions. The large fans are controlled using\nair pressure as the control variable. The fan speeds are adjusted to maintain a min-\nimum pressure difference between the hot and cold aisles.\nTo cool this hot air, they add large-scale fan-coils at either end of the rows of\nracks. Hot air from the racks is delivered to the fan-coils above via a horizontal\nplenum inside the hot aisle. (Two rows share the pair of cooling coils, as they\nare placed above the cold aisle between the two rows.) The cooled air is sent\nvia a plenum in the ceiling to the wall with the big fans in Figure 6.28, which return\nthe cooled air to the room containing the racks.\nWe\u2019ll describe how to remove the heat from the water in the cooling coils\nshortly, but let\u2019s reflect on the architecture so far. It separates the racks from\nthe cooling capacity provided by the fan-coils, which allows for sharing of cooling\nacross two rows of racks in the WSC. Thus, it efficiently provides more cooling to\nhigh-power racks and less to low-power racks. With thousands of racks in a WSC,\nthey are unlikely to be identical, so power variability between racks is common,\nwhich this design accommodates.\nCool water is supplied to the individual fan-coils via a network of pipes from a\ncooling plant. Heat is transferred into the water via forced convection in the cooling\ncoils, and warm water returns to a cooling plant.\nFigure 6.28 The cool air blows into the room containing the aisles of servers. The hot\nair goes through large vents into the ceilings where it is cooled before returning to\nthese fans.\n6.7\nPutting It All Together: A Google Warehouse-Scale Computer\n\u25a0\n507"
    },
    {
        "page": 540,
        "text": "To improve the efficiency of WSCs, architects try to use the local environment\nto remove the heat whenever possible. Evaporative cooling towers are common in\nWSCs to leverage the colder outside air to cool the water instead of it being chilled\nmechanically. The temperature that matters is called the wet-bulb temperature,\nwhich is the lowest temperature that can be achieved by evaporating water with\nair. It is the temperature a parcel of air would have if it were cooled to saturation\n(100% relative humidity) by the evaporation of water into it, with the latent heat\nbeing supplied by the parcel. Wet-bulb temperature is measured by blowing air at\nthe bulb end of a thermometer that has water on it.\nWarm water is sprayed inside in the cooling tower and collected in pools at the\nbottom, transferring heat to the outside air via evaporation and thereby cooling the\nwater. This technique is called water-side economization. Figure 6.29 shows the\nsteam rising above cooling towers. An alternative is to use cold water instead of\ncrisp air. Google\u2019s WSC in Finland uses a water-to-water heat exchanger that takes\nthe frigid water from the Gulf of Finland to chill the warm water from inside\nthe WSC.\nThe cooling tower system uses water caused by evaporation in the cooling\ntowers. For example, an 8-MW facility might need 70,000\u2013200,000 gallons of\nwater per day, thus the desire for the WSC to be located near ample sources\nof water.\nAlthough the cooling plant is designed so that heat can be removed without\nartificial cooling most of the time, mechanical chillers aid in rejecting the heat\nin some regions when the weather is warm.\nFigure 6.29 Steam rising from the cooling towers that transfer heat to the air from the water used to cool\nequipment.\n508\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 541,
        "text": "Racks of a Google WSC\nWe saw how Google gets power to rack and how it cools the hot air that exhausts\nfrom the rack. Now we\u2019re ready to explore the rack itself. Figure 6.30 shows a\ntypical rack found inside a Google WSC. To put this rack into context, a WSC\nconsists of multiple arrays (which Google calls clusters). Although arrays vary\nin size, some have one to two dozen rows with each row holding two to three\ndozen racks.\nThe 20 slots shown the middle of the rack in Figure 6.30 hold the servers.\nDependingontheirwidth,uptofourserverscanbeplacedinasingletray.Thepower\nconverters near the top of the rack turn the 240 V AC current into 48 V DC, which is\nrun on copper bus bars down the back of the rack to power the servers.\nThe diesel generators that provide backup power for the whole WSC take tens\nof seconds before they can offer power. Instead of populating a large room with\nBattery\nbackup\nPower\nconversion\nNetwork\nswitches\nConfigurable\npayload bay\nFigure 6.30 A Google rack for its WSC. Its dimensions are about 7 ft high, 4 ft wide,\nand 2 ft deep (2 m\u00d71.2 m\u00d70.5 m). The Top of Rack switches are indeed at the top of\nthis rack. Next comes the power converter that converts from 240 V AC to 48 V DC for\nthe servers in the rack using a bus bar at the back of the rack. Next is the 20 slots\n(depending on the height of the server) that can be configured for the various types\nof servers that can be placed in the rack. Up to four servers can be placed per tray.\nAt the bottom of the rack are high-efficiency distributed modular DC uninterruptible\npower supply (UPS) batteries.\n6.7\nPutting It All Together: A Google Warehouse-Scale Computer\n\u25a0\n509"
    },
    {
        "page": 542,
        "text": "enough batteries to power the whole WSC for several minutes\u2014which was a com-\nmon practice in the early WSCs\u2014Google puts small batteries at the bottom of each\nrack. Because UPS is distributed to each rack, the cost is incurred only as racks are\ndeployed, instead of paying upfront for the UPS capacity of a full WSC. These\nbatteries are also better than the traditional batteries because they are on the DC\nside after the voltage conversions, and they use an efficient charging scheme. In\naddition, replacing the 94%-efficient lead batteries with the 99.99%-efficient local\nUPS helps to lower the PUE. It\u2019s a very efficient UPS system.\nIt is comforting that the top of the rack in Figure 6.30 does indeed contain the\nTop of Rack switch, which we describe next.\nNetworking in a Google WSC\nThe Google WSC network uses a topology called Clos, which is named after the\ntelecommunications expert who invented it (Clos, 1953). Figure 6.31 shows the\nstructure of the Google Clos network. It is a multistage network that uses low\nport-count (\u201clow radix\u201d) switches, offers fault tolerance, and increases both the\nnetwork scale and its bisection bandwidth. Google increases the scale simply by\nadding stages to the multistage network. The fault tolerance is provided by its\ninherent redundancy, which means a failure of any link has only a small impact\non the overall network capacity.\nAs Section 6.6 describes, Google builds customer switches from standard\ncommodity switch chips and uses centralized control for network routing and man-\nagement. Every switch is given a consistent copy of the current topology of the\nnetwork, which simplifies the more complex routing of a Clos network.\nSpine\nBlock 1\nEdge Aggregation\nBlock 1\nEdge Aggregation\nBlock 2\nEdge Aggregation\nBlock N\nSpine\nBlock 2\nSpine\nBlock 3\nSpine\nBlock 4\nSpine\nBlock M\nServer\nracks\nwith ToR\nswitches\nFigure 6.31 A Clos network has three logical stages containing crossbar switches: ingress, middle, and egress.\nEach input to the ingress stage can go through any of the middle stages to be routed to any output of the egress\nstage. In this figure, the middle stages are the M Spine Blocks, and the ingress and egress stages are in the N Edge\nActivation Blocks. Figure 6.22 shows the changes in the Spine Blocks and the Edge Aggregation Blocks over many\ngenerations of Clos networks in Google WSCs.\n510\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 543,
        "text": "The latest Google switch is Jupiter, which is the switch\u2019s sixth generation.\nFigure 6.32 shows the building blocks of the switch, and Figure 6.33 shows the\nwiring of the middle blocks housed in racks. All the cables use bundles of optical\nfibers.\nThe commodity switch chip for Jupiter is a 16\u000116 crossbar using 40 Gbps\nlinks. The Top of Rack switch has four of these chips, which are configured with 48\n40-Gbps links to the servers and 16 40-Gbps links to the network fabric, yielding\nan oversubscription of just 3:1, which is better than earlier generations. Moreover,\nthis generation was the first time that servers were offered with 40-Gbps links.\nThe middle blocks in Figures 6.32 and 6.33 consist of 16 of the switch chips.\nThey use two stages, with 256 10-Gbps links for the Top of Rack connectivity and\n64 40-Gbps links to connect to the rest of the network fabric through the spine.\nEach of the chips in the Top of Rack switch connects to eight middle blocks using\ndual redundant 10-Gbps links.\nEach aggregation block is connected to the spine block with 512 40-Gbps\nlinks. A spine block uses 24 switch chips to offer 128 40-Gbps ports to the aggre-\ngation blocks. At the largest scale, they use 64 aggregation blocks to provide dual\nredundant links. At this maximum size, the bisection bandwidth is an impressive\n1.3 Pbit (1015) per second.\nNote that the whole Internet might have a bisection bandwidth of just\n0.2 Pbit/s; one reason is that Jupiter was built for a high bisection bandwidth,\nbut the Internet was not.\nMerchant\nsilicon\nMiddle block (MB)\nSpine block\nCentauri\n16\u00d740G\n32\u00d7 40G down\n128 \u00d7 40G down to 64 aggregation blocks\nAggregation block (512 \u00d740G to 256 spine blocks)\n256 \u00d710G down\n32\u00d7 40G up\n64\u00d740G up\n1 \u00d740G\n1\u00d740G\n2\u00d710G\n\u00d732\nMB\n1\nMB\n2\nMB\n3\nMB\n4\nMB\n5\nMB\n6\nMB\n7\nMB\n8\nFigure 6.32 Building blocks of the Jupiter Clos network.\n6.7\nPutting It All Together: A Google Warehouse-Scale Computer\n\u25a0\n511"
    },
    {
        "page": 544,
        "text": "Servers in a Google WSC\nNow that we have seen how to power, cool, and communicate, we are finally ready\nto see the computers that do the actual work of the WSC.\nThe example server in Figure 6.34 has two sockets, each containing an 18-core\nIntel Haswell processor running at 2.3 GHz (see Section 5.8). The photo shows\n16 DIMMs, and these servers are typically deployed with 256 GB total of\nDDR3-1600 DRAM. The Haswell memory hierarchy has two 32 KiB L1 caches,\na 256 KiB L2 cache, and 2.5 MiB of L3 cache per core, resulting in a 45 MiB L3\ncache. The local memory bandwidth is 44 GB/s with a latency of 70 ns, and the\nintersocket bandwidth is 31 GB/s with a latency of 140 ns to remote memory.\nKanev et al. (2015) highlighted the differences between the SPEC benchmark suite\nand a WSC workload. An L3 cache is barely needed for SPEC, but it is useful for a\nreal WSC workload.\nThe baseline design has a single network interface card (NIC) for a 10 Gbit/s\nEthernet link, although 40 Gbit/s NICs are available. (Other cloud providers\nMiddle\nMiddle\nBlock\nBlock\nMiddle\nBlock\nFigure 6.33 Middle blocks of the Jupiter switches housed in racks. Four are packed in\na rack. A rack can hold two spine blocks.\n512\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 545,
        "text": "moved to 25 Gbit/s or multiples thereof.) While the photo in Figure 6.34 shows two\nSATA disk drives, each of which can contain up to 8 TB, the server also can\nbe configured with SSD flash drives with 1 TB of storage. The peak power of\nthe baseline is about 150 watts. Four of these servers can fit in a slot of the rack\nin Figure 6.30.\nThis baseline node is supplemented to offer a storage (or \u201cdiskfull\u201d) node. The\nsecond unit contains 12 SATA disks and is connected to the server over PCIe. Peak\npower for a storage node is about 300 watts.\nConclusion\nIn the previous edition, the Google WSC we described had a PUE of 1.23 in 2011.\nAs of 2017, the average PUE of the whole Google fleet of 16 sites dropped to 1.12,\nwith the Belgium WSC leading the way with a 1.09 PUE. The energy-saving\ntechniques include\n\u25a0\nOperating servers at higher temperatures means that air has to be chilled only to\n80+\u00b0F (27\u00b0C) instead of the traditional 64\u201371\u00b0F (18\u201322\u00b0C).\n\u25a0\nA higher target for cold air temperature helps put the facility more often within\nthe range that can be sustained by cooling towers, which are more energy-\nefficient than traditional chillers.\nFigure 6.34 An example server from a Google WSC. The Haswell CPUs (2 sockets\u000118\ncores\u00012 threads\u00bc72 \u201cvirtual cores\u201d per machine) have 2.5 MiB last level cache per core\nor 45 MiB using DDR3-1600. They use the Wellsburg Platform Controller Hub and have a\nTFP of 150 W.\n6.7\nPutting It All Together: A Google Warehouse-Scale Computer\n\u25a0\n513"
    },
    {
        "page": 546,
        "text": "\u25a0\nDeploying WSCs in temperate climates to allow use of evaporative cooling\nexclusively for large portions of the year.\n\u25a0\nAdding large fans for entire rooms to work in concert with the small fans of the\nservers to reduce energy while satisfying worst-case scenarios.\n\u25a0\nAveraging the cooling per server to whole racks of servers by deploying the\ncooling coils per row to accommodate warmer and cooler racks.\n\u25a0\nDeploying extensive monitoring hardware and software to measure actual PUE\nversus designed PUE improves operational efficiency.\n\u25a0\nOperating more servers than the worst-case scenario for the power distribution\nsystem would suggest. It is safe since it\u2019s statistically improbable that thou-\nsands of servers would all be highly busy simultaneously as long as there is\na monitoring system to off-load work in the unlikely case that they did (Fan\net al., 2007; Ranganathan et al., 2006). PUE improves because the facility is\noperating closer to its fully designed capacity, where it is at its most efficient\nbecause the servers and cooling systems are not energy-proportional. Such\nincreased utilization reduces demand for new servers and new WSCs.\nIt will be interesting to see what innovations remain to further improve the WSC\nefficiency so that we are good guardians of our environment. It is hard to imagine\nnow how engineers might halve the power and cooling overhead of a WSC prior to\nthe next edition of this book, as they did between the previous edition and this one.\n6.8\nFallacies and Pitfalls\nDespite WSC being just 15 years old, WSC architects like those at Google have\nalready uncovered many pitfalls and fallacies about WSCs, often the hard way.\nAs we said in the introduction, WSC architects are today\u2019s Seymour Crays.\nFallacy\nCloud computing providers are losing money.\nWhen AWS was announced, a popular question about cloud computing was\nwhether it was profitable at the low prices at the time. Amazon Web Services\nhas grown so large that it must be recorded separately in Amazon\u2019s quarterly\nreports. To the surprise of some, AWS has proved to be the most profitable portion\nof the company. AWS had $12.2 billion in revenue for 2016, with an operating\nmargin of 25%, whereas Amazon\u2019s retail operations had an operating margin of\nless than 3%. AWS is consistently responsible for three-fourths of Amazon\u2019s\nprofits.\nPitfall\nFocusing on average performance instead of 99th percentile performance.\nAs Dean and Barroso (2013) observed, developers of WSC services worry about\nthe tail more than they care about the mean. If some customers get terrible perfor-\nmance, that experience can drive them away to a competitor, and they\u2019ll never\nreturn.\n514\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 547,
        "text": "Pitfall\nUsing too wimpy a processor when trying to improve WSC cost-performance.\nAmdahl's Law still applies to WSC. There will be some serial work for each request\nand that can increase request latency if this work runs on a slow server (H\u20acolzle,\n2010; Lim et al., 2008). If the serial work increases latency, then the cost of using\na wimpy processor must include the software development costs to optimize the\ncode to return it to the lower latency. The larger number of threads of many slow\nservers can also be more difficult to schedule and load balance, and thus the var-\niability in thread performance can lead to longer latencies. When required to wait\nfor the longest task, a 1-in-1000 chance of bad scheduling is probably not an issue\nwith 10 tasks, but problematic with 1000 tasks.\nMany smaller servers can also lead to lower utilization because it\u2019s clearly\neasier to schedule fewer things. Finally, even some parallel algorithms get\nless efficient when the problem is partitioned too finely. The Google rule of\nthumb is to use the low-end range of server class computers (Barroso and\nH\u20acolzle, 2009).\nAs a concrete example, Reddi et al. (2010) compared embedded microproces-\nsors (Atom) and server microprocessors (Nehalem Xeon) running the Bing search\nengine. They found that the latency of a query was about three times longer on\nAtom than on Xeon. Moreover, the Xeon was more robust. As load increases\non Xeon, quality of service degrades gradually and modestly. The Atom design\nquickly violates its quality-of-service target as it tries to absorb additional load.\nAlthough the Atom design is more energy-efficient, the response time affects rev-\nenue, and the revenue loss is likely much greater than the cost savings of less\nenergy. Energy-efficient designs that cannot match the response-time goals are\nunlikely to be deployed; we\u2019ll see another version of this pitfall lesson in the next\nchapter (Section 7.9).\nThis behavior translates directly into search quality. Given the importance of\nlatency to the user, as Figure 6.12 suggests, the Bing search engine uses multiple\nstrategies to refine search results if the query latency has not yet exceeded a cutoff\nlatency. The lower latency of the larger Xeon nodes means they can spend more\ntime refining search results. Thus, even when the Atom had almost no load, it gave\nworse answers in 1% of the queries than Xeon. At normal loads, 2% of the answers\nwere worse.\nKanev et al. (2015) has more recent, yet consistent, results.\nPitfall\nInconsistent measure of PUE by different companies.\nGoogle\u2019s PUE measurements start from the power before it reaches the substa-\ntion. Some measure at the entrance to the WSC, which skips voltage step downs\nthat represent a 6% loss. There will also be different results depending on the\nseason of the year if the WSC relies on the atmosphere to help cool the system.\nFinally, some report the design goal of the WSC instead of measuring the result-\ning system. The most conservative and best PUE measurement is a running\naverage of the past 12 months of the measured PUE, starting from the feed\nof the utility.\n6.8\nFallacies and Pitfalls\n\u25a0\n515"
    },
    {
        "page": 548,
        "text": "Fallacy\nCapital costs of the WSC facility are higher than for the servers that it houses.\nAlthough a quick look at Figure 6.13 on page 453 might lead one to that conclu-\nsion, that quick glimpse ignores the length of amortization for each part of the full\nWSC. However, the facility lasts 10\u201315 years, whereas the servers need to be\nrepurchased every 3 or 4 years. Using the amortization times in Figure 6.13 of\n10 years and 3 years, respectively, the capital expenditures over a decade are\n$72 million for the facility and 3.3\u0001$67 million, or $221 million, for servers.\nThus, the capital costs for servers in a WSC over a decade are a factor of three\nhigher than for the WSC facility.\nPitfall\nTrying to save power with inactive low power modes versus active low\npower modes.\nFigure 6.3 on page 441 shows that the average utilization of servers is between\n10% and 50%. Given the concern about operational costs of a WSC from\nSection 6.4, one would think low power modes would be a huge help.\nAs Chapter 1 mentions, DRAMs or disks cannot be accessed in these inactive\nlow power modes, so they must be returned to fully active mode to read or write, no\nmatter how low the rate. The pitfall is that the time and energy required to return to\nfully active mode make inactive low power modes less attractive. Figure 6.3 shows\nthat almost all servers average at least 10% utilization, so long periods of low\nactivity might be expected, but not long periods of inactivity (Lo et al., 2014).\nIn contrast, processors still run in lower power modes at a small multiple of\nthe regular rate, so active low power modes are much easier to use. Note that\nthe time to move to fully active mode for processors is also measured in microsec-\nonds, so active low power modes also address the latency concerns about low\npower modes.\nFallacy\nGiven improvements in DRAM dependability and the fault tolerance of WSC\nsystems software, there is no need to spend extra for ECC memory in a WSC.\nBecauseECCadds8bitstoevery64bitsofDRAM,potentiallyaninthoftheDRAM\ncosts could be saved by eliminating error-correcting code (ECC), especially since\nmeasurements of DRAM have claimed failure rates of 1000\u20135000 FIT (failures\nper billion hours of operation) per megabit (Tezzaron Semiconductor, 2004).\nSchroeder et al. (2009) studied measurements of the DRAMs with ECC pro-\ntection at the majority of Google\u2019s WSCs, which was surely many hundreds of\nthousands of servers, over a 2.5-year period. They found 15\u201325 times higher\nFIT rates than had been published, or 25,000\u201370,000 failures per megabit. Failures\naffected more than 8% of DIMMs, and the average DIMM had 4000 correctable\nerrors and 0.2 uncorrectable errors per year. Measured at the server, about a third\nexperienced DRAM errors each year, with an average of 22,000 correctable errors\nand 1 uncorrectable error per year. That is, for one-third of the servers, one memory\nerror was corrected every 2.5 h. Note that these systems used the more powerful\nChipkill codes rather than the simpler SECDED codes. If the easier scheme had\nbeen used, the uncorrectable error rates would have been 4\u201310 times higher.\n516\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 549,
        "text": "In a WSC that had only parity error protection, the servers would have to reboot\nfor each memory parity error. If the reboot time were 5 min, one-third of the\nmachines would spend 20% of their time rebooting! Such behavior would lower\nthe performance of the expensive facility by about 6%. Moreover, these systems\nwould suffer many uncorrectable errors without operators being notified that they\noccurred.\nIn the early years, Google used DRAM that did not even have parity protection.\nIn 2000, during testing before shipping the next release of the search index, it\nstarted suggesting random documents in response to test queries (Barroso and\nH\u20acolzle, 2009). The reason was a stuck-at-zero fault in some DRAMs, which cor-\nrupted the new index. Google added consistency checks to detect such errors in the\nfuture. As WSC grew in size and as ECC DIMMs became more affordable, ECC\nbecame the standard in Google WSCs. ECC has the added benefit of making it\nmuch easier to find broken DIMMs during repair.\nSuch data suggest why the Fermi GPU (Chapter 4) adds ECC to its memory\nwhere its predecessors didn\u2019t even have parity protection. Moreover, these FIT\nrates cast doubts on efforts to use the Intel Atom processor in a WSC\u2014because\nof its improved power efficiency\u2014since the chip set did not support ECC DRAM.\nPitfall\nCoping effectively with microsecond delays as opposed to nanosecond\nor millisecond delays.\nBarroso et al. (2017) point out that modern computer systems make it easy for pro-\ngrammers to mitigate latencies in the nanosecond and millisecond timescales (such\nas cache and DRAM accesses at tens of nanoseconds and disk accesses at a few\nmilliseconds) but that such systems significantly lack support for microsecond-\nscale events. Programmers get a synchronous interface to the memory hierarchy,\nwith hardware doing heroic work so that such accesses appear consistent and\ncoherent (Chapter 2). Operating systems offer programmers a similar synchronous\ninterface for a disk read, with many lines of OS code enabling the safe switching to\nanother process while waiting for the disk and then returning again to the original\nprocess when the data is ready. We need new mechanisms to cope with the micro-\nsecond delays of memory technologies like Flash or the fast network interfaces like\n100 Gbit/s Ethernet.\nFallacy\nTurning off hardware during periods of low activity improves cost-performance\nof a WSC.\nFigure 6.14 on page 454 shows that the cost of amortizing the power distribution\nand cooling infrastructure is 50% higher than the entire monthly power bill. Thus,\nalthough it certainly would save some money to compact workloads and turn off\nidle machines, even if half the power were saved, the monthly operational bill\nwould be reduced only by 7%. There would also be practical problems to overcome\nbecause the extensive WSC monitoring infrastructure depends on being able to\npoke equipment and see it respond. Another advantage of energy proportionality\nand active low power modes is that they are compatible with the WSC monitoring\ninfrastructure, which allows a single operator to be responsible for more than 1000\n6.8\nFallacies and Pitfalls\n\u25a0\n517"
    },
    {
        "page": 550,
        "text": "servers. Note also that preventive maintenance is one of the important tasks that\ntake place during idle time.\nThe conventional WSC wisdom is to run other valuable tasks during periods of\nlittle activity to recoup the investment in power distribution and cooling. A prime\nexample is the batch MapReduce jobs that create indices for search. Another exam-\nple of getting value from meager utilization is spot pricing on AWS, which the\nexample in Figure 6.17 on page 461 illustrates. AWS users who are flexible about\nwhen their tasks are run can save up to a factor of four for computation by letting\nAWS schedule the tasks more flexibly using spot instances, such as when the WSC\nwould otherwise have low utilization.\n6.9\nConcluding Remarks\nInheriting the title of building the world\u2019s biggest computers, computer architects\nof WSCs are designing the large part of the future IT that supports the mobile client\nand IoT devices. Many of us use WSCs many times a day, and the number of times\nper day and the number of people using WSCs will surely increase in the next\ndecade. Already more than six billion of the seven billion people on the planet have\ncell phone subscriptions. As these devices become Internet-ready, many more\npeople from around the world will be able to benefit from WSCs.\nMoreover, the economies of scale uncovered by WSC have realized the long-\ndreamed-of goal of computing as a utility. Cloud computing means anyone any-\nwhere with good ideas and business models can tap thousands of servers to deliver\ntheir vision almost instantly. Of course, there are important obstacles that could\nlimit the growth of cloud computing around standards, privacy, the rate of growth\nof Internet bandwidth, and the pitfalls we mention in Section 6.8, but we foresee\nthem being addressed so that cloud computing can continue to flourish.\nAmong the many attractive features of cloud computing is that it offers eco-\nnomic incentives for conservation. Whereas it is hard to convince cloud computing\nproviders to turn off unused equipment to save energy given the cost of the infra-\nstructure investment, it is easy to convince cloud computing users to give up idle\ninstances since they are paying for them, whether or not they are doing anything\nuseful. Similarly, charging by use encourages programmers to use computation,\ncommunication, and storage efficiently, which can be difficult to encourage\nwithout an understandable pricing scheme. The explicit pricing also makes it\npossible for researchers to evaluate innovations in cost-performance instead of just\nperformance, because costs are now easily measured and believable. Finally, cloud\ncomputing means that researchers can evaluate their ideas at the scale of thousands\nof computers, which in the past only large companies could afford.\nWe believe that WSCs are changing the goals and principles of server design,\njust as the needs of mobile clients and IoT are changing the goals and principles of\nmicroprocessor design. Both are revolutionizing the software industry, as well.\nPerformance per dollar and performance per joule drive both mobile client\nhardware and the WSC hardware, and parallelism and domain-specific accelerators\nare key to delivering on those sets of goals. Architects will play a vital role in both\nhalves of this exciting future world.\n518\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 551,
        "text": "Looking forward, the end of Moore\u2019s Law and Dennard scaling (Chapter 1)\nmeans that the single-thread performance of the newest processors is not that much\nfaster than their predecessors, which will likely stretch the lifetimes of the servers\nin the WSCs. Thus, the money formerly spent replacing older servers will instead\nbe used to expand to the cloud, which could mean that the cloud will be even more\neconomically attractive in the next decade than it is today. The Moore\u2019s Law era\ncombined with innovations in the design and operation of WSCs caused the\nperformance-cost-energy curve of WSCs to improve continuously. With the end\nof that glorious era, plus the removal of the largest causes of inefficiency in WSCs,\nthe field will likely need to look to for innovations in computer architecture of the\nchips that populate the WSC for sustained improvement, which is the topic of the\nnext chapter.\n6.10\nHistorical Perspectives and References\nSection M.8 (available online) covers the development of clusters that were the\nfoundation of WSC and of utility computing. (Readers interested in learning more\nshould start with Barroso et al. (2013) and the blog postings of James Hamilton at\nhttp://perspectives.mvdirona.com plus his talks at the annual Amazon Re-Invent\nconference.)\nCase Studies and Exercises by Parthasarathy\nRanganathan\nCase Study 1: Total Cost of Ownership Influencing\nWarehouse-Scale Computer Design Decisions\nConcepts illustrated by this case study\n\u25a0\nTotal Cost of Ownership (TCO)\n\u25a0\nInfluence of Server Cost and Power on the Entire WSC\n\u25a0\nBenefits and Drawbacks of Low-Power Servers\nTotal cost of ownership is an important metric for measuring the effectiveness of a\nwarehouse-scale computer (WSC). TCO includes both the CAPEX and OPEX\ndescribed in Section 6.4, and reflects the ownership cost of the entire datacenter\nto achieve a certain level of performance. In considering different servers,\nnetworks, and storage architectures, TCO is often the most important comparison\nmetric used by datacenter owners to decide which options are best; however, TCO\nis a multidimensional computation that takes into account many different factors.\nThe goal of this case study is to take a detailed look into WSCs, to see how different\narchitectures influence TCO, and to understand how TCO drives operator deci-\nsions. This case study will use the numbers from Figures 6.13 and 6.14 and Section\n6.4, and assumes that the described WSC achieves the operator\u2019s target level of\nCase Studies and Exercises by Parthasarathy Ranganathan\n\u25a0\n519"
    },
    {
        "page": 552,
        "text": "performance. TCO is often used to compare different server options that have mul-\ntiple dimensions. The exercises in this case study examine how such comparisons\nare made in the context of WSCs and the complexity involved in making the\ndecisions.\n6.1\n[5/5/10] <6.2, 6.4>In this chapter, data-level parallelism has been discussed as a\nway for WSCs to achieve high performance on large problems. Conceivably, even\ngreater performance can be obtained by using high-end servers; however, higher\nperformance servers often come with a nonlinear price increase.\na. [5] <6.4>Assuming servers that are 10% faster at the same utilization, but are\n20% more expensive, what is the CAPEX for the WSC?\nb. [5] <6.4>If those servers also use 15% more power, what is the OPEX of the\nwarehouse-scale computer?\nc. [10] <6.2, 6.4>Given the speed improvement and power increase, what must\nthe cost of the new servers be to be comparable to the original cluster? (Hint:\nBased on this TCO model, you may have to change the critical load of the\nfacility.)\n6.2\n[5/10] <6.4, 6.6, 6.8>To achieve a lower OPEX, one appealing alternative is to\nuse low-power versions of servers to reduce the total electricity required to run the\nservers; however, similar to high-end servers, low-power versions of high-end\ncomponents also have nonlinear trade-offs.\na. [5] <6.4, 6.6, 6.8>If low-power server options offered 15% lower power at the\nsame performance but are 20% more expensive, are they a good trade-off?\nb. [10] <6.4, 6.6, 6.8>At what cost do the servers become comparable to the\noriginal cluster? What if the price of electricity doubles?\n6.3\n[5/10/15] <6.4, 6.6>Servers that have different operating modes offer opportuni-\nties for dynamically running different configurations in the cluster to match work-\nload usage. Use the data in Figure 6.35 for the power/performance modes for a\ngiven low-power server.\na. [5] <6.4, 6.6>If a server operator decided to save power costs by running all\nservers at medium performance, how many servers would be needed to achieve\nthe same level of performance?\nb. [10] <6.4, 6.6>What are the CAPEX and OPEX of such a configuration?\nMode\nPerformance\nPower\nHigh\n100%\n100%\nMedium\n75%\n60%\nLow\n59%\n38%\nFigure 6.35 Power\u2013performance modes for low-power servers.\n520\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 553,
        "text": "c. [15] <6.4, 6.6>If there was an alternative where you could purchase a server\nthat is 20% cheaper but x% slower and uses y% less power, find the perfor-\nmance\u2013power curve that provides a TCO comparable to the baseline server.\n6.4\n[Discussion] <6.4>Discuss the trade-offs and benefits of the two options in Exer-\ncise 6.3, assuming a constant workload being run on the servers.\n6.5\n[Discussion] <6.2, 6.4>Unlike high-performance computing (HPC) clusters,\nWSCs often experience significant workload fluctuation throughout the day. Dis-\ncuss the trade-offs and benefits of the two options in Exercise 6.3, this time assum-\ning a workload that varies.\n6.6\n[Discussion] <6.4, 6.7>The TCO model presented so far abstracts away a signif-\nicant amount of lower level details. Discuss the impact of these abstractions to the\noverall accuracy of the TCO model. When are these abstractions safe to make? In\nwhat cases would greater detail provide significantly different answers?\nCase Study 2: Resource Allocation in WSCs and TCO\nConcepts illustrated by this case study\n\u25a0\nServer and Power Provisioning within a WSC\n\u25a0\nTime Variance of Workloads\n\u25a0\nEffects of Variance on TCO\nSome of the key challenges to deploying efficient WSCs are provisioning\nresources properly and utilizing them to their fullest capacity. This problem is com-\nplex due to the size of WSCs as well as the potential variance of the workloads\nbeing run. The exercises in this case study show how different uses of resources\ncan affect TCO. Assume data from Figures 6.13 and 6.14 as appropriate.\n6.7\n[5/5/10] <6.4>One of the challenges in provisioning a WSC is determining the\nproper power load, given the facility size. As described in the chapter, nameplate\npower is often a peak value that is rarely encountered.\na. [5] <6.4>Estimate how the per-server TCO changes if the nameplate server\npower is 200 W and the cost is $3000.\nb. [5] <6.4>Also consider a higher power, but cheaper server option whose\npower is 300 W and costs $2000.\nc. [10] <6.4>How does the per-server TCO change if the actual average power\nusage of the servers is only 70% of the nameplate power?\n6.8\n[15/10] <6.2, 6.4>One assumption in the TCO model is that the critical load of\nthe facility is fixed, and the amount of servers fits that critical load. In reality, due to\nthe variations of server power based on load, the critical power used by a facility\ncan vary at any given time. Operators must initially provision the datacenter based\non its critical power resources and an estimate of how much power is used by the\ndatacenter components.\nCase Studies and Exercises by Parthasarathy Ranganathan\n\u25a0\n521"
    },
    {
        "page": 554,
        "text": "a. [15] <6.2, 6.4>Extend the TCO model to initially provision a WSC based on a\nserver with a nameplate power of 300 W, but also calculate the actual monthly\ncritical power used and TCO assuming the server averages 40% utilization and\nso consumes only 225 W. How much capacity is left unused?\nb. [10] <6.2, 6.4>Repeat this exercise with a 500-W server that averages 20%\nutilization and consumes only 300 W.\n6.9\n[10] <6.4, 6.5>WSCs are often used in an interactive manner with end users, as\nmentioned in Section 6.5. This interactive usage often leads to time-of-day fluctu-\nations, with peaks correlating to specific time periods. For example, for Netflix\nrentals there is a peak during the evening periods of 8\u201310 p.m.; the entirety of these\ntime-of-day effects is significant. Compare the per-server TCO of a datacenter with\na capacity to match the utilization at 4 a.m. compared to 9 p.m.\n6.10\n[Discussion/15] <6.4, 6.5>Discuss some options to better utilize the excess\nservers during the off-peak hours or find ways to save costs. Given the interactive\nnature of WSCs, what are some of the challenges to aggressively reducing power\nusage?\n6.11\n[Discussion/25] <6.4, 6.6, 6.8>Propose one possible way to improve TCO by\nfocusing on reducing server power. What are the challenges to evaluating your pro-\nposal? Estimate the TCO improvements based on your proposal. What are some\nadvantages and drawbacks?\nExercises\n6.12\n[10/10/10] <6.1, 6.2>One of the important enablers of WSC is ample request-\nlevel parallelism, in contrast to instruction- or thread-level parallelism. This ques-\ntion explores the implication of different types of parallelism on computer archi-\ntecture and system design.\na. [10] <6.1>Discuss scenarios where improving the instruction- or thread-level\nparallelism would provide greater benefits than those achievable through\nrequest-level parallelism.\nb. [10] <6.1, 6.2>What are the software design implications of increasing\nrequest-level parallelism?\nc. [10] <6.1, 6.2>What are potential drawbacks of increasing request-level\nparallelism?\n6.13\n[Discussion/15/15] <6.2, 6.3>When a cloud computing service provider receives\njobs consisting of multiple Virtual Machines (VMs) (e.g., a MapReduce job),\nmany scheduling options exist. The VMs can be scheduled in a round-robin man-\nner to spread across all available processors and servers, or they can be consoli-\ndated to use as few processors as possible. Using these scheduling options, if a\njob with 24 VMs was submitted and 30 processors were available in the cloud\n(each able to run up to 3 VMs), round-robin would use 24 processors, while con-\nsolidated scheduling would use 8 processors. The scheduler can also find available\nprocessor cores at different scopes: socket, server, rack, and an array of racks.\n522\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 555,
        "text": "a. [Discussion] <6.2, 6.3>Assuming that the submitted jobs are all compute-\nheavy workloads, possibly with different memory bandwidth requirements,\nwhat are the pros and cons of round-robin versus consolidated scheduling in\nterms of power and cooling costs, performance, and reliability?\nb. [15] <6.2, 6.3>Assuming that the submitted jobs are all I/O-heavy workloads,\nwhat are the pros and cons of round-robin versus consolidated scheduling, at\ndifferent scopes?\nc. [15] <6.2, 6.3>Assuming that the submitted jobs are network-heavy work-\nloads, what are the pros and cons of round-robin versus consolidated schedul-\ning, at different scopes?\n6.14\n[15/15/10/10] <6.2, 6.3>MapReduce enables large amounts of parallelism by\nhaving data-independent tasks run on multiple nodes, often using commodity hard-\nware; however, there are limits to the level of parallelism. For example, for redun-\ndancy MapReduce will write data blocks to multiple nodes, consuming disk and,\npotentially, network bandwidth. Assume a total dataset size of 300 GB, a network\nbandwidth of 1 Gb/s, a 10 s/GB map rate, and a 20 s/GB reduce rate. Also assume\nthat 30% of the data must be read from remote nodes, and each output file is written\nto two other nodes for redundancy. Use Figure 6.6 for all other parameters.\na. [15] <6.2, 6.3>Assume that all nodes are in the same rack. What is the\nexpected runtime with 5 nodes? 10 nodes? 100 nodes? 1000 nodes? Discuss\nthe bottlenecks at each node size.\nb. [15] <6.2, 6.3>Assume that there are 40 nodes per rack and that any remote\nread/write has an equal chance of going to any node. What is the expected run-\ntime at 100 nodes? 1000 nodes?\nc. [10] <6.2, 6.3>An important consideration is minimizing data movement as\nmuch as possible. Given the significant slowdown of going from local to rack\nto array accesses, software must be strongly optimized to maximize locality.\nAssume that there are 40 nodes per rack, and 1000 nodes are used in the MapRe-\nduce job. What is the runtime if remote accesses are within the same rack 20% of\nthe time? 50% of the time? 80% of the time?\nd. [10] <6.2, 6.3>Given the simple MapReduce program in Section 6.2, discuss\nsome possible optimizations to maximize the locality of the workload.\n6.15\n[20/20/10/20/20/20] <6.2, 6.3>WSC programmers often use data replication to\novercome failures in the software. Hadoop HDFS, for example, employs three-way\nreplication (one local copy, one remote copy in the rack, and one remote copy in a\nseparate rack), but it\u2019s worth examining when such replication is needed.\na. [20] <6.2>Let us assume that Hadoop clusters are relatively small, with 10\nnodes or less, and with dataset sizes of 10 TB or less. Using the failure fre-\nquency data in Figure 6.1, what kind of availability does a 10-node Hadoop\ncluster have with one-, two-, and three-way replications?\nb. [20] <6.2>Assuming the failure data in Figure 6.1 and a 1000-node Hadoop\ncluster, what kind of availability does it have with one-, two-, and three-way\nreplications? What can you reason about the benefits of replication, at scale?\nCase Studies and Exercises by Parthasarathy Ranganathan\n\u25a0\n523"
    },
    {
        "page": 556,
        "text": "c. [10] <6.2, 6.3>The relative overhead of replication varies with the amount of\ndata written per local compute hour. Calculate the amount of extra I/O traffic\nand network traffic (within and across rack) for a 1000-node Hadoop job that\nsorts 1 PB of data, where the intermediate results for data shuffling are written\nto the HDFS.\nd. [20] <6.2, 6.3>Using Figure 6.6, calculate the time overhead for two- and three-\nwayreplications.UsingthefailureratesshowninFigure6.1,comparetheexpected\nexecution times for no replication versus two- and three-way replications.\ne. [20] <6.2, 6.3>Now consider a database system applying replication on logs,\nassuming each transaction on average accesses the hard disk once and generates\n1 KB of log data. Calculate the time overhead for two- and three-way replica-\ntions. What if the transaction is executed in-memory and takes 10 \u03bcs?\nf. [20] <6.2, 6.3>Now consider a database system with ACID consistency that\nrequires two network round trips for two-phase commitment. What is the time\noverhead for maintaining consistency as well as replications?\n6.16\n[15/15/20/Discussion] <6.1, 6.2, 6.8>Although request-level parallelism allows\nmany machines to work on a single problem in parallel, thereby achieving greater\noverall performance, one of the challenges is how to avoid dividing the problem\ntoo finely. If we look at this problem in the context of service level agreements\n(SLAs), using smaller problem sizes through greater partitioning can require\nincreased effort to achieve the target SLA. Assume an SLA of 95% of queries\nrespond at 0.5 s or faster, and a parallel architecture similar to MapReduce that\ncan launch multiple redundant jobs to achieve the same result. For the following\nquestions, assume the query\u2013response time curve shown in Figure 6.36. The curve\nshows the latency of response, based on the number of queries per second, for a\nbaseline server as well as a \u201csmall\u201d server that uses a slower processor model.\nLatency (s)\n1\n3\n2.5\n1.5\n0.5\n2\n1\n0\n2\n3\n4\n5\n6\n7\n8\n9\n10\nQueries per second, for one server\nBaseline\nSmall\nFigure 6.36 Query\u2013response time curve.\n524\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 557,
        "text": "a. [15] <6.1, 6.2, 6.8>How many servers are required to achieve this SLA,\nassuming the query-response time curve shown in Figure 6.36 and the WSC\nreceiving 30,000 queries per second? How many \u201csmall\u201d servers are required\nto achieve this SLA, given this response-time probability curve? Looking only\nat server costs, how much cheaper must the \u201csmall\u201d servers be than the normal\nservers to achieve a cost advantage for the target SLA?\nb. [15] <6.1, 6.2, 6.8>Often, \u201csmall\u201d servers are also less reliable due to cheaper\ncomponents. Using the numbers from Figure 6.1, assume that the number of\nevents due to flaky machines and bad memories increases by 30%. How many\n\u201csmall\u201d servers are required now? How much cheaper must those servers be\nthan the standard servers?\nc. [20] <6.1, 6.2, 6.8>Now assume a batch processing environment. The \u201csmall\u201d\nservers provide 30% of the overall performance of the regular servers. Still\nassuming the reliability numbers from Exercise 6.15 part (b), how many \u201csmall\u201d\nnodes are required to provide the same expected throughput of a 2400-node\narray of standard servers, assuming perfect linear scaling of performance to\nnode size and an average task length of 10 min per node? What if the scaling\nis 85%? 60%?\nd. [Discussion] <6.1, 6.2, 6.8>Often the scaling is not a linear function, but\ninstead a logarithmic function. A natural response may be instead to purchase\nlarger nodes that have more computational power per node to minimize the\narray size. Discuss some of the trade-offs with this architecture.\n6.17\n[10/10/15/Discussion] <6.3, 6.8>One trend in high-end servers is toward the\ninclusion of nonvolatile flash memory in the memory hierarchy, either through\nsolid-state disks (SSDs) or PCI Express-attached cards. Typical SSDs have a band-\nwidth of 250 MB/s and latency of 75 \u03bcs, whereas PCIe cards have a bandwidth of\n600 MB/s and latency of 35 \u03bcs.\na. [10] Take Figure 6.7 and include these points in the local server hierarchy.\nAssuming that identical performance scaling factors like DRAM are accessed\nat different hierarchy levels, how do these flash memory devices compare when\naccessed across the rack? Across the array?\nb. [10] Discuss some software-based optimizations that can utilize the new level\nof the memory hierarchy.\nc. [15] As discussed in \u201cFallacies and Pitfalls\u201d (Section 6.8), replacing all disks\nwith SSDs is not necessarily a cost-effective strategy. Consider a WSC operator\nthat uses it to provide cloud services. Discuss some scenarios where using SSDs\nor other flash memory would make sense.\nd. [Discussion] Recently, some vendors have discussed new memory tech-\nnologies that are much faster than flash. As an example, look up the spec-\nifications for Intel 3D X-point memory and discuss how it would factor in\nFigure 6.7.\nCase Studies and Exercises by Parthasarathy Ranganathan\n\u25a0\n525"
    },
    {
        "page": 558,
        "text": "6.18\n[20/20/Discussion] <6.3>Memory Hierarchy: Caching is heavily used in some\nWSC designs to reduce latency, and there are multiple caching options to satisfy\nvarying access patterns and requirements.\na. [20] Let\u2019s consider the design options for streaming rich media from the Web\n(e.g., Netflix). First we need to estimate the number of videos, number of encode\nformats per video, and concurrent viewing users. Assume a streaming video pro-\nvider that has 12,000 titles for online streaming, each title having at least four\nencode formats (at 500, 1000, 1600, and 2200 kbps). Let\u2019s also assume that there\nare 100,000 concurrent viewers for the entire site, and an average video is 75 min\nlong (accounting for both 30-min shows and 2-h videos). Estimate the total\nstorage capacity, I/O and network bandwidths, and video-streaming-related\ncomputation requirements.\nb. [20] What are the access patterns and reference locality characteristics per user,\nper video, and across all videos? (Hint: Random versus sequential, good versus\npoor temporal and spatial locality, relatively small versus large working set\nsize.)\nc. [Discussion] What movie storage options exist by using DRAM, SSD, and hard\ndrives? Compare them in performance and TCO. Would new memory technol-\nogies like those in Problem 6.17(d) be useful?\n6.19\n[Discussion/20/Discussion/Discussion] <6.3>Consider a social networking web-\nsite with 100 million active users posting updates about themselves (in text and\npictures) as well as browsing and interacting with updates in their social networks.\nTo provide low latency, Facebook and many other websites use memcached as a\ncaching layer before the backend storage/database tiers. Assume that at any given\ntime the average user is browsing megabytes of content, and on any given day the\naverage user uploads megabytes of content.\na. [20] For the social networking website discussed here, how much DRAM is\nneeded to host its working set? Using servers each having 96 GB DRAM, esti-\nmate how many local versus remote memory accesses are needed to generate a\nuser\u2019s home page?\nb. [Discussion] Now consider two candidate memcached server designs, one\nusing conventional Xeon processors and the other using smaller cores, such\nas Atom processors. Given that memcached requires large physical memory\nbut has low CPU utilization, what are the pros and cons of these two designs?\nc. [Discussion] Today\u2019s tight coupling between memory modules and processors\noften requires an increase in CPU socket count in order to provide large memory\nsupport. List other designs to provide large physical memory without propor-\ntionally increasing the number of sockets in a server. Compare them based\non performance, power, costs, and reliability.\nd. [Discussion] The same user\u2019s information can be stored in both the memcached\nand storage servers, and such servers can be physically hosted in different ways.\nDiscuss the pros and cons of the following server layout in the WSC: (1)\n526\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 559,
        "text": "memcached collocated on the same storage server, (2) memcached and storage\nservers on separate nodes in the same rack, or (3) memcached servers on the\nsame racks and storage servers collocated on separate racks.\n6.20\n[5/5/10/10/Discussion/Discussion/Discussion] <6.3, 6.5, 6.6>Datacenter Net-\nworking: MapReduce and WSC are a powerful combination to tackle large-scale\ndata processing. For this problem, we will assume we sort one petabyte (1 PB) of\nrecords in 6 h using 4000 servers and 48,000 hard drives (Google discussed doing\nthis in 2008).\na. [5]DerivediskbandwidthfromFigure6.6andassociatedtext.Howmanyseconds\ndoes it take to read the data into main memory and write the sorted results back?\nb. [5] Assuming each server has two 1 Gb/s Ethernet network interface cards\n(NICs) and the WSC switch infrastructure is oversubscribed by a factor of 4,\nhow many seconds does it take to shuffle the entire dataset across 4000 servers?\nc. [10] Assuming network transfer is the performance bottleneck for petabyte sort,\ncan you estimate what oversubscription ratio Google has in its datacenter?\nd. [10] Now let\u2019s examine the benefits of having 10 Gb/s Ethernet without\noversubscription\u2014for example, using a 48-port 10 Gb/s Ethernet (this was used\nby the 2010 Indy sort benchmark winner TritonSort). How long does it take to\nshuffle 1 PB of data?\ne. [Discussion] Compare the two approaches here: (1) the massive scale-out\napproach with high network oversubscription ratio, and (2) a relatively small-\nscale system with a high-bandwidth network. What are their potential bottle-\nnecks? What are their advantages and disadvantages, in terms of scalability\nand TCO?\nf. [Discussion] Sort and many important scientific computing workloads are\ncommunication-heavy, while many other workloads are not. List three example\nworkloads that do not benefit from high-speed networking. What EC2 instances\nwould you recommend to use for these two classes of workloads?\ng. [Discussion] Look up the various benchmarks in www.sortbenchmark.org and\nrecent winners in each category. How do these results match the insights from\nthe discussion in part (e) above? How does the cloud instance used for the most\nrecent winner of CloudSort compare with your answer in part (f) above?\n6.21\n[10/25/Discussion] <6.4, 6.6>Because of the massive scale of WSCs, it is very\nimportant to properly allocate network resources based on the workloads that are\nexpected to be run. Different allocations can have significant impacts on both the\nperformance and total cost of ownership.\na. [10] Using the numbers in the spreadsheet detailed in Figure 6.13, what is the\noversubscription ratio at each access-layer switch? What is the impact on TCO\nif the oversubscription ratio is cut in half? What if it is doubled?\nb. [25] Reducing the oversubscription ratio can potentially improve performance\nif a workload is network-limited. Assume a MapReduce job that uses 120\nCase Studies and Exercises by Parthasarathy Ranganathan\n\u25a0\n527"
    },
    {
        "page": 560,
        "text": "servers and reads 5 TB of data. Assume the same ratio of read/intermediate/out-\nput data as in Figure 6.2, Sep-09, and use Figure 6.6 to define the bandwidths of\nthe memory hierarchy. When reading data, assume that 50% of data is read from\nremote disks; of that, 80% is read from within the rack and 20% is read from\nwithin the array. For intermediate data and output data, assume that 30% of the\ndata uses remote disks; of that, 90% is within the rack and 10% is within the\narray. What is the overall performance improvement when reducing the over-\nsubscription ratio by half? What is the performance if the oversubscription ratio\nis doubled? Calculate the TCO in each case.\nc. [Discussion] We are seeing the trend to more cores per system. We are also see-\ning the increasing adoption of optical communication (with potentially higher\nbandwidth and improved energy efficiency). How do you think these and other\nemerging technology trends will affect the design of future WSCs?\n6.22\n[5/15/15/20/25/Discussion] <6.5>Realizing the Capability of the Cloud: Imagine\nyou are the site operation and infrastructure manager of an Alexa.com top site and\nare considering using Amazon Web Services (AWS). What factors do you need to\nconsider in determining whether to migrate to AWS? What services and instance\ntypes could you use, and how much cost could you save? You can use Alexa and\nsite traffic information (e.g., Wikipedia provides page view stats) to estimate the\namount of traffic received by a top site, or you can take concrete examples from the\nWeb, such as the following example: http://2bits.com/sites/2bits.com/files/drupal-\nsingle-server-2.8-million-page-views-a-day.pdf. The slides describe an Alexa\n#3400 site that receives 2.8 million page views per day, using a single server.\nThe server has two quad-core Xeon 2.5 GHz processors with 8 GB DRAM and\nthree 15 K RPM SAS hard drives in a RAID1 configuration, and it costs about\n$400 per month. The site uses caching heavily, and the CPU utilization ranges\nfrom 50% to 250% (roughly 0.5\u20132.5 cores busy).\na. [5] Looking at the available EC2 instances (http://aws.amazon.com/ec2/\ninstance-types/), what instance types match or exceed the current server\nconfiguration?\nb. [15] Looking at the EC2 pricing information (http://aws.amazon.com/ec2/\npricing/), select the most cost-efficient EC2 instances (combinations allowed)\nto host the site on AWS. What is the monthly cost for EC2?\nc. [15] Now add the costs for IP address and network traffic to the equation, and\nsuppose the site transfers 100 GB/day in and out on the Internet. What is the\nmonthly cost for the site now?\nd. [20] AWS also offers a micro instance for free for 1 year to new customers and\n15 GB bandwidth each for traffic going in and out across AWS. Based on your\nestimation of peak and average traffic from your department Web server, can\nyou host it for free on AWS?\n528\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 561,
        "text": "e. [25] Based on the service characteristics, if a much larger site like Netflix.com\nmigrates its streaming and encoding infrastructure to AWS, what AWS services\ncould be used by Netflix and for what purposes?\nf. [Discussion] Look at similar offerings from other cloud providers (Google,\nMicrosoft, Alibaba, etc.). How do the answers to parts (a)\u2013(e) change?\ng. [Discussion] \u201cServerless computing\u201d allows you to build and run higher-level\napplications and services without thinking about specific servers. Examples\ninclude AWS Lambda, Google Cloud Functions, Microsoft Azure Functions,\netc. Continuing to wear your site operation and infrastructure manager hat,\nwhen would you consider serverless computing?\n6.23\n[Discussion/Discussion/20/20/Discussion] <6.4, 6.8>Figure 6.12 shows the\nimpact of user perceived response time on revenue, and motivates the need to\nachieve high-throughput while maintaining low latency.\na. [Discussion] Taking Web search as an example, what are the possible ways of\nreducing query latency?\nb. [Discussion] What monitoring statistics can you collect to help understand\nwhere time is spent? How do you plan to implement such a monitoring tool?\nc. [20] Assuming that the number of disk accesses per query follows a normal dis-\ntribution, with an average of 2 and standard deviation of 3, what kind of disk\naccess latency is needed to satisfy a latency SLA of 0.1 s for 95% of the queries?\nd. [20] In-memory caching can reduce the frequencies of long-latency events (e.g.,\naccessing hard drives). Assuming a steady-state hit rate of 40%, hit latency of\n0.05 s, and miss latency of 0.2 s, does caching help meet a latency SLA of 0.1 s\nfor 95% of the queries?\ne. [Discussion] When can cached content become stale or even inconsistent? How\noften can this happen? How can you detect and invalidate such content?\n6.24\n[15/15/20/Discussion] <6.4, 6.6>The efficiency of typical power supply units\n(PSUs) varies as the load changes; for example, PSU efficiency can be about\n80% at 40% load (e.g., output 40 W from a 100-W PSU), 75% when the load\nis between 20% and 40%, and 65% when the load is below 20%.\na. [15] Assume a power-proportional server whose actual power is proportional to\nCPU utilization, with a utilization curve as shown in Figure 6.3. What is the\naverage PSU efficiency?\nb. [15] Suppose the server employs 2 N redundancy for PSUs (i.e., doubles the\nnumber of PSUs) to ensure stable power when one PSU fails. What is the aver-\nage PSU efficiency?\nc. [20] Blade server vendors use a shared pool of PSUs not only to provide redun-\ndancy but also to dynamically match the number of PSUs to the server\u2019s actual\npower consumption. The HP c7000 enclosure uses up to six PSUs for a total of\n16 servers. In this case, what is the average PSU efficiency for the enclosure of\nserver with the same utilization curve?\nCase Studies and Exercises by Parthasarathy Ranganathan\n\u25a0\n529"
    },
    {
        "page": 562,
        "text": "d. [Discussion] Consider the impact of the different efficiency numbers in the con-\ntext of the broader TCO discussions in Figures 6.13 and 6.14: how do the dif-\nferent design impact the total TCO? Given these, how would you optimize\ndesigns for future warehouse-scale computers?\n6.25\n[5/Discussion/10/15/Discussion/Discussion/Discussion]\n<6.4,\n6.8>Power\nstranding is a term used to refer to power capacity that is provisioned but not used\nin a datacenter. Consider the data presented in Figure 6.37 [Fan, Weber, and\nBarroso, 2007] for different groups of machines. (Note that what this paper calls\na \u201ccluster\u201d is what we have referred to as an \u201carray\u201d in this chapter.)\na. [5] What is the stranded power at (1) the rack level, (2) the power distribution\nunit level, and (3) the array (cluster) level? What are the trends with oversub-\nscription of power capacity at larger groups of machines?\nb. [Discussion] What do you think causes the differences between power\nstranding at different groups of machines?\nc. [10] Consider an array-level collection of machines where the total machines\nnever use more than 72% of the aggregate power (this is sometimes also referred\nto as the ratio between the peak-of-sum and sum-of-peaks usage). Using the cost\nmodel in the case study, compute the cost savings from comparing a datacenter\nprovisioned for peak capacity and one provisioned for actual use.\nd. [15] Assume that the datacenter designer chose to include additional servers at the\narray level to take advantage of the stranded power. Using the example config-\nuration and assumptions in part (a), compute how many more servers can now be\nincluded in the warehouse-scale computer for the same total power provisioning.\ne. [Discussion] What is needed to make the optimization of part (d) work in a real-\nworld deployment? (Hint: Think about what needs to happen to cap power in\nthe rare case when all the servers in the array are used at peak power.)\n1\n1\n0.99\n0.98\n0.97\n0.96\n0.95\n0.8\n0.6\n0.4\n0.2\n0\n0.4\n0.5\n0.6\n7\n.\n0\n8\n.\n0\n0.9\n1\n5\n9\n.\n0\n5\n8\n.\n0\n9\n.\n0\n8\n.\n0\n5\n7\n.\n0\n7\n.\n0\n5\n6\n.\n0\nNormalized power\nNormalized power\n(a) Full distribution\n(b) Zoomed view\nCDF\nCDF\n1\nRack\nPDU\nCluster\nRack\nPDU\nCluster\nFigure 6.37 Cumulative distribution function (CDF) of a real datacenter.\n530\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 563,
        "text": "f. [Discussion] Two kinds of policies can be envisioned to manage power caps\n[Ranganathan et al., 2006]: (1) preemptive policies where power budgets are\npredetermined (\u201cdon\u2019t assume you can use more power; ask before you\ndo!\u201d) or (2) reactive policies where power budgets are throttled in the event\nof a power budget violation (\u201cuse as much power as needed until told you\ncan\u2019t!\u201d). Discuss the trade-offs between these approaches and when you would\nuse each type.\ng. [Discussion] What happens to the total stranded power if systems become more\nenergy-proportional (assume workloads similar to that of Figure 6.4)?\n6.26\n[5/20/Discussion] <6.4, 6.7>Section 6.7 discussed the use of per-server battery\nsources in the Google design. Let us examine the consequences of this design.\na. [5] Assume that the use of a battery as a mini-server-level UPS is 99.99% effi-\ncient and eliminates the need for a facility-wide UPS that is only 92% efficient.\nAssume that substation switching is 99.7% efficient and that the efficiency for\nthe PDU, step-down stages, and other electrical breakers are 98%, 98%, and\n99%, respectively. Calculate the overall power infrastructure efficiency\nimprovements from using a per-server battery backup.\nb. [20] Assume that the UPS is 10% of the cost of the IT equipment. Using the rest\nof the assumptions from the cost model in the case study, what is the break-even\npoint for the costs of the battery (as a fraction of the cost of a single server) at\nwhich the total cost of ownership for a battery-based solution is better than that\nfor a facility-wide UPS?\nc. [Discussion] What are the other trade-offs between these two approaches? In\nparticular, how do you think the manageability and failure model will change\nacross these two different designs?\n6.27\n[5/5/Discussion] <6.4>For this exercise, consider a simplified equation for the\ntotal operational power of a WSC as follows:\nTotal operational power \u00bc 1 + Cooling inefficiency multiplier\n\u00f0\n\u00de*ITequipment power:\na. [5] Assume an 8 MW datacenter at 80% power usage, electricity costs of $0.10\nper kilowatt-hour, and a cooling-inefficiency multiplier of 0.8. Compare the\ncost savings from (1) an optimization that improves cooling efficiency by\n20%, and (2) an optimization that improves the energy efficiency of the IT\nequipment by 20%.\nb. [5] What is the percentage improvement in IT equipment energy efficiency\nneeded to match the cost savings from a 20% improvement in cooling efficiency?\nc. [Discussion/10] What conclusions can you draw about the relative importance\nof optimizations that focus on server energy efficiency and cooling energy\nefficiency?\n6.28\n[5/5/Discussion] <6.4>As discussed in this chapter, the cooling equipment in\nWSCs can themselves consume a lot of energy. Cooling costs can be lowered\nCase Studies and Exercises by Parthasarathy Ranganathan\n\u25a0\n531"
    },
    {
        "page": 564,
        "text": "by proactively managing temperature. Temperature-aware workload placement is\none optimization that has been proposed to manage temperature to reduce cooling\ncosts. The idea is to identify the cooling profile of a given room and map the hotter\nsystems to the cooler spots, so that at the WSC level the requirements for overall\ncooling are reduced.\na. [5] The coefficient of performance (COP) of a computer room air conditioning\nunit (CRAC) is a measure of its efficiency, and is defined as the ratio of heat\nremoved (Q) to the amount of work necessary (W) to remove that heat. The\nCOP of a CRAC unit increases with the temperature of the air the CRAC unit\npushes into the plenum. If air returns to the CRAC unit at 20\u00b0C and we remove\n10 KW of heat with a COP of 1.9, how much energy do we expend in the CRAC\nunit? If it takes a COP of 3.1 to cool the same volume of air, but the air is\nreturned at 25\u00b0C, how much energy do we now expend in the CRAC unit?\nb. [5] Assume a workload distribution algorithm is able to match the hot work-\nloads well with the cool spots to allow the computer room air-conditioning\n(CRAC) unit to be run at a higher temperature to improve cooling efficiencies\nlike in the exercise above. What is the power savings between the two cases\ndescribed above?\nc. [Discussion] Given the scale of WSC systems, power management can be a com-\nplex, multifaceted problem. Optimizations to improve energy efficiency can be\nimplemented in hardware and in software, at the system level, and at the cluster\nlevel for the IT equipment or the cooling equipment, etc. It is important to con-\nsider these interactions when designing an overall energy-efficiency solution for\nthe WSC. Consider a consolidation algorithm that looks at server utilization and\nconsolidates different workload classes on the same server to increase server uti-\nlization (this can potentially have the server operating at a higher energy effi-\nciency if the system is not energy-proportional). How would this optimization\ninteract with a concurrent algorithm that tried to use different power states (see\nACPI, Advanced Configuration Power Interface, for some examples)? What\nother examples can you think of where multiple optimizations can potentially\nconflict with one another in a WSC? How would you solve this problem?\n6.29\n[5/10/15/20] <6.2, 6.6>Energy proportionality (sometimes also referred to as\nenergy scale-down) is the attribute of the system to consume no power when idle,\nbut more importantly gradually consume more power in proportion to the activity\nlevel and work done. In this exercise, we will examine the sensitivity of energy\nconsumption to different energy proportionality models. In the exercises below,\nunless otherwise mentioned, use the data in Figure 6.4 as the default.\na. [5] A simple way to reason about energy proportionality is to assume linearity\nbetween activity and power usage. Using just the peak power and idle power\ndata from Figure 6.4 and a linear interpolation, plot the energy-efficiency trends\nacross varying utilizations. (Energy efficiency is expressed as performance per\nwatt.) What happens if idle power (at 0% activity) is half of what is assumed in\nFigure 6.4? What happens if idle power is zero?\n532\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 565,
        "text": "b. [10] Plot the energy-efficiency trends across varying activities, but use the data\nfrom column 3 of Figure 6.4 for power variation. Plot the energy efficiency\nassuming that the idle power (alone) is half of what is assumed in Figure\n6.4. Compare these plots with the linear model in the previous exercise. What\nconclusions can you draw about the consequences of focusing purely on idle\npower alone?\nc. [15] Assume the system utilization mix in column 7 of Figure 6.4. For simplic-\nity, assume a discrete distribution across 1000 servers, with 109 servers at\n0% utilization, 80 servers at 10% utilization, etc. Compute the total performance\nand total energy for this workload mix using the assumptions in part (a) and part\n(b).\nd. [20] One could potentially design a system that has a sublinear power versus\nload relationship in the region of load levels between 0% and 50%. This would\nhave an energy-efficiency curve that peaks at lower utilizations (at the expense\nof higher utilizations). Create a new version of column 3 from Figure 6.4 that\nshows such an energy-efficiency curve. Assume the system utilization mix in\ncolumn 7 of Figure 6.4. For simplicity, assume a discrete distribution across\n1000 servers, with 109 servers at 0% utilization, 80 servers at 10% utilizations,\netc. Compute the total performance and total energy for this workload mix.\n6.30\n[15/20/20] <6.2, 6.6>This exercise illustrates the interactions of energy propor-\ntionality models with optimizations such as server consolidation and energy-\nefficient server designs. Consider the scenarios shown in Figures 6.38 and 6.39.\na. [15] Consider two servers with the power distributions shown in Figure 6.38: case\nA (the server considered in Figure 6.4) and case B (a less energy-proportional but\nmore energy-efficient server than case A). Assume the system utilization mix in\ncolumn 7 of Figure 6.4. For simplicity, assume a discrete distribution across 1000\nservers, with 109 servers at 0% utilization, 80 servers at 10% utilizations, etc., as\nActivity (%)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nPower, case A (W)\n181\n308\n351\n382\n416\n451\n490\n533\n576\n617\n662\nPower, case B (W)\n250\n275\n325\n340\n395\n405\n415\n425\n440\n445\n450\nFigure 6.38 Power distribution for two servers.\nActivity (%)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nNo. servers, case A and B \n109\n80\n153\n246\n191\n115\n51\n21\n15\n12\n8\nNo. servers, case C \n504\n6\n8\n11\n26\n57\n95\n123\n76\n40\n54\nFigure 6.39 Utilization distributions across cluster, without and with consolidation.\nCase Studies and Exercises by Parthasarathy Ranganathan\n\u25a0\n533"
    },
    {
        "page": 566,
        "text": "shown in row 1 of Figure 6.39. Assume performance variation based on column 2\nof Figure 6.4. Compare the total performance and total energy for this workload\nmix for the two server types.\nb. [20] Consider a cluster of 1000 servers with data similar to the data shown in\nFigure 6.4 (and summarized in the first rows of Figures 6.38 and 6.39). What are\nthe total performance and total energy for the workload mix with these assump-\ntions? Now assume that we were able to consolidate the workloads to model the\ndistribution shown in case C (second row of Figure 6.39). What are the total\nperformance and total energy now? How does the total energy compare with\na system that has a linear energy-proportional model with idle power of zero\nwatts and peak power of 662 W?\nc. [20] Repeat part (b), but with the power model of server B, and compare with the\nresults of part (a).\n6.31\n[10/Discussion] <6.2, 6.4, 6.6>System-Level Energy Proportionality Trends:\nConsider the following breakdowns of the power consumption of a server:\nCPU, 50%; memory, 23%; disks, 11%; networking/other, 16%\nCPU, 33%; memory, 30%; disks, 10%; networking/other, 27%\na. [10] Assume a dynamic power range of 3.0\u0001 for the CPU (i.e., the power con-\nsumption of the CPU at idle is one-third that of its power consumption at peak).\nAssume that the dynamic range of the memory systems, disks, and the network-\ning/other categories above are, respectively, 2.0\u0001, 1.3\u0001, and 1.2\u0001. What is the\noverall dynamic range for the total system for the two cases?\nb. [Discussion/10] What can you learn from the results of part (a)? How would we\nachieve better energy proportionality at the system level? (Hint: Energy propor-\ntionality at a system level cannot be achieved through CPU optimizations alone,\nbut instead requires improvement across all components.)\n6.32\n[30] <6.4>Pitt Turner IV et al. [2008] presented a good overview of datacenter\ntier classifications. Tier classifications define site infrastructure performance. For\nsimplicity, consider the key differences as shown in Figure 6.40 (adapted from Pitt\nTurner IV et al. [2008]). Using the TCO model in the case study as a guiding frame-\nwork, compare the cost implications of the different tiers shown.\n6.33\n[Discussion] <6.4>Based on the observations in Figures 6.12 and 6.13, what can\nyou say qualitatively about the trade-offs between revenue loss from downtime and\ncosts incurred for uptime?\n6.34\n[15/Discussion] <6.4>Some recent studies have defined a metric called TPUE,\nwhich stands for \u201ctrue PUE\u201d or \u201ctotal PUE.\u201d TPUE is defined as PUE * SPUE.\nPUE, the power utilization effectiveness, is defined in Section 6.4 as the ratio of\nthe total facility power over the total IT equipment power. SPUE, or server PUE,\nis a new metric analogous to PUE, but instead applied to computing equipment.\nSPUE is defined as the ratio of total server input power to its useful power, where\nuseful power is defined as the power consumed by the electronic components\n534\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 567,
        "text": "directly involved in the computation: motherboard, disks, CPUs, DRAM, I/O cards,\nand so on. In other words, the SPUE metric captures inefficiencies associated with\nthe power supplies, voltage regulators, and fans housed on a server.\na. [15] <6.4>Consider a design that uses a higher supply temperature for the\ncomputer room air conditioning (CRAC) units. The efficiency of the CRAC\nunit is approximately a quadratic function of the temperature, and this design\ntherefore improves the overall PUE, let\u2019s assume by 7%. (Assume baseline\nPUE of 1.7.) However, the higher temperature at the server level triggers the\non-board fan controller to operate the fan at much higher speeds. The fan power\nis a cubic function of speed, and the increased fan speed leads to a degradation\nof SPUE. Assume a fan power model:\nFan power \u00bc 284*ns*ns*ns\u000475*ns*ns,\nwhere ns is the normalized fan speed=fan speed in rpm/18,000 and a baseline\nserver power of 350 W. Compute the SPUE if the fan speed increases from (1)\n10,000\u201312,500 rpm and (2) 10,000\u201318,000 rpm. Compare the PUE and TPUE\nin both these cases. (For simplicity, ignore the inefficiencies with power deliv-\nery in the SPUE model.)\nb. [Discussion]Part(a)illustratesthat,whilePUEisanexcellentmetrictocapturethe\noverhead of the facility, it does not capture the inefficiencies within the IT equip-\nment itself. Can you identify another design where changes to the TPUE are\npotentially lower than the changes to traditional PUE? (Hint: See Exercise 6.26.)\n6.35\n[Discussion/30/Discussion] <6.2>Two benchmarks provide a good starting point\nfor energy-efficiency accounting in servers\u2014the SPECpower_ssj2008 benchmark\n(available at http://www.spec.org/power_ssj2008/) and the JouleSort metric\n(available at http://sortbenchmark.org/ ).\na. [Discussion] <6.2>Look up the descriptions of the two benchmarks. How are\nthey similar? How are they different? What would you do to improve these\nbenchmarks to better address the goal of improving WSC energy efficiency?\nTier 1\nSingle path for power and cooling distributions, without \nredundant components\n99.0%\nTier 2\n(N + 1) redundancy = two power and cooling distribution \npaths\n99.7%\nTier 3\n(N + 2) redundancy = three power and cooling distribution \npaths for uptime even during maintenance\n99.98%\nTier 4\nTwo active power and cooling distribution paths, with \nredundant components in each path, to tolerate any single \nequipment failure without impacting the load\n99.995%\nFigure 6.40 Overview of data center tier classifications. (Adapted from Pitt Turner IV\net al. [2008].).\nCase Studies and Exercises by Parthasarathy Ranganathan\n\u25a0\n535"
    },
    {
        "page": 568,
        "text": "b. [30] <6.2>JouleSort measures the total system energy to perform an out-of-core\nsort and attempts to derive a metric that enables the comparison of systems rang-\ning from embedded devices to supercomputers. Look up the description of the\nJouleSort metric at http://sortbenchmark.org. Download a publicly available\nversion of the sort algorithm and run it on different classes of machines\u2014a laptop,\na PC, a mobile phone, etc.\u2014or with different configurations. What can you learn\nfrom the JouleSort ratings for different setups?\nc. [Discussion] <6.2>Consider the system with the best JouleSort rating from\nyour experiments above. How would you improve the energy efficiency? For\nexample, try rewriting the sort code to improve the JouleSort rating. What does\nrunning sort in the cloud do to energy efficiency?\n6.36\n[10/10/15] <6.1, 6.2>Figure 6.1 is a listing of outages in an array of servers.\nWhen dealing with the large scale of WSCs, it is important to balance cluster\ndesign and software architectures to achieve the required uptime without incurring\nsignificant costs. This question explores the implications of achieving availability\nthrough hardware only.\na. [10] <6.1, 6.2>Assuming that an operator wishes to achieve 95% availability\nthrough server hardware improvements alone, how many events of each type\nwould have to be reduced? For now, assume that individual server crashes\nare completely handled through redundant machines.\nb. [10] <6.1, 6.2>How does the answer to part (a) change if the individual server\ncrashes are handled by redundancy 50% of the time? 20% of the time? None of\nthe time?\nc. [15] <6.1, 6.2>Discuss the importance of software redundancy to achieving a\nhigh level of availability. If a WSC operator considered buying machines that\nwere cheaper but 10% less reliable, what implications would this have on the soft-\nware architecture? What are the challenges associated with software redundancy?\nd. [Discussion] <6.1>Discuss the importance of eventual consistency in how\nwarehouse-scale computers can scale.\n6.37\n[15] <6.1, 6.8>Look up the current prices of standard DDR4 DRAM versus\nDDR4 DRAM that has error-correcting code (ECC). What is the increase in price\nper bit for achieving the higher reliability that ECC provides? Using the DRAM\nprices alone, and the data provided in Section 6.8, what is the uptime per dollar\nof a WSC with non-ECC versus ECC DRAM?\n6.38\n[5/Discussion] <6.1, 6.8>WSC Reliability and Manageability Concerns:\na. [5] Consider a cluster of servers costing $2000 each. Assuming an annual fail-\nure rate of 5%, an average of an hour of service time per repair, and replacement\nparts requiring 10% of the system cost per failure, what is the annual mainte-\nnance cost per server? Assume an hourly rate of $100 per hour for a service\ntechnician.\n536\n\u25a0\nChapter Six Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism"
    },
    {
        "page": 569,
        "text": "b. [Discussion] Comment on the differences between this manageability model\nversus that in a traditional enterprise datacenter with a large number of small-\nor medium-sized applications each running on its own dedicated hardware\ninfrastructure.\nc. [Discussion] Discuss the trade-offs in having heterogeneous machines in a\nwarehouse-scale computer.\n6.39\n[Discussion] <6.4, 6.7, 6.8>The OpenCompute project at www.opencompute.\norg provides a community to design and share efficient designs for warehouse-\nscale computers. Look at some of the recently proposed designs. How do they\ncompare with the design trade-offs discussed in this chapter? How do the designs\ndiffer from the Google case study discussed in Section 6.7?\n6.40\n[15/15] <6.3, 6.4, 6.5>Assume that the MapReduce job from Page #438 in Sec-\ntion 6.2 is executing a task with 2^40 bytes of input data, 2^37 bytes of intermediate\ndata, and 2^30 bytes of output data. This job is entirely memory/storage bound, so\nits performance can be quantified by the DRAM/Disk bandwidth of Figure 6.6.\na. How much does the job cost to run on m4.16xlarge and m4.large in Figure 6.15?\nWhich EC2 instance provides better performance? Which EC2 instance pro-\nvides better cost?\nb. How much would the job cost if an SSD was added to the system, as in m3.\nmedium? How do the performance and cost of m3.medium compare with\nthe best instance from part (a) above?\n6.41\n<6.1, 6.4>[5/5/10/Discussion] Imagine you have created a web service that runs\nvery well (responds within 100 ms latency) 99% of the time, and has performance\nissues 1% of the time (maybe the CPU went into a lower power state and the\nresponse took 1000 ms, etc.).\na. [5] Your service grows popular, and you now have 100 servers and your com-\nputation has to touch all these servers to handle the user request. What is the\npercentage of time your query is likely to have a slow response time, across\n100 servers?\nb. [5] Instead of \u201ctwo nines\u201d (99%) single server latency SLA, how many \u201cnines\u201d\ndo we need to have for the single server latency SLA so that the cluster latency\nSLA has bad latencies only 10% of the time or lower?\nc. [10] How do the answers to parts (a) and (b) change if we have 2000 servers?\nd. [Discussion] Section 6.4 (page 452) discusses \u201ctail-tolerant\u201d designs. What\nkind of design optimizations would you need to make in your web service\n(Hint: Look at the \u201cTail at Scale\u201d paper from Dean and Barroso [2013]).\nCase Studies and Exercises by Parthasarathy Ranganathan\n\u25a0\n537"
    },
    {
        "page": 570,
        "text": "7.1\nIntroduction\n540\n7.2\nGuidelines for DSAs\n543\n7.3\nExample Domain: Deep Neural Networks\n544\n7.4\nGoogle\u2019s Tensor Processing Unit, an Inference Data Center Accelerator 557\n7.5\nMicrosoft Catapult, a Flexible Data Center Accelerator\n567\n7.6\nIntel Crest, a Data Center Accelerator for Training\n579\n7.7\nPixel Visual Core, a Personal Mobile Device Image Processing Unit\n579\n7.8\nCross-Cutting Issues\n592\n7.9\nPutting It All Together: CPUs Versus GPUs Versus DNN Accelerators\n595\n7.10\nFallacies and Pitfalls\n602\n7.11\nConcluding Remarks\n604\n7.12\nHistorical Perspectives and References\n606\nCase Studies and Exercises by Cliff Young\n606"
    },
    {
        "page": 571,
        "text": "7\nDomain-Specific Architectures\nMoore\u2019s Law can\u2019t continue forever \u2026 We have another 10 to\n20 years before we reach a fundamental limit\nGordon Moore,\nIntel Co-Founder (2005)\nComputer Architecture. https://doi.org/10.1016/B978-0-12-811905-1.00007-9\n\u00a9 2019 Elsevier Inc. All rights reserved."
    },
    {
        "page": 572,
        "text": "7.1\nIntroduction\nGordon Moore not only predicted the amazing growth of transistors per chip in\n1965, but the opening chapter quote shows that he also predicted its demise\n50 years later. As evidence, Figure 7.1 shows that even the company he\nfounded\u2014which for decades proudly used Moore\u2019s Law as a guideline for capital\ninvestment\u2014is slowing its development of new semiconductor processes.\nDuring the semiconductor boom time, architects rode Moore\u2019s Law to create\nnovel mechanisms that could turn the cornucopia of transistors into higher perfor-\nmance. The resources for a five-stage pipeline, 32-bit RISC processor\u2014which\nneeded as little as 25,000 transistors in the 1980s\u2014grew by a factor of 100,000\nto enable features that accelerated general-purpose code on general-purpose\nprocessors, as earlier chapters document:\n\u25a0\n1st-level, 2nd-level, 3rd-level, and even 4th-level caches\n\u25a0\n512-bit SIMD floating-point units\n\u25a0\n15+ stage pipelines\n\u25a0\nBranch prediction\n\u25a0\nOut-of-order execution\n\u25a0\nSpeculative prefetching\n\u25a0\nMultithreading\n\u25a0\nMultiprocessing\nThese sophisticated architectures targeted million-line programs written in effi-\ncient languages like C++. Architects treated such code as black boxes, generally\n2000\n2002\n2004\n2006\n2008\nIntel process (nm)\n2010\n2012\n2014\n2016\n14\n22\n32\n45\n65\n90\n130\n180\n10\n100\nFigure 7.1 Time before new Intel semiconductor process technology measured\nin nm. The y-axis is log scale. Note that the time stretched previously from about\n24 months per new process step to about 30 months since 2010.\n540\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 573,
        "text": "without understanding either the internal structure of the programs or even what\nthey were trying to do. Benchmark programs like those in SPEC2017 were just\nartifacts to measure and accelerate. Compiler writers were the people at the\nhardware-software interface, which dates back to the RISC revolution in the\n1980s, but they have limited understanding of the high-level application behavior;\nthat\u2019s why compilers cannot even bridge the semantic gap between C or C++ and\nthe architecture of GPUs.\nAs Chapter 1 described, Dennard scaling ended much earlier than Moore\u2019s\nLaw. Thus more transistors switching now means more power. The energy budget\nis not increasing, and we\u2019ve already replaced the single inefficient processor with\nmultiple efficient cores. Hence, we have nothing left up our sleeves to continue\nmajor improvements in cost-performance and energy efficiency for general-\npurpose architectures. Because the energy budget is limited (because of electromi-\ngration, mechanical and thermal limits of chips), if we want higher performance\n(higher operations/second), we need to lower the energy per operation.\nFigure 7.2 is another take on the relative energy costs of memory and logic\nmentioned in Chapter 1, this time calculated as overhead for an arithmetic instruc-\ntion. Given this overhead, minor twists to existing cores may get us 10% improve-\nments, but if we want order-of-magnitude improvements while offering\nprogrammability, we need to increase the number of arithmetic operations per\ninstruction from one to hundreds. To achieve that level of efficiency, we need a\ndrastic change in computer architecture from general-purpose cores to domain-\nspecific architectures (DSAs).\nThus, just as the field switched from uniprocessors to multiprocessors in the\npast decade out of necessity, desperation is the reason architects are now working\non DSAs. The new normal is that a computer will consist of standard processors to\nrun conventional large programs such as operating systems along with domain-\nspecific processors that do only a narrow range of tasks, but they do them\nextremely well. Thus such computers will be much more heterogeneous than\nthe homogeneous multicore chips of the past.\nRISC instruction\nOverhead\n125 pJ\nALU\n32-bit addition\n7 pJ\n+\n8-bit addition\n0.2\u20130.5 pJ\n+\nSP floating point\n15\u201320 pJ\n+\nD-$\nLoad/Store\nOverhead\n150 pJ\nALU\nFigure 7.2 Energy costs in picoJoules for a 90 nm process to fetch instructions or\naccess a data cache compared to the energy cost of arithmetic operations\n(Qadeer et al., 2015).\n7.1\nIntroduction\n\u25a0\n541"
    },
    {
        "page": 574,
        "text": "Part of the argument is that the preceding architecture innovations from the past\nfew decades that leveraged Moore\u2019s Law (caches, out-of-order execution, etc.)\nmay not be a good match to some domains\u2014especially in terms of energy\nusage\u2014so their resources can be recycled to make the chip a better match to\nthe domain. For example, caches are excellent for general-purpose architectures,\nbut not necessarily for DSAs; for applications with easily predictable memory\naccess patterns or huge data sets like video that have little data reuse, multilevel\ncaches are overkill, hording area and energy that could be put to better use. There-\nfore the promise of DSAs is both improved silicon efficiency and better energy\nefficiency, with the latter typically being the more important attribute today.\nArchitects probably won\u2019t create a DSA for a large C++ program like a com-\npiler as found in the SPEC2017 benchmark. Domain-specific algorithms are\nalmost always for small compute-intensive kernels of larger systems, such as\nfor object recognition or speech understanding. DSAs should focus on the subset\nand not plan to run the entire program. In addition, changing the code of the bench-\nmark is no longer breaking the rules; it is a perfectly valid source of speedup for\nDSAs. Consequently, if they are going to make useful contributions, architects\ninterested in DSA must now shed their blinders and learn application domains\nand algorithms.\nIn addition to needing to expand their areas of expertise, a challenge for\ndomain-specific architects is to find a target whose demand is large enough to jus-\ntify allocating dedicated silicon on an SOC or even a custom chip. The nonrecur-\nring engineering (NRE) costs of a custom chip and supporting software are\namortized over the number of chips manufactured, so it is unlikely to make eco-\nnomic sense if you need only 1000 chips.\nOne way to accommodate smaller volume applications is to use reconfigurable\nchips such as FPGAs because they have lower NRE than custom chips and because\nseveral different applications may be able to reuse the same reconfigurable hard-\nware to amortize its costs (see Section 7.5). However, since the hardware is less\nefficient than custom chips, the gains from FPGAs are more modest.\nAnother DSA challenge is how to port software to it. Familiar programming\nenvironments like the C++ programming language and compiler are rarely the\nright vehicles for a DSA.\nThe rest of this chapter provides five guidelines for the design of DSAs and\nthen a tutorial on our example domain, which is deep neural networks (DNNs).\nWe chose DNNs because they are revolutionizing many areas of computing today.\nUnlike some hardware targets, DNNs are applicable to a wide range of problems,\nso we can reuse a DNN-specific architecture for solutions in speech, vision, lan-\nguage, translation, search ranking, and many more areas.\nWe follow with four examples of DSAs: two custom chips for the data center\nthat accelerate DNNs, an FPGA for the data center that accelerates many domains,\nand an image-processing unit designed for personal mobile devices (PMDs). We\nthen compare the cost-performance of the DSAs along with CPUs and GPUs using\nDNN benchmarks, and conclude with a prediction of an upcoming renaissance for\ncomputer architecture.\n542\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 575,
        "text": "7.2\nGuidelines for DSAs\nHere are five principles that generally guided the designs of the four DSAs we\u2019ll\nsee in Sections 7.4\u20137.7. Not only do these five guidelines lead to increased area and\nenergy efficiency, they also provide two valuable bonus effects. First, they lead to\nsimpler designs, which reduce the cost of NRE of DSAs (see the fallacy in\nSection 7.10). Second, for user-facing applications that are commonplace with\nDSAs, accelerators that follow these principles are a better match to the 99th-\npercentile response-time deadlines than the time-varying performance optimiza-\ntions of traditional processors, as we will see in Section 7.9. Figure 7.3 shows\nhow the four DSAs followed these guidelines.\n1. Use dedicated memories to minimize the distance over which data is moved.\nThe many levels of caches in general-purpose microprocessors use a great deal\nof area and energy trying to move data optimally for a program. For example, a\ntwo-way set associative cache uses 2.5 times as much energy as an equivalent\nsoftware-controlled scratchpad memory. By definition, the compiler writers\nand programmers of DSAs understand their domain, so there is no need for\nthe hardware to try to move data for them. Instead, data movement is reduced\nwith software-controlled memories that are dedicated to and tailored for specific\nfunctions within the domain.\n2. Invest the resources saved from dropping advanced microarchitectural optimi-\nzations into more arithmetic units or bigger memories.\nAs Section 7.1 describes, architects turned the bounty from Moore\u2019s Law into\nthe resource-intensive optimizations for CPUs and GPUs (out-of-order execu-\ntion, multithreading, multiprocessing, prefetching, address coalescing, etc.).\nGuideline\nTPU\nCatapult\nCrest\nPixel Visual Core\nDesign target\nData center ASIC\nData center FPGA\nData center ASIC\nPMD ASIC/SOC IP\n1. Dedicated\nmemories\n24 MiB Unified Buffer,\n4 MiB Accumulators\nVaries\nN.A.\nPer core: 128 KiB line\nbuffer, 64 KiB P.E.\nmemory\n2. Larger\narithmetic unit\n65,536 Multiply-\naccumulators\nVaries\nN.A.\nPer core: 256 Multiply-\naccumulators (512 ALUs)\n3. Easy\nparallelism\nSingle-threaded, SIMD,\nin-order\nSIMD, MISD\nN.A.\nMPMD, SIMD, VLIW\n4. Smaller data\nsize\n8-Bit, 16-bit integer\n8-Bit, 16-bit integer\n32-bit Fl. Pt.\n21-bit Fl. Pt.\n8-bit, 16-bit, 32-bit integer\n5. Domain-\nspecific lang.\nTensorFlow\nVerilog\nTensorFlow\nHalide/TensorFlow\nFigure 7.3 The four DSAs in this chapter and how closely they followed the five guidelines. Pixel Visual Core typ-\nically has 2\u201316 cores. The first implementation of Pixel Visual Core does not support 8-bit arithmetic.\n7.2\nGuidelines for DSAs\n\u25a0\n543"
    },
    {
        "page": 576,
        "text": "Given the superior understanding of the execution of programs in these nar-\nrower domains, these resources are much better spent on more processing units\nor larger on-chip memory.\n3. Use the easiest form of parallelism that matches the domain.\nTarget domainsforDSAsalmost alwayshaveinherentparallelism.Thekeydeci-\nsionsforaDSAarehowtotakeadvantageofthatparallelismandhowtoexposeit\nto the software. Design the DSA around the natural granularity of the parallelism\nof the domain and expose that parallelism simply in the programming model. For\nexample,withrespecttodata-levelparallelism,ifSIMDworksinthedomain,it\u2019s\ncertainly easier for the programmer and the compiler writer than MIMD. Simi-\nlarly, if VLIW can express the instruction-level parallelism for the domain, the\ndesign can be smaller and more energy-efficient than out-of-order execution.\n4. Reduce data size and type to the simplest needed for the domain.\nAs we will see, applications in many domains are typically memory-bound, so\nyou can increase the effective memory bandwidth and on-chip memory utiliza-\ntion by using narrower data types. Narrower and simpler data also let\u2019s you pack\nmore arithmetic units into the same chip area.\n5. Use a domain-specific programming language to port code to the DSA.\nAs Section 7.1 mentions, a classic challenge for DSAs is getting applications to\nrun on your novel architecture. A long-standing fallacy is assuming that your\nnew computer is so attractive that programmers will rewrite their code just for\nyour hardware. Fortunately, domain-specific programming languages were\nbecoming popular even before architects were forced to switch their attention\nto DSAs. Examples are Halide for vision processing and TensorFlow for DNNs\n(Ragan-Kelley et al., 2013; Abadi et al., 2016). Such languages make porting\napplications to your DSA much more feasible. As previously mentioned, only a\nsmall, compute-intensive portion of the application needs to run on the DSA in\nsome domains, which also simplifies porting.\nDSAs introduce many new terms, mostly from the new domains but also from\nnovel architecture mechanisms not seen in conventional processors. As we did\nin Chapter 4, Figure 7.4 lists the new acronyms, terms, and short explanations\nto aid the reader.\n7.3\nExample Domain: Deep Neural Networks\nArtificial intelligence (AI) is not only the next big wave in computing\u2014it\u2019s the\nnext major turning point in human history\u2026 the Intelligence Revolution will be\ndriven by data, neural networks and computing power. Intel is committed to\nAI [thus]\u2026 we\u2019ve added a set of leading-edge accelerants required for the\ngrowth and widespread adoption of AI.\nBrian Krzanich,\nIntel CEO (2016)\n544\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 577,
        "text": "Area\nTerm\nAcronym\nShort explanation\nGeneral\nDomain-specific\narchitectures\nDSA\nA special-purpose processor designed for a particular domain. It relies on\nother processors to handle processing outside that domain\nIntellectual\nproperty block\nIP\nA portable design block that can be integrated into an SOC. They enable a\nmarketplace where organizations offer IP blocks to others who compose them\ninto SOCs\nSystem on a chip\nSOC\nA chip that integrates all the components of a computer; commonly found in\nPMDs\nDeep neural networks\nActivation\n\u2014\nResult of \u201cactivating\u201d the artificial neuron; the output of the nonlinear\nfunctions\nBatch\n\u2014\nA collection of datasets processed together to lower the cost of fetching\nweights\nConvolutional\nneural network\nCNN\nA DNN that takes as inputs a set of nonlinear functions of spatially nearby\nregions of outputs from the prior layer, which are multiplied by the weights\nDeep neural\nnetwork\nDNN\nA sequence of layers that are collections of artificial neurons, which consist of\na nonlinear function applied to products of weights times the outputs of the\nprior layer\nInference\n\u2014\nThe production phase of DNNs; also called prediction\nLong short-term\nmemory\nLSTM\nAn RNN well suited to classify, process, and predict time series. It is a\nhierarchical design consisting of modules called cells\nMultiLayer\nperceptron\nMLP\nA DNN that takes as inputs a set of nonlinear functions of all outputs from the\nprior layer multiplied by the weights. These layers are called fully connected\nRectified linear\nunit\nReLU\nA nonlinear function that performs f(x)\u00bcmax(x,0). Other popular\nnonlinear functions are sigmoid and hyperbolic tangent (tanh)\nRecurrent neural\nnetwork\nRNN\nA DNN whose inputs are from the prior layer and the previous state\nTraining\n\u2014\nThe development phase of DNNs; also called learning\nWeights\n\u2014\nThe values learned during training that are applied to inputs; also called\nparameters\nTPU\nAccumulators\n\u2014\nThe 4096 25632-bit registers (4 MiB) that collect the output of the MMU\nand are input to the Activation Unit\nActivation unit\n\u2014\nPerforms the nonlinear functions (ReLU, sigmoid, hyperbolic tangent, max\npool, and average pool). Its input comes from the Accumulators and its output\ngoes to the Unified Buffer\nMatrix multiply\nunit\nMMU\nA systolic array of 256256 8-bit arithmetic units that perform multiply-add.\nIts inputs are the Weight Memory and the Unified Buffer, and its output is the\nAccumulators\nSystolic array\n\u2014\nAn array of processing units that in lockstep input data from upstream\nneighbors, compute partial results, and pass some inputs and results to\ndownstream neighbors\nUnified buffer\nUB\nA 24 MiB on-chip memory that holds the activations. It was sized to try to\navoid spilling activations to DRAM when running a DNN\nWeight memory\n\u2014\nAn 8 MiB external DRAM chip containing the weights for the MMU. Weights\nare transferred to a Weight FIFO before entering the MMU\nFigure 7.4 A handy guide to DSA terms used in Sections 7.3\u20137.6. Figure 7.29 on page 472 has a guide for\nSection 7.7."
    },
    {
        "page": 578,
        "text": "Artificial intelligence (AI) has made a dramatic comeback since the turn of the cen-\ntury. Instead of building artificial intelligence as a large set of logical rules, the\nfocus switched to machine learning from example data as the path to artificial intel-\nligence. The amount of data needed to learn was much greater than thought. The\nwarehouse scale computers (WSCs) of this century, which harvest and store peta-\nbytes of information found on the Internet from the billions of users and their\nsmartphones, supply the ample data. We also underestimated the amount of com-\nputation needed to learn from the massive data, but GPUs\u2014which have excellent\nsingle-precision floating-point cost-performance\u2014embedded in the thousands of\nservers of WSCs deliver sufficient computing.\nOne part of machine learning, called DNNs, has been the AI star for the past\nfive years. Example DNN breakthroughs are in language translation, which DNNs\nimproved more in a single leap than all the advances from the prior decade (Tung,\n2016; Lewis-Kraus, 2016); the switch to DNNs in the past five years reduced the\nerror rate in an image recognition competition from 26% to 3.5% (Krizhevsky\net al., 2012; Szegedy et al., 2015; He et al., 2016); and in 2016, DNNs enabled\na computer program for the first time to beat a human champion at Go (Silver\net al., 2016). Although many of these run in the cloud, they have also enabled Goo-\ngle Translate on smartphones, which we described in Chapter 1. In 2017 new, sig-\nnificant DNN results appear nearly every week.\nReaders interested in learning more about DNNs than found in this section\nshould download and try the tutorials in TensorFlow (TensorFlow Tutorials,\n2016), or for the less adventurous, consult a free online textbook on DNNs\n(Nielsen, 2016).\nThe Neurons of DNNs\nDNNs were inspired by the neuron of the brain. The artificial neuron used for neu-\nral networks simply computes the sum over a set of products of weights or param-\neters and data values that is then put through a nonlinear function to determine its\noutput. As we will see, each artificial neuron has a large fan-in and a large fan-out.\nFor an image-processing DNN, the input data would be the pixels of a photo,\nwith the pixel values multiplied by the weights. Although many nonlinear func-\ntions have been tried, a popular one today is simply f(x)\u00bcmax(x,0), which\nreturns 0 if the x is negative or the original value if positive or zero. (This simple\nfunction goes by the complicated name rectified linear unit or ReLU.) The output\nof a nonlinear function is called an activation, in that it is the output of the artificial\nneuron that has been \u201cactivated.\u201d\nA cluster of artificial neurons might process different portions of the input, and\nthe output of that cluster becomes the input to the next layer of artificial neurons. The\nlayers between the input layer and the output layer are called hidden layers. For\nimage processing, you can think of each layer as looking for different types of fea-\ntures, going from lower-level ones like edges and angles to higher-level ones like\neyes and ears. If the image-processing application was trying to decide if the image\n546\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 579,
        "text": "contained a dog, the output of the last layer could be a probability number between\n0 and 1 or perhaps a list of probabilities corresponding to a list of dog breeds.\nThe number of layers gave DNNs their name. The original lack of data\nand computing horsepower kept most neural networks relatively shallow.\nFigure 7.5 shows the number of layers for a variety of recent DNNs, the number\nof weights, and the number of operations per weight fetched. In 2017 some DNNs\nhave 150 layers.\nTraining Versus Inference\nThe preceding discussion concerns DNNs that are in production. DNN develop-\nment starts by defining the neural network architecture, picking the number and\ntype of layers, the dimensions of each layer, and the size of the data. Although\nexperts may develop new neural network architectures, most practitioners will\nchoose among the many existing designs (e.g., Figure 7.5) that have been shown\nto perform well on problems similar to theirs.\nOnce the neural architecture has been selected, the next step is to learn the\nweights associated with each edge in the neural network graph. The weights deter-\nmine the behavior of the model. Depending on the choice of neural architecture,\nthere can be anywhere from thousands to hundreds of millions of weights in a sin-\ngle model (see Figure 7.5). Training is the costly process of tuning these weights so\nthat the DNN approximates the complex function (e.g., mapping from pictures to\nthe objects in that picture) described by the training data.\nThis development phase is universally called training or learning, whereas\nthe production phase has many names: inference, prediction, scoring, implemen-\ntation, evaluation, running, or testing. Most DNNs use supervised learning in\nthat they are given a training set to learn from where the data is preprocessed\nin order to have the correct labels. Thus, in the ImageNet DNN competition\n(Russakovsky et al., 2015), the training set consists of 1.2 million photos, and\neach photo has been labeled as one of 1000 categories. Several of these categories\nName\nDNN layers\nWeights\nOperations/Weight\nMLP0\n5\n20M\n200\nMLP1\n4\n5M\n168\nLSTM0\n58\n52M\n64\nLSTM1\n56\n34M\n96\nCNN0\n16\n8M\n2888\nCNN1\n89\n100M\n1750\nFigure 7.5 Six DNN applications that represent 95% of DNN workloads for inference\nat Google in 2016, which we use in Section 7.9. The columns are the DNN name, the\nnumber of layers in the DNN, the number of weights, and operations per weight (oper-\national intensity). Figure 7.41 on page 595 goes into more detail on these DNNs.\n7.3\nExample Domain: Deep Neural Networks\n\u25a0\n547"
    },
    {
        "page": 580,
        "text": "are quite detailed, such as specific breeds of dogs and cats. The winner is deter-\nmined by evaluating a separate secret set of 50,000 photos to see which DNN has\nthe lowest error rate.\nSetting the weights is an iterative process that goes backward through the neu-\nral network using the training set. This process is called backpropagation. For\nexample, because you know the breed of a dog image in the training set, you\nsee what your DNN says about the image, and then you adjust the weights to\nimprove the answer. Amazingly, the weights at the start of the training process\nshould be set to random data, and you just keep iterating until you\u2019re satisfied with\nthe DNN accuracy using the training set.\nFor the mathematically inclined, the goal of learning is to find a function that\nmaps the inputs to the correct outputs over the multilayer neural network architec-\nture. Backpropagation stands for \u201cback propagation of errors.\u201d It calculates a gra-\ndient over all the weights as input to an optimization algorithm that tries to\nminimize the errors by updating the weights. The most popular optimization\nalgorithm for DNNs is stochastic gradient descent. It adjusts the weights propor-\ntionally to maximize the descent of the gradient obtained from backpropagation.\nReaders interested in learning more should see Nielsen (2016) or TensorFlow\nTutorials (2016).\nTraining can take weeks of computation, as Figure 7.6 shows. The inference\nphase is often below 100 ms per data sample, which is a million times less.\nAlthough training takes much longer than a single inference, the total compute time\nfor inference is a product of the number of customers of the DNN and how\nfrequently they invoke it.\nAfter training, you deploy your DNN, hoping that your training set is\nrepresentative of the real world, and that your DNN will be so popular that\nyour users will spend much more time employing it than you\u2019ve put into devel-\noping it!\nType of data\nProblem area\nSize of\nbenchmark\u2019s\ntraining set\nDNN\narchitecture\nHardware\nTraining\ntime\ntext [1]\nWord prediction\n(word2vec)\n100 billion words\n(Wikipedia)\n2-layer skip\ngram\n1 NVIDIA Titan X\nGPU\n6.2 hours\naudio [2]\nSpeech recognition\n2000 hours (Fisher\nCorpus)\n11-layer RNN\n1 NVIDIA K1200\nGPU\n3.5 days\nimages [3]\nImage\nclassification\n1 million images\n(ImageNet)\n22-layer CNN\n1 NVIDIA K20\nGPU\n3 weeks\nvideo [4]\nactivity recognition\n1 million videos\n(Sports-1M)\n8-layer CNN\n10 NVIDIA GPUs\n1 month\nFigure 7.6 Training set sizes and training time for several DNNs (Iandola, 2016).\n548\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 581,
        "text": "There are tasks that don\u2019t have training datasets, such as when trying to predict\nthe future of some real-world event. Although we won\u2019t cover it here, reinforce-\nment learning (RL) is a popular algorithm for such learning in 2017. Instead of a\ntraining set to learn from, RL acts on the real world and then gets a signal from a\nreward function, depending on whether that action made the situation better\nor worse.\nAlthough it\u2019s hard to imagine a faster changing field, only three types\nof DNNs reign as most popular in 2017: MultiLayer Perceptrons (MLPs),\nConvolutional Neural Networks (CNNs), and Recurrent Neural Networks\n(RNNs). They are all examples of supervised learning, which rely on\ntraining sets.\nMultilayer Perceptron\nMLPs were the original DNNs. Each new layer is a set of nonlinear functions F of\nweighted sum of all outputs from a prior one yn\u00bcF(Wyn\u00031). The weighted sum\nconsists of a vector-matrix multiply of the outputs with the weights (see\nFigure 7.7). Such a layer is called fully connected because each output neuron\nresult depends on all input neurons of the prior layer.\nWe can calculate the number of neurons, operations, and weights per layer for\neach of the DNN types. The easiest is MLP because it is just a vector-matrix\nDim[i]\nDim[i]\nVector matrix multiply\nNonlinear function\nDim[i-1]\nDim[i-1]\nLayer[i-1]\nLayer[i]\nnlf\nVMX\nOutput\nWeights\nInput\nnlf\nVMX\nFigure 7.7 MLP showing the input Layer[i21] on the left and the output Layer[i] on\nthe right. ReLU is a popular nonlinear function for MLPs. The dimensions of the input\nand output layers are often different. Such a layer is called fully connected because it\ndepends on all the inputs from the prior layer, even if many of them are zeros. One study\nsuggested that 44% were zeros, which presumably is in part because ReLU turns neg-\native numbers into zeros.\n7.3\nExample Domain: Deep Neural Networks\n\u25a0\n549"
    },
    {
        "page": 582,
        "text": "multiply of the input vector times the weights array. Here are the parameters and\nthe equations to determine weights and operations for inference (we count multiply\nand add as two operations):\n\u25a0\nDim[i]: Dimension of the output vector, which is the number of neurons\n\u25a0\nDim[i\u00031]: Dimension of the input vector\n\u25a0\nNumber of weights: Dim[i\u00031]Dim[i]\n\u25a0\nOperations: 2Number of weights\n\u25a0\nOperations/Weight: 2\nThis final term is the operational intensity from the Roofline model discussed in\nChapter 4. We use operations per weight because there can be millions of\nweights, which usually don\u2019t fit on the chip. For example, the dimensions of\none stage of an MLP in Section 7.9 has Dim[i\u00031]\u00bc4096 and Dim[i]\u00bc2048,\nso for that layer, the number of neurons is 2048, number of weights is\n8,388,608, the number of operations is 16,777,216, and the operational intensity\nis 2. As we recall from the Roofline model, low operational intensity makes it\nharder to deliver high performance.\nConvolutional Neural Network\nCNNs are widely used for computer vision applications. As images have a two-\ndimensional structure, neighboring pixels are the natural place to look to find rela-\ntionships. CNNs take as inputs a set of nonlinear functions from spatially nearby\nregions of outputs from the prior layer and then multiplies by the weights, which\nreuses the weights many times.\nThe idea behind CNNs is that each layer raises the level of abstraction of the\nimage. For example, the first layer might identify only horizontal lines and vertical\nlines. The second layer might combine them to identify corners. The next step\nmight be rectangles and circles. The following layer could use that input to detect\nportions of a dog, like eyes or ears. The higher layers would be trying to identify\ncharacteristics of different breeds of dogs.\nEach neural layer produces a set of two-dimensional feature maps, where each\ncell of the two-dimensional feature map is trying to identify one feature in the cor-\nresponding area of the input.\nFigure 7.8 shows the starting point where a 22 stencil computation from the\ninput image creates the elements of the first feature map. A stencil computation\nuses neighboring cells in a fixed pattern to update all the elements of an array.\nThe number of output feature maps will depend on how many different features\nyou are trying to capture from the image and the stride used to apply the stencil.\nThe process is actually more complicated because the image is usually not just\na single, flat two-dimensional layer. Typically, a color image will have three levels\nfor red, green, and blue. For example, a 22 stencil will access 12 elements: 22\n550\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 583,
        "text": "of red pixels, 22 of green pixels, and 22 of blue pixels. In this case, you need\n12 weights per output feature map for a 22 stencil on three input levels of\nan image.\nFigure 7.9 shows the general case of an arbitrary number of input and output\nfeature maps, which occurs after that first layer. The calculation is a three-\ndimensional stencil over all the input feature maps with a set of weights to produce\none output feature map.\nFor the mathematically oriented, if the number of input feature maps and output\nfeature maps both equal 1 and the stride is 1, then a single layer of a two-dimensional\nCNN is the same calculation as a two-dimensional discrete convolution.\nAs we see in Figure 7.9, CNNs are more complicated than MLPs. Here are the\nparameter and the equations to calculate the weights and operations:\n\u25a0\nDimFM[i\u00031]: Dimension of the (square) input Feature Map\n\u25a0\nDimFM[i]: Dimension of the (square) output Feature Map\n\u25a0\nDimSten[i]: Dimension of the (square) stencil\n\u25a0\nNumFM[i\u00031]: Number of input Feature Maps\n\u25a0\nNumFM[i]: Number of output Feature Maps\n\u25a0\nNumber of neurons: NumFM[i]DimFM[i]2\nOutput feature map\nWeights\nInput image\nnlf\nVMX\nVector matrix multiply\nNonlinear function\nnlf\nVMX\nFigure 7.8 Simplified first step of a CNN. In this example, every group of four pixels of\nthe input image are multiplied by the same four weights to create the cells of the output\nfeature map. The pattern depicted shows a stride of two between the groups of input\npixels, but other strides are possible. To relate this figure to MLP, you can think of each\n22 convolution as a tiny fully connected operation to produce one point of the output\nfeature map. Figure 7.9 shows how multiple feature maps turn the points into a vector in\nthe third dimension.\n7.3\nExample Domain: Deep Neural Networks\n\u25a0\n551"
    },
    {
        "page": 584,
        "text": "\u25a0\nNumber of weights per output Feature Map: NumFM[i\u00031]DimSten[i]2\n\u25a0\nTotal number of weights per layer: NumFM[i]Number of weights per output\nFeature Map\n\u25a0\nNumber of operations per output Feature Map: 2DimFM[i]2Number of\nweights per output Feature Map\n\u25a0\nTotal number of operations per layer: NumFM[i]Number of operations per\noutput Feature Map\u00bc2DimFM[i]2NumFM[i]Number of weights per\noutput Feature Map\u00bc2DimFM[i]2Total number of weights per layer\n\u25a0\nOperations/Weight: 2DimFM[i]2\nA CNN in Section 7.9 has a layer with DimFM[i\u00031]\u00bc28, DimFM[i]\u00bc14, Dim-\nSten[i]\u00bc3, NumFM[i\u00031]\u00bc64 (number of input feature maps), and NumFM[i]\u00bc\n128 (number of output feature maps). That layer has 25,088 neurons, 73,728\nweights, does 28,901,376 operations, and has an operational intensity of 392.\nAs our example indicates, CNN layers generally have fewer weights and greater\noperational intensity than the fully connected layers found in MLPs.\nVector matrix multiply\nNonlinear function\nnlf\nVMX\nWeights\nNumFM[i -1]\nNumFM[i]\nDimFM[i ]\nDimFM[i ]\nDimFM[i-1]\nDimFM[i-1]\nnlf\nVMX\nDimSten[i]\nDimSten[i]\nNumFM[i-1]\nLayer[i-1]\n(input feature maps)\nLayer[i]\n(output feature maps)\nFigure 7.9 CNN general step showing input feature maps of Layer[i21] on the left,\nthe output feature maps of Layer[i] on the right, and a three-dimensional stencil over\ninput feature maps to produce a single output feature map. Each output feature map\nhas its own unique set of weights, and the vector-matrix multiply happens for every one.\nThe dotted lines show future output feature maps in this figure. As this figure illustrates,\nthe dimensions and number of the input and output feature maps are often different. As\nwith MLPs, ReLU is a popular nonlinear function for CNNs.\n552\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 585,
        "text": "Recurrent Neural Network\nThe third type of DNN is RNNs, which are popular for speech recognition or lan-\nguage translation. RNNs add the ability to explicitly model sequential inputs by\nadding state to the DNN model so that RNNs can remember facts. It\u2019s analogous\nto the difference in hardware between combinational logic and a state machine. For\nexample, you might learn the gender of the person, which you would want to pass\nalong to remember later when translating words. Each layer of an RNN is a col-\nlection of weighted sums of inputs from the prior layer and the previous state. The\nweights are reused across time steps.\nLong short-term memory (LSTM) is by far the most popular RNN today.\nLSTMs mitigate a problem that previous RNNs had with their inability to remem-\nber important long-term information.\nUnlike the other two DNNs, LSTM is a hierarchical design. LSTM consists of\nmodules called cells. You can think of cells as templates or macros that are linked\ntogether to create the full DNN model, similar to how layers of an MLP line up to\nform a complete DNN model.\nFigure 7.10 shows how the LSTM cells are linked together. They are hooked\nup from left to right, connecting the output of one cell to the input of the next. They\nare also unrolled in time, which runs top down in Figure 7.10. Thus a sentence is\ninput a word at a time per iteration of the unrolled loop. The long-term and short-\nterm memory information that gives the LSTM its name is also passed top-down\nfrom one iteration to the next.\nFigure 7.11 shows the contents of an LSTM cell. As we would expect from\nFigure 7.10, the input is on the left, the output is on the right, the two memory\ninputs are at the top, and the two memory outputs are at the bottom.\nEach cell does five vector-matrix multiplies using five unique sets of weights.\nThe matrix multiply on the input is just like the MLP in Figure 7.7. Three others are\ncalled gates in that they gate or limit how much information from one source is\npassed along to the standard output or the memory output. The amount of infor-\nmation sent per gate is set by their weights. If the weights are mostly zeros or small\nvalues, then little gets through; conversely, if they are mostly large, then the gate\nlets most information flow. The three gates are called the input gate, the output\ngate, and the forget gate. The first two filter the input and output, and the last\none determines what to forget along the long-term memory path.\nThe short-term memory output is a vector-matrix multiply using the Short\nTerm Weights and the output of this cell. The short-term label is applied because\nit does not directly use any of the inputs to the cell.\nBecause the LSTM cell inputs and outputs are all connected together, the size\nof the three input-output pairs must be the same. Looking inside the cell, there are\nenough dependencies that all of the inputs and outputs are often the same size.\nLet\u2019s assume they are all the same size, called Dim.\nEven so, the vector-matrix multiplies are not all the same size. The vectors for\nthe three gate multiplies are 3Dim, because the LSTM concatenates all three\ninputs. The vector for the input multiply is 2Dim, because the LSTM\n7.3\nExample Domain: Deep Neural Networks\n\u25a0\n553"
    },
    {
        "page": 586,
        "text": "concatenates the input with the short-term memory input as the vector. The vector\nfor the last multiply is just 1Dim, because it is just the output.\nNow we can finally calculate the weights and operations:\n\u25a0\nNumber\nof\nweights\nper\ncell:\n3(3DimDim)+(2DimDim)\n+(1DimDim)\u00bc12Dim2\n\u25a0\nNumber of operations for the 5 vector-matrix multiplies per cell: 2Number\nof weights per cell\u00bc24Dim2\n\u25a0\nNumber of operations for the 3 element-wise multiplies and 1 addition (vectors\nare all the size of the output): 4Dim\n\u25a0\nTotal number of operations per cell (5 vector-matrix multiplies and the 4\nelement-wise operations): 24Dim2+4Dim\n\u25a0\nOperations/Weight: \u00042\nLSTM0\nLSTM1\n. . .\nLSTMn\n\u201cnow\u201d\nLSTM0\nLSTM1\n. . .\nLSTMn\n\u201cis\u201d\nLSTM0\nLSTM1\n. . .\nLSTMn\n\u201cthe\u201d\nTime\nLSTM0\nLSTM1\n. . .\nLSTMn\n\u201ctime\u201d\nLSTM0\nLSTM1\n. . .\nLSTMn\n<end_input>\nLSTM0\nLSTM1\n. . .\nLSTMn\n\u201cmomento\u201d\nLSTM0\nLSTM1\n. . .\nLSTMn\n\u201cel\u201d\nLSTM0\nLSTM1\n. . .\nLSTMn\n\u201ces\u201d\nLSTM0\nLSTM1\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\nLSTMn\n\u201cahora\u201d\n\u201cmomento\u201d\n\u201cel\u201d\n\u201ces\u201d\n\u201cahora\u201d\n<end_output>\nFigure 7.10 LSTM cells connected together. The inputs are on the left (English words),\nand the outputs are on the right (the translated Spanish words). The cells can be thought\nof as being unrolled over time, from top to bottom. Thus the short-term and long-term\nmemory of LSTM is implemented by passing information top-down between unrolled\ncells. They are unrolled enough to translate whole sentences or even paragraphs. Such\nsequence-to-sequence translation models delay their output until they get to the end\nof the input (Wu et al., 2016). They produce the translation in reverse order, using the most\nrecent translated word as input to the next step, so \u201cnow is the time\u201d becomes \u201cahora es el\nmomento.\u201d (This figure and the next are often shown turned 90 degrees in LSTM litera-\nture, but we\u2019ve rotated them to be consistent with Figures 7.7 and 7.8.)\n554\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 587,
        "text": "Dim is 1024 for one of the six cells of an LSTM in Section 7.9. Its number of\nweights is 12,582,912, its number of operations is 25,169,920, and its operational\nintensity is 2.0003. Thus LSTMs are like MLPs in that they typically have more\nweights and a lower operational intensity than CNNs.\nOutput\nInput\nLTMemoryin\nSTMemoryin\nLTMemoryout\nSTMemoryout\nnlf\nVMX\nnlf\nnlf\n:\n:\n:\n\u2022\n\u2022\n+\n+\n\u2022\nVMX\nVMX\nnlf\nVMX\n\u2022\nnlf\nOutput gate\nweights\nForget gate\nweights\nInput gate\nweights\nVMX\nShort term\nweights\nnlf\nVMX\nInput\nweights\nVector matrix multiply\nElement-wise multiply\nElement-wise addition\nNonlinear function\nConcatenation\nFigure 7.11 This LSTM cell contains 5 vector-matrix multiplies, 3 element-wise multiplies, 1 element-wise add,\nand 6 nonlinear functions. The standard input and short-term memory input are concatenated to form the vector\noperand for the input vector-matrix multiply. The standard input, long-term memory input, and short-term memory\ninput are concatenated to form the vector that is used in three of the other four vector-matrix multiplies. The non-\nlinear functions for the three gates are Sigmoids f(x)\u00bc1/(1+exp(\u0003x)); the others are hyperbolic tangents. (This figure\nand the previous one are often shown turned 90 degrees in LSTM literature, but we\u2019ve rotated them to be consistent\nwith Figures 7.7 and 7.8.)\n7.3\nExample Domain: Deep Neural Networks\n\u25a0\n555"
    },
    {
        "page": 588,
        "text": "Batches\nBecause DNNs can have many weights, a performance optimization is to reuse\nthe weights once they have been fetched from memory across a set of inputs,\nthereby increasing effective operational intensity. For example, an image-\nprocessing DNN might work on a set of 32 images at a time to reduce the\neffective cost of fetching weights by a factor of 32. Such datasets are called\nbatches or minibatches. In addition to improving the performance of inference,\nbackpropagation needs a batch of examples instead of one at a time in order to\ntrain well.\nLooking at an MLP in Figure 7.7, a batch can be seen as a sequence of input\nrow vectors, which you can think of as a matrix with a height dimension that\nmatches the batch size. A sequence of row vector inputs to the five matrix multi-\nplies of LSTMs in Figure 7.11 can also be considered a matrix. In both cases, com-\nputing them as matrices instead of sequentially as independent vectors improves\ncomputing efficiency.\nQuantization\nNumerical precision is less important for DNNs than for many applications. For\nexample, there is no need for double-precision floating-point arithmetic, which\nis the standard bearer of high-performance computing. It\u2019s even unclear that\nyou need the full accuracy of the IEEE 754 floating-point standard, which aims\nto be accurate within one-half of a unit in the last place of the floating-point\nsignificand.\nTo take advantage of the flexibility in numerical precision, some devel-\nopers use fixed point instead of floating point for the inference phase. (Train-\ning is almost always done in floating-point arithmetic.) This conversion is\ncalled quantization, and such a transformed application is said to be quantized\n(Vanhoucke et al., 2011). The fixed-point data width is usually 8 or 16 bits,\nwith the standard multiply-add operation accumulating at twice the width\nof the multiplies. This transformation typically occurs after training, and\nit can reduce DNN accuracy by a few percentage points (Bhattacharya and\nLane, 2016).\nSummary of DNNs\nEven this quick overview suggests that DSAs for DNNs will need to perform at\nleast these matrix-oriented operations well: vector-matrix multiply, matrix-matrix\nmultiply, and stencil computations. They will also need support for the nonlinear\nfunctions, which include at a minimum ReLU, Sigmoid, and tanh. These modest\nrequirements still leave open a very large design space, which the next four\nsections explore.\n556\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 589,
        "text": "7.4\nGoogle\u2019s Tensor Processing Unit, an Inference Data\nCenter Accelerator\nThe Tensor Processing Unit (TPU)1 is Google\u2019s first custom ASIC DSA for WSCs.\nIts domain is the inference phase of DNNs, and it is programmed using the Tensor-\nFlow framework, which was designed for DNNs. The first TPU was been deployed\nin Google data centers in 2015.\nThe heart of the TPU is a 65,536 (256256) 8-bit ALU Matrix Multiply Unit\nand a large software-managed on-chip memory. The TPU\u2019s single-threaded, deter-\nministic execution model is a good match to the 99th-percentile response-time\nrequirement of the typical DNN inference application.\nTPU Origin\nStarting as far back as 2006, Google engineers had discussions about deploying\nGPUs, FPGAs, or custom ASICs in their data centers. They concluded that the\nfew applications that could run on special hardware could be done virtually for free\nusing the excess capacity of the large data centers, and it\u2019s hard to improve on free.\nThe conversation changed in 2013 when it was projected that if people used voice\nsearch for three minutes a day using speech recognition DNNs, it would have\nrequired Google\u2019s data centers to double in order to meet computation demands.\nThat would be very expensive to satisfy with conventional CPUs. Google then\nstarted a high-priority project to quickly produce a custom ASIC for inference\n(and bought off-the-shelf GPUs for training). The goal was to improve cost-\nperformance by 10 over GPUs. Given this mandate, the TPU was designed, ver-\nified (Steinberg, 2015), built, and deployed in data centers in just 15 months.\nTPU Architecture\nTo reduce the chances of delaying deployment, the TPU was designed to be a\ncoprocessor on the PCIe I/O bus, which allows it to be plugged into existing\nservers. Moreover, to simplify hardware design and debugging, the host server\nsends instructions over the PCIe bus directly to the TPU for it to execute, rather\nthan having the TPU fetch the instructions. Thus the TPU is closer in spirit to\nan FPU (floating-point unit) coprocessor than it is to a GPU, which fetches instruc-\ntions from its memory.\nFigure 7.12 shows the block diagram of the TPU. The host CPU sends TPU\ninstructions over the PCIe bus into an instruction buffer. The internal blocks are\ntypically connected together by 256-byte-wide (2048-bits) paths. Starting in the\nupper-right corner, the Matrix Multiply Unit is the heart of the TPU. It contains\n1This section is based on the paper \u201cIn-Datacenter Performance Analysis of a Tensor Processing Unit\u201d Jouppi et al., 2017,\nof which one of your book authors was a coauthor.\n7.4\nGoogle\u2019s Tensor Processing Unit, an Inference Data Center Accelerator\n\u25a0\n557"
    },
    {
        "page": 590,
        "text": "256256 ALUs that can perform 8-bit multiply-and-adds on signed or unsigned\nintegers. The 16-bit products are collected in the 4 MiB of 32-bit Accumulators\nbelow the matrix unit. When using a mix of 8-bit weights and 16-bit activations\n(or vice versa), the Matrix Unit computes at half-speed, and it computes at a\nquarter-speed when both are 16 bits. It reads and writes 256 values per clock cycle\nand can perform either a matrix multiply or a convolution. The nonlinear functions\nare calculated by the Activation hardware.\nThe weights for the matrix unit are staged through an on-chip Weight FIFO that\nreads from an off-chip 8 GiB DRAM called Weight Memory (for inference,\nweights are read-only; 8 GiB supports many simultaneously active models).\nThe intermediate results are held in the 24 MiB on-chip Unified Buffer, which\ncan serve as inputs to the Matrix Multiply Unit. A programmable DMA controller\ntransfers data to or from CPU Host memory and the Unified Buffer.\nControl\nControl\nControl\nOff-chip I/O\nData buffer\nComputation\nControl\nPCIe Gen3 x16\ninterface\nHost interface\nControl\nControl\nUnified\nbuffer\n(local\nactivation\nstorage)\nSystolic\ndata\nsetup\nDDR3-2133\ninterfaces\nWeight FIFO\n(weight fetcher)\nAccumulators\nActivation\nNormalize / Pool\n10\nGiB/s\n167 GiB/s\n167\nGiB/s\n14 GiB/s\n30 GiB/s\n30 GiB/s\n30 GiB/s\nDDR3 DRAM chips\n14\nGiB/s\n14\nGiB/s\nInstr\nMatrix multiply\nunit\n(64K per cycle)\nFigure 7.12 TPU Block Diagram. The PCIe bus is Gen3 16. The main computation part is the light-shaded Matrix\nMultiply Unit in the upper-right corner. Its inputs are the medium-shaded Weight FIFO and the medium-shaded Uni-\nfied Buffer and its output is the medium-shaded Accumulators. The light-shaded Activation Unit performs the non-\nlinear functions on the Accumulators, which go to the Unified Buffer.\n558\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 591,
        "text": "TPU Instruction Set Architecture\nAs instructions are sent over the relatively slow PCIe bus, TPU instructions follow\nthe CISC tradition, including a repeat field. The TPU does not have a program\ncounter, and it has no branch instructions; instructions are sent from the host\nCPU. The clock cycles per instruction (CPI) of these CISC instructions are typi-\ncally 10\u201320. It has about a dozen instructions overall, but these five are the key\nones:\n1. Read_Host_Memory reads data from the CPU host memory into the\nUnified Buffer.\n2. Read_Weights reads weights from Weight Memory into the Weight FIFO as\ninput to the Matrix Unit.\n3. MatrixMultiply/Convolve causes the Matrix Multiply Unit to perform a\nmatrix-matrix multiply, a vector-matrix multiply, an element-wise matrix multi-\nply, an element-wise vector multiply, or a convolution from the Unified Buffer\ninto the Accumulators. A matrix operation takes a variable-sized B*256 input,\nmultiplies it by a 256256 constant input, and produces a B*256 output, taking\nB pipelined cycles to complete. For example, if the input were 4 vectors of 256\nelements, B would be 4, so it would take 4 clock cycles to complete.\n4. Activate performs the nonlinear function of the artificial neuron, with\noptions for ReLU, Sigmoid, tanh, and so on. Its inputs are the Accumulators,\nand its output is the Unified Buffer.\n5. Write_Host_Memory writes data from the Unified Buffer into the CPU host\nmemory.\nThe other instructions are alternate host memory read/write, set configuration, two\nversions of synchronization, interrupt host, debug-tag, nop, and halt. The CISC\nMatrixMultiply instruction is 12 bytes, of which 3 are Unified Buffer address; 2\nare accumulator address; 4 are length (sometimes 2 dimensions for convolutions);\nand the rest are opcode and flags.\nThe goal is to run whole inference models in the TPU to reduce interactions\nwith the host CPU and to be flexible enough to match the DNN needs of 2015\nand beyond, instead of just what was required for 2013 DNNs.\nTPU Microarchitecture\nThe microarchitecture philosophy of the TPU is to keep the Matrix Multiply Unit\nbusy. The plan is to hide the execution of the other instructions by overlapping their\nexecution with the MatrixMultiply instruction. Thus each of the preceding\nfour general categories of instructions have separate execution hardware (with read\nand write host memory combined into the same unit). To increase instruction par-\nallelism further, the Read_Weights instruction follows the decoupled access/\n7.4\nGoogle\u2019s Tensor Processing Unit, an Inference Data Center Accelerator\n\u25a0\n559"
    },
    {
        "page": 592,
        "text": "execute philosophy (Smith, 1982b) in that they can complete after sending their\naddresses but before the weights are fetched from Weight Memory. The matrix unit\nhas not-ready signals from the Unified Buffer and the Weight FIFO that will cause\nthe matrix unit to stall if their data are not yet available.\nNote that a TPU instruction can execute for many clock cycles, unlike the\ntraditional RISC pipeline with one clock cycle per stage.\nBecause reading a large SRAM is much more expensive than arithmetic,\nthe Matrix Multiply Unit uses systolic execution to save energy by reducing\nreads\nand\nwrites\nof\nthe\nUnified\nBuffer (Kung\nand\nLeiserson,\n1980;\nRamacher et al., 1991; Ovtcharov et al., 2015b). A systolic array is a two-\ndimensional collection of arithmetic units that each independently compute a\npartial result as a function of inputs from other arithmetic units that are con-\nsidered upstream to each unit. It relies on data from different directions arriv-\ning at cells in an array at regular intervals where they are combined. Because\nthe data flows through the array as an advancing wave front, it is similar to\nblood being pumped through the human circulatory system by the heart, which\nis the origin of the systolic name.\nFigure 7.13 demonstrates how a systolic array works. The six circles at the bot-\ntom are the multiply-accumulate units that are initialized with the weights wi. The\nstaggered input data xi are shown coming into the array from above. The 10 steps of\nthe figure represent 10 clock cycles moving down from top to bottom of the page.\nThe systolic array passes the inputs down and the products and sums to the right.\nThe desired sum of products emerges as the data completes its path through the\nsystolic array. Note that in a systolic array, the input data is read only once from\nmemory, and the output data is written only once to memory.\nIn the TPU, the systolic array is rotated. Figure 7.14 shows that the weights are\nloaded from the top and the input data flows into the array in from the left. A given\n256-element multiply-accumulate operation moves through the matrix as a diag-\nonal wave front. The weights are preloaded and take effect with the advancing\nwave alongside the first data of a new block. Control and data are pipelined to give\nthe illusion that the 256 inputs are read at once, and after a feed delay, they update\none location of each of 256 accumulator memories. From a correctness perspec-\ntive, software is unaware of the systolic nature of the matrix unit, but for perfor-\nmance, it does worry about the latency of the unit.\nTPU Implementation\nThe TPU chip was fabricated using the 28-nm process. The clock rate is 700 MHz.\nFigure 7.15 shows the floor plan of the TPU. Although the exact die size is not\nrevealed, it is less than half the size of an Intel Haswell server microprocessor,\nwhich is 662 mm2.\nThe 24 MiB Unified Buffer is almost a third of the die, and the Matrix Multiply\nUnit is a quarter, so the datapath is nearly two-thirds of the die. The 24 MiB size\nwas picked in part to match the pitch of the Matrix Unit on the die and, given the\n560\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 593,
        "text": "X3\nX2\nX1\nW11\nW21\nW12\nW22\n(A)\nW13\nW23\nW12\nX2\nW11\nX3\n(E)\nW13\nW23\nW22\nW21\nX1\n(B)\nX3\nX2\nX1\nW11\nW21\nW12\nW22\nW13\nW23\n*\n(C)\nX3\nX2\nW11\nX1\nW21\nW12\nW22\nW13\nW23\nW12\nX2\nW11\nX3\n(F)\nW13\nW23\nW22\nW21\nX1\n*\n+\n+\n(G)\nW13\nX3\nW11\nW12\nW23\nW21\nW22\nX2\n(H)\nW13\nX3\nW11\nW12\nW23\nW21\nW22\nX2\n+\ny1 = w11x1 + w12x2 + w13x3\n(I)\nW11\nW12\nW13\nW23\nX3\nW21\nW22\ny1 = w11x1 + w12x2 + w13x3\n(D)\nX3\nX2\nW11\nX1\nW21\nW12\nW22\nW13\nW23\n+\n*\n(J)\nW11\nW12\nW13\nW23\nX3\nW21\nW22\ny1 = w11x1 + w12x2 + w13x3\ny2 = w21x1 + w22x2 + w23x3\nFigure 7.13 Example of systolic array in action, from top to bottom on the page. In this example, the six weights\nare already inside the multiply-accumulate units, as is the norm for the TPU. The three inputs are staggered in time to\nget the desired effect, and in this example are shown coming in from the top. (In the TPU, the data actually comes in\nfrom the left.) The array passes the data down to the next element and the result of the computation to the right to\nthe next element. At the end of the process, the sum of products is found to the right. Drawings courtesy of Yaz Sato."
    },
    {
        "page": 594,
        "text": "Local Unified Buffer for\nactivations\n(96Kx256x8b = 24 MiB)\n29% of  chip\nMatrix multiply unit\n(256x256x8b = 64K MAC)\n24%\nHost\nInterf. 2%\nAccumulators\n(4Kx256x32b = 4 MiB) 6%\nD\nR\nA\nM\nport\nddr3\n3%\nD\nR\nA\nM\nport\nddr3\n3%\nActivation pipeline 6%\nControl 2%\nPCle\nInterface 3%\nMisc. I/O 1%\nFigure 7.15 Floor plan of TPU die. The shading follows Figure 7.14. The light data\nbuffers are 37%, the light computation units are 30%, the medium I/O is 10%, and\nthe dark control is just 2% of the die. Control is much larger (and much more difficult\nto design) in a CPU or GPU. The unused white space is a consequence of the emphasis\non time to tape-out for the TPU.\nData\nPartial sums\n+\n+\n+\nDone\n. . .\n. . .\nControl\n+\nFigure 7.14 Systolic data flow of the Matrix Multiply Unit.\n562\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 595,
        "text": "short development schedule, in part to simplify the compiler. Control is just 2%.\nFigure 7.16 shows the TPU on its printed circuit card, which inserts into existing\nservers in a SATA disk slot.\nTPU Software\nThe TPU software stack had to be compatible with that developed for CPUs and\nGPUs so that applications could be ported quickly. The portion of the application\nrun on the TPU is typically written using TensorFlow and is compiled into an API\nthat can run on GPUs or TPUs (Larabel, 2016). Figure 7.17 shows TensorFlow\ncode for a portion of an MLP.\nLike GPUs, the TPU stack is split into a User Space Driver and a Kernel\nDriver. The Kernel Driver is lightweight and handles only memory management\nand interrupts. It is designed for long-term stability. The User Space driver\nchanges frequently. It sets up and controls TPU execution, reformats data into\nTPU order, and translates API calls into TPU instructions and turns them into\nan application binary. The User Space driver compiles a model the first time\nit is evaluated, caching the program image and writing the weight image into\nthe TPU Weight Memory; the second and following evaluations run at full speed.\nThe TPU runs most models completely from inputs to outputs, maximizing the\nratio of TPU compute time to I/O time. Computation is often done one layer at a\ntime, with overlapped execution allowing the matrix unit to hide most noncritical\npath operations.\nFigure 7.16 TPU printed circuit board. It can be inserted into the slot for an SATA disk\nin a server, but the card uses the PCIe bus.\n7.4\nGoogle\u2019s Tensor Processing Unit, an Inference Data Center Accelerator\n\u25a0\n563"
    },
    {
        "page": 596,
        "text": "Improving the TPU\nThe TPU architects looked at variations of the microarchitecture to see whether\nthey could have improved the TPU.\nLike an FPU, the TPU coprocessor has a relatively easy microarchitecture\nto evaluate, so the TPU architects created a performance model and estimated\nperformance as the memory bandwidth, the matrix unit size, and the clock rate\nand number of accumulators varied. Measurements using TPU hardware coun-\nters found that the modeled performance was on average within 8% of the\nhardware.\n# Network Parameters\nn_hidden_1 = 256 # 1st layer number of features\nn_hidden_2 = 256 # 2nd layer number of features\nn_input = 784 # MNIST data input (img shape: 28*28)\nn_classes = 10 # MNIST total classes (0-9 digits)\n# tf Graph input\nx = tf.placeholder(\"float\", [None, n_input])\ny = tf.placeholder(\"float\", [None, n_classes])\n# Create model\ndef multilayer_perceptron(x, weights, biases):\n# Hidden layer with ReLU activation\nlayer_1 = tf.add(tf.matmul(x, weights[\u2019h1\u2019]), biases[\u2019b1\u2019])\nlayer_1 = tf.nn.relu(layer_1)\n# Hidden layer with ReLU activation\nlayer_2 = tf.add(tf.matmul(layer_1, weights[\u2019h2\u2019]), biases[\u2019b2\u2019])\nlayer_2 = tf.nn.relu(layer_2)\n# Output layer with linear activation\nout_layer = tf.matmul(layer_2, weights[\u2019out\u2019]) + biases[\u2019out\u2019]\nreturn out_layer\n# Store layers weight & bias\nweights = {\n\u2019h1\u2019: tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n\u2019h2\u2019: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n\u2019out\u2019: tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n}\nbiases = {\n\u2019b1\u2019: tf.Variable(tf.random_normal([n_hidden_1])),\n\u2019b2\u2019: tf.Variable(tf.random_normal([n_hidden_2])),\n\u2019out\u2019: tf.Variable(tf.random_normal([n_classes]))\n}\nFigure 7.17 Portion of the TensorFlow program for the MNIST MLP. It has two hidden 256256 layers, with each\nlayer using a ReLU as its nonlinear function.\n564\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 597,
        "text": "Figure 7.18 shows the performance sensitivity of the TPU as these parameters\nscale over the range for 0.25 to 4. (Section 7.9 lists the benchmarks used.) In\naddition to evaluating the impact of only raising clock rates (clock in Figure 7.18),\nFigure 7.18 also plots a design (clock+) that increases the clock rate and scales the\nnumber of accumulators correspondingly so that the compiler can keep more mem-\nory references in flight. Likewise, Figure 7.18 plots matrix unit expansion if the\nnumber of accumulators increase with the square of the rise in one dimension\n(matrix+), because the matrix grows in both dimensions, as well as only increasing\nthe matrix unit (matrix).\nFirst, increasing memory bandwidth (memory) has the biggest impact: perfor-\nmance improves 3 on average when memory bandwidth increases 4, because it\nreduces the time waiting for weight memory. Second, clock rate has little benefit on\naverage with or without more accumulators. Third, the average performance in\nFigure 7.18 slightly degrades when the matrix unit expands from 256256 to\n512512 for all applications, whether or not they get more accumulators. The\nissue is analogous to internal fragmentation of large pages, only worse because\nit\u2019s in two dimensions.\nConsider the 600600 matrix used in LSTM1. With a 256256 matrix unit,\nit takes nine steps to tile 600600, for a total of 18 \u03bcs of time. The larger\n512512 unit requires only four steps, but each step takes four times longer,\nor 32 \u03bcs of time. The TPU\u2019s CISC instructions are long, so decode is insignificant\nand does not hide the overhead of loading from the DRAM.\n0.0\n0.0\n0.5\n1.0\n1.5\n2.0\nScale relative to original MPU\nPerformance relative to original\nMPU\n2.5\n3.0\n3.5\n4.0\nmemory\nmatrix+\nmatrix\nclock+\nclock\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nFigure 7.18 Performance as metrics scale from 0.25\u00d7 to 4\u00d7: memory bandwidth,\nclock rate+accumulators, clock rate, matrix unit dimension+accumulators, and\none dimension of the square matrix unit This is the average performance calculated\nfrom six DNN applications in Section 7.9. The CNNs tend to be computation-bound, but\nthe MLPs and LSTMs are memory-bound. Most applications benefit from a faster mem-\nory, but a faster clock makes little difference, and a bigger matrix unit actually hurts per-\nformance. This performance model is only for code running inside the TPU and does not\nfactor in the CPU host overhead.\n7.4\nGoogle\u2019s Tensor Processing Unit, an Inference Data Center Accelerator\n\u25a0\n565"
    },
    {
        "page": 598,
        "text": "Given these insights from the performance model, the TPU architects next\nevaluated an alternative and hypothetical TPU that they might have designed\nin the same process technology if they\u2019d had more than 15 months to do so. More\naggressive logic synthesis and block design might have increased the clock rate\nby 50%. The architects found that designing an interface circuit for GDDR5\nmemory, as used by the K80, would improve Weight Memory bandwidth by\nmore than a factor of five. As Figure 7.18 shows, increasing clock rate to\n1050 MHz, but not helping memory, made almost no change in performance.\nIf the clock is left at 700 MHz, but it uses GDDR5 instead for Weight Memory,\nperformance is increased by 3.2, even accounting for the host CPU overhead of\ninvoking the DNN on the revised TPU. Doing both does not improve average\nperformance further.\nSummary: How TPU Follows the Guidelines\nDespite living on an I/O bus and having relatively little memory bandwidth that\nlimits full utilization of the TPU, a small fraction of a big number can, nonetheless,\nbe relatively large. As we will see in Section 7.9, the TPU delivered on its goal of a\ntenfold improvement in cost-performance over the GPU when running DNN infer-\nence applications. Moreover, a redesigned TPU with the only change being a\nswitch to the same memory technology as in the GPU would be three times faster.\nOne way to explain the TPU\u2019s success is to see how it followed the guidelines\nin Section 7.2.\n1. Use dedicated memories to minimize the distance over which data is moved.\nThe TPU has the 24 MiB Unified Buffer that holds the intermediate matrices and\nvectors of MLPs and LSTMs and the feature maps of CNNs. It is optimized for\naccesses of 256 bytes at a time. It also has the 4 MiB Accumulators, each 32-bits\nwide, that collect the output of the Matrix Unit and act as input to the hardware\nthat calculates the nonlinear functions. The 8-bit weights are stored in a separate\noff-chipweight memoryDRAMandareaccessedviaanon-chipweightFIFO.In\ncontrast, all these types and sizes of data would exist in redundant copies at sev-\neral levels of the inclusive memory hierarchy of a general-purpose CPU.\n2. Invest the resources saved from dropping advanced microarchitectural optimi-\nzations into more arithmetic units or bigger memories.\nThe TPU offers 28 MiB of dedicated memory and 65,536 8-bit ALUs, which\nmeans it has about 60% of the memory and 250 times as many ALUs as a\nserver-class CPU, despite being half its size and power (see Section 7.9). Com-\npared to a server-class GPU, the TPU has 3.5 times the on-chip memory and 25\ntimes as many ALUs.\n3. Use the easiest form of parallelism that matches the domain.\nThe TPU delivers its performance via a two-dimensional SIMD parallelism\nwith its 256256 Matrix Multiply Unit, which is internally pipelined with a\nsystolic organization, plus a simple overlapped execution pipeline of its\n566\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 599,
        "text": "instructions. GPUs rely instead on multiprocessing, multithreading, and one-\ndimensional SIMD, and CPUs rely on multiprocessing, out-of-order execution,\nand one-dimensional SIMD.\n4. Reduce data size and type to the simplest needed for the domain.\nThe TPU computes primarily on 8-bit integers, although it supports 16-bit\nintegers and accumulates in 32-bit integers. CPUs and GPUs also support\n64-bit integers and 32-bit and 64-bit floating point.\n5. Use a domain-specific programming language to port code to the DSA.\nThe TPU is programmed using the TensorFlow programming framework,\nwhereas GPUs rely on CUDA and OpenCL and CPUs must run virtually\neverything.\n7.5\nMicrosoft Catapult, a Flexible Data Center Accelerator\nAt the same time that Google was thinking about deploying a custom ASIC in its\ndata centers, Microsoft was considering accelerators for theirs. The Microsoft\nperspective was that any solution had to follow these guidelines:\n\u25a0\nIt had to preserve homogeneity of servers to enable rapid redeployment of\nmachines and to avoid making maintenance and scheduling even more com-\nplicated, even if that notion is a bit at odds with the concept of DSAs.\n\u25a0\nIt had to scale to applications that might need more resources than could fit into\na single accelerator without burdening all applications with multiple\naccelerators.\n\u25a0\nIt needed to be power-efficient.\n\u25a0\nIt couldn\u2019t become a dependability problem by being a single point of failure.\n\u25a0\nIt had to fit within the available spare space and power in existing servers.\n\u25a0\nIt could not hurt data center network performance or reliability.\n\u25a0\nThe accelerator had to improve the cost-performance of the server.\nThe first rule prevented deploying an ASIC that helped only some applications on\nsome servers, which was the decision that Google made.\nMicrosoft started a project called Catapult that placed an FPGA on a PCIe bus\nboard into data center servers. These boards have a dedicated network for appli-\ncations that need more than one FPGA. The plan was to use the flexibility of\nthe FPGA to tailor its use for varying applications both on different servers and\nto reprogram the same server to accelerate distinct applications over time. This plan\nincreased the return on its investment of the accelerator. Another advantage of\nFPGAs is that they should have lower NRE than ASICs, which could again\nimprove return on investment. We discuss two generations of Catapult, showing\nhow the design evolved to meet the needs of WSCs.\n7.5\nMicrosoft Catapult, a Flexible Data Center Accelerator\n\u25a0\n567"
    },
    {
        "page": 600,
        "text": "One interesting upside of FPGAs is that each application\u2014or even each phase\nof an application\u2014can be thought of as its own DSA, so in this section, we get to\nsee many examples of novel architectures in one hardware platform.\nCatapult Implementation and Architecture\nFigure 7.19 shows a PCIe board that Microsoft designed to fit within its\nservers, which limited power and cooling to 25 W. This constraint led to\nthe selection of the 28-nm Altera Stratix V D5 FPGA for its first implemen-\ntation of Catapult. The board also has 32 MiB of flash memory and includes\ntwo banks of DDR3-1600 DRAM with a total capacity of 8 GiB. The FPGA\nhas 3926 18-bit ALUs, 5 MiB of on-chip memory, and 11 GB/s bandwidth to\nDDR3 DRAMs.\nFigure 7.19 The Catapult board design. (A) shows the block diagram, and (B) is a pho-\ntograph of both sides of the board, which is 10 cm9 cm16 mm. The PCIe and inter-\nFPGA network are wired to a connector on the bottom of the board that plugs directly\ninto the motherboard. (C) is a photograph of the server, which is 1U (4.45 cm) high and\nhalf a standard rack wide. Each server has two 12-core Intel Sandy Bridge Xeon CPUs, 64\nGiB of DRAM, 2 solid-state drives, 4 hard-disk drives, and a 10-Gbit Ethernet network\ncard. The highlighted rectangle on the right in (C) shows the location of the Catapult\nFPGA board on the server. The cool air is sucked in from the left in (C), and the hot\nair exhausts to the right, which passes over the Catapult board. This hot spot and\nthe amount of the power that the connector could deliver mean that the Catapult board\nis limited to 25 watts. Forty-eight servers share an Ethernet switch that connects to the\ndata center network, and they occupy half of a data center rack.\n568\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 601,
        "text": "Each of the 48 servers in half of a data center rack contains a Catapult board.\nCatapult follows the preceding guidelines about supporting applications that need\nmore than a single FPGA without affecting the performance of the data center net-\nwork. It adds a separate low-latency 20 Gbit/s network that connects 48 FPGAs.\nThe network topology is a two-dimensional 68 torus network.\nTo follow the guideline about not being a single point of failure, this network\ncan be reconfigured to operate even if one of the FPGAs fails. The board also has\nSECDED protection on all memories outside the FPGA, which is required for\nlarge-scale deployment in a data center.\nBecause FPGAs use a great deal of memory on the chip to deliver programma-\nbility, they are more vulnerable than ASICs to single-event upsets (SEUs) because\nof radiation as the process geometries shrink. The Altera FPGA in Catapult boards\nincludes mechanisms to detect and correct SEUs inside the FPGA and reduces the\nchances of SEUs by periodically scrubbing the FPGA configuration state.\nThe separate network has an added benefit of reducing the variability of com-\nmunication performance as compared to a data center network. Network unpredict-\nability increases tail latency\u2014which is especially detrimental for applications that\nface end users\u2014so a separate network makes it easier to successfully offload work\nfrom the CPU to the accelerator. This FPGA network can run a much simpler pro-\ntocol than in the data center because the error rates are considerably lower and the\nnetwork topology is well defined.\nNote that resiliency requires care when reconfiguring FPGAs so that they nei-\nther appear as failed nodes nor crash the host server or corrupt their neighbors.\nMicrosoft developed a high-level protocol for ensuring safety when reconfiguring\none or more FPGAs.\nCatapult Software\nPossibly the biggest difference between Catapult and the TPU is having to program\nin a hardware-description language such as Verilog or VHDL. As the Catapult\nauthors write (Putnam et al., 2016):\nGoing forward, the biggest obstacle to widespread adoption of FPGAs in the\ndatacenter is likely to be programmability. FPGA development still requires\nextensive hand-coding in Register Transfer Level and manual tuning.\nTo reduce the burden of programming Catapult FPGAs, the Register Transfer Level\n(RTL) code is divided into the shell and the role, as Figure 7.20 shows. The shell\ncode is like the system library on an embedded CPU. It contains the RTL code that\nwill be reused across applications on the same FPGA board, such as data marshaling,\nCPU-to-FPGA communication, FPGA-to-FPGA communication, data movement,\nreconfiguration, and health monitoring. The shell RTL code is 23% of the Altera\nFPGA. The role code is the application logic, which the Catapult programmer writes\nusing the remaining 77% of the FPGA resources. Having a shell has the added\nbenefit of offering a standard API and standard behavior across applications.\n7.5\nMicrosoft Catapult, a Flexible Data Center Accelerator\n\u25a0\n569"
    },
    {
        "page": 602,
        "text": "CNNs on Catapult\nMicrosoft developed a configurable CNN accelerator as an application for\nCatapult. Configuration parameters include the number of neural network\nlayers, the dimension of those layers, and even the numerical precision to be\nused. Figure 7.21 shows the block diagram of the CNN accelerator. Its key\nfeatures are:\n\u25a0\nRun-time configurable design, without requiring recompilation using the\nFPGA tools.\n\u25a0\nTo minimize memory accesses, it offers efficient buffering of CNN data struc-\ntures (see Figure 7.21).\n\u25a0\nA two-dimensional array of Processing Elements (PEs) that can scale up to\nthousands of units.\nShell\nRole\nDDR3 Core 0\n4 GB DDR3-1333\nECC SO-DIMM\nDDR3 Core 1\n72\n4 GB DDR3-1333\nECC SO-DIMM\n256 Mb\nQSPI\nConfig\nFlash\nConfig\nFlash\n(RSU)\nJTAG\nLEDs\nTemp\nSensors\nxcvr\nreconfig\nx8 PCIe\nCore\nHost\nCPU\nDMA\nEngine\nSEU\nI2C\n72\n4\n8\n2\n2\n2\n2\nInter-FPGA Router\nNorth\nSLIII\nSouth\nSLIII\nEast\nSLIII\nWest\nSLIII\nApplication\nFigure 7.20 Components of Catapult shell and role split of the RTL code.\n570\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 603,
        "text": "Images are sent to DRAM and then input into a multibank buffer in the FPGA. The\ninputs are sent to multiple PEs to perform the stencil computations that produce the\noutput feature maps. A controller (upper left in Figure 7.21) orchestrates the flow\nof data to each PE. The final results are then recirculated to the input buffers to\ncompute the next layer of the CNN.\nLike the TPU, the PEs are designed to be used as a systolic array. Figure 7.22\nshows the details of the PE design.\nTop\ncontroller\nLayer\ncontroller\nAddress\ngeneration\nScan chain\nBroad-cast\nInput\nvolume\nSegment 0\nInput\nvolume\nSegment 1\nInput\nvolume\nSegment N-2\nInput volume\nInput\nvolume\nSegment N-1\nLayer\nconfig.\nData re-distribution\nOutput volume\nz\nz\nx\nx\ny\ny\nOutput\nfeature\nmap\nInput\nkernel\nweight\n0\nPE\nPE\nPE\nPE\nOutput\nfeature\nmap\nInput\nkernel\nweight\n1\nPE\nPE\nPE\nPE\nOutput\nfeature\nmap\nInput\nkernel\nweight\nM-2\nPE\nPE\nPE\nPE\nOutput\nfeature\nmap\nInput\nkernel\nweight\nM-1\nPE\nPE\nPE\nPE\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\nFigure 7.21 CNN Accelerator for Catapult. The Input Volume of the left correspond to Layer[i\u00031] on the left of\nFigure 7.20, with NumFM[i\u00031] corresponding to y and DimFM[i\u00031] corresponding to z. Output Volume at the top\nmaps to Layer[i], with z mapping to NumFM[i] and DimFM[i] mapping to x. The next figure shows the inside of\nthe Processing Element (PE).\n7.5\nMicrosoft Catapult, a Flexible Data Center Accelerator\n\u25a0\n571"
    },
    {
        "page": 604,
        "text": "IBW0\nFU0,0\nMax pool\ncontrol\nMax pool command\nBias data load\nInput double\nBuffer array\nUmi Dram\nFetcher\nDRAM\nPCle\nUmi\ncommand\nShallow\nFIFO\nArray\nControl\nAddress\nRing\nArbitor\nFunctional unit array\nIBD0\nOB0\nFU1,0\nFU2,0\nFUn,0\n+b0\nMPE0\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n0\nIBW1\nFU0,0\nOB1\nFU1,1\nFU2,1\nFUn,1\n+b1\nMPE1\n\u2022\u2022\u2022\n1\nIBW2\nFU0,0\nOB2\nFU1,2\nFU2,2\nFUn,2\n+b2\nMPE2\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n2\nIBWn\nFU0,n\nOBn\nFU1,n\nFU2,n\nFUn,n\n+bn\nMPEn\n\u2022\u2022\u2022\nn\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\nIBD0\nIBD0\nIBD0\nSingle Layer\nControl\nRegister\ninterface\nMulti layer\ncontrol\nKernel weights\nBuffer array\nOutput buffer\nArray\nBias buffer\nArray\nActivation\nFunction array\nMax pooling\narray\nUmi command\nFigure 7.22 The Processing Element (PE) of the CNN Accelerator for Catapult in Figure 7.21. The two-dimension\nFunctional Units (FU) consist of just an ALU and a few registers.\n572\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 605,
        "text": "Search Acceleration on Catapult\nTheprimaryapplicationtotestthereturnoninvestmentofCatapultwasacriticalfunc-\ntionoftheMicrosoftBingsearchenginecalledranking.Itrankstheorderoftheresults\nfrom a search. The output is a document score, which determines the position of the\ndocumentonthewebpagethatispresentedtotheuser.Thealgorithmhasthreestages:\n1. Feature Extraction extracts thousands of interesting features from a document\nbased on the search query, such as the frequency that the query phrase appears\nin a document.\n2. Free-Form Expressions calculates thousands of combinations of features from\nthe prior stage.\n3. Machine-Learned Scoring uses machine-learning algorithms to evaluate the\nfeatures from the first two stages to calculate a floating-point score of a docu-\nment that is returned to the host search software.\nThe Catapult implementation of ranking produces identical results to equivalent\nBing software, even reproducing known bugs!\nTaking advantage of one of the preceding guidelines, the ranking function does\nnot have to fit within a single FPGA. Here is how the ranking stages are split across\neight FPGAs:\n\u25a0\nOne FPGA does Feature Extraction.\n\u25a0\nTwo FPGAs do Free-Form Expressions.\n\u25a0\nOne FPGA does a compression stage that increases scoring engine efficiency.\n\u25a0\nThree FPGA do Machine-Learned Scoring.\nThe remaining FPGA is a spare used to tolerate faults. Using multiple FPGAs for\none application works well because of the dedicated FPGA network.\nFigure 7.23 shows the Feature Extraction stage organization. It uses 43 feature-\nextraction state machines to compute in parallel 4500 features per document-\nquery pair.\nNext is the following Free-Form Expressions stage. Rather than implement the\nfunctions directly in gates or in state machines, Microsoft developed a 60-core pro-\ncessor that overcomes long-latency operations with multithreading. Unlike a GPU,\nMicrosoft\u2019s processor does not require SIMD execution. It has three features that\nlet it match the latency target:\n1. Each core supports four simultaneous threads where one can stall on a long\noperation but the others can continue. All functional units are pipelined, so they\ncan accept a new operation every clock cycle.\n2. Threads are statically prioritized using a priority encoder. Expressions with the\nlongest latency use thread slot 0 on all cores, then the next slowest is in slot 1 on\nall cores, and so on.\n7.5\nMicrosoft Catapult, a Flexible Data Center Accelerator\n\u25a0\n573"
    },
    {
        "page": 606,
        "text": "3. Expressions that are too large to fit in the time allocated for a single FPGA can\nbe split across the two FPGAs used for free-form expressions.\nOne cost of the reprogrammability in an FPGA is a slower clock rate than custom\nchips. Machine-Learned Scoring uses two forms of parallelism to try to overcome\nthat disadvantage. The first is to have a pipeline that matches the available pipeline\nparallelism in the application. For ranking, the limit is 8 \u03bcs per stage. The second\nversion of parallelism is the rarely seen multiple instruction streams, single data\nstream (MISD) parallelism, where a large number of independent instruction\nstreams operate in parallel on a single document.\nFigure 7.24 shows the performance of the ranking function on Catapult. As we\nwill see in Section 7.9, user-facing applications often have rigid response times; it\ndoesn\u2019t matter how high the throughput is if the application misses the deadline.\nThe x-axis shows the response-time limit, with 1.0 as the cutoff. At this maximum\nlatency, Catapult is 1.95 times as fast as the host Intel server.\nCatapult Version 1 Deployment\nBefore populating a whole warehouse-scale computer with tens of thousands of\nservers, Microsoft did a test deployment of 17 full racks, which contained\n17482 or 1632 Intel servers. The Catapult cards and network links were tested\nat manufacture and system integration, but at deployment, seven of the 1632 cards\nfailed (0.43%), and one of the 3264 FPGA network links (0.03%) was defective.\nAfter several months of deployment, nothing else failed.\nFeature extraction FSMs\nFeature-\ngathering\nnetwork\nHit vector\npreprocessing\nFSM\nFigure 7.23 The architecture of FPGA implementation of the Feature Extraction\nstage. A hit vector, which describes the locations of query words in each document,\nis streamed into the hit vector preprocessing state machine and then split into control\nand data tokens. These tokens are issued in parallel to the 43 unique feature state\nmachines. The feature-gathering network collects generated feature and value pairs\nand forwards them to the following Free-Form Expressions stage.\n574\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 607,
        "text": "Catapult Version 2\nAlthough the test deployment was successful, Microsoft changed the architecture\nfor the real deployment to enable both Bing and Azure Networking to use the same\nboards and architecture (Caulfield et al., 2016). The main problem with the V1\narchitecture was that the independent FPGA network did not enable the FPGA\nto see and process standard Ethernet/IP packets, which prevented it from being\nused to accelerate the data center network infrastructure. In addition, the cabling\nwas expensive and complicated, it was limited to 48 FPGAs, and the rerouting\nof traffic during certain failure patterns reduced performance and could\nisolate nodes.\nThe solution was to place the FPGA logically between the CPU and NIC, so\nthat all network traffic goes through the FPGA. This \u201cbump-on-a-wire\u201d placement\nremoves many weaknesses of the FPGA network in Catapult V1. Moreover, it\nenables the FPGAs to run their own low-latency network protocol that allows them\nto be treated as a global pool of all the FPGAs in the data center and even across\ndata centers.\nThree changes occurred between V1 and V2 to overcome the original concerns\nof Catapult applications interfering with data center network traffic. First, the data\ncenter network was upgraded from 10 Gbit/s to 40 Gbit/s, increasing the headroom.\nSecond, Catapult V2 added a rate limiter for FPGA logic, ensuring that an FPGA\napplication could not overwhelm the network. The final and perhaps most\n0\n0\n0.5\n1\nLatency (normalized to 95th percentile target)\n95% more\n95th percentile latency versus throughput\nThroughput (normalized)\n1.5\n2\n1\n2\n3\n4\n5\nSoftware\nFPGA\n29% lower latency\nThroughput\nFigure 7.24 Performance for the ranking function on Catapult for a given\nlatency bound. The x-axis shows the response time for the Bing ranking function.\nThe maximum response time at the 95th percentile for the Bing application on the\nx-axis is 1.0, so data points to the right may have a higher throughput but arrive too\nlate to be useful. The y-axis shows the 95% throughputs on Catapult and pure software\nfor a given response time. At a normalized response time of 1.0, Catapult has 1.95 the\nthroughput of Intel server running in pure software mode. Stated alternatively, if Cat-\napult matches the throughput that the Intel server has at 1.0 normalized response time,\nCatapult\u2019s response time is 29% less.\n7.5\nMicrosoft Catapult, a Flexible Data Center Accelerator\n\u25a0\n575"
    },
    {
        "page": 608,
        "text": "important change was that the networking engineers would now had their own use\ncases for the FPGA, given its bump-in-the-wire placement. That placement trans-\nformed these former interested bystanders into enthusiastic collaborators.\nBy deploying Catapult V2 in the majority of its new servers, Microsoft essen-\ntially has a second supercomputer composed of distributed FPGAs that shares the\nsame network wires as the CPU servers and is at the same scale, as there is one\nFPGA per server. Figures 7.25 and 7.26 show the block diagram and the board\nfor Catapult V2.\nCatapult V2 follows the same shell and role split of the RTL to simplify pro-\ngramming, but at the time of publication, the shell uses almost half of the FPGA\nresources (44%) because of the more complicated network protocol that shares the\ndata center network wires.\nCatapult V2 is used for both Ranking acceleration and function network accel-\neration. In Ranking acceleration, rather than perform nearly all of the ranking func-\ntion inside the FPGA, Microsoft implemented only the most compute-intensive\nportions and left the rest to the host CPU:\n\u25a0\nThe feature functional unit (FFU) is a collection of finite state machines that\nmeasure standard features in search, such as counting the frequency of a par-\nticular search term. It is similar in concept to the Feature Extraction stage of\nCatapult V1.\n2-socket server blade\nTOR\n40Gb/s\n40Gb/s\nDRAM\nCPU\nFPGA\nNIC\nDRAM\nCPU\nDRAM\nGen3 2x8\nAccelerator card\nGen3 x8\nQPI\nQSFP\nQSFP\nQSFP\nFigure 7.25 The Catapult V2 block diagram. All network traffic is routed through the\nFPGA to the NIC. There is also a PCIe connector to the CPUs, which allows the FPGA to be\nused as a local compute accelerator, as in Catapult V1.\n576\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 609,
        "text": "\u25a0\nThe dynamic programming feature (DPF) creates a Microsoft proprietary set\nof features using dynamic programming and bears some similarity to the Free-\nForm Expressions stage of Catapult V1.\nBoth are designed so that they can use non-local FPGAs for these tasks, which\nsimplifies scheduling.\nFigure 7.27 shows the performance of Catapult V2 compared to software in\na format similar to Figure 7.24. The throughput can now be increased 2.25\nwithout endangering latency, whereas the speedup was previously 1.95.\nWhen ranking was deployed and measured in production, Catapult V2 had bet-\nter tail latencies than software; that is, the FPGA latencies never exceeded the\nsoftware latencies at any given demand despite being able to absorb twice the\nworkload.\nSummary: How Catapult Follows the Guidelines\nMicrosoft reported that adding Catapult V1 to the servers in the pilot phase\nincreased the total cost of ownership (TCO) by less than 30%. Thus, for this appli-\ncation, the net gain in cost-performance for Ranking was at least 1.95/1.30, or a\nreturn on investment of about 1.5. Although no comment was made about TCO\nconcerning Catapult V2, the board has a similar number of the same type of chips,\nso we might guess that the TCO is no higher. If so, the cost-performance of Cat-\napult V2 is about 2.25/1.30, or 1.75 for Ranking.\nHere is how Catapult followed the guidelines from Section 7.2.\n40G QSFP Ports\n(NIC and TOR)\nStratix V\nD5 FPGA\n4GB DDR3\nFigure 7.26 The Catapult V2 board uses a PCIe slot. It uses the same FPGA as Catapult\nV1 and has a TDP of 32 W. A 256-MB Flash chip holds the golden image for the FPGA that\nis loaded at power on, as well as one application image.\n7.5\nMicrosoft Catapult, a Flexible Data Center Accelerator\n\u25a0\n577"
    },
    {
        "page": 610,
        "text": "1. Use dedicated memories to minimize the distance over which data is moved.\nThe Altera V FPGA has 5 MiB of memory on-chip, which an application can\ncustomize for its use. For example, for CNNs, it is used for the input and output\nfeature maps of Figure 7.21.\n2. Invest the resources saved from dropping advanced microarchitectural optimi-\nzations into more arithmetic units or bigger memories.\nThe Altera V FPGA also has 3926 18-bit ALUs that are tailored to the appli-\ncation. For CNNs, they are used to create the systolic array that drives the Pro-\ncessing Elements in Figure 7.22, and they form the datapaths of the 60-core\nmultiprocessor used by Free Form Expression stage of ranking.\n3. Use the easiest form of parallelism that matches the domain.\nCatapult picks the form of parallelism that matches the application. For exam-\nple, Catapult uses two-dimensional SIMD parallelism for the CNN application\nand MISD parallelism in the Machine Scoring phase stream Ranking.\n4. Reduce data size and type to the simplest needed for the domain.\nCatapult can use whatever size and type of data that the application wants, from\nan 8-bit integer to a 64-bit floating point.\n5. Use a domain-specific programming language to port code to the DSA.\nIn this case, programming is done in the hardware register-transfer language\n(RTL) Verilog, which is an even less productive language than C or C++. Micro-\nsoft didnot (andpossiblycould not)follow this guidelinegiven itsuse ofFPGAs.\nAlthough this guideline concerns the one-time porting of an application from software\ntoFPGA,applicationsarenotfrozenintime.Almostbydefinition,softwareevolvesto\nadd features or fix bugs, especially for something as important as web search.\n0\n0\n1\n2\n3\n4\n0.5\n1\nLatency (normalized to 99th percentile target)\nThroughput (normalized)\n1.5\n2\nFPGA\nSoftware\nFigure 7.27 Performance for the ranking function on Catapult V2 in the same format\nas Figure 7.24. Note that this version measures 99th percentile while the earlier figure\nplots 95th percentile.\n578\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 611,
        "text": "Maintenance of successful programs can be most of software\u2019s development costs.\nMoreover,whenprogramminginanRTL,softwaremaintenanceisevenmoreburden-\nsome. The Microsoft developers, like all others who use FPGAs as accelerators, hope\nthat future advances in domain-specific languages and systems for hardware-software\nco-design will reduce the difficulty of programming FPGAs.\n7.6\nIntel Crest, a Data Center Accelerator for Training\nThe quotation by the Intel CEO that opens Section 7.3 came from the press release\nannouncing that Intel was going to start shipping DSAs (\u201caccelerants\u201d) for DNN. The\nfirst example was Crest, which was announced while we were writing this edition.\nDespitethelimiteddetails,weincludeit herebecauseof thesignificanceof atraditional\nmicroprocessor manufacturer like Intel taking this bold step of embracing DSAs.\nCrest is aimed at DNN training. The Intel CEO said the goal is to accelerate\nDNN training a hundredfold over the next three years. Figure 7.6 shows that train-\ning can take a month. There is likely to be a demand to decrease the DNN training\nto just eight hours, which would be 100 times quicker than the CEO predicted.\nDNNs will surely become even more complex over the next 3 years, which will\nrequire a much greater training effort. Thus there seems little danger that a\n100 improvement in training is overkill.\nCrest instructions operate on blocks of 3232 matrices. Crest uses a number\nformat called flex point, which is a scaled fixed-point representation: 3232 matri-\nces of 16-bit data share a single 5-bit exponent that is provided as part of the\ninstruction set.\nFigure 7.28shows the block diagram of the Lake Crest chip. To compute onthese\nmatrices, Crest uses the12 processing clusters of Figure 7.28. Each cluster includes a\nlarge SRAM, a big linear algebra processing unit, and a small amount of logic for on-\nand off-chiprouting.The four8 GiB HBM2 DRAMmodules offer 1 TB/s ofmemory\nbandwidth, which should lead to an attractive Roofline model for the Crest chip. In\naddition to high-bandwidth paths to main memory, Lake Crest supports high band-\nwidth interconnects directly between compute cores inside the processing clusters,\nwhich facilitates quick core-to-core communication without passing through shared\nmemory. Lake Crest\u2019s goal is a factor of 10 improvement in training over GPUs.\nFigure 7.28 shows 12 Inter-Chip Links (ICLs) and 2 Inter-Chip Controllers\n(ICCs), so Crest is clearly designed to allow many Crest chips to collaborate, sim-\nilar in spirit to the dedicated network connecting the 48 FPGAs in Catapult. It\u2019s\nlikely that the 100 improvement in training will require ganging together several\nCrest chips.\n7.7\nPixel Visual Core, a Personal Mobile Device Image\nProcessing Unit\nPixel Visual Core is a programmable, scalable DSA intended for image processing\nand computer vision from Google, initially for cell phones and tablets running the\n7.7\nPixel Visual Core, a Personal Mobile Device Image Processing Unit\n\u25a0\n579"
    },
    {
        "page": 612,
        "text": "Android operating system, and then potentially for Internet of Things (IoT)\ndevices. It is a multicore design, supporting between 2 and 16 cores to deliver a\ndesired cost-performance. It is designed either to be its own chip or to be part\nof a system on a chip (SOC). It has a much smaller area and energy budget than\nits TPU cousin. Figure 7.29 lists terms and acronyms found in this section.\nPixel Visual Core is an example of a new class of domain specific architectures\nfor vision processing that we call image processing units (IPUs). IPUs solve the\ninverse problem of GPUs: they analyze and modify an input image in contrast\nto generating an output image. We call them IPUs to signal that, as a DSA, they\ndo not need to do everything well because there will also be CPUs (and GPUs) in\nthe system to perform non-input-vision tasks. IPUs rely on stencil computations\nmentioned above for CNNs.\nThe innovations of Pixel Visual Core include replacing the one-dimensional\nSIMD unit of CPUs with a two-dimensional array of processing elements\n(PEs). They provide a two-dimensional shifting network for the PEs that is aware\nof the two-dimensional spatial relationship between the elements, and a two-\ndimensional version of buffers that reduces accesses to off-chip memory. This\nnovel hardware makes it easy to perform stencil computations that are central to\nboth vision processing and CNN algorithms.\nISPs, the Hardwired Predecessors of IPUs\nMost portable mobile devices (PMDs) have multiple cameras for input, which has\nled to hardwired accelerators called image signal processors (ISPs) for enhancing\n8GB HBM2\nInterposer\n8GB HBM2\n8GB HBM2\n8GB HBM2\nHBM\nPHY\nMem\nCtrlr\nHBM\nPHY\nMem\nCtrlr\nPCl Express \u00d716\nPCle Controller & DMA\nSPI, IC2,\nGPIO\nMem\nCtrlr\nHBM\nPHY\nMem\nCtrlr\nHBM\nPHY\nMGMT\nCPU\nICC\nICC\nICL\nICL\nICL\nICL\nICL\nICL\nICL\nICL\nICL\nICL\nICL\nICL\nProcessing\nCluster\nProcessing\nCluster\nProcessing\nCluster\nProcessing\nCluster\nProcessing\nCluster\nProcessing\nCluster\nProcessing\nCluster\nProcessing\nCluster\nProcessing\nCluster\nProcessing\nCluster\nProcessing\nCluster\nProcessing\nCluster\nFigure 7.28 Block diagram of the Intel Lake Crest processor. Before being acquired by Intel, Crest said that the chip\nis almost a full reticle in TSMC 28 nm, which would make the die size 600\u2013700 mm2. This chip should be available in\n2017. Intel is also building Knights Crest, which is a hybrid chip containing Xeon x86 cores and Crest accelerators.\n580\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 613,
        "text": "input images. The ISP is usually a fixed function ASIC. Virtually every PMD today\nincludes an ISP.\nFigure 7.30 shows a typical organization of an image-processing system,\nincluding the lens, sensor, ISP, CPU, DRAM, and display. The ISP receives\nimages, removes artifacts in images from the lens and the sensor, interpolates miss-\ning colors, and significantly improves the overall visual quality of the image.\nPMDs tend to have small lens and thus tiny noisy pixels, so this step is critical\nto producing high-quality photos and videos.\nAn ISP processes the input image in raster scan order by calculating a series of\ncascading algorithms via software configurable hardware building blocks, typi-\ncally organized as a pipeline to minimize memory traffic. At each stage of the pipe-\nline and for each clock cycle, a few pixels are input, and a few are output.\nComputation is typically performed over small neighborhoods of pixels\n(stencils). Stages are connected by buffers called line buffers. The line buffers help\nTerm\nAcronym\nShort explanation\nCore\n\u2013\nA processor. Pixel Visual Core can have 2\u201316 cores. The first implementation has 8;\nalso called stencil processor (STP)\nHalide\n\u2013\nA domain-specific programming language for image processing that separates the\nalgorithm from its execution schedule\nHalo\n\u2013\nAn extended region around the 1616 computation array to handle stencil computation\nnear the borders of the array. It holds values, but doesn\u2019t compute\nImage signal\nprocessors\nISP\nA fixed function ASIC that improves the visual quality of an image; found in virtually\nall PMDs with cameras\nImage processing\nunit\nIPU\nA DSA that solves the inverse problem of a GPU: it analyzes and modifies an input\nimage in contrast to generating an output image\nLine buffer pool\nLB\nA line buffer is designed to capture a sufficient number of full lines of an intermediate image\nto keep the next stage busy. Pixel Visual Core uses two-dimensional line buffers, each\nChange64to128KiB.TheLineBufferPoolcontainsoneLBpercoreplusoneLBforDMA\nNetwork on chip\nNOC\nThe network that connects the cores in Pixel Visual Core\nPhysical ISA\npISA\nThe Pixel Visual Core instruction set architecture (ISA) that is executed by the hardware\nProcessing\nelement array\n\u2013\nThe 1616 array of Processing Elements plus the halo that performs the 16-bit\nmultiply-add operations. Each Processing Element includes a Vector Lane and local\nmemory. It can shift data en mass to neighbors in any of four directions\nSheet generator\nSHG\nDoes memory accesses of blocks of 1  1 to 31  31 pixels, which are called sheets.\nThe different sizes allow the option of including the space for the halo or not\nScalar lane\nSCL\nSame operations as the Vector Lane except it adds instructions that handle jumps,\nbranches, and interrupts, controls instruction flow to the vector array, and schedules all\nthe loads and stores for the sheet generator. It also has a small instruction memory. It\nplays the same role as the scalar processor in a vector architecture\nVector lane\nVL\nPortion of the Processing Element that performs the computer arithmetic\nVirtual ISA\nvISA\nThePixelVisualCoreISAgeneratedby thecompiler.Itismappedto pISAbeforeexecution\nFigure 7.29 A handy guide to Pixel Visual Core terms in Section 7.7. Figure 7.4 on page 437 has a guide for\nSections 7.3\u20137.6.\n7.7\nPixel Visual Core, a Personal Mobile Device Image Processing Unit\n\u25a0\n581"
    },
    {
        "page": 614,
        "text": "keep the processing stages utilized via spatial locality by capturing just enough full\nlines of an intermediate image to facilitate the computation required by the\nnext stage.\nThe enhanced image is either sent to a display or to DRAM for storage or for\nlater processing. The ISP also sends statistics about the image (e.g., color and luma\nhistograms, sharpness, and so on) to the CPU, which in turn it processes and sends\ninformation to help the system adapt.\nAlthough efficient, ISPs have two major downsides. Given the increasing\ndemand for improved image quality in handheld devices, the first is the inflexibil-\nity of an ISP, especially as it takes years to design and manufacture a new ISP\nwithin an SOC. The second is that these computing resources can be used only\nfor the image-enhancing function, no matter what is needed at the time on the\nPMD. Current generation ISPs handle workloads at up to 2 Tera-operations per\nsecond on a PMD power budget, so a DSA replacement has to achieve similar per-\nformance and efficiency.\nPixel Visual Core Software\nPixel Visual Core generalized the typical hardwired pipeline organization of ker-\nnels of an ISP into a directed acyclic graph (DAG) of kernels. Pixel Visual Core\nimage-processing programs are typically written in Halide, which is a domain-\nspecific functional programming language for image processing. Figure 7.31 is\na Halide example that blurs an image. Halide has a functional section to express\nthe function being programmed and a separate schedule section to specify how\nto optimize that function to the underlying hardware.\nOutput\nimage\n(Display)\nImage\nImg &\nStats\nAWB\nAE\nAF\nSensor\n(CCD or CMOS)\nLens\nDRAM\nCPU\nISP\nBUS\nFigure 7.30 Diagram showing interconnection of the Image Signal Processor (ISP),\nCPU, DRAM, lens, and sensor. The ISP sends statistics to the CPU as well as the\nimproved image either to the display or to DRAM for storage or later processing. The\nCPU then processes the image statistics and sends information to let the system adapt:\nAuto White Balance (AWB) to the ISP, Auto Exposure (AE) to the sensor, and Auto Focus\n(AF) to the lens, known as the 3As.\n582\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 615,
        "text": "Pixel Visual Core Architecture Philosophy\nThe power budget of PMDs is 6\u20138 W for bursts of 10\u201320 seconds, dropping down\nto tens of milliwatts when the screen is off. Given the challenging energy goals of a\nPMD chip, the Pixel Visual Core architecture was strongly shaped by the relative\nenergy costs for the primitive operations mentioned in Chapter 1 and made explicit\nin Figure 7.32. Strikingly, a single 8-bit DRAM access takes as much energy as\n12,500 8-bit additions or 7\u2013100 8-bit SRAM accesses, depending on the organi-\nzation of the SRAM. The 22 to 150 higher cost of IEEE 754 floating-point\noperations over 8-bit integer operations, plus the die size and energy benefits of\nstoring narrower data, strongly favor using narrow integers whenever algorithms\ncan accommodate them.\nIn addition to the guidelines from Section 7.2, these observations led to other\nthemes that guided the Pixel Visual Core design:\n\u25a0\nTwo-dimensional is better than one-dimensional: Two-dimensional organiza-\ntions can be beneficial for processing images as it minimizes communication\ndistance and because the two- and three-dimensional nature of image data can\nutilize such organizations.\n\u25a0\nCloser is better than farther: Moving data is expensive. Moreover, the relative\ncost of moving data to an ALU operation is increasing. And of course DRAM\ntime and energy costs far exceed any local data storage or movement.\nA primary goal in going from an ISP to an IPU is to get more reuse of the hardware\nvia programmability. Here are the three main features of the Pixel Visual Core:\nFunc buildBlur(Func input) {\n// Functional portion (independent of target processor)\nFunc blur_x(\"blur_x\"), blur_y(\"blur_y\");\nblur_x(x,y) = (input(x\u00031,y) + input(x,y)*2 + input(x+1,y)) / 4;\nblur_y(x,y) = (blur_x(x,y\u00031) + blur_x(x,y)*2 + blur_x(x,y+1)) / 4;\nif (has_ipu) {\n// Schedule portion (directs how to optimize for target processor)\nblur_x.ipu(x,y);\nblur_y.ipu(x,y);\n}\nreturn blur_y;\n}\nFigure 7.31 Portion of a Halide example to blur an image. The ipu(x,y) suffix schedules the function to Pixel\nVisual Core. A blur has the effect of looking at the image through a translucent screen, which reduces noise and detail.\nA Gaussian function is often used to blur the image.\n7.7\nPixel Visual Core, a Personal Mobile Device Image Processing Unit\n\u25a0\n583"
    },
    {
        "page": 616,
        "text": "1. Following the theme that two-dimensional is better than one-dimensional, Pixel\nVisual Core uses a two-dimensional SIMD architecture instead of one-\ndimensional SIMD architecture. Thus it has a two-dimensional array of indepen-\ndent processing elements (PEs), each of which contains 2 16-bit ALUs, 1 16-bit\nMAC unit, 10 16-bit registers, and 10 1-bit predicate registers. The 16-bit arith-\nmetic follows the guideline of providing only the precision needed by the domain.\n2. Pixel Visual Core needs temporary storage at each PE. Following the guideline\nfrom Section 7.2 of avoiding caches, this PE memory is a compiler-managed\nscratchpad memory. The logical size of each PE memory is 128 entries of\n16 bits, or just 256 bytes. Because it would be inefficient to implement a sep-\narate small SRAM in each PE, Pixel Visual Core instead groups the PE memory\nof 8 PEs together in a single wide SRAM block. Because the PEs operate in\nSIMD fashion, Pixel Visual Core can bind all the individual reads and writes\ntogether to form a \u201csquarer\u201d SRAM, which is more efficient than narrow\nand deep or wide and shallow SRAMs. Figure 7.33 shows four PEs.\n3. To be able to perform simultaneous stencil computations in all PEs, Pixel Visual\nCore needs to collect inputs from nearest neighbors. This communication pat-\ntern requires a \u201cNSEW\u201d (North, South, East, West) shift network: it can shift\ndata en masse between the PEs in any compass direction. So that it doesn\u2019t lose\npixels along the edges as it shifts images, Pixel Visual Core connects the end-\npoints of the network together to form a torus.\nNote that the shift network is in contrast with the systolic array of processing element\narrays in the TPU and Catapult. In this case, software explicitly moves the data in the\ndesired direction across the array, whereas the systolic approach is a hardware-\ncontrolled, two-dimensional pipeline that moves data as a wavefront that is invisible\nto the software.\nThe Pixel Visual Core Halo\nA 33, 55, or 77 stencil is going to get inputs from 1, 2, or 3 external pixels at\nthe edges of the two-dimensional subset being computed (half of the dimension of\nthe stencil minus one-half). That leaves two choices. Either Pixel Visual Core\nOperation\nEnergy (pJ)\nOperation\nEnergy (pJ)\nOperation\nEnergy (pJ)\n8b DRAM LPDDR3\n125.00\n8b SRAM\n1.2\u201317.1\n16b SRAM\n2.4\u201334.2\n32b Fl. Pt. muladd\n2.70\n8b int muladd\n0.12\n16b int muladd\n0.43\n32b Fl. Pt. add\n1.50\n8b int add\n0.01\n16b int add\n0.02\nFigure 7.32 Relative energy costs per operation in picoJoules assuming TSMC 28-nm HPM process, which was\nthe process Pixel Visual Core used [17][18][19][20]. The absolute energy cost are less than in Figure 7.2 because of\nusing 28 nm instead of 90 nm, but the relative energy costs are similarly high.\n584\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 617,
        "text": "under utilizes the hardware in the elements near the border, because they only pass\ninput values, or Pixel Visual Core slightly extends the two-dimensional PEs with\nsimplified PEs that leave out the ALUs. Because the difference in size between a\nstandard PE and a simplified PE is about 2.2, Pixel Visual Core has an extended\narray. This extended region is called the halo. Figure 7.34 shows two rows of a halo\nsurrounding an 8x8 PE array and illustrates how an example 55 stencil compu-\ntation in the upper-left corner relies on the halo.\nA Processor of the Pixel Visual Core\nThe collection of 1616 PEs and 4 halo lanes in each dimension, called the PE\narray or vector array, is the main computation unit of the Pixel Visual Core. It also\nhas a load-store unit called a Sheet Generator (SHG). SHG refers to memory\naccesses of blocks of 1  1 to 256  256 pixels, which are called sheets. This hap-\npens during downsampling, and typical values are 16  16 or 20  20.\nAn implementation of Pixel Visual Core can have any even number of 2 or\nmore cores, depending on the resources available. Thus it needs a network to con-\nnect them together, so every core also has an interface to the Network on Chip\n(NOC). A typical NOC implementation for Pixel Visual Core will not be an expen-\nsive cross switch, however, because those require data to travel a long distance,\nwhich is expensive. Leveraging the pipeline nature of the application, the NOC\ntypically needs to communicate only to neighboring cores. It is implemented as\na two-dimensional mesh, which allows power gating of pairs of cores under soft-\nware control.\nMem\nMem\nS\nN\nMem\nMem\nS\nN\nE\nE\nS\nW\nW\nFigure 7.33 The two-dimensional SIMD includes two-dimensional shifting \u201cN,\u201d \u201cS,\u201d \u201cE,\u201d \u201cW,\u201d show the direction\nof the shift (North, South, East, West). Each PE has a software-controlled scratchpad memory.\n7.7\nPixel Visual Core, a Personal Mobile Device Image Processing Unit\n\u25a0\n585"
    },
    {
        "page": 618,
        "text": "Finally, the Pixel Visual Core also includes a scalar processor that is called a\nscalar lane (SCL). It is identical to the vector lane, except it adds instructions that\nhandle jumps, branches, and interrupts, controls instruction flow to the vector\narray, and schedules all the loads and stores for the sheet generator. It also has\na small instruction memory. Note that Pixel Visual Core has a single instruction\nstream that controls the scalar and vector units, similar to how a CPU core has\na single instruction stream for its scalar and SIMD units.\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\npe\n5x5 stencil\nFigure 7.34 The two-dimensional array of full processing elements (shown as\nunshaded circles) surrounded by two layers of simplified processing elements\n(shaded diamonds) called a halo. In this figure, there are 88 or 64 full PEs with 80\nsimplified PEs in the halo. (Pixel Visual Core actually has 1616 or 256 full PEs and\ntwo layers in its halo and thus 144 simplified PEs.) The edges of the halo are connected\n(shown as gray lines) to form a torus. Pixel Visual Core does a series of two-dimensional\nshifts across all processing elements to move the neighbor portions of each stencil com-\nputation into the center PE of the stencil. An example 55 stencil is shown in the upper-\nleft corner. Note that 16 of the 25 pieces of data for this 55 stencil location come from\nhalo processing elements.\n586\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 619,
        "text": "In addition to cores, there is also a DMA engine to transfer data between\nDRAM and the line buffers while efficiently converting between image memory\nlayout formats (e.g., packing/unpacking). As well as sequential DRAM accesses,\nthe DMA engines perform vector-like gather reads of DRAM as well as sequential\nand strided reads and writes.\nPixel Visual Core Instruction Set Architecture\nLike GPUs, Pixel Visual Core uses a two-step compilation process. The first step is\ncompiling the programs from the target language (e.g., Halide) into vISA instruc-\ntions. The Pixel Visual Core vISA (virtual Instruction Set Architecture) is inspired\nin part by the RISC-V instruction set, but it uses an image-specific memory model\nand extends the instruction set to handle image processing, and in particular, the\ntwo-dimensional notion of images. In vISA, the two-dimensional array of a core is\ninfinite, the number of register is unbounded, and memory size is similarly unlim-\nited. vISA instructions contain pure functions that don\u2019t directly access DRAM\n(see Figure 7.36), which greatly simplifies mapping them onto the hardware.\nThe next step is to compile the vISA program into a pISA (physical Instruction\nSet Architecture) program. Using vISA as the target of compilers allows the proces-\nsor to be software-compatible with past programs and yet accept changes to the pISA\ninstruction set, so vISA plays the same role that PTX does for GPUs (see Chapter 4).\nLowering from vISA to pISA takes two steps: compilation and mapping with\nearly-bound parameters, and then patching the code with late-bound parameters.\nThe parameters that must be bound include STP size, halo size, number of STPs,\nmapping of line buffers, mapping of kernels to processors, as well as register and\nlocal memory allocations.\nFigure 7.35 shows that pISA is a very long instruction word (VLIW) instruc-\ntion set with 119-bit-wide instructions. The first 43-bit field is for the Scalar\nLane, the next 38-bit field specifies the computation by the two-dimensional\nPE array, and the third 12-bit field specifies the memory accesses by the two-\ndimensional PE array. The last two fields are immediates for computation or\naddressing. The operations for all the VLIW fields are what you\u2019d expect: two\u2019s\ncomplement integer arithmetic, saturating integer arithmetic, logical operations,\nshifts, data transfers, and a few special ones like divide iteration and count lead-\ning zeros. The Scalar Lane supports a superset of the operations in the two-\ndimensional PE array, plus it adds instructions for control-flow and sheet-\ngenerator control. The 1-bit Predicate registers mentioned above enables condi-\ntional moves to registers (e.g., A \u00bc B if C).\nField\nScalar\nMath\nMemory\nImm\nMemImm\n# Bits\n43\n38\n12\n16\n10\nFigure 7.35 VLIW format of the 119-bit pISA instruction.\n7.7\nPixel Visual Core, a Personal Mobile Device Image Processing Unit\n\u25a0\n587"
    },
    {
        "page": 620,
        "text": "Although the pISA VLIW instruction is very wide, Halide kernels are short,\noften just 200\u2013600 instructions. Recall that as an IPU, it only needs to execute\nthe compute-intensive portion of an application, leaving the rest of the function-\nality to CPUs and GPUs. Thus the instruction memory of a Pixel Visual Core holds\njust 2048 pISA instructions (28.5 KiB).\nThe Scalar Lane issues sheet generator instructions that access line buffers.\nUnlike other memory accesses within Pixel Visual Core, the latency can be more\nthan 1 clock cycle, so they have a DMA-like interface. The lane first sets up the\naddresses and transfer size in special registers.\nPixel Visual Core Example\nFigure 7.36 shows the vISA code that is output from the Halide compiler for the\nblur example in Figure 7.31, with comments added for clarity. It calculates a blur\nfirst in the x direction and then in the y direction using 16-bit arithmetic. The vISA\ncode matches the functional part of the Halide program. This code can be thought\nof as executing across all the pixels of an image.\nPixel Visual Core Processing Element\nOne of the architectural decisions was how big to build the halo. Pixel Visual Core\nuses 1616 PEs, and it adds a halo of 2 extra elements, so it can support 55\n// vISA inner loop blur in x dimension\ninput.b16\nt1 <- _input[x*1+(\u00031)][y*1+0][0]; // t1 = input[x\u00031,y]\ninput.b16\nt2 <- _input[x*1+0][y*1+0][0];\n// t2 = input[x,y]\nmov.b16\nst3 <- 2;\nmul.b16\nt4 <- t2, st3;\n//t4 = input[x,y] * 2\nadd.b16\nt5 <- t1, t4;\n//t5 = input[x\u00031,y] + input[x,y]*2\ninput.b16\nt6 <- _input[x*1+1][y*1+0][0];\n// t6 = input[x+1,y]\nadd.b16\nt7 <- t5, t6;\n//t7 = input[x+1,y]+input[x,y]+input[x\u00031,y]*2\nmov.b16\nst8 <- 4;\ndiv.b16\nt9 <- t7, st8;\n//t9 = t7/4\noutput.b16 _blur_x[x*1+0][y*1+0][0] <- t9; // blur_x[x,y] = t7/4\n// vISA inner loop blur in y dimension\ninput.b16\nt1 <- _blur_x[x*1+0][y*1+(\u00031)][0]; // t1 = blur_x[x,y\u00031]\ninput.b16\nt2 <- _blur_x[x*1+0][y*1+0][0];\n// t2 = blur_x[x,y]\nmov.b16\nst3 <- 2;\nmul.b16\nt4 <- t2, st3;\n//t4 = blur_x[x,y] * 2\nadd.b16\nt5 <- t1, t4;\n//t5 = blur_x[x,y\u00031] + blur_x[x,y]*2\ninput.b16\nt6 <- _blur_x[x*1+0][y*1+1][0];\n// t6 = blur_x[x,y+1]\nadd.b16\nt7 <- t5, t6;\n//t7 = blurx[x,y+1]+blurx[x,y\u00031]+blurx[x,y]*2\nmov.b16\nst8 <- 4;\ndiv.b16\nt9 <- t7, st8;\n//t9 = t7/4\noutput.b16 _blur_y[x*1+0][y*1+0][0] <- t9; // blur_y[x,y] = t7/4\nFigure 7.36 Portion of the vISA instructions compiled from the Halide Blur code in Figure 7.31. This vISA code\ncorresponds to the functional part of the Halide code.\n588\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 621,
        "text": "stencils directly. Note that the bigger the array of PEs, the less the halo overhead to\nsupport a given stencil size.\nFor Pixel Visual Core, the smaller size of the halo PEs and the 1616 arrays\nmeans it only costs 20% more area for the halo. For a 55 stencil, Pixel Visual\nCore can calculate 1.8 times as many results per clock cycle (162/122), and the ratio\nis 1.3 for a 33 stencil (162/142).\nThe design of the arithmetic unit of the PE is driven by multiply-accumulate\n(MAC), which is a primitive of stencil computation. Pixel Visual Core native\nMACs are 16-bits wide for the multiplies, but they can accumulate at a 32-bit\nwidth. Pipelining MAC would use energy unnecessarily because of the reading\nand writing of the added pipeline register. Thus the multiply-add hardware deter-\nmines the clock cycle. The other operations, previously mentioned, are the tradi-\ntional logical and arithmetic operations along with saturating versions of the\narithmetic operations and a few specialized instructions.\nThe PE has two 16-bit ALUs that can operate in a variety of ways within a\nsingle clock cycle:\n\u25a0\nIndependently, producing two 16-bit results: A op B, C op D.\n\u25a0\nFused, producing just one 16-bit result: A op (C op D).\n\u25a0\nJoined, producing one 32-bit result: A:C op B:D.\nTwo-Dimensional Line Buffers and Their Controller\nBecause DRAM accesses use so much energy (see Figure 7.32), the Pixel Visual\nCore memory system was carefully designed to minimize the number of DRAM\naccesses. The key innovation is the two-dimensional line buffer.\nKernels are logically running on separate cores, and they are connected in a\nDAG with input from the sensor or DRAM and output to DRAM. The line buffers\nhold portions of the image being calculated between kernels. Figure 7.37 shows the\nlogical use of line buffers in Pixel Visual Core.\n2D stencil\nprocessor\n2D stencil\nprocessor\nDRAM\nDRAM\nLens\n2D stencil\nprocessor\n2D stencil\nprocessor\nLineBuffer\nLineBuffer\nLineBuffer\nLineBuffer\nLineBuffer\nLineBuffer\nLineBuffer\nFigure 7.37 Programmer view of Pixel Visual Core: a directed-acyclic graph of kernels.\n7.7\nPixel Visual Core, a Personal Mobile Device Image Processing Unit\n\u25a0\n589"
    },
    {
        "page": 622,
        "text": "Here are four features that the two-dimensional line buffer must support:\n1. It must support two-dimensional stencil computations of various sizes, which\nare unknown at design time.\n2. Because of the halo, for the 1616 PE array in Pixel Visual Core, the STPs will\nwant to read 2020 blocks of pixels from the line buffer and write 1616\nblocks of pixels to the line buffer. (As previously mentioned, they call these\nblocks of pixels sheets.)\n3. Because the DAG is programmable, we need line buffers that can be allocated\nby software between any two cores.\n4. Several cores may need to read data from the same line buffer. Thus a line buffer\nshould support multiple consumers, although it needs just one producer.\nLine buffers in Pixel Visual Core are really a multi-reader, two-dimensional FIFO\nabstraction on top of a relatively large amount of SRAM: 128 KiB per instance. It\ncontains temporary \u201cimages\u201d that are used just once, so a small, dedicated local\nFIFO is much more efficient than a cache for data in distant memory.\nTo accommodate the size mismatch between reading 2020 blocks of pixels\nand writing 1616 blocks, the fundamental unit of allocation in the FIFO is a\ngroup of 44 pixels. Per stencil processor, there is one Line Buffer Pool (LBP)\nthat can have eight logical line buffers (LB), plus one LBP for DMA of I/O.\nThe LBP has three levels of abstraction:\n1. At the top, the LBP controller supports eight LBs as logical instances. Each LB\nhas one FIFO producer and up to eight FIFO consumers per LB.\n2. The controller keeps track of a set of head and tail pointers for each FIFO. Note\nthat the sizes of the line buffers inside the LBP are flexible and up to the\ncontroller.\n3. At the bottom are many physical memory banks to support the bandwidth\nrequirements. Pixel Visual Core has eight physical memory banks, each having\na 128-bit interface and 16 KiB of capacity.\nThe controller for the LBP is challenging because it must fulfill the bandwidth\ndemands of the STPs and I/O DMAs as well as schedule all their reads and writes\nto the banks of physical SRAM memory. The LBP controller is one of the most\ncomplicated pieces of Pixel Visual Core.\nPixel Visual Core Implementation\nThe first implementation of Pixel Visual Core was as a separate chip. Figure 7.38\nshows the floorplan of the chip, which has 8 cores. It was fabricated in a TSMC\n28 nm technology in 2016. The chip dimensions are 67.2 mm, it runs at\n426 MHz, it is stacked with 512 MB DRAM as Silicon in Package, and consumes\n590\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 623,
        "text": "(including the DRAM) 187\u20134500 mW depending on the workload. About 30% of\nthe power for the chip is for an ARMv7 A53 core for control, the MIPI, the PCIe,\nthe PCIe, and the LPDDR interfaces, interface is just over half this die at 23 mm2.\nPower for Pixel Visual Core running a worst case \u201cpower virus\u201d can go as high as\n3200 mW. Figure 7.39 shows the floor plan of a core.\nSummary: How Pixel Visual Core Follows the Guidelines\nPixel Visual Core is a multicore DSA for image and vision processing intended as a\nstand-alone chip or as an IP block for mobile device SOCs. As we will see in\nSection7.9,itsperformanceperwattforCNNsarefactorsof25\u2013100betterthanCPUs\nand GPUs. Here is how the Pixel Visual core followed the guidelines in Section 7.2.\n1. Use dedicated memories to minimize the distance over which data is moved.\nPerhaps the most distinguishing architecture feature of Pixel Visual Core is the\nsoftware-controlled, two-dimensional line buffers. At 128 KiB per core, they\nare a significant fraction of the area. Each core also has 64 KiB of software-\ncontrolled PE memory for temporary storage.\n2. Invest the resources saved from dropping advanced microarchitectural optimi-\nzations into more arithmetic units or bigger memories.\nTwo other key features of Pixel Visual Core are a 1616 two-dimensional\narray of processing elements per core and a two-dimensional shifting network\nbetween the processing elements. It offers a halo region that acts as a buffer to\nallow full utilization of its 256 arithmetic units.\n3 x MIPI-In\n2 x MIPI-Out\nA53\nLPDDR4\nPixel Visual Core\nPCIE\n4 x\nGen3\nFigure 7.38 Floor plan of the 8-core Pixel Visual Core chip. A53 is an ARMv7 core.\nLPDDR4 is a DRAM controller. PCIE and MIPI are I/O buses.\n7.7\nPixel Visual Core, a Personal Mobile Device Image Processing Unit\n\u25a0\n591"
    },
    {
        "page": 624,
        "text": "3. Use the easiest form of parallelism that matches the domain.\nPixel Visual Core relies on two-dimensional SIMD parallelism using its PE\narray, VLIW to express instruction-level parallelism, and multiple program\nmultiple data (MPMD) parallelism to utilize multiple cores.\n4. Reduce data size and type to the simplest needed for the domain.\nPixel Visual Core relies primarily on 8-bit and 16-bit integers, but it also works\nwith 32-bit integers, albeit more slowly.\n5. Use a domain-specific programming language to port code to the DSA.\nPixel Visual Core is programmed in the domain-specific language Halide for\nimage processing and in TensorFlow for CNNs.\n7.8\nCross-Cutting Issues\nHeterogeneity and System on a Chip (SOC)\nThe easy way to incorporate DSAs into a system is over the I/O bus, which is the\napproach of the data center accelerators in this chapter. To avoid fetching memory\noperands over the slow I/O bus, these accelerators have local DRAM.\nFigure 7.39 Floor plan of a Pixel Visual Core. From left to right, and top down: the sca-\nlar lane (SCL) is 4% of the core area, NOC is 2%, the line buffer pool (LBP) is 15%, the sheet\ngenerator (SHG) is 5%, the halo is 11%, and the processing element array is 62%. The torus\nconnection of the halo makes each of the four edges of the array logical neighbors. It is\nmore area-efficient to collapse the halo to just two sides, which preserves the topology.\n592\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 625,
        "text": "Amdahl\u2019s Law reminds us that the performance of an accelerator is limited by\nthe frequency of shipping data between the host memory and the accelerator mem-\nory. There will surely be applications that would benefit from the host CPU and the\naccelerators to be integrated into the same system on a chip (SOC), which is one of\nthe goals of Pixel Visual Core and eventually the Intel Crest.\nSuch a design is called an IP block, standing for Intellectual Property, but a\nmore descriptive name might be portable design block. IP blocks are typically\nspecified in a hardware description language like Verilog or VHDL to be inte-\ngrated into the SOC. IP blocks enable a marketplace where many companies\nmake IP blocks that other companies can buy to build the SOCs for their appli-\ncations without having to design everything themselves. Figure 7.40 indicates\nthe importance of IP blocks by plotting the number of IP blocks across genera-\ntions of Apple PMD SOCs; they tripled in just four years. Another indication of\nthe importance of IP blocks is that the CPU and GPU get only one-third of the\narea of the Apple SOCs, with IP blocks occupying the remainder (Shao and\nBrooks, 2015).\nDesigning an SOC is like city planning, where independent groups lobby for\nlimited resources, and finding that the right compromise is difficult. CPUs, GPUs,\ncaches, video encoders, and so on have adjustable designs that can shrink or\nexpand to use more or less area and energy to deliver more or less performance.\nBudgets will differ depending on whether the SOC is for tablets or for IoT. Thus an\nIP block must be scalable in area, energy, and performance. Moreover, it is espe-\ncially important for a new IP block to offer a small resource version because it may\nnot already have a well-established foothold in the SOC ecosystem; adoption is\nmuch easier if the initial resource request can be modest. The Pixel Visual Core\napproach is a multicore design, allowing the SOC engineer to choose between 2\nand 16 cores to match the area and power budget and desired performance.\nA8\n(2014)\nA7\n(2013)\nA6\n(2012)\n# of specialized IP blocks\nA5\n(2011)\nA4\n(2010)\n0\n10\n20\n30\nFigure 7.40 Number of IP blocks in Apple SOCs for the iPhone and iPad between\n2010 and 2014 (Shao and Brooks, 2015).\n7.8\nCross-Cutting Issues\n\u25a0\n593"
    },
    {
        "page": 626,
        "text": "It will be interesting to see whether the attractiveness of integration leads to\nmost data center processors coming from traditional CPU companies with IP accel-\nerators integrated into the CPU die, or whether systems companies will continue\ndesigning their own accelerators and include IP CPUs in their ASICs.\nAn Open Instruction Set\nOne challenge for designers of DSAs is determining how to collaborate with a CPU\nto run the rest of the application. If it\u2019s going to be on the same SOC, then a major\ndecision is which CPU instruction set to choose, because until recently virtually\nevery instruction set belonged to a single company. Previously, the practical first\nstep of an SOC was to sign a contract with a company to lock in the instruction set.\nThe alternative was to design your own custom RISC processor and to port a\ncompiler and libraries to it. The cost and hassle of licensing IP cores led to a sur-\nprisingly large number of do-it-yourself simple RISC processors in SOCs. One\nAMD engineer estimated that there were 12 instruction sets in a modern\nmicroprocessor!\nRISC-V offers a third choice: a viable free and open instruction set with plenty\nof opcode space reserved for adding instructions for domain-specific coprocessors,\nwhich enables the previously mentioned tighter integration between CPUs and\nDSAs. SOC designers can now select a standard instruction set that comes with\na large base of support software without having to sign a contract.\nThey still have to pick the instruction set early in the design, but they don\u2019t have\nto pick one company and sign a contract. They can design a RISC-V core them-\nselves, they can buy one from the several companies that sell RISC-V IP blocks, or\nthey can download one of the free open-source RISC-V IP blocks developed by\nothers. The last case is analogous to open-source software, which offers web\nbrowsers, compilers, operating systems, and so on that volunteers maintain for\nusers to download and use for free.\nAs a bonus, the open nature of the instruction set improves the business case for\nsmall companies offering RISC-V technology because customers don\u2019t have to\nworry about the long-term viability of a company with its own unique\ninstruction set.\nAnother attraction of RISC-V for DSAs is that the instruction set is not as\nimportant as it is for general-purpose processors. If DSAs are programmed at\nhigher levels using abstractions like DAGs or parallel patterns, as is the case\nfor Halide and TensorFlow, then there is less to do at the instruction set level.\nMoreover, in a world where performance-cost and energy-cost advances come\nfrom adding DSAs, binary compatibility may not play as important a role as\nin the past.\nAt the time of this writing, the future of the open RISC-V instruction set\nappears promising. (We wish we could peer into the future and learn the status\nof RISC-V from now to the next edition of this book!)\n594\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 627,
        "text": "7.9\nPutting It All Together: CPUs Versus GPUs Versus\nDNN Accelerators\nWe now use the DNN domain to compare the cost-performance of the accelerators\nin this chapter.2 We start with a thorough comparison of the TPU to standard CPUs\nand GPUs and then add brief comparisons to Catapult and Pixel Visual Core.\nFigure 7.41 shows the six benchmarks we use in this comparison. They consist\nof two examples of each of the three types of DNNs in Section 7.3. These six\nbenchmarks represent 95% of TPU inference workload in Google data centers\nin 2016. Typically written in TensorFlow, they are surprisingly short: just 100\u2013\n1500 lines of code. They are small pieces of larger applications that run on the host\nserver, which can be thousands to millions of lines of C++ code. The applications\nare typically user-facing, which leads to rigid response-time limits, as we will see.\nFigures 7.42 and 7.43 show the chips and servers being compared. They are\nserver-class computers deployed in Google data centers at the same time that TPUs\nwere deployed. To be deployed in Google data centers, they must at least check for\ninternal memory errors, which excluded some choices, such as the Nvidia Maxwell\nGPU. For Google to purchase and deploy them, the machines had to be sensibly\nconfigured, and not awkward artifacts assembled solely to win benchmarks.\nThe traditional CPU server is represented by an 18-core, dual-socket Haswell\nprocessor from Intel. This platform is also the host server for GPUs or TPUs.\n2This section is also largely based upon the paper \u201cIn-Datacenter Performance Analysis of a Tensor Processing Unit\u201d\nJouppi et al., 2017, of which one of your book authors was a coauthor.\nName\nLOC\nDNN layers\nWeights\nTPU Ops/Weight\n% deployed\nTPUs 2016\nFC\nConv\nElement\nPool\nTotal\nMLP0\n100\n5\n5\n20M\n200\n61%\nMLP1\n1000\n4\n4\n5M\n168\nLSTM0\n1000\n24\n34\n58\n52M\n64\n29%\nLSTM1\n1500\n37\n19\n56\n34M\n96\nCNN0\n1000\n16\n16\n8M\n2888\n5%\nCNN1\n1000\n4\n72\n13\n89\n100M\n1750\nFigure 7.41 Six DNN applications (two per DNN type) that represent 95% of the TPU\u2019s workload. The 10 columns\nare the DNN name; the number of lines of code; the types and number of layers in the DNN (FC is fully connected;\nConv is convolution; Element is element-wise operation of LSTM, see Section 7.3; and Pool is pooling, which is a\ndownsizing stage that replaces a group of elements with its average or maximum); the number of weights; TPU oper-\national intensity; and TPU application popularity in 2016. The operational intensity varies between TPU, CPU, and GPU\nbecause the batch sizes vary. The TPU can have larger batch sizes while still staying under the response time limit.\nOne DNN is RankBrain (Clark, 2015), one LSTM is GNM Translate (Wu et al., 2016), and one CNN is DeepMind AlphaGo\n(Silver et al., 2016; Jouppi, 2016).\n7.9\nPutting It All Together: CPUs Versus GPUs Versus DNN Accelerators\n\u25a0\n595"
    },
    {
        "page": 628,
        "text": "Haswell is fabricated in an Intel 22-nm process. Both the CPU and GPU are very\nlarge dies: about 600 mm2!\nThe GPU accelerator is the Nvidia K80. Each K80 card contains two dies and\noffers SECDED on internal memory and DRAM. Nvidia states that (Nvidia, 2016)\nthe K80 Accelerator dramatically lowers datacenter cost by delivering applica-\ntion performance with fewer, more powerful servers.\nDNN researchers frequently used K80s in 2015, which is when they were deployed\nat Google. Note that K80s were also chosen for new cloud-based GPUs by Amazon\nWeb Services and by Microsoft Azure in late 2016.\nBecause the number of dies per benchmarked server varies between 2 and 8,\nthe following figures show results normalized per die, except for Figure 7.50,\nwhich compares the performance/watt of whole servers.\nPerformance: Rooflines, Response Time, and Throughput\nTo illustrate the performance of the six benchmarks on the three processors, we\nadapt the Roofline performance model in Chapter 4. To use the Roofline model\nfor the TPU, when DNN applications are quantized, we first replace floating-point\noperations with integer multiply-accumulate operations. As weights do not\nnormally fit in on-chip memory for DNN applications, the second change is to\nredefine operational intensity to be integer operations per byte of weights read\n(Figure 7.41).\nChip model\nmm2\nnm\nMHz\nTDP\nMeasured\nTOPS/s\nGB/s\nOn-chip memory\nIdle\nBusy\n8b\nFP\nIntel Haswell\n662\n22\n2300\n145W\n41W\n145W\n2.6\n1.3\n51\n51 MiB\nNVIDIA K80\n561\n28\n560\n150W\n25W\n98W\n\u2013\n2.8\n160\n8 MiB\nTPU\n<331*\n28\n700\n75W\n28W\n40W\n92\n\u2013\n34\n28 MiB\n*The TPU die size is less than half of the Haswell die size.\nFigure 7.42 The chips used by the benchmarked servers are Haswell CPUs, K80 GPUs, and TPUs. Haswell has 18\ncores, and the K80 has 13 SMX processors.\nServer\nDies/Server\nDRAM\nTDP\nMeasured power\nIdle\nBusy\nIntel Haswell\n2\n256 GiB\n504W\n159W\n455W\nNVIDIA K80 (2 dies/card)\n8\n256 GiB (host)+12 GiB8\n1838W\n357W\n991W\nTPU\n4\n256 GiB (host)+8 GiB4\n861W\n290W\n384W\nFigure 7.43 Benchmarked servers that use the chips in Figure 7.42. The low-power TPU allows for better rack-level\ndensity than the high-power GPU. The 8 GiB DRAM per TPU is Weight Memory.\n596\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 629,
        "text": "Figure 7.44 shows the Roofline model for a single TPU on log-log scales. The\nTPU has a long \u201cslanted\u201d part of its Roofline, where operational intensity means\nthat performance is limited by memory bandwidth rather than by peak compute.\nFive of the six applications are happily bumping their heads against the ceiling:\nthe MLPs and LSTMs are memory-bound, and the CNNs are computation-bound.\nThe single DNN that is not bumping its head against the ceiling is CNN1. Despite\nCNNs having very high operational intensity, CNN1 is running at only 14.1 Tera\nOperations Per Second (TOPS), while CNN0 runs at a satisfying 86 TOPS.\nFor readers interested into a deep dive into what happened with CNN1,\nFigure 7.45 uses performance counters to give partial visibility into the utilization\nof the TPU. The TPU spends less than half of its cycles performing matrix oper-\nations for CNN1 (column 7, row 1). On each of those active cycles, only about half\nof the 65,536 MACs hold useful weights because some layers in CNN1 have shal-\nlow feature depths. About 35% of cycles are spent waiting for weights to load from\nmemory into the matrix unit, which occurs during the four fully connected layers\nthat run at an operational intensity of just 32. This leaves roughly 19% of cycles not\n100\n1000\n1\n0.1\n10\n0.5\n1\n5\n10\n50\nRoofline\nTPU log-log\nTeraOps/sec (log scale)\nLSTM0\nLSTM1\nMLP1\nMLP0\nCNN0\nCNN1\n14.1\n86.0\n12.3\n9.7\n2.8\n3.7\nOperational intensity: MAC Ops/weight byte (log scale)\nFigure 7.44 TPU Roofline. Its ridge point is far to the right at 1350 multiply-\naccumulate operations per byte of weight memory. CNN1 is much further below\nits Roofline than the other DNNs because it spends about a third of the time waiting\nfor weights to be loaded into the matrix unit and because the shallow depth of some\nlayers in the CNN results in only half of the elements within the matrix unit holding use-\nful values ( Jouppi et al., 2017).\n7.9\nPutting It All Together: CPUs Versus GPUs Versus DNN Accelerators\n\u25a0\n597"
    },
    {
        "page": 630,
        "text": "explained by the matrix-related counters. Because of overlapped execution on the\nTPU, we do not have exact accounting for those cycles, but we can see that 23% of\ncycles have stalls for RAW dependences in the pipeline and that 1% are spent\nstalled for input over the PCIe bus.\nFigures 7.46 and 7.47 show Rooflines for Haswell and the K80. The six NN\napplications are generally further below their ceilings than the TPU in Figure 7.44.\nResponse-time limits are the reason. Many of these DNN applications are parts of\nservices that are part of end-user-facing services. Researchers have demonstrated\nthat small increases in response time cause customers to use a service less (see\nChapter 6). Thus, although training may not have hard response-time deadlines,\ninference usually does. That is, inference cares about throughput only while it\nis maintaining the latency bound.\nFigure 7.48 illustrates the impact of the 99th percentile response-time limit of 7\nms for MLP0 on Haswell and the K80, which was required by the application\ndeveloper. (The inferences per second and 7-ms latency include the server host\ntime as well as the accelerator time.) They can operate at 42% and 37%, respec-\ntively, with the highest throughput achievable for MLP0, if the response-time limit\nis relaxed. Thus, although CPUs and GPUs have potentially much higher through-\nput, it\u2019s wasted if they don\u2019t meet the response-time limit. These bounds affect the\nTPU as well, but at 80% in Figure 7.48, it is operating much closer to its highest\nMLP0 throughput. As compared with CPUs and GPUs, the single-threaded TPU\nhas none of the sophisticated microarchitectural features discussed in Section 7.1\nthat consume transistors and energy to improve the average case but not the 99th-\npercentile case.\nApplication\nMLP0\nMLP1\nLSTM0\nLSTM1\nCNN0\nCNN1\nMean\nRow\nArray active cycles\n12.7%\n10.6%\n8.2%\n10.5%\n78.2%\n46.2%\n28%\n1\nUseful MACs in 64K matrix (% peak)\n12.5%\n9.4%\n8.2%\n6.3%\n78.2%\n22.5%\n23%\n2\nUnused MACs\n0.3%\n1.2%\n0.0%\n4.2%\n0.0%\n23.7%\n5%\n3\nWeight stall cycles\n53.9%\n44.2%\n58.1%\n62.1%\n0.0%\n28.1%\n43%\n4\nWeight shift cycles\n15.9%\n13.4%\n15.8%\n17.1%\n0.0%\n7.0%\n12%\n5\nNon-matrix cycles\n17.5%\n31.9%\n17.9%\n10.3%\n21.8%\n18.7%\n20%\n6\nRAW stalls\n3.3%\n8.4%\n14.6%\n10.6%\n3.5%\n22.8%\n11%\n7\nInput data stalls\n6.1%\n8.8%\n5.1%\n2.4%\n3.4%\n0.6%\n4%\n8\nTeraOp/s (92 Peak)\n12.3\n9.7\n3.7\n2.8\n86.0\n14.1\n21.4\n9\nFigure 7.45 Factors limiting TPU performance of the NN workload based on hardware performance counters.\nRows 1, 4, 5, and 6 total 100% and are based on measurements of activity of the matrix unit. Rows 2 and 3 further\nbreak down the fraction of 64K weights in the matrix unit that hold useful weights on active cycles. Our counters\ncannot exactly explain the time when the matrix unit is idle in row 6; rows 7 and 8 show counters for two possible\nreasons, including RAW pipeline hazards and PCIe input stalls. Row 9 (TOPS) is based on measurements of production\ncode while the other rows are based on performance-counter measurements, so they are not perfectly consistent.\nHost server overhead is excluded here. The MLPs and LSTMs are memory-bandwidth limited, but CNNs are not. CNN1\nresults are explained in the text.\n598\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 631,
        "text": "Figure 7.49 gives the bottom line of relative inference performance per die,\nincluding the host server overhead for the two accelerators. Recall that architects\nuse the geometric mean when they don\u2019t know the actual mix of programs that will\nbe run. For this comparison, however, we do know the mix (Figure 7.41). The\nHaswell log-log\nTeraOps/sec (log scale)\nRoofline\nLSTM0\nLSTM1\nMLP1\nMLP0\nCNN0\nCNN1\n100\n1000\n1\n0.1\n10\n0.6\n0.6\n0.5\n1.1\n0.2\n0.3\nOperational intensity: MAC Ops/weight byte (log scale)\n0.2\n0.4\n0.6\n0.8\n1\n2\nFigure 7.46 Intel Haswell CPU Roofline with its ridge point at 13 multiply-accumulate operations/byte, which is\nmuch further to the left than in Figure 7.44.\nK80 log-log\nTeraOps/sec (log scale)\nRoofline\nLSTM0\nLSTM1\nMLP1\nMLP0\nCNN0\nCNN1\n0.9\n0.7\n0.7\n0.5\n0.2\n1.0\n100\n1000\n1\n0.2\n10\nOperational intensity: MAC Ops/weight byte (log scale)\n0.4\n0.6\n0.8\n1\n2\nFigure 7.47 NVIDIA K80 GPU die Roofline. The much higher memory bandwidth moves the ridge point to 9\nmultiply-accumulate operations per weight byte, which is even further to the left than in Figure 7.46.\n7.9\nPutting It All Together: CPUs Versus GPUs Versus DNN Accelerators\n\u25a0\n599"
    },
    {
        "page": 632,
        "text": "weighted mean in the last column of Figure 7.49 using the actual mix makes the\nGPU up to 1.9 times, and the TPU is 29.2 times as fast as the CPU, so the TPU is\n15.3 times as fast as the GPU.\nCost-Performance, TCO, and Performance/Watt\nWhen buying computers by the thousands, cost-performance trumps general per-\nformance. The best cost metric in a data center is total cost of ownership (TCO).\nThe actual price Google pays for thousands of chips depends on negotiations\nbetween the companies involved. For business reasons, Google can\u2019t publish such\nprice information or data that might let them be deduced. However, power is cor-\nrelated with TCO, and Google can publish watts per server, so we use performance/\nwatt as our proxy for performance/TCO. In this section, we compare servers\n(Figure 7.43) rather than single dies (Figure 7.42).\nFigure 7.50 shows the weighted mean performance/watt for the K80 GPU and\nTPU relative to the Haswell CPU. We present two different calculations of perfor-\nmance/watt. The first (\u201ctotal\u201d) includes the power consumed by the host CPU\nserver when calculating performance/watt for the GPU and TPU. The second\n(\u201cincremental\u201d) subtracts the host CPU server power from the total for the GPU\nand TPU beforehand.\nType\nBatch\n99th% response\nInf/s (IPS)\n% max IPS\nCPU\n16\n7.2 ms\n5482\n42%\nCPU\n64\n21.3 ms\n13,194\n100%\nGPU\n16\n6.7 ms\n13,461\n37%\nGPU\n64\n8.3 ms\n36,465\n100%\nTPU\n200\n7.0 ms\n225,000\n80%\nTPU\n250\n10.0 ms\n280,000\n100%\nFigure 7.48 99th% response time and per die throughput (IPS) for MLP0 as batch\nsize varies. The longest allowable latency is 7 ms. For the GPU and TPU, the maximum\nMLP0 throughput is limited by the host server overhead.\nType\nMLP0\nMLP1\nLSTM0\nLSTM1\nCNN0\nCNN1\nMean\nGPU\n2.5\n0.3\n0.4\n1.2\n1.6\n2.7\n1.9\nTPU\n41.0\n18.5\n3.5\n1.2\n40.3\n71.0\n29.2\nRatio\n16.7\n60.0\n8.0\n1.0\n25.4\n26.3\n15.3\nFigure 7.49 K80 GPU and TPU performance relative to CPU for the DNN workload. The mean uses the actual mix of\nthe six applications in Figure 7.41. Relative performance for the GPU and TPU includes host server overhead.\nFigure 7.48 corresponds to the second column of this table (MLP0), showing relative IPS that meet the 7-ms latency\nthreshold.\n600\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 633,
        "text": "For total-performance/watt, the K80 server is 2.1 Haswell. For incremental-\nperformance/watt, when Haswell power is omitted, the K80 server is 2.9.\nThe TPU server has 34 times better total-performance/watt than Haswell,\nwhich makes the TPU server 16 times the performance/watt of the K80 server.\nThe relative incremental-performance/watt\u2014which was Google\u2019s justification\nfor a custom ASIC\u2014is 83 for the TPU, which lifts the TPU to 29 times the per-\nformance/watt of the GPU.\nEvaluating Catapult and Pixel Visual Core\nCatapult V1 runs CNNs 2.3 as fast as a 2.1 GHz, 16-core, dual-socket server\n(Ovtcharov et al., 2015a). Using the next generation of FPGAs (14-nm Arria\n10), performance goes up 7, and perhaps even 17 with more careful floorplan-\nning and scaling up of the Processing Elements (Ovtcharov et al., 2015b). In both\ncases, Catapult increases power by less than 1.2. Although it\u2019s apples versus\noranges, the TPU runs its CNNs 40 to 70 versus a somewhat faster server\n(see Figures 7.42, 7.43, and 7.49).\nBecause Pixel Visual Core and the TPU are both made by Google, the good\nnews is that we can directly compare performance for CNN1, which is a common\nDNN, although it had to be translated from TensorFlow. It runs with batch size of 1\ninstead of 32 as in the TPU. The TPU runs CNN1 about 50 times as fast as Pixel\nVisual Core, which makes Pixel Visual Core about half as fast as the GPU and a\nlittle faster than Haswell. Incremental performance/watt for CNN1 raises Pixel\nVisual Core to about half the TPU, 25 times the GPU, and 100 times the CPU.\nPerformance/Watt relative to\nCPU or GPU\nIncremental performance/Watt\nTPU/GPU\nTotal performance/Watt\n0\n25\n50\n75\n100\n2.9\n83\n29\n2.1\n34\n16\nTPU/CPU\nGPU/CPU\nFigure 7.50 Relative performance/watt of GPU and TPU servers to CPU or GPU\nservers. Total performance/watt includes host server power, but incremental doesn\u2019t.\nIt is a widely quoted metric, but we use it as a proxy for performance/TCO in the\ndata center.\n7.9\nPutting It All Together: CPUs Versus GPUs Versus DNN Accelerators\n\u25a0\n601"
    },
    {
        "page": 634,
        "text": "Because the Intel Crest is designed for training rather than inference, it wouldn\u2019t be\nfair to include it in this section, even if it were available to measure.\n7.10\nFallacies and Pitfalls\nIn these early days of both DSAs and DNNs, fallacies abound.\nFallacy\nIt costs $100 million to design a custom chip.\nFigure 7.51 shows a graph from an article that debunks the widely quoted $100-\nmillion myth that it was \u201conly\u201d $50 million, with most of the cost being salaries\n(Olofsson, 2011). Note that the author\u2019s estimate is for sophisticated processors\nthat include features that DSAs by definition omit, so even if there were no\nimprovement to the development process, you would expect the cost of a DSA\ndesign to be less.\nWhy are we more optimistic six years later, when, if anything, mask costs are\neven higher for the smaller process technologies?\nFirst, software is the largest category, at almost a third of the cost. The avail-\nability of applications written in domain-specific languages allows the compilers to\ndo most of the work of porting the applications to your DSA, as we saw for the TPU\nand Pixel Visual Core. The open RISC-V instruction set will also help reduce the\ncost of getting system software as well as cut the large IP costs.\nMaskandfabricationcostscanbesavedbyhavingmultipleprojectsshareasingle\nreticle. As long as you have a small chip, amazingly enough, for $30,000 anyone can\nget 100 untested parts in 28-nm TSMC technology (Patterson and Nikoli\u0001c, 2015).\nSOFTWARE,\n$15,750,000\nHARDWARE,\n$13,500,000\nEDA TOOLS,\n$9,000,000\nFABRICATION,\n$5,000,000\nIP,\n$5,000,000\nSales+\nManagement,\n$4,500,000\nFigure 7.51 The breakdown of the $50 million cost of a custom ASIC that came from\nsurveying others (Olofsson, 2011). The author wrote that his company spent just $2\nmillion for its ASIC.\n602\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 635,
        "text": "Perhaps the biggest change is to hardware engineering, which is more than a\nquarter of the cost. Hardware engineers have begun to follow their software col-\nleagues to use agile development. The traditional hardware process not only has\nseparate phases for design requirements, architecture, logical design, layout, ver-\nification, and so on, but also it uses different job titles for the people who perform\neach of the phases. This process is heavy on planning, documentation, and sched-\nuling in part because of the change in personnel each phase.\nSoftware used to follow this \u201cwaterfall\u201d model as well, but projects were so com-\nmonly late, over budget, and even canceled that it led to a radically different\napproach. The Agile Manifesto in 2001 basically said that it was much more likely\nthat a small team that iterated on an incomplete but working prototype shown reg-\nularly to customers would produce useful software on schedule and on budget more\nthan the traditional plan-and-document approach of the waterfall process would.\nSmall hardware teams now do agile iterations (Lee et al., 2016). To ameliorate\nthe long latency of a chip fabrication, engineers do some iterations using FPGAs\nbecause modern design systems can produce both the EDIF for FPGAs and chip\nlayout from a single design. FPGA prototypes run 10\u201320 times slower than chips,\nbut that is still much faster than simulators. They also do \u201ctape-in\u201d iterations, where\nyou do all the work of a tape-out for your working but incomplete prototype, but\nyou don\u2019t pay the costs of fabricating a chip.\nIn addition to an improved development process, more modern hardware design\nlanguages to support them (Bachrach et al., 2012), and advances in automatic gen-\neration of hardware from high-level domain-specific languages (Canis et al., 2013;\nHuang et al., 2016; Prabhakar et al., 2016). Open source cores that you can download\nfor free and modify should also lower the cost of hardware design.\nPitfall\nPerformance counters added as an afterthought for DSA hardware.\nThe TPU has 106 performance counters, and the designers wanted even more (see\nFigure 7.45). The raison d\u2019^etre for DSAs is performance, and it is way too early in\ntheir evolution to have a good idea about what is going on.\nFallacy\nArchitects are tackling the right DNN tasks.\nThe architecture community is paying attention to deep learning: 15% of the papers\nat ISCA 2016 were on hardware accelerators for DNNs! Alas, all nine papers\nlooked at CNNs, and only two mentioned other DNNs. CNNs are more complex\nthan MLPs and are prominent in DNN competitions (Russakovsky et al., 2015),\nwhich might explain their allure, but they are only about 5% of the Google data\ncenter NN workload. It seems wise try to accelerate MLPs and LSTMs with at least\nas much gusto.\nFallacy\nFor DNN hardware, inferences per second (IPS) is a fair summary performance\nmetric.\nIPS is not appropriate as a single, overall performance summary for DNN hardware\nbecause it\u2019s simply the inverse of the complexity of the typical inference in the\napplication (e.g., the number, size, and type of NN layers). For example, the\n7.10\nFallacies and Pitfalls\n\u25a0\n603"
    },
    {
        "page": 636,
        "text": "TPU runs the 4-layer MLP1 at 360,000 IPS but the 89-layer CNN1 at only 4700\nIPS; thus TPU IPS varies by 75X! Therefore using IPS as the single-speed sum-\nmary is much more misleading for NN accelerators than MIPS or FLOPS is for\ntraditional processors, so IPS should be even more disparaged. To compare\nDNN machines better, we need a benchmark suite written at a high level to port\nit to the wide variety of DNN architectures. Fathom is a promising new attempt\nat such a benchmark suite (Adolf et al., 2016).\nPitfall\nBeing ignorant of architecture history when designing a DSA.\nIdeas that didn\u2019t fly for general-purpose computing may be ideal for DSAs, thus\nhistory-aware architects could have a competitive edge. For the TPU, three impor-\ntant architectural features date back to the early 1980s: systolic arrays (Kung and\nLeiserson, 1980), decoupled-access/execute (Smith, 1982b), and CISC instruc-\ntions (Patterson and Ditzel, 1980). The first reduced the area and power of the large\nMatrix Multiply Unit, the second fetched weights concurrently during operation of\nthe Matrix Multiply Unit, and the third better utilized the limited bandwidth of the\nPCIe bus for delivering instructions. We advise mining the historical perspectives\nsections at the end of every chapter of this book to discover jewels that could\nembellish DSAs that you design.\n7.11\nConcluding Remarks\nIn this chapter, we\u2019ve seen several commercial examples of the recent shift from\nthe traditional goal of improving general-purpose computers so that all programs\nbenefit to accelerating a subset of programs with DSAs.\nBoth versions of Catapult preserved data-center homogeneity by designing a\nsmall, low-power FPGA board that could fit inside a server. The hope is that\nthe flexibility of FPGAs will allow Catapult to be useful to many current applica-\ntions and the new ones that appeared after deployment. Catapult runs search rank\nand CNNs faster than GPUs, offering a 1.5\u20131.75 gain in performance/TCO for\nranking over CPUs.\nThe TPU project actually began with FPGAs but abandoned them when the\ndesigners concluded that the FPGAs of that time were not competitive in perfor-\nmance compared to the GPUs. They also believed the TPU would use much less\npower than GPUs, while being as fast or faster, potentially making the TPU much\nbetter than FPGAs and GPUs. Finally, the TPU was not the device that broke data\ncenter homogeneity at Google because some servers in its data centers already had\nGPUs. The TPU basically followed in the footsteps of the GPU and was just\nanother type of accelerator.\nThe nonrecurring engineering costs were likely much higher for the TPU than\nfor Catapult, but the rewards were also greater: both performance and performance/\nwatt were much higher for an ASIC than for an FPGA. The risk was that the TPU\nwas appropriate only for DNN inference, but as we mentioned, DNNs are an attrac-\ntive target because they can potentially be used for many applications. In 2013\n604\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 637,
        "text": "Google\u2019s management took a leap of faith by trusting that the DNN requirements in\n2015 and beyond would justify investment in the TPU.\nThe deterministic execution model of both Catapult and the TPU is a better\nmatch to the response-time deadline of user-facing applications than are the\ntime-varying optimizations of CPUs and GPUs (caches, out-of-order execution,\nmultithreading, multiprocessing, prefetching, etc.) that help average throughput\nmore than latency. The lack of such features helps explain why, despite having\nmyriad ALUs and a big memory, the TPU is relatively small and low powered.\nThis achievement suggests a \u201cCornucopia Corollary\u201d to Amdahl\u2019s Law: low uti-\nlization of a huge, cheap resource can still deliver high, cost-effective\nperformance.\nIn summary, the TPU succeeded for DNNs because of the large matrix unit; the\nsubstantial software-controlled on-chip memory; the ability to run whole inference\nmodels to reduce dependence on host CPU; a single-threaded, deterministic exe-\ncution model that proved to be a good match to 99th-percentile response-time\nlimits; enough flexibility to match the DNNs of 2017 as well as of 2013; the omis-\nsion of general-purpose features that enabled a small and low-power die despite the\nlarger datapath and memory; the use of 8-bit integers by the quantized applications;\nand the fact that applications were written using TensorFlow, which made it easy to\nport them to the DSA at high-performance rather than having to rewrite them in\norder for them to run well on the very different hardware.\nPixel Visual Core demonstrated the constraints of designing a DSA for a PMD\nin terms of die size and power. Unlike the TPU, it is a separate processor from the\nhost that fetches its own instructions. Despite being aimed primarily at computer\nvision, Pixel Visual Core can run CNNs one to two orders of magnitude better in\nperformance/watt than the K80 GPU and the Haswell CPU.\nIt\u2019s too early to render judgment on the Intel Crest, although its enthusiastic\nannouncement by the Intel CEO signals a shift in the computing landscape.\nAn Architecture Renaissance\nFor at least the past decade, architecture researchers have been publishing innova-\ntions based on simulations using limited benchmarks claiming improvements for\ngeneral-purpose processors of 10% or less while companies are now reporting\ngains for DSA hardware products of 10 times or more.\nWe think that is a signthat the field isundergoinga transformation, and we expect\nto see a renaissance in architecture innovation in the next decade because of\n\u25a0\nthe historic end of both Dennard scaling and Moore\u2019s Law, which means\nimproving cost-energy-performance will require innovation in computer\narchitecture;\n\u25a0\nthe productivity advances in building hardware from both Agile hardware\ndevelopment and new hardware design languages that leverage advances in\nmodern programming languages;\n7.11\nConcluding Remarks\n\u25a0\n605"
    },
    {
        "page": 638,
        "text": "\u25a0\nthe reduced cost of hardware development because of free and open instruction\nsets, open-source IP blocks, and commercial IP blocks (which so far is where\nmost DSAs are found);\n\u25a0\nthe improvements mentioned above in productivity and cost of development\nmeans researchers can afford to demonstrate their ideas by building them in\nFPGAs or even in custom chips, instead of trying to convince skeptics with\nsimulators; and\n\u25a0\nthe potential upside of DSAs and their synergy with domain-specific program-\nming languages.\nWe believe that many architecture researchers will build DSAs that will raise the\nbar still higher than those discussed in this chapter. And we can\u2019t wait to see what\nthe computer architecture world will look like by the next edition of this book!\n7.12\nHistorical Perspectives and References\nSection M.9 (available online) covers the development of DSAs.\nCase Studies and Exercises by Cliff Young\nCase Study: Google\u2019s Tensor Processing Unit and Acceleration\nof Deep Neural Networks\nConcepts illustrated by this case study\n\u25a0\nStructure of matrix multiplication operations\n\u25a0\nCapacities of memories and rates of computations (\u201cspeeds and feeds\u201d) for a\nsimple neural network model\n\u25a0\nConstruction of a special-purpose ISA\n\u25a0\nInefficiencies in mapping convolutions to TPU hardware\n\u25a0\nFixed-point arithmetic\n\u25a0\nFunction approximation\n7.1\n[10/20/10/25/25] <7.3,7.4>Matrix multiplication is a key operation supported in\nhardware by the TPU. Before going into details of the TPU hardware, it\u2019s worth\nanalyzing the matrix multiplication calculation itself. One common way to depict\nmatrix multiplication is with the following triply nested loop:\nfloat a[M][K], b[K][N], c[M][N];\n// M, N, and K are constants.\nfor (int i = 0; i < M; ++i)\nfor (int j = 0; j < N; ++j)\nfor (int k = 0; k < K; ++k)\nc[i][j] += a[i][k] * b[k][j];\n606\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 639,
        "text": "a. [10] Suppose that M, N, and K are all equal. What is the asymptotic complexity\nin time of this algorithm? What is the asymptotic complexity in space of the\narguments? What does this mean for the operational intensity of matrix multi-\nplication as M, N, and K grow large?\nb. [20] Suppose that M=3, N=4, and K=5, so that each of the dimensions are\nrelatively prime. Write out the order of accesses to memory locations in each\nof the three matrices A, B, and C (you might start with two-dimensional indices,\nthen translate those to memory addresses or offsets from the start of each\nmatrix). For which matrices are the elements accessed sequentially? Which\nare not? Assume row-major (C-language) memory ordering.\nc. [10] Suppose that you transpose matrix B, swapping its indices so that they are\nB[N][K] instead. So, now the innermost statement of the loop looks like:\nc[i][j] += a[i][k] * b[j][k];\nNow, for which matrices are the elements accessed sequentially?\nd. [25] The innermost (k-indexed) loop of our original routine performs a\ndot-product operation. Suppose that you are a given a hardware unit that can\nperform an 8-element dot-product more efficiently than the raw C code, behav-\ning effectively like this C function:\nvoid hardware_dot(float *accumulator,\nconst float *a_slice, const float *b_slice) {\nfloat total = 0.;\nfor (int k = 0; k < 8; ++k) {\ntotal += a_slice[k] * b_slice[k];\n}\n*accumulator += total;\n}\nHow would you rewrite the routine with the transposed B matrix from part (c) to\nuse this function?\ne. [25] Suppose that instead, you are given a hardware unit that performs an\n8-element \u201csaxpy\u201d operation, which behaves like this C function:\nvoid hardware_saxpy(float *accumulator,\nfloat a, const float *input) {\nfor (int k = 0; k < 8; ++k) {\naccumulator[k] += a * input[k];\n}\n}\nWrite another routine that uses the saxpy primitive to deliver equivalent results to\nthe original loop, without the transposed memory ordering for the B matrix.\n7.2\n[15/10/10/20/15/15/20/20] <7.3,7.4>Consider the neural network model MLP0\nfrom Figure 7.5. That model has 20 M weights in five fully connected layers (neural\nnetwork researchers count the input layer as if it were a layer in the stack, but it has no\nCase Studies and Exercises by Cliff Young\n\u25a0\n607"
    },
    {
        "page": 640,
        "text": "weights associated with it). For simplicity, let\u2019s assume that those layers are each the\nsame size, so each layer holds 4 M weights. Then assume that each layer has identical\ngeometry, so each group of 4 M weights represents a 2 K*2 K matrix. Because the\nTPU typically uses 8-bit numerical values, 20 M weights take up 20 MB.\na. [15] For batch sizes of 128, 256, 512, 1024, and 2048, how big are the input\nactivations for each layer of the model (which, except for the input layer, are\nalso the output activations of the previous layer)? Now considering the whole\nmode (i.e., there\u2019s just the input to the first layer and the output from the last\nlayer), for each batch size, what is the transfer time for input and output over\nPCIe Gen3 x16, which has a transfer speed of about 100 Gibit/s?\nb. [10] Given the memory system speed of 30 GiB/s, give a lower bound for the\ntime the TPU takes to read the weights of MLP0 from memory. How much time\ndoes it take for the TPU to read a 256256 \u201ctile\u201d of weights from memory?\nc. [10] Show how to calculate the TPU\u2019s 92 T operations/second, given that we\nknow that the systolic array matrix multiplier has 256256 elements, each of\nwhich performs an 8-bit multiply-accumulate operation (MAC) each cycle. In\nhigh-performance-computingmarketingterms,aMACcountsastwooperations.\nd. [20] Once a weight tile has been loaded into the matrix unit of the TPU, it can be\nreused to multiply a 256-element input vector by the 256256 weight matrix\nrepresented by the tile to produce a 256-element output vector every cycle.\nHow many cycles pass during the time it takes to load a weight tile? This is\nthe \u201cbreak-even\u201d batch size, where compute and memory-load times are equal,\nalso known as the \u201cridge\u201d of the roofline.\ne. [15] The compute peak for the Intel Haswell x86 server is about 1 T FLOPS,\nwhile the compute peak for the NVIDIA K80 GPU is about 3 T FLOPS. Sup-\nposing that they hit these peak numbers, calculate their best-case compute time\nfor batch size 128. How do these times compare to the time the TPU takes to\nload all 20 M weights from memory?\nf. [15] Assuming that the TPU program does not overlap computation with I/O\nover PCIe, calculate the time elapsed from when the CPU starts to send the first\nbyte of data to the TPU until the time that the last byte of output is returned.\nWhat fraction of PCIe bandwidth is used?\ng. [20] Suppose that we deployed a configuration where one CPU was connected\nto five TPUs across a single PCIe Gen3 x16 bus (with appropriate PCIe\nswitches). Assume that we parallelize by placing one layer of MLP0 on each\nTPU, and that the TPUs can communicate directly with each other over PCIe.\nAt batch=128, what is the best-case latency for calculating a single inference,\nand what throughput, in terms of inferences per second, would such a config-\nuration deliver? How does this compare to a single TPU?\nh. [20] Suppose that each example in a batch of inferences requires 50 core-micro-\nseconds of processing time on the CPU. How many cores on the host CPU will\nbe required to drive a single-TPU configuration at batch=128?\n7.3\n[20/25/25/25/Discussion] <7.3,7.4>Consider a pseudo-assembly language for\nthe TPU, and consider the program that handles a batch of size 2048 for a tiny fully\n608\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 641,
        "text": "connected layer with a 256256 weight matrix. If there were no constraints on the\nsizes or alignments of computations in each instruction, the entire program for that\nlayer might look like the following:\nread_host u#0, 256*2048\nread_weights w#0, 256*256\n// matmul weights are implicitly read from the FIFO.\nactivate u#256*2048, a#0, 256*2048\nwrite_host, u#256*2048, 256*2048\nIn this pseudo-assembly language, a prefix of \u201cu#\u201d refers to a memory address in\nthe unified buffer; a prefix of \u201cw#\u201d refers to a memory address in the off-chip\nweight DRAM, and a prefix of \u201ca#\u201d refers to an accumulator address. The\nlast argument of each assembly instruction describes the number of bytes to be\noperated upon.\nLet\u2019s walk through the program instruction by instruction:\n\u25a0\nThe read_host instruction reads 512 KB of data from host memory, storing it at\nthe very beginning of the unified buffer (u#0).\n\u25a0\nThe read_weights instruction tells the weight fetching unit to read 64 KB of\nweights, loading them into the on-chip weight FIFO. These 64 KB of weights\nrepresent a single, 256256 matrix of weights, which we will call a \u201cweight\ntile.\u201d\n\u25a0\nThe matmul instruction reads the 512 KB of input data from address 0 in the\nunified buffer, performs a matrix multiplication with the tile of weights, and\nstores the resulting 256*2048=524,288, 32-bit activations at accumulator\naddress 0 (a#0). We have intentionally glossed over the details of the ordering\nof weights; the exercise will expand on these details.\n\u25a0\nThe activate instruction takes those 524,288 32-bit accumulators at a#0, applies\nan activation function to them, and stores the resulting 524,288, 8-bit output\nvalues at the next free location in the unified buffer, u#524288.\n\u25a0\nThe write_host instruction writes the 512 KB of output activations, starting at\nu#524288, back to the host CPU.\nWe will progressively add realistic details to the pseudo-assembly language to\nexplore some aspects of TPU design.\na. [20] While we have written our pseudo-code in terms of bytes and byte\naddresses (or in the case of the accumulators, in terms of addresses to 32-bit\nvalues), the TPU operates on a natural vector length of 256. This means that\nthe unified buffer is typically addressed at 256-byte boundaries, the accumula-\ntors are addressed in groups of 256 32-bit values (or at 1 KB boundaries), and\nweights are loaded in groups of 65,536 8-bit values. Rewrite the program\u2019s\naddresses and transfer sizes to take these vector and weight-tile lengths into\naccount. How many 256-element vectors of input activations will be read\nby the program? How many bytes of accumulated values will be used while\ncomputing the results? How many 256-element vectors of output activations\nwill be written back to the host?\nCase Studies and Exercises by Cliff Young\n\u25a0\n609"
    },
    {
        "page": 642,
        "text": "b. [25] Suppose that the application requirements change, and instead of a\nmultiplication by a 256256 weight matrix, the shape of the weight matrix\nnow becomes 1024256. Think of the matmul instruction as putting the\nweights as the right argument of the matrix multiplication operator, so 1024\ncorresponds to K, the dimension in which the matrix multiplication adds up\nvalues. Suppose that there are now two variants of the accumulate instruction,\none of which overwrites the accumulators with its results, and the other of\nwhich adds the matrix multiplication results to the specified accumulator.\nHow would you change the program to handle this 1024256 matrix? Do\nyou need more accumulators? The size of the matrix unit remains the same\nat 256256; how many 256256 weight tiles does your program need?\nc. [25] Now write the program to handle a multiplication by a weight matrix of\nsize 256512. Does your program need more accumulators? Can you rewrite\nyour program so that it uses only 2048, 256-entry accumulators? How many\nweight tiles does your program need? In what order should they be stored in\nthe weight DRAM?\nd. [25] Next, write the program to handle a multiplication by a weight matrix of\nsize 1024768. How many weight tiles does your program need? Write your\nprogram so that it uses only 2048, 256-entry accumulators. In what order\nshould the weight tiles be stored in the weight DRAM? For this calculation,\nhow many times did each input activation get read?\ne. [Discussion] What would it take to build an architecture that reads each\n256-element set of input activations just once? How many accumulators would\nthat require? If you did it that way, how big would the accumulator memory\nhave to be? Contrast this approach with the TPU, which uses 4096 accumula-\ntors, so that one set of 2048 accumulators can be written by the matrix unit\nwhile another is being used for activations.\n7.4\n[15/15/15] <7.3,7.4>Consider the first convolutional layer of AlexNet, which\nuses a 77 convolutional kernel, with an input feature depth of 3 and an output\nfeature depth of 48. The original image width is 220220.\na. [15] Ignore the 77 convolutional kernel for the moment, and consider just\nthe center element of that kernel. A 11 convolutional kernel is mathemati-\ncally equivalent to a matrix multiplication, using a weight matrix that is\ninput_depthoutput_depth in dimensions. With these depths, and using a stan-\ndard matrix multiplication, what fraction of the TPU\u2019s 65,536 ALUs can be\nused?\nb. [15] For convolutional neural networks, the spatial dimensions are also sources\nof weight reuse, since the convolutional kernel gets applied to many different\n(x,y) coordinate positions. Suppose that the TPU reaches balanced compute and\nmemory at a batch size of 1400 (as you might have computed in exercise 1d).\nWhat is the smallest square image size that the TPU can process efficiently at a\nbatch size of 1?\nc. [15] The first convolutional layer of AlexNet implements a kernel stride of 4,\nwhich means that rather than moving by one X or Y pixel at each application,\n610\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 643,
        "text": "the 77 kernel moves by 4 pixels at a time. This striding means that we can\npermute the input data from 2202203 to be 555548 (dividing the X\nand Y dimensions by 4 and multiplying the input depth by 16), and simulta-\nneously we can restack the 77348 convolutional weights to be\n224848 (just as the input data gets restacked by 4 in X and Y, we do\nthe same to the 77 elements of the convolutional kernel, ending up with\nceiling(7/4)=2 elements in each of the X and Y dimensions). Because the\nkernel is now 22, we need to perform only four matrix multiplication oper-\nations, using weight matrices of size 4848. What is the fraction of the 65,536\nALUs that can be used now?\n7.5\n[15/10/20/20/20/25]<7.3>The TPU uses fixed-point arithmetic (sometimes also\ncalled quantized arithmetic, with overlapping and conflicting definitions), where\nintegers are used to represent values on the real number line. There are a number of\ndifferent schemes for fixed-point arithmetic, but they share the common theme that\nthere is an affine projection from the integer used by hardware to the real number\nthat the integer represents. An affine projection has the form r=i*s+b, where i is\nthe integer, r is the represented real value, and s and b are a scale and bias. You can\nof course write the projection in either direction, from integers to reals or vice versa\n(although you need to round when converting from reals to integers).\na. [15] The simplest activation function supported by the TPU is \u201cReLUX,\u201d which\nis a rectified linear unit with a maximum of X. For example, ReLU6 is defined by\nRelu6(x)={ 0, when x<0; x, when 0<=x<=6; and 6, when x>6 }. So 0.0 and\n6.0 on the real number line are the minimum and maximum values that Relu6\nmight produce. Assume that you use an 8-bit unsigned integer in hardware,\nand that you want to make 0 map to 0.0 and 255 map to 6.0. Solve for s and b.\nb. [10] How many values on the real number line are exactly representable by\nan 8-bit quantized representation of ReLU6 output? What is the real-number\nspacing between them?\nc. [20] The difference between representable values is sometimes called a \u201cunit in\nthe least place,\u201d or ulp, when performing numerical analysis. If you map a real\nnumber to its fixed-point representation, then map back, you only rarely get\nback the original real number. The difference between the original number\nand its representation is called the quantization error. When mapping a\nreal number in the range [0.0,6.0] to an 8-bit integer, show that the worst-case\nquantization error is one-half of an ulp (make sure you round to the nearest rep-\nresentable value). You might consider graphing the errors as a function of the\noriginal real number.\nd. [20] Keep the real-number range [0.0,6.0] for an 8-bit integer from the last step.\nWhat 8-bit unsigned integer represents 1.0? What is the quantization error for\n1.0? Suppose that you ask the TPU to add 1.0 to 1.0. What answer do you get\nback, and what is the error in that result?\ne. [20] If you pick a random number uniformly in the range [0.0, 6.0], then\nquantize it to an 8-bit unsigned integer, what distribution would you expect\nto see for the 256 integer values?\nCase Studies and Exercises by Cliff Young\n\u25a0\n611"
    },
    {
        "page": 644,
        "text": "f. [25] The hyperbolic tangent function, tanh, is another commonly used\nactivation function in deep learning: tanh x\n\u00f0 \u00de \u00bc 1 \u0003 e\u00032x\n1 + e\u00032x\nTanh also has a bounded range, mapping the entire real number line to the interval\n\u00031:0,1:0\n\u00f0\n\u00de. Solve for s and b for this range, using an 8-bit unsigned representation.\nThen solve for s and b using an 8-bit two\u2019s complement representation. For both\ncases, what real number does the integer 0 represent? Which integer represents the\nreal number 0.0? Can you imagine any issues that might result from the quantiza-\ntion error incurred when representing 0.0?\n7.6\n[20/25/15/15/30/30/30/40/40/25/20/Discussion]\n<7.3>In\naddition\nto\ntanh,\nanothers-shapedsmoothfunction,thelogisticsigmoidfunctiony=1/(1+exp(\u0003x)),\nlogistic_sigmoid x\n\u00f0 \u00de \u00bc\n1\n1 + e\u0003x\nis commonly used as an activation function in neural networks. A common way to\nimplement them in fixed-point arithmetic uses a piecewise quadratic approxima-\ntion, where the most significant bits of the input value select which table entry to\nuse. Then the least significant bits of the input value are sent to a degree-2 polyno-\nmialthat describes aparabolathat isfit tothesubrangeoftheapproximatedfunction.\na. [20] Using a graphing tool (we like www.desmos.com/calculator), draw the\ngraphs for the logistic sigmoid and tanh functions.\nb. [25] Now draw the graph of y=tanh(x/2)/2. Compare that graph with the logis-\ntic sigmoid function. How much do they differ by? Build an equation that\nshows how to transform one into the other. Prove that your equation is correct.\nc. [15] Given this algebraic identity, do you need to use two different sets of coef-\nficients to approximate logistic sigmoid and tanh?\nd. [15] Tanh is an odd function, meaning that f(\u0003x)=\u0003f(x). Can you exploit this\nfact to save table space?\ne. [30] Let\u2019s focus our attention on approximating tanh over the interval\nx 2 0:0, 6:4\n\u00bd\n\u0005 on the number line. Using floating-point arithmetic, write a pro-\ngram that divides the interval into 64 subintervals (each of length 0.1), and then\napproximates the value of tanh over each subinterval using a single constant\nfloating-point value (so you\u2019ll need to pick 64 different floating-point values,\none for each subinterval). If you spot-check 100 different values (randomly\nchosen is fine) within each subinterval, what is the worst-case approximation\nerror you see over all subintervals? Can you choose your constant to minimize\nthe approximation error for each subinterval?\nf. [30] Now consider building a floating-point linear approximation for each sub-\ninterval. In this case, you want to pick a pair of floating-point values m and b,\nfor the traditional line equation y \u00bc mx + b, to approximate each of the 64 sub-\nintervals. Come up with a strategy that you think is reasonable to build this lin-\near interpolation over 64 subintervals for tanh. Measure the worst-case\napproximation error over the 64 intervals. Is your approximation monotonic\nwhen it reaches a boundary between subintervals?\n612\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 645,
        "text": "g. [40] Next, build a quadratic approximation, using the standard formula\ny \u00bc ax2 + bx + c: Experiment with a number of different ways to fit the formula.\nTry fitting the parabola to the endpoints and midpoint of the bucket, or using a\nTaylor approximation around a single point in the bucket. What worst-case\nerror do you get?\nh. [40] (extra credit) Let\u2019s combine the numerical approximations of this exercise\nwith the fixed-point arithmetic of the previous exercise. Suppose that the input\nx 2 0:0, 6:4\n\u00bd\n\u0005 is represented by a 15-bit unsigned value, with 0x0000 represent-\ning 0.0 and 0x7FFF representing 6.4. For the output, similarly use a 15-bit\nunsigned value, with 0x0000 representing 0.0 and 0x7FFF representing 1.0.\nFor each of your constant, linear, and quadratic approximations, calculate the\ncombined effect of approximation and quantization errors. Since there are so\nfew input values, you can write a program to check them exhaustively.\ni. [25] For the quadratic, quantized approximation, is your approximation mono-\ntonic within each subinterval?\nj. [20] A difference of one ulp in the output scale should correspond to an error of\n1.0 / 32767. How many ulps of error are you seeing in each case?\nk. [Discussion] By choosing to approximate the interval [0.0,6.4], we effectively\nclipped the \u201ctail\u201d of the hyperbolic tangent function, for values of x > 6:4. It\u2019s\nnot an unreasonable approximation to set the output value for all of the tail to\n1.0. What\u2019s the worst-case error, in terms of both real numbers and ulps, of\ntreating the tail this way? Is there a better place we might have clipped the tail\nto improve our accuracy?\nExercises\n7.7\n[10/20/10/15] <7.2,7.5>One popular family of FPGAs, the Virtex-7 series, is\nbuilt by Xilinx. A Virtex-7 XC7VX690T FPGA contains 3,600 25x18-bit\ninteger multiply-add \u201cDSP slices.\u201d Consider building a TPU-style design on such\nan FPGA.\na. [10] Using one 2518 integer multiplier per systolic array cell, what\u2019s the larg-\nest matrix multiplication unit one could construct? Assume that the matrix mul-\ntiplication unit must be square.\nb. [20] Suppose that you could build a rectangular, nonsquare matrix multiplica-\ntion unit. What implications would such a design have for hardware and soft-\nware? (Hint: think about the vector length that software must handle.)\nc. [10] Many FPGA designs are lucky to reach 500 MHz operation. At that speed,\ncalculate the peak 8-bit operations per second that such a device might achieve.\nHow does that compare to the 3 T FLOPS of a K80 GPU?\nd. [15] Assume that you can make up the difference between 3600 and 4096 DSP\nslices using LUTs, but that doing so will reduce your clock rate to 350 MHz. Is\nthis a worthwhile trade-off to make?\nCase Studies and Exercises by Cliff Young\n\u25a0\n613"
    },
    {
        "page": 646,
        "text": "7.8\n[15/15/15] <7.9>Amazon Web Services (AWS) offers a wide variety of \u201ccom-\nputing instances,\u201d which are machines configured to target different applications\nand scales. AWS prices tell us useful data about the Total Cost of Ownership\n(TCO) of various computing devices, particularly as computer equipment is often\ndepreciated1 on a 3-year schedule. As of July 2017, a dedicated, compute-oriented\n\u201cc4\u201d computing instance includes two x86 chips with 20 physical cores in total. It\nrents on-demand for $1.75/hour, or $17,962 for 3 years. In contrast, a dedicated\n\u201cp2\u201d computing instance also has two x86 chips but with 36 cores in total, and adds\n16 NVIDIA K80 GPUs. A p2 rents on-demand for $15.84/hour, or $184,780 for\n3 years.\na. [15] The c4 instance uses Intel Xeon E5-2666 v3 (Haswell) processors. The p2\ninstance uses Intel Xeon E5-2686 v4 (Broadwell) processors. Neither part num-\nber is listed officially on Intel\u2019s product website, which suggests that these parts\nare specially built for Amazon by Intel. The E5-2660 v3 part has a similar core\ncount to the E5-2666 v3 and has a street price of around $1500. The E5-2697 v4\npart has a similar core count to the E5-2686 v4 and has a street price of around\n$3000. Assume that the non-GPU portion of the p2 instance would have a price\nproportional to the ratio of street prices. What is the TCO, over 3 years, for a\nsingle K80 GPU?\nb. [15] Suppose that you have a compute- and throughput-dominated workload\nthat runs at rate 1 on the c4 instance and at rate T on the GPU-accelerated\np2 instance. How large must T be for the GPU-based solution to be more\ncost-effective? Suppose that each general-purpose CPU core can compute at\na rate of about 30G single-precision FLOPS. Ignoring the CPUs of the p2\ninstance, what fraction of peak K80 FLOPs would be required to reach the same\nrate of computation as the c4 instance?\nc. [15] AWS also offers \u201cf1\u201d instances that include 8 Xilinx Ultrascale+VU9P\nFPGAs. They rent at $13.20/hour, or $165,758 for 3 years. Each VU9P device\nincludes 6840 DSP slices, which can perform 2718-bit integer multiply-\naccumulate operations (recall that one multiply-accumulate counts as two\n\u201coperations\u201d). At 500 MHz, what is the peak multiply-accumulate opera-\ntions/cycle that an f1-based system might achieve, counting all 8 FPGAs\ntoward the computation total? Assuming that the integer operations on the\nFPGAs can substitute for floating-point operations, how does this compare\nto the peak single-precision multiply-accumulate operations/cycle of the GPUs\nof the p2 instance? How do they compare in terms of cost-effectiveness?\n7.9\n[20/20/25] <7.7>As shown in Figure 7.34 (but simplified to fewer PEs), each\nPixel Visual Core includes a 1616 set of full processing elements, surrounded\n1Capital expenses are accounted for over the lifetime of an asset, using a \u201cdepreciation schedule.\u201d Rather than taking a\none-time charge at the point where an asset is acquired, standard accounting practice spreads out the capital cost over\nthe lifetime of the asset. So one might account for a $30,000 device that has a useful life of 3 years by assigning\n$10,000 in depreciation to each year.\n614\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 647,
        "text": "by an additional two layers of \u201csimplified\u201d processing elements. Simplified\nPEs can store and communicate data but omit the computation hardware of\nfull PEs. Simplified PEs store copies of data that might be the \u201chome data\u201d of a\nneighboring core, so there are (16+2+2)2=400 PEs in total, 256 full and 144\nsimplified.\na. [20] Suppose that you wanted to process a 6432 grayscale image with a 55\nstencil using 8 Pixel Visual Cores. For now, assume that the image is laid out in\nraster-scan order (pixels that are adjacent in X are adjacent in memory, while\npixels that are adjacent in Y are 64 memory locations apart). For each of the\n8 cores, describe the memory region that the core should import to handle\nits part of the image. Make sure to include the halo region. Which parts of\nthe halo region should be zeroed by software to ensure correct operation?\nYou may find it convenient to refer to subregions of the image using a 2D slice\nnotation, where for example image[2:5][6:13] refers to the set of pixels whose x\ncomponent is 2<=x<5 and whose y component is 6<=y<13 (the slices are\nhalf-open following Python slicing practice).\nb. [20] If we change to a 33 stencil, how do the regions imported from memory\nchange? How many halo-simplified PEs go unused?\nc. [25] Now consider how to support a 77 stencil. In this case, we don\u2019t have as\nmany hardware-supported simplified PEs as we need to cover the three pixels\nworth of halo data that \u201cbelong to\u201d neighboring cores. To handle this, we use\nthe outermost ring of full PEs as if they were simplified PEs. How many pixels\ncan we handle in a single core using this strategy? How many \u201ctiles\u201d are now\nrequired to handle our 6432 input image? What is the utilization of our full\nPEs over the complete processing time for the 77 stencil over the 6432\nimage?\n7.10\n[20/20/20/25/25] <7.7>Consider a case in which each of the eight cores on a\nPixel Visual Core device is connected through a four-port switch to a 2D SRAM,\nforming a core+memory unit. The remaining two ports on the switch link these\nunits in a ring, so that each core is able to access any of the eight SRAMs. However,\nthis ring-based network-on-chip topology makes some data access patterns more\nefficient than others.\nCore\nSwitch\nSRAM\nCore\nSwitch\nSRAM\nCore\nSwitch\nSRAM\nCore\nSwitch\nSRAM\n\u00b7\u00b7\u00b7\nCase Studies and Exercises by Cliff Young\n\u25a0\n615"
    },
    {
        "page": 648,
        "text": "a. [20] Suppose that each link in the NOC has the same bandwidth B, and that\neach link is full-duplex, so it can simultaneously transfer bandwidth B in each\ndirection. Links connect the core to the switch, the switch to SRAM, and pairs\nof switches in the ring. Assume that each local memory has at least B band-\nwidth, so it can saturate its link. Consider a memory access pattern where each\nof the eight PEs access only the closest memory (the one connected via the\nswitch of the core+memory unit). What is the maximum memory bandwidth\nthat the core will be able to achieve?\nb. [20] Now consider an off-by-one access pattern, where core i accesses memory\ni+1, going through three links to reach that memory (core 7 will access mem-\nory 0, because of the ring topology). What is the maximum memory bandwidth\nthat the core will be able to achieve in this case? To achieve that bandwidth, do\nyou need to make any assumptions about the capabilities of the 4-port switch?\nWhat if the switch can only move data at rate B?\nc. [20] Consider an off-by-two access pattern, where core i access memory i+2.\nOnce again, what is the maximum memory bandwidth that the core will be able\nto achieve in this case? Where are the bottleneck links in the network-on-chip?\nd. [25] Consider a uniform random memory access pattern, where each core uses\neach of the SRAMs for \u215bof its memory requests. Assuming this traffic pattern,\nhow much traffic traverses a switch-to-switch link, compared to the amount of\ntraffic between a core and its associated switch or between an SRAM and its\nassociated switch?\ne. [25] (advanced) Can you conceive of a case (workload) where this network can\ndeadlock? From the standpoint of software-only solutions, what should the\ncompiler do to avoid such a scenario? If you can make changes to hardware,\nwhat changes in routing topology (and routing scheme) would guarantee no\ndeadlocks?\n7.11\n<7.2>The first Anton molecular dynamics supercomputer typically simulated a\nbox of water that was 64 \u00c5 on a side. The computer itself might be approximated as\na box with 1 m side length. A single simulation step represented 2.5 fs of simula-\ntion time, and took about 10 \u03bcs of wall-clock time. The physics models used in\nmolecular dynamics act as if every particle in the system exerts a force on every\nother particle in the system on each (\u201couter\u201d) time step, requiring what amounts to a\nglobal synchronization across the entire computer.\na. Calculate the spatial expansion factor from simulation space to hardware in real\nspace.\nb. Calculate the temporal slowdown factor from simulated time to wall-clock\ntime.\nc. These two numbers come out surprisingly close. Is this just a coincidence, or is\nthere some other limit that constrains them in some way? (Hint: the speed of\nlight applies to both the simulated chemical system and the hardware that does\nthe simulation.)\n616\n\u25a0\nChapter Seven Domain-Specific Architectures"
    },
    {
        "page": 649,
        "text": "d. Given these limits, what would it take to use a warehouse-scale supercomputer\nto perform molecular dynamics simulations at Anton rates? That is, what\u2019s the\nfastest simulation step time that might be achieved with a machine 102 or 103 m\non a side? What about simulating on a world-spanning Cloud service?\n7.12\n<7.2>The Anton communication network is a 3D, 888 torus, where each\nnode in the system has six links to neighboring nodes. Latency for a packet to tran-\nsit single link is about 50 ns. Ignore on-chip switching time between links for this\nexercise.\na. What is the diameter (maximum number of hops between a pair of nodes) of the\ncommunication network? Given that diameter, what is the shortest latency\nrequired to broadcast a single value from one node of the machine to all 512\nnodes of the machine?\nb. Assuming that adding up two values takes zero time, what is the shortest\nlatency to add up a sum over 512 values to a single node, where each value\nstarts on a different node of the machine?\nc. Once again assume that you want to perform the sum over 512 values, but you\nwant each of the 512 nodes of the system to end up with a copy of the sum. Of\ncourse you could perform a global reduction followed by a broadcast. Can you\ndo the combined operation in less time? This pattern is called an all-reduce.\nCompare the times of your all-reduce pattern to the time of a broadcast from\na single node or a global sum to a single node. Compare the bandwidth used\nby the all-reduce pattern with the other patterns.\nCase Studies and Exercises by Cliff Young\n\u25a0\n617"
    },
    {
        "page": 650,
        "text": "A.1\nIntroduction\nA-2\nA.2\nClassifying Instruction Set Architectures\nA-3\nA.3\nMemory Addressing\nA-7\nA.4\nType and Size of Operands\nA-13\nA.5\nOperations in the Instruction Set\nA-15\nA.6\nInstructions for Control Flow\nA-16\nA.7\nEncoding an Instruction Set\nA-21\nA.8\nCross-Cutting Issues: The Role of Compilers\nA-24\nA.9\nPutting It All Together: The RISC-V Architecture\nA-33\nA.10\nFallacies and Pitfalls\nA-42\nA.11\nConcluding Remarks\nA-46\nA.12\nHistorical Perspective and References\nA-47\nExercises by Gregory D. Peterson\nA-47"
    },
    {
        "page": 651,
        "text": "A\nInstruction Set Principles\nA n\nAdd the number in storage location n into the\naccumulator.\nE n\nIf the number in the accumulator is greater than or equal\nto zero execute next the order which stands in storage\nlocation n; otherwise proceed serially.\nZ\nStop the machine and ring the warning bell.\nWilkes and Renwick,\nSelection from the List of 18 Machine\nInstructions for the EDSAC (1949)"
    },
    {
        "page": 652,
        "text": "A.1\nIntroduction\nIn this appendix we concentrate on instruction set architecture\u2014the portion of the\ncomputer visible to the programmer or compiler writer. Most of this material should\nbe review for readers of this book; we include it here for background. This appendix\nintroduces the wide variety of design alternatives available to the instruction set\narchitect. In particular, we focus on four topics. First, we present a taxonomy of\ninstruction set alternatives and give some qualitative assessment of the advantages\nand disadvantages of various approaches. Second, we present and analyze some\ninstruction set measurements that are largely independent of a specific instruction\nset. Third, we address the issue of languages and compilers and their bearing on\ninstruction set architecture. Finally, the \u201cPutting It All Together\u201d section shows\nhow these ideas are reflected in the RISC-V instruction set, which is typical of RISC\narchitectures. We conclude with fallacies and pitfalls of instruction set design.\nTo illustrate the principles further and to provide a comparison with RISC-V,\nAppendix K also gives four examples of other general-purpose RISC architectures\n(MIPS, Power ISA, SPARC, and Armv8), four embedded RISC processors (ARM\nThumb2, RISC-V Compressed, microMIPS), and three older architectures (80x86,\nIBM 360/370, and VAX). Before we discuss how to classify architectures, we need\nto say something about instruction set measurement.\nThroughout this appendix, we examine a wide variety of architectural measure-\nments. Clearly, these measurements depend on the programs measured and on the\ncompilers used in making the measurements. The results should not be interpreted\nas absolute, and you might see different data if you did the measurement with a\ndifferent compiler or a different set of programs. We believe that the measurements\nin this appendix are reasonably indicative of a class of typical applications. Many\nof the measurements are presented using a small set of benchmarks, so that the data\ncan be reasonably displayed and the differences among programs can be seen. An\narchitect for a new computer would want to analyze a much larger collection of\nprograms before making architectural decisions. The measurements shown are\nusually dynamic\u2014that is, the frequency of a measured event is weighed by the\nnumber of times that event occurs during execution of the measured program.\nBefore starting with the general principles, let\u2019s review the three application\nareas from Chapter 1. Desktop computing emphasizes the performance of pro-\ngrams with integer and floating-point data types, with little regard for program\nsize. For example, code size has never been reported in the five generations of\nSPEC benchmarks. Servers today are used primarily for database, file server,\nand Web applications, plus some time-sharing applications for many users. Hence,\nfloating-point performance is much less important for performance than integers\nand character strings, yet virtually every server processor still includes floating-\npoint instructions. Personal mobile devices and embedded applications value cost\nand energy, so code size is important because less memory is both cheaper and\nlower energy, and some classes of instructions (such as floating point) may be\noptional to reduce chip costs, and a compressed version of the instructions set\ndesigned to save memory space may be used.\nA-2\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 653,
        "text": "Thus, instruction sets for all three applications are very similar. In fact, archi-\ntectures similar to RISC-V, which we focus on here, have been used successfully in\ndesktops, servers, and embedded applications.\nOne successful architecture very different from RISC is the 80x86 (see Appen-\ndix K). Surprisingly, its success does not necessarily belie the advantages of a RISC\ninstruction set. The commercial importance of binary compatibility with PC soft-\nware combined with the abundance of transistors provided by Moore\u2019s Law led\nIntel to use a RISC instruction set internally while supporting an 80x86 instruction\nset externally. Recent 80x86 microprocessors, including all the Intel Core micropro-\ncessors built in the past decade, use hardware to translate from 80x86 instructions to\nRISC-like instructions and then execute the translated operations inside the chip.\nThey maintain the illusion of 80x86 architecture to the programmer while allowing\nthe computer designer to implement a RISC-style processor for performance. There\nremain, however, serious disadvantages for a complex instruction set like the\n80x86, and we discuss these further in the conclusions.\nNow that the background is set, we begin by exploring how instruction set\narchitectures can be classified.\nA.2\nClassifying Instruction Set Architectures\nThe type of internal storage in a processor is the most basic differentiation, so in this\nsection we will focus on the alternatives for this portion of the architecture. The\nmajor choices are a stack, an accumulator, or a set of registers. Operands may be\nnamed explicitly or implicitly: The operands in a stack architecture are implicitly\non the top of the stack, and in an accumulator architecture one operand is implicitly\nthe accumulator. The general-purpose register architectures have only explicit\noperands\u2014either registers or memory locations. Figure A.1 shows a block diagram\nof such architectures, and Figure A.2 shows how the code sequence C\u00bcA+B would\ntypically appear in these three classes of instruction sets. The explicit operands may\nbe accessed directly from memory or may need to be first loaded into temporary\nstorage, depending on the class of architecture and choice of specific instruction.\nAs the figures show, there are really two classes of register computers. One\nclass can access memory as part of any instruction, called register-memory archi-\ntecture, and the other can access memory only with load and store instructions,\ncalled load-store architecture. A third class, not found in computers shipping\ntoday, keeps all operands in memory and is called a memory-memory architecture.\nSome instruction set architectures have more registers than a single accumulator\nbut place restrictions on uses of these special registers. Such an architecture is\nsometimes called an extended accumulator or special-purpose register computer.\nAlthough most early computers used stack or accumulator-style architectures,\nvirtually every new architecture designed after 1980 uses a load-store register archi-\ntecture. The major reasons for the emergence of general-purpose register (GPR)\ncomputers are twofold. First, registers\u2014like other forms of storage internal to the\nprocessor\u2014are faster than memory. Second, registers are more efficient for a\nA.2\nClassifying Instruction Set Architectures\n\u25a0\nA-3"
    },
    {
        "page": 654,
        "text": "Stack\nAccumulator\nRegister-memory\nTOS\nALU\n. . .\n. . .\n. . .\nALU\n. . .\n. . .\nALU\n. . .\n. . .\n. . .\n. . .\nRegister-register/\nload-store\nALU\n. . .\n. . .\n. . .\n. . .\nMemory\nProcessor\n(A)\n(B)\n(C)\n(D)\nFigure A.1 Operand locations for four instruction set architecture classes. The arrows indicate whether the oper-\nand is an input or the result of the arithmetic-logical unit (ALU) operation, or both an input and result. Lighter shades\nindicate inputs, and the dark shade indicates the result. In (A), a top of stack (TOS) register points to the top input\noperand, which is combined with the operand below. The first operand is removed from the stack, the result takes the\nplace of the second operand, and TOS is updated to point to the result. All operands are implicit. In (B), the accu-\nmulator is both an implicit input operand and a result. In (C), one input operand is a register, one is in memory,\nand the result goes to a register. All operands are registers in (D) and, like the stack architecture, can be transferred\nto memory only via separate instructions: push or pop for (A) and load or store for (D).\nStack\nAccumulator\nRegister\n(register-memory)\nRegister\n(load-store)\nPush A\nLoad A\nLoad R1,A\nLoad R1,A\nPush B\nAdd B\nAdd\nR3,R1,B\nLoad R2,B\nAdd\nStore C\nStore R3,C\nAdd\nR3,R1,R2\nPop C\nStore R3,C\nFigure A.2 The code sequence for C5A+B for four classes of instruction sets. Note\nthat the Add instruction has implicit operands for stack and accumulator architectures\nand explicit operands for register architectures. It is assumed that A, B, and C all belong\nin memory and that the values of A and B cannot be destroyed. Figure A.1 shows the\nAdd operation for each class of architecture.\nA-4\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 655,
        "text": "compiler to use than other forms of internal storage. For example, on a register com-\nputer the expression (A * B)+(B * C) \u2013 (A * D) may be evaluated by doing the\nmultiplications in any order, which may be more efficient because of the location of\nthe operands or because of pipelining concerns (see Chapter 3). Nevertheless, on a\nstackcomputer thehardwaremust evaluate the expression in onlyoneorder, because\noperands are hidden on the stack, and it may have to load an operand multiple times.\nMore importantly, registers can be used to hold variables. When variables are\nallocated to registers, the memory traffic reduces, the program speeds up (because\nregisters are faster than memory), and the code density improves (because a reg-\nister can be named with fewer bits than can a memory location).\nAs explained in Section A.8, compiler writers would prefer that all registers be\nequivalent and unreserved. Older computers compromise this desire by dedicating\nregisters to special uses, effectively decreasing the number of general-purpose reg-\nisters. If the number of truly general-purpose registers is too small, trying to allo-\ncate variables to registers will not be profitable. Instead, the compiler will reserve\nall the uncommitted registers for use in expression evaluation.\nHow many registers are sufficient? The answer, of course, depends on the effec-\ntiveness of the compiler. Most compilers reserve some registers for expression eval-\nuation,usesomeforparameterpassing,andallowtheremaindertobeallocatedtohold\nvariables. Modern compiler technology and its ability to effectively use larger num-\nbers of registers has led to an increase in register counts in more recent architectures.\nTwo major instruction set characteristics divide GPR architectures. Both char-\nacteristics concern the nature of operands for a typical arithmetic or logical instruc-\ntion (ALU instruction). The first concerns whether an ALU instruction has two or\nthree operands. In the three-operand format, the instruction contains one result\noperand and two source operands. In the two-operand format, one of the operands\nis both a source and a result for the operation. The second distinction among GPR\narchitectures concerns how many of the operands may be memory addresses in\nALU instructions. The number of memory operands supported by a typical\nALU instruction may vary from none to three. Figure A.3 shows combinations\nof these two attributes with examples of computers. Although there are seven\nNumber of memory\naddresses\nMaximum number\nof operands\nallowed\nType of architecture\nExamples\n0\n3\nLoad-store\nARM, MIPS, PowerPC, SPARC, RISC-V\n1\n2\nRegister-memory\nIBM 360/370, Intel 80x86, Motorola\n68000, TI TMS320C54x\n2\n2\nMemory-memory\nVAX (also has three-operand formats)\n3\n3\nMemory-memory\nVAX (also has two-operand formats)\nFigure A.3 Typical combinations of memory operands and total operands per typical ALU instruction with\nexamples of computers. Computers with no memory reference per ALU instruction are called load-store or\nregister-register computers. Instructions with multiple memory operands per typical ALU instruction are called\nregister-memory or memory-memory, according to whether they have one or more than one memory operand.\nA.2\nClassifying Instruction Set Architectures\n\u25a0\nA-5"
    },
    {
        "page": 656,
        "text": "possible combinations, three serve to classify nearly all existing computers. As we\nmentioned earlier, these three are load-store (also called register-register), register-\nmemory, and memory-memory.\nFigure A.4 shows the advantages and disadvantages of each of these alter-\nnatives. Of course, these advantages and disadvantages are not absolutes: they\nare qualitative and their actual impact depends on the compiler and implemen-\ntation strategy. A GPR computer with memory-memory operations could eas-\nily be ignored by the compiler and used as a load-store computer. One of the\nmost pervasive architectural impacts is on instruction encoding and the num-\nber of instructions needed to perform a task. We see the impact of these archi-\ntectural alternatives on implementation approaches in Appendix C and\nChapter 3.\nSummary: Classifying Instruction Set Architectures\nHere and at the end of Sections A.3\u2013A.8 we summarize those characteristics we\nwould expect to find in a new instruction set architecture, building the foundation\nfor the RISC-V architecture introduced in Section A.9. From this section we should\nclearly expect the use of general-purpose registers. Figure A.4, combined with\nAppendix C on pipelining, leads to the expectation of a load-store version of a\ngeneral-purpose register architecture.\nWith the class of architecture covered, the next topic is addressing operands.\nType\nAdvantages\nDisadvantages\nRegister-register\n(0, 3)\nSimple, fixed-length instruction encoding.\nSimple code generation model. Instructions\ntake similar numbers of clocks to execute\n(see Appendix C)\nHigher instruction count than architectures with\nmemory references in instructions. More instructions\nand lower instruction density lead to larger programs,\nwhich may have some instruction cache effects\nRegister-memory\n(1, 2)\nData can be accessed without a separate load\ninstruction first. Instruction format tends to\nbe easy to encode and yields good density\nOperands are not equivalent because a source\noperand in a binary operation is destroyed. Encoding\na register number and a memory address in each\ninstruction may restrict the number of registers.\nClocks per instruction vary by operand location\nMemory-\nmemory (2, 2)\nor (3, 3)\nMost compact. Doesn\u2019t waste registers for\ntemporaries\nLarge variation in instruction size, especially for\nthree-operand instructions. In addition, large\nvariation in work per instruction. Memory accesses\ncreate memory bottleneck. (Not used today.)\nFigure A.4 Advantages and disadvantages of the three most common types of general-purpose register com-\nputers. The notation (m, n) means m memory operands and n total operands. In general, computers with fewer alter-\nnatives simplify the compiler\u2019s task because there are fewer decisions for the compiler to make (see Section A.8).\nComputers with a wide variety of flexible instruction formats reduce the number of bits required to encode the pro-\ngram. The number of registers also affects the instruction size because you need log2 (number of registers) for each\nregister specifier in an instruction. Thus, doubling the number of registers takes three extra bits for a register-register\narchitecture, or about 10% of a 32-bit instruction.\nA-6\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 657,
        "text": "A.3\nMemory Addressing\nIndependent of whether the architecture is load-store or allows any operand to\nbe a memory reference, it must define how memory addresses are interpreted\nand how they are specified. The measurements presented here are largely,\nbut not completely, computer independent. In some cases the measurements\nare significantly affected by the compiler technology. These measurements\nhave been made using an optimizing compiler, because compiler technology\nplays a critical role.\nInterpreting Memory Addresses\nHow is a memory address interpreted? That is, what object is accessed as a func-\ntion of the address and the length? All the instruction sets discussed in this book\nare byte addressed and provide access for bytes (8 bits), half words (16 bits), and\nwords (32 bits). Most of the computers also provide access for double words\n(64 bits).\nThere are two different conventions for ordering the bytes within a larger\nobject. Little Endian byte order puts the byte whose address is \u201cx \u2026 x000\u201d at\nthe least-significant position in the double word (the little end). The bytes are\nnumbered:\n7\n6\n5\n4\n3\n2\n1\n0\nBig Endian byte order puts the byte whose address is \u201cx \u2026 x000\u201d at the most-\nsignificant position in the double word (the big end). The bytes are numbered:\n0\n1\n2\n3\n4\n5\n6\n7\nWhen operating within one computer, the byte order is often unnoticeable\u2014\nonly programs that access the same locations as both, say, words and bytes, can\nnotice the difference. Byte order is a problem when exchanging data among com-\nputers with different orderings, however. Little Endian ordering also fails to match\nthe normal ordering of words when strings are compared. Strings appear\n\u201cSDRAWKCAB\u201d (backwards) in the registers.\nA second memory issue is that in many computers, accesses to objects larger\nthan a byte must be aligned. An access to an object of size s bytes at byte address A\nis aligned if A mod s\u00bc0. Figure A.5 shows the addresses at which an access is\naligned or misaligned.\nWhy would someone design a computer with alignment restrictions? Misalign-\nment causes hardware complications, because the memory is typically aligned on a\nmultiple of a word or double-word boundary. A misaligned memory access may,\ntherefore, take multiple aligned memory references. Thus, even in computers that\nallow misaligned access, programs with aligned accesses run faster.\nA.3\nMemory Addressing\n\u25a0\nA-7"
    },
    {
        "page": 658,
        "text": "Even if data are aligned, supporting byte, half-word, and word accesses requires\nan alignment network to align bytes, half words, and words in 64-bit registers. For\nexample, in Figure A.5, suppose we read a byte from an address with its 3 low-order\nbitshavingthevalue4.Wewillneedtoshiftright3bytestoalignthebytetotheproper\nplaceina64-bitregister.Dependingontheinstruction,thecomputermayalsoneedto\nsign-extendthequantity.Storesareeasy:onlytheaddressedbytesinmemorymaybe\naltered.Onsome computersa byte,half-word, andwordoperationdoes notaffectthe\nupper portion of a register. Although all the computers discussed in this book permit\nbyte, half-word, and word accesses to memory, only the IBM 360/370, Intel 80x86,\nand VAX support ALU operations on register operands narrower than the full width.\nNow that we have discussed alternative interpretations of memory addresses,\nwe can discuss the ways addresses are specified by instructions, called addressing\nmodes.\nAddressing Modes\nGiven an address, we now know what bytes to access in memory. In this sub-\nsection we will look at addressing modes\u2014how architectures specify the address\nValue of three low-order bits of byte address\nWidth of object\n0\n1\n2\n3\n4\n5\n6\n7\n1 byte (byte)\nAligned\nAligned\nAligned\nAligned\nAligned\nAligned\nAligned\nAligned\n2 bytes (half word)\nAligned\nAligned\nAligned\nAligned\n2 bytes (half word)\nMisaligned\nMisaligned\nMisaligned\nMisaligned\n4 bytes (word)\nAligned\nAligned\n4 bytes (word)\nMisaligned\nMisaligned\n4 bytes (word)\nMisaligned\nMisaligned\n4 bytes (word)\nMisaligned\nMisaligned\n8 bytes (double word)\nAligned\n8 bytes (double word)\nMisaligned\n8 bytes (double word)\nMisaligned\n8 bytes (double word)\nMisaligned\n8 bytes (double word)\nMisaligned\n8 bytes (double word)\nMisaligned\n8 bytes (double word)\nMisaligned\n8 bytes (double word)\nMisaligned\nFigure A.5 Aligned and misaligned addresses of byte, half-word, word, and double-word objects for byte-\naddressed computers. For each misaligned example some objects require two memory accesses to complete. Every\naligned object can always complete in one memory access, as long as the memory is as wide as the object. The figure\nshows the memory organized as 8 bytes wide. The byte offsets that label the columns specify the low-order three bits\nof the address.\nA-8\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 659,
        "text": "of an object they will access. Addressing modes specify constants and registers\nin addition to locations in memory. When a memory location is used, the actual\nmemory address specified by the addressing mode is called the effective address.\nFigure A.6 shows all the data addressing modes that have been used in recent\ncomputers. Immediates or literals are usually considered memory addressing\nmodes (even though the value they access is in the instruction stream), although\nregisters are often separated because they don\u2019t usually have memory addresses.\nWe have kept addressing modes that depend on the program counter, called\nPC-relative addressing, separate. PC-relative addressing is used primarily for\nspecifying code addresses in control transfer instructions, discussed in Section A.6.\nAddressing\nmode\nExample instruction\nMeaning\nWhen used\nRegister\nAdd R4,R3\nRegs[R4] Regs[R4]\n+Regs[R3]\nWhen a value is in a register\nImmediate\nAdd R4,3\nRegs[R4] Regs[R4]+3\nFor constants\nDisplacement\nAdd R4,100(R1)\nRegs[R4] Regs[R4]\n+Mem[100+Regs[R1]]\nAccessing local variables\n(+ simulates register indirect, direct\naddressing modes)\nRegister\nindirect\nAdd R4,(R1)\nRegs[R4] Regs[R4]\n+Mem[Regs[R1]]\nAccessing using a pointer or a\ncomputed address\nIndexed\nAdd R3,(R1+R2)\nRegs[R3] Regs[R3]\n+Mem[Regs[R1]+Regs\n[R2]]\nSometimes useful in array\naddressing: R1\u00bcbase of array;\nR2\u00bcindex amount\nDirect or\nabsolute\nAdd R1,(1001)\nRegs[R1] Regs[R1]\n+Mem[1001]\nSometimes useful for accessing\nstatic data; address constant may\nneed to be large\nMemory\nindirect\nAdd R1,@(R3)\nRegs[R1] Regs[R1]\n+Mem[Mem[Regs[R3]]]\nIf R3 is the address of a pointer p,\nthen mode yields *p\nAutoincrement\nAdd R1,(R2)+\nRegs[R1] Regs[R1]\n+ Mem[Regs[R2]]\nRegs[R2] Regs[R2]+d\nUseful for stepping through arrays\nwithin a loop. R2 points to start of\narray; each reference increments R2\nby size of an element, d\nAutodecrement\nAdd R1, \u2013(R2)\nRegs[R2] Regs[R2] \u2013 d\nRegs[R1] Regs[R1]\n+Mem[Regs[R2]]\nSame use as autoincrement.\nAutodecrement/-increment can also\nact as push/pop to implement a\nstack.\nScaled\nAdd R1,100(R2)[R3]\nRegs[R1] Regs[R1]\n+Mem[100+Regs[R2]\n+Regs[R3] * d]\nUsed to index arrays. May be\napplied to any indexed addressing\nmode in some computers\nFigure A.6 Selection of addressing modes with examples, meaning, and usage. In autoincrement/-decrement and\nscaled addressing modes, the variable d designates the size of the data item being accessed (i.e., whether the instruc-\ntion is accessing 1, 2, 4, or 8 bytes). These addressing modes are only useful when the elements being accessed are\nadjacent in memory. RISC computers use displacement addressing to simulate register indirect with 0 for the address\nand to simulate direct addressing using 0 in the base register. In our measurements, we use the first name shown for\neach mode. The extensions to C used as hardware descriptions are defined on page A.38.\nA.3\nMemory Addressing\n\u25a0\nA-9"
    },
    {
        "page": 660,
        "text": "Figure A.6 shows the most common names for the addressing modes, though the\nnames differ among architectures. In this figure and throughout the book, we will use\nan extension of the C programming language as a hardware description notation. In\nthis figure, only one non-C feature is used: the left arrow ( ) is used for assignment.\nWe also use the array Mem as the name for main memory and the array Regs for\nregisters. Thus, Mem[Regs[R1]] refers to the contents of the memory location\nwhose address is given by the contents of register 1 (R1). Later, we will introduce\nextensions for accessing and transferring data smaller than a word.\nAddressing modes have the ability to significantly reduce instruction counts;\nthey also add to the complexity of building a computer and may increase the aver-\nage clock cycles per instruction (CPI) of computers that implement those modes.\nThus, the usage of various addressing modes is quite important in helping the\narchitect choose what to include.\nFigure A.7 shows the results of measuring addressing mode usage patterns\nin three programs on the VAX architecture. We use the old VAX architecture\nfor a few measurements in this appendix because it has the richest set of addressing\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n24%\n11%\n39%\n32%\n40%\n3%\n43%\n17%\n55%\n0%\n6%\n16%\nScaled\nRegister indirect\nImmediate\nDisplacement\nTeX\nspice\ngcc\nTeX\nspice\ngcc\nTeX\nspice\ngcc\nTeX\nspice\ngcc\n1%\n6%\nMemory indirect\nTeX\nspice\ngcc\n1%\nFrequency of the addressing mode\nFigure A.7 Summary of use of memory addressing modes (including immediates).\nThese major addressing modes account for all but a few percent (0%\u20133%) of the mem-\nory accesses. Register modes, which are not counted, account for one-half of the oper-\nand references, while memory addressing modes (including immediate) account for the\nother half. Of course, the compiler affects what addressing modes are used; see\nSection A.8. The memory indirect mode on the VAX can use displacement, autoincre-\nment, or autodecrement to form the initial memory address; in these programs, almost\nall the memory indirect references use displacement mode as the base. Displacement\nmode includes all displacement lengths (8, 16, and 32 bits). The PC-relative addressing\nmodes, used almost exclusively for branches, are not included. Only the addressing\nmodes with an average frequency of over 1% are shown.\nA-10\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 661,
        "text": "modes and the fewest restrictions on memory addressing. For example, Figure A.6\non page A.9 shows all the modes the VAX supports. Most measurements in this\nappendix, however, will use the more recent register-register architectures to show\nhow programs use instruction sets of current computers.\nAs Figure A.7 shows, displacement and immediate addressing dominate\naddressing mode usage. Let\u2019s look at some properties of these two heavily\nused modes.\nDisplacement Addressing Mode\nThe major question that arises for a displacement-style addressing mode is that of\nthe range of displacements used. Based on the use of various displacement sizes, a\ndecision of what sizes to support can be made. Choosing the displacement field\nsizes is important because they directly affect the instruction length. Figure A.8\n30%\n35%\n40%\n25%\n20%\n15%\n10%\n5%\n0%\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nPercentage of displacement\nNumber of bits of displacement\nFloating-point average\nInteger average\n15\nFigure A.8 Displacement values are widely distributed. There are both a large num-\nber of small values and a fair number of large values. The wide distribution of displace-\nment values is due to multiple storage areas for variables and different displacements to\naccess them (see Section A.8) as well as the overall addressing scheme the compiler\nuses. The x-axis is log2 of the displacement, that is, the size of a field needed to represent\nthe magnitude of the displacement. Zero on the x-axis shows the percentage of dis-\nplacements of value 0. The graph does not include the sign bit, which is heavily affected\nby the storage layout. Most displacements are positive, but a majority of the largest dis-\nplacements (14+ bits) are negative. Because these data were collected on a computer\nwith 16-bit displacements, they cannot tell us about longer displacements. These data\nwere taken on the Alpha architecture with full optimization (see Section A.8) for SPEC\nCPU2000, showing the average of integer programs (CINT2000) and the average of\nfloating-point programs (CFP2000).\nA.3\nMemory Addressing\n\u25a0\nA-11"
    },
    {
        "page": 662,
        "text": "shows the measurements taken on the data access on a load-store architecture using\nour benchmark programs. We look at branch offsets in Section A.6\u2014data acces-\nsing patterns and branches are different; little is gained by combining them,\nalthough in practice the immediate sizes are made the same for simplicity.\nImmediate or Literal Addressing Mode\nImmediates can be used in arithmetic operations, in comparisons (primarily for\nbranches), and in moves where a constant is wanted in a register. The last case\noccurs for constants written in the code\u2014which tend to be small\u2014and for address\nconstants, which tend to be large. For the use of immediates it is important to know\nwhether they need to be supported for all operations or for only a subset. Figure A.9\nshows the frequency of immediates for the general classes of integer and floating-\npoint operations in an instruction set.\nAnother important instruction set measurement is the range of values for imme-\ndiates. Like displacement values, the size of immediate values affects instruction\nlength. As Figure A.10 shows, small immediate values are most heavily used. Large\nimmediates are sometimes used, however, most likely in addressing calculations.\nSummary: Memory Addressing\nFirst, because of their popularity, we would expect a new architecture to support at\nleast the following addressing modes: displacement, immediate, and register indi-\nrect. Figure A.7 shows that they represent 75%\u201399% of the addressing modes used\n0%\n5%\n10%\n15%\n20%\n25%\nLoads\nALU operations\nAll instructions\n21%\n16%\n25%\n19%\n23%\n22%\n30%\nFloating-point average\nInteger average\nFigure A.9 About one-quarter of data transfers and ALU operations have an imme-\ndiate operand. The bottom bars show that integer programs use immediates in about\none-fifth of the instructions, while floating-point programs use immediates in about\none-sixth of the instructions. For loads, the load immediate instruction loads 16 bits into\neither half of a 32-bit register. Load immediates are not loads in a strict sense because\nthey do not access memory. Occasionally a pair of load immediates is used to load a 32-\nbit constant, but this is rare. (For ALU operations, shifts by a constant amount are\nincluded as operations with immediate operands.) The programs and computer used\nto collect these statistics are the same as in Figure A.8.\nA-12\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 663,
        "text": "in our measurements. Second, we would expect the size of the address for displace-\nment mode to be at least 12\u201316 bits, because the caption in Figure A.8 suggests\nthese sizes would capture 75%\u201399% of the displacements. Third, we would expect\nthe size of the immediate field to be at least 8\u201316 bits. This claim is not substan-\ntiated by the caption of the figure to which it refers.\nHaving covered instruction set classes and decided on register-register archi-\ntectures, plus the previous recommendations on data addressing modes, we next\ncover the sizes and meanings of data.\nA.4\nType and Size of Operands\nHow is the type of an operand designated? Usually, encoding in the opcode\ndesignates the type of an operand\u2014this is the method used most often. Alterna-\ntively, the data can be annotated with tags that are interpreted by the hardware.\nThese tags specify the type of the operand, and the operation is chosen accordingly.\nComputers with tagged data, however, can only be found in computer museums.\nLet\u2019s start with desktop and server architectures. Usually the type of an oper-\nand\u2014integer, single-precision floating point, character, and so on\u2014effectively\n30%\n35%\n40%\n45%\n25%\n20%\n15%\n10%\n5%\n0%\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nPercentage of immediates\nNumber of bits needed for immediate\nFloating-point average\nInteger average\n15\nFigure A.10 The distribution of immediate values. The x-axis shows the number of\nbits needed to represent the magnitude of an immediate value\u20140 means the imme-\ndiate field value was 0. The majority of the immediate values are positive. About 20%\nwere negative for CINT2000, and about 30% were negative for CFP2000. These measure-\nments were taken on an Alpha, where the maximum immediate is 16 bits, for the same\nprograms as in Figure A.8. A similar measurement on the VAX, which supported 32-bit\nimmediates, showed that about 20%\u201325% of immediates were longer than 16 bits.\nThus, 16 bits would capture about 80% and 8 bits about 50%.\nA.4\nType and Size of Operands\n\u25a0\nA-13"
    },
    {
        "page": 664,
        "text": "gives its size. Common operand types include character (8 bits), half word (16\nbits), word (32 bits), single-precision floating point (also 1 word), and double-\nprecision floating point (2 words). Integers are almost universally represented\nas two\u2019s complement binary numbers. Characters are usually in ASCII, but the\n16-bit Unicode (used in Java) is gaining popularity with the internationalization\nof computers. Until the early 1980s, most computer manufacturers chose their\nown floating-point representation. Almost all computers since that time follow\nthe same standard for floating point, the IEEE standard 754, although this level\nof accuracy has recently been abandoned in application-specific processors. The\nIEEE floating-point standard is discussed in detail in Appendix J.\nSome architectures provide operations on character strings, although such oper-\nations are usually quite limited and treat each byte in the string as a single character.\nTypical operations supported on character strings are comparisons and moves.\nFor business applications, some architectures support a decimal format, usually\ncalled packed decimal or binary-coded decimal\u20144 bits are used to encode the\nvalues 0\u20139, and 2 decimal digits are packed into each byte. Numeric character\nstrings are sometimes called unpacked decimal, and operations\u2014called packing\nand unpacking\u2014are usually provided for converting back and forth between them.\nOne reason to use decimal operands is to get results that exactly match decimal\nnumbers, as some decimal fractions do not have an exact representation in binary.\nFor example, 0.1010 is a simple fraction in decimal, but in binary it requires an\ninfinite set of repeating digits: 0:0001100110011\u20262. Thus, calculations that are\nexact in decimal can be close but inexact in binary, which can be a problem for\nfinancial transactions. (See Appendix J to learn more about precise arithmetic.)\nThe SPEC benchmarks use byte or character, half-word (short integer), word\n(integer and single precision floating point), double-word (long integer), and\nfloating-point data types. Figure A.11 shows the dynamic distribution of the sizes\nof objects referenced from memory for these programs. The frequency of access to\n0%\n20%\n40%\n60%\n80%\nByte\n(8 bits)\nHalf word\n(16 bits)\nWord\n(32 bits)\nDouble word\n(64 bits)\n10%\n1%\n5%\n0%\n26%\n29%\n59%\n70%\nFloating-point average\nInteger average\nFigure A.11 Distribution of data accesses by size for the benchmark programs. The\ndouble-word data type is used for double-precision floating point in floating-point pro-\ngrams and for addresses, because the computer uses64-bit addresses. Ona 32-bit address\ncomputer the 64-bit addresses would be replaced by 32-bit addresses, and so almost all\ndouble-word accesses in integer programs would become single-word accesses.\nA-14\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 665,
        "text": "different data types helps in deciding what types are most important to support effi-\nciently. Should the computer have a 64-bit access path, or would taking two cycles\nto access a double word be satisfactory? As we saw earlier, byte accesses require an\nalignment network: how important is it to support bytes as primitives? Figure A.11\nuses memory references to examine the types of data being accessed.\nIn some architectures, objects in registers may be accessed as bytes or half\nwords. However, such access is very infrequent\u2014on the VAX, it accounts for\nno more than 12% of register references, or roughly 6% of all operand accesses\nin these programs.\nA.5\nOperations in the Instruction Set\nThe operators supported by most instruction set architectures can be categorized as\nin Figure A.12. One rule of thumb across all architectures is that the most widely\nexecuted instructions are the simple operations of an instruction set. For example,\nFigure A.13 shows 10 simple instructions that account for 96% of instructions exe-\ncuted for a collection of integer programs running on the popular Intel 80x86.\nHence, the implementor of these instructions should be sure to make these fast,\nas they are the common case.\nOperator type\nExamples\nArithmetic and\nlogical\nInteger arithmetic and logical operations: add, subtract, and, or,\nmultiply, divide\nData transfer\nLoads-stores (move instructions on computers with memory\naddressing)\nControl\nBranch, jump, procedure call and return, traps\nSystem\nOperating system call, virtual memory management instructions\nFloating point\nFloating-point operations: add, multiply, divide, compare\nDecimal\nDecimal add, decimal multiply, decimal-to-character conversions\nString\nString move, string compare, string search\nGraphics\nPixel and vertex operations, compression/decompression operations\nFigure A.12 Categories of instruction operators and examples of each. All computers\ngenerally provide a full set of operations for the first three categories. The support for\nsystem functions in the instruction set varies widely among architectures, but all com-\nputers must have some instruction support for basic system functions. The amount of\nsupport in the instruction set for the last four categories may vary from none to an\nextensive set of special instructions. Floating-point instructions will be provided in\nany computer that is intended for use in an application that makes much use of floating\npoint. These instructions are sometimes part of an optional instruction set. Decimal and\nstring instructions are sometimes primitives, as in the VAX or the IBM 360, or may be\nsynthesized by the compiler from simpler instructions. Graphics instructions typically\noperate on many smaller data items in parallel\u2014for example, performing eight 8-bit\nadditions on two 64-bit operands.\nA.5\nOperations in the Instruction Set\n\u25a0\nA-15"
    },
    {
        "page": 666,
        "text": "As mentioned before, the instructions in Figure A.13 are found in every computer\nfor every application\u2013\u2013desktop, server, embedded\u2013\u2013with the variations of operations\nin Figure A.12 largely depending on which data types the instruction set includes.\nA.6\nInstructions for Control Flow\nBecause the measurements of branch and jump behavior are fairly independent of\nothermeasurementsandapplications,wenowexaminetheuseofcontrolflowinstruc-\ntions, which have little in common with the operations of the previous sections.\nThere is no consistent terminology for instructions that change the flow of con-\ntrol. In the 1950s they were typically called transfers. Beginning in 1960 the name\nbranch began to be used. Later, computers introduced additional names. Through-\nout this book we will use jump when the change in control is unconditional and\nbranch when the change is conditional.\nWe can distinguish four different types of control flow change:\n\u25a0\nConditional branches\n\u25a0\nJumps\n\u25a0\nProcedure calls\n\u25a0\nProcedure returns\nWe want to know the relative frequency of these events, as each event is different,\nmay use different instructions, and may have different behavior. Figure A.14\nshows the frequencies of these control flow instructions for a load-store computer\nrunning our benchmarks.\nRank\n80x86 instruction\nInteger average\n% total executed)\n1\nLoad\n22%\n2\nConditional branch\n20%\n3\nCompare\n16%\n4\nStore\n12%\n5\nAdd\n8%\n6\nAnd\n6%\n7\nSub\n5%\n8\nMove register-register\n4%\n9\nCall\n1%\n10\nReturn\n1%\nTotal\n96%\nFigure A.13 The top 10 instructions for the 80x86. Simple instructions dominate this\nlist and are responsible for 96% of the instructions executed. These percentages are the\naverage of the five SPECint92 programs.\nA-16\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 667,
        "text": "Addressing Modes for Control Flow Instructions\nThe destination address of a control flow instruction must always be specified. This\ndestination is specified explicitly in the instruction in the vast majority of cases\u2014\nprocedure return being the major exception, because for return the target is not\nknown at compile time. The most common way to specify the destination is to supply\na displacement that is added to the program counter (PC). Control flow instructions\nof this sort are called PC-relative. PC-relative branches or jumps are advantageous\nbecause the target is often near the current instruction, and specifying the position\nrelative to the current PC requires fewer bits. Using PC-relative addressing also per-\nmits the code to run independently of where it is loaded. This property, called posi-\ntion independence, can eliminate some work when the program is linked and is also\nuseful in programs linked dynamically during execution.\nTo implement returns and indirect jumps when the target is not known at com-\npile time, a method other than PC-relative addressing is required. Here, there must\nbe a way to specify the target dynamically, so that it can change at runtime. This\ndynamic address may be as simple as naming a register that contains the target\naddress; alternatively, the jump may permit any addressing mode to be used to sup-\nply the target address.\nThese register indirect jumps are also useful for four other important\nfeatures:\n\u25a0\nCase or switch statements, found in most programming languages (which\nselect among one of several alternatives).\n\u25a0\nVirtual functions or methods in object-oriented languages like C++ or Java\n(which allow different routines to be called depending on the type of the\nargument).\n0%\n25%\n50%\n75%\nCall/return\nJump\nConditional branch\n75%\n82%\n6%\n10%\n19%\n8%\n100%\nFrequency of branch instructions\nFloating-point average\nInteger average\nFigure A.14 Breakdown of control flow instructions into three classes: calls or\nreturns, jumps, and conditional branches. Conditional branches clearly dominate. Each\ntype is counted in one of three bars. The programs and computer used to collect these\nstatistics are the same as those in Figure A.8.\nA.6\nInstructions for Control Flow\n\u25a0\nA-17"
    },
    {
        "page": 668,
        "text": "\u25a0\nHigh-order functions or function pointers in languages like C or C++ (which\nallow functions to be passed as arguments, giving some of the flavor of object-\noriented programming).\n\u25a0\nDynamically shared libraries (which allow a library to be loaded and linked at\nruntime only when it is actually invoked by the program rather than loaded and\nlinked statically before the program is run).\nIn all four cases the target address is not known at compile time, and hence is usu-\nally loaded from memory into a register before the register indirect jump.\nAs branches generally use PC-relative addressing to specify their targets, an\nimportant question concerns how far branch targets are from branches. Knowing\nthe distribution of these displacements will help in choosing what branch offsets to\nsupport, and thus will affect the instruction length and encoding. Figure A.15\nshows the distribution of displacements for PC-relative branches in instructions.\nAbout 75% of the branches are in the forward direction.\nConditional Branch Options\nBecause most changes in control flow are branches, deciding how to specify the\nbranch condition is important. Figure A.16 shows the three primary techniques in\nuse today and their advantages and disadvantages.\n30%\n25%\n20%\n15%\n10%\n5%\n0%\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nPercentage of distance\nBits of branch displacement\nFloating-point average\nInteger\naverage\n15\n16\n17\n18\n19\n20\nFigure A.15 Branch distances in terms of number of instructions between the target and the branch instruction.\nThe most frequent branches in the integer programs are to targets that can be encoded in 4\u20138 bits. This result tells us\nthat short displacement fields often suffice for branches and that the designer can gain some encoding density by\nhaving a shorter instruction with a smaller branch displacement. These measurements were taken on a load-store\ncomputer (Alpha architecture) with all instructions aligned on word boundaries. An architecture that requires fewer\ninstructions for the same program, such as a VAX, would have shorter branch distances. However, the number of bits\nneeded for the displacement may increase if the computer has variable-length instructions to be aligned on any byte\nboundary. The programs and computer used to collect these statistics are the same as those in Figure A.8.\nA-18\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 669,
        "text": "One of the most noticeable properties of branches is that a large number of the\ncomparisons are simple tests, and a large number are comparisons with zero. Thus,\nsome architectures choose to treat these comparisons as special cases, especially if\na compare and branch instruction is being used. Figure A.17 shows the frequency\nof different comparisons used for conditional branching.\nProcedure Invocation Options\nProcedure calls and returns include control transfer and possibly some state saving;\nat a minimum the return address must be saved somewhere, sometimes in a special\nlink register or just a GPR. Some older architectures provide a mechanism to save\nmany registers, while newer architectures require the compiler to generate stores\nand loads for each register saved and restored.\nThere are two basic conventions in use to save registers: either at the call site or\ninside the procedure being called. Caller saving means that the calling procedure\nmust save the registers that it wants preserved for access after the call, and thus the\ncalled procedure need not worry about registers. Callee saving is the opposite: the\ncalled procedure must save the registers it wants to use, leaving the caller unre-\nstrained. There are times when caller save must be used because of access patterns\nto globally visible variables in two different procedures. For example, suppose we\nhave a procedure P1 that calls procedure P2, and both procedures manipulate the\nName\nExamples\nHow condition is tested\nAdvantages\nDisadvantages\nCondition\ncode (CC)\n80x86, ARM,\nPowerPC,\nSPARC, SuperH\nTests special bits set by\nALU operations,\npossibly under program\ncontrol\nSometimes condition\nis set for free.\nCC is extra state. Condition\ncodes constrain the ordering of\ninstructionsbecause they pass\ninformation from one\ninstruction to a branch\nCondition\nregister/\nlimited\ncomparison\nAlpha, MIPS\nTests arbitrary register\nwith the result of a simple\ncomparison (equality or\nzero tests)\nSimple\nLimited compare may affect\ncritical path or require extra\ncomparison for general\ncondition\nCompare\nand branch\nPA-RISC, VAX,\nRISC-V\nCompare is part of the\nbranch. Fairly general\ncompares are allowed\n(greater then, less then)\nOne instruction\nrather than two for a\nbranch\nMay set critical path for branch\ninstructions\nFigure A.16 The major methods for evaluating branch conditions, their advantages, and their disadvantages.\nAlthough condition codes can be set by ALU operations that are needed for other purposes, measurements on pro-\ngrams show that this rarely happens. The major implementation problems with condition codes arise when the con-\ndition code is set by a large or haphazardly chosen subset of the instructions, rather than being controlled by a bit in the\ninstruction. Computers with compare and branch often limit the set of compares and use a separate operation and\nregister for more complex compares. Often, different techniques are used for branches based on floating-point com-\nparison versus those based on integer comparison. This dichotomy is reasonable because the number of branches that\ndepend on floating-point comparisons is much smaller than the number depending on integer comparisons.\nA.6\nInstructions for Control Flow\n\u25a0\nA-19"
    },
    {
        "page": 670,
        "text": "global variable x. If P1 had allocated x to a register, it must be sure to save x to a\nlocation known by P2 before the call to P2. A compiler\u2019s ability to discover when a\ncalled procedure may access register-allocated quantities is complicated by the\npossibility of separate compilation. Suppose P2 may not touch x but can call\nanother procedure, P3, that may access x, yet P2 and P3 are compiled separately.\nBecause of these complications, most compilers will conservatively caller save any\nvariable that may be accessed during a call.\nIn the cases where either convention could be used, some programs will be more\noptimal with callee save and some will be more optimal with caller save. As a result,\nmost real systems today use a combination of the two mechanisms. This convention\nis specified in an application binary interface (ABI) that sets down the basic rules as\nto which registers should be caller saved and which should be callee saved. Later in\nthis appendix we will examine the mismatch between sophisticated instructions for\nautomatically saving registers and the needs of the compiler.\nSummary: Instructions for Control Flow\nControl flow instructions are some of the most frequently executed instructions.\nAlthough there are many options for conditional branches, we would expect\nGreater than\nGreater than or equal\nEqual\nNot equal\nLess than or equal\n Less than\n35%\n34%\n33%\n44%\n0%\n0%\n0%\n11%\n18%\n16%\n2%\n5%\n0%\n10%\n20%\n30%\n40%\n50%\nFrequency of comparison types in branches\nFloating-point average\nInteger average\nFigure A.17 Frequency of different types of compares in conditional branches. Less\nthan (or equal) branches dominate this combination of compiler and architecture. These\nmeasurements include both the integer and floating-point compares in branches. The\nprograms and computer used to collect these statistics are the same as those in\nFigure A.8.\nA-20\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 671,
        "text": "branch addressing in a new architecture to be able to jump to hundreds of instruc-\ntions either above or below the branch. This requirement suggests a PC-relative\nbranch displacement of at least 8 bits. We would also expect to see register indirect\nand PC-relative addressing for jump instructions to support returns as well as many\nother features of current systems.\nWe have now completed our instruction architecture tour at the level seen by\nan assembly language programmer or compiler writer. We are leaning toward a\nload-store architecture with displacement, immediate, and register indirect addres-\nsing modes. These data are 8-, 16-, 32-, and 64-bit integers and 32- and 64-bit\nfloating-point data. The instructions include simple operations, PC-relative condi-\ntional branches, jump and link instructions for procedure call, and register indirect\njumps for procedure return (plus a few other uses).\nNow we need to select how to represent this architecture in a form that makes it\neasy for the hardware to execute.\nA.7\nEncoding an Instruction Set\nClearly, the choices mentioned herein will affect how the instructions are encoded\ninto a binary representation for execution by the processor. This representation\naffects not only the size of the compiled program but also the implementation\nof the processor, which must decode this representation to quickly find the oper-\nation and its operands. The operation is typically specified in one field, called the\nopcode. As we shall see, the important decision is how to encode the addressing\nmodes with the operations.\nThis decision depends on the range of addressing modes and the degree of\nindependence between opcodes and modes. Some older computers have one\nto five operands with 10 addressing modes for each operand (see Figure A.6).\nFor such a large number of combinations, typically a separate address\nspecifier is needed for each operand: the address specifier tells what addres-\nsing mode is used to access the operand. At the other extreme are load-store\ncomputers with only one memory operand and only one or two addressing\nmodes; obviously, in this case, the addressing mode can be encoded as part\nof the opcode.\nWhen encoding the instructions, the number of registers and the number of\naddressing modes both have a significant impact on the size of instructions, as\nthe register field and addressing mode field may appear many times in a single\ninstruction. In fact, for most instructions many more bits are consumed in\nencoding addressing modes and register fields than in specifying the opcode.\nThe architect must balance several competing forces when encoding the\ninstruction set:\n1. The desire to have as many registers and addressing modes as possible.\n2. The impact of the size of the register and addressing mode fields on the average\ninstruction size and hence on the average program size.\nA.7\nEncoding an Instruction Set\n\u25a0\nA-21"
    },
    {
        "page": 672,
        "text": "3. A desire to have instructions encoded into lengths that will be easy to han-\ndle in a pipelined implementation. (The value of easily decoded instructions\nis discussed in Appendix C and Chapter 3.) As a minimum, the architect\nwants instructions to be in multiples of bytes, rather than an arbitrary bit\nlength. Many desktop and server architects have chosen to use a fixed-\nlength instruction to gain implementation benefits while sacrificing average\ncode size.\nFigure A.18 shows three popular choices for encoding the instruction set. The first\nwe call variable, because it allows virtually all addressing modes to be with all\noperations. This style is best when there are many addressing modes and opera-\ntions. The second choice we call fixed, because it combines the operation and\nthe addressing mode into the opcode. Often fixed encoding will have only a single\nOperation and\nno. of operands\nAddress\nspecifier 1\nAddress\nfield 1\nAddress\nfield 1\nOperation\nAddress\nfield 2\nAddress\nfield 3\nAddress\nspecifier\nOperation\nAddress\nfield \nAddress\nspecifier 1\nOperation\nAddress\nspecifier 2\nAddress\nfield\nAddress\nspecifier\nOperation\nAddress\nfield 1\nAddress\nfield 2 \nAddress\nspecifier n\nAddress\nfield n\n(A) Variable (e.g., Intel 80x86, VAX)\n(B) Fixed (e.g., RISC V, ARM, MIPS, PowerPC, SPARC)\n(C) Hybrid (e.g., RISC V Compressed (RV32IC), IBM 360/370, microMIPS, Arm Thumb2)\nFigure A.18 Three basic variations in instruction encoding: variable length, fixed\nlength, and hybrid. The variable format can support any number of operands, with\neach address specifier determining the addressing mode and the length of the spec-\nifier for that operand. It generally enables the smallest code representation, because\nunused fields need not be included. The fixed format always has the same number of\noperands, with the addressing modes (if options exist) specified as part of the\nopcode. It generally results in the largest code size. Although the fields tend not\nto vary in their location, they will be used for different purposes by different instruc-\ntions. The hybrid approach has multiple formats specified by the opcode, adding one\nor two fields to specify the addressing mode and one or two fields to specify the\noperand address.\nA-22\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 673,
        "text": "size for all instructions; it works best when there are few addressing modes and\noperations. The trade-off between variable encoding and fixed encoding is size\nof programs versus ease of decoding in the processor. Variable tries to use as\nfew bits as possible to represent the program, but individual instructions can vary\nwidely in both size and the amount of work to be performed.\nLet\u2019s look at an 80x86 instruction to see an example of the variable encoding:\nadd EAX,1000(EBX)\nThe name add means a 32-bit integer add instruction with two operands, and\nthis opcode takes 1 byte. An 80x86 address specifier is 1 or 2 bytes, specifying the\nsource/destination register (EAX) and the addressing mode (displacement in\nthis case) and base register (EBX) for the second operand. This combination takes\n1 byte to specify the operands. When in 32-bit mode (see Appendix K), the size of\nthe address field is either 1 byte or 4 bytes. Because1000 is bigger than 28, the total\nlength of the instruction is\n1 + 1 + 4 \u00bc 6 bytes\nThe length of 80x86 instructions varies between 1 and 17 bytes. 80x86 programs\nare generally smaller than the RISC architectures, which use fixed formats (see\nAppendix K).\nGiven these two poles of instruction set design of variable and fixed, the third\nalternative immediately springs to mind: reduce the variability in size and work of\nthe variable architecture but provide multiple instruction lengths to reduce code\nsize. This hybrid approach is the third encoding alternative, and we\u2019ll see examples\nshortly.\nReduced Code Size in RISCs\nAs RISC computers started being used in embedded applications, the 32-bit fixed\nformat became a liability because cost, and hence smaller code, are important. In\nresponse, several manufacturers offered a new hybrid version of their RISC instruc-\ntion sets, with both 16-bit and 32-bit instructions. The narrow instructions support\nfewer operations, smaller address and immediate fields, fewer registers, and the\ntwo-address format rather than the classic three-address format of RISC computers.\nRISC-V offers such an extension, called RV32IC, the C standing for compressed.\nCommon instruction occurrences, such as intermediates with small values and com-\nmon ALU operations with the source and destination register being identical, are\nencoded in 16-bit formats. Appendix K gives two other examples, the ARM Thumb\nand microMIPS, which both claim a code size reduction of up to 40%.\nIn contrast to these instruction set extensions, IBM simply compresses its stan-\ndard instruction set and then adds hardware to decompress instructions as they are\nfetched from memory on an instruction cache miss. Thus, the instruction cache\ncontains full 32-bit instructions, but compressed code is kept in main memory,\nROMs, and the disk. The advantage of a compressed format, such as RV32IC,\nA.7\nEncoding an Instruction Set\n\u25a0\nA-23"
    },
    {
        "page": 674,
        "text": "microMIPS and Thumb2 is that instruction caches act as if they are about 25%\nlarger, while IBM\u2019s CodePack means that compilers need not be changed to handle\ndifferent instruction sets and instruction decoding can remain simple.\nCodePack starts with run-length encoding compression on any PowerPC pro-\ngram and then loads the resulting compression tables in a 2 KB table on chip.\nHence, every program has its own unique encoding. To handle branches, which\nare no longer to an aligned word boundary, the PowerPC creates a hash table in\nmemory that maps between compressed and uncompressed addresses. Like a\nTLB (see Chapter 2), it caches the most recently used address maps to reduce\nthe number of memory accesses. IBM claims an overall performance cost of\n10%, resulting in a code size reduction of 35%\u201340%.\nSummary: Encoding an Instruction Set\nDecisions made in the components of instruction set design discussed in previous\nsections determine whether the architect has the choice between variable and fixed\ninstruction encodings. Given the choice, the architect more interested in code size\nthan performance will pick variable encoding, and the one more interested in per-\nformance than code size will pick fixed encoding. RISC-V, MIPS, and ARM all\nhave an instruction set extension that uses 16-bit instruction, as well as 32-bit;\napplications with serious code size constraints can opt to use the 16-bit variant\nto decrease code size. Appendix E gives 13 examples of the results of architects\u2019\nchoices. In Appendix C and Chapter 3, the impact of variability on performance of\nthe processor will be discussed further.\nWe have almost finished laying the groundwork for the RISC-V instruction set\narchitecture that will be introduced in Section A.9. Before we do that, however, it\nwill be helpful to take a brief look at compiler technology and its effect on program\nproperties.\nA.8\nCross-Cutting Issues: The Role of Compilers\nToday almost all programming is done in high-level languages for desktop and\nserver applications. This development means that because most instructions exe-\ncuted are the output of a compiler, an instruction set architecture is essentially\na compiler target. In earlier times for these applications, architectural decisions\nwere often made to ease assembly language programming or for a specific kernel.\nBecause the compiler will significantly affect the performance of a computer,\nunderstanding compiler technology today is critical to designing and efficiently\nimplementing an instruction set.\nOnce it was popular to try to isolate the compiler technology and its effect on\nhardware performance from the architecture and its performance, just as it was\npopular to try to separate architecture from its implementation. This separation\nis essentially impossible with today\u2019s desktop compilers and computers. Architec-\ntural choices affect the quality of the code that can be generated for a computer and\nthe complexity of building a good compiler for it, for better or for worse.\nA-24\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 675,
        "text": "In this section, we discuss the critical goals in the instruction set primarily from\nthe compiler viewpoint. It starts with a review of the anatomy of current compilers.\nNext we discuss how compiler technology affects the decisions of the architect,\nand how the architect can make it hard or easy for the compiler to produce good\ncode. We conclude with a review of compilers and multimedia operations, which\nunfortunately is a bad example of cooperation between compiler writers and\narchitects.\nThe Structure of Recent Compilers\nTo begin, let\u2019s look at what optimizing compilers are like today. Figure A.19 shows\nthe structure of recent compilers.\nA compiler writer\u2019s first goal is correctness\u2014all valid programs must be\ncompiled correctly. The second goal is usually speed of the compiled code. Typ-\nically, a whole set of other goals follows these two, including fast compilation,\ndebugging support, and interoperability among languages. Normally, the passes\nLanguage dependent;\nmachine independent\nDependencies\nTransform language to\ncommon intermediate form\nFunction\nFront end per \nlanguage\nHigh-level\noptimizations\nGlobal\noptimizer\nCode generator\nIntermediate\nrepresentation\nFor example, loop \ntransformations and \nprocedure inlining\n(also called \nprocedure integration)\nIncluding global and local\noptimizations+ register\nallocation\nDetailed instruction selection\nand machine-dependent\noptimizations; may include\nor be followed by assembler\nSomewhat language\ndependent; largely machine\nindependent\nSmall language dependencies;\nmachine dependencies slight \n(e.g., register counts/types)\nHighly machine dependent;\nlanguage independent\nFigure A.19 Compilers typically consist of two to four passes, with more highly opti-\nmizing compilers having more passes. This structure maximizes the probability that a\nprogram compiled at various levels of optimization will produce the same output when\ngiven the same input. The optimizing passes are designed to be optional and may be\nskipped when faster compilation is the goal and lower-quality code is acceptable. A pass\nis simply one phase in which the compiler reads and transforms the entire program.\n(The term phase is often used interchangeably with pass.) Because the optimizing passes\nare separated, multiple languages can use the same optimizing and code generation\npasses. Only a new front end is required for a new language.\nA.8\nCross-Cutting Issues: The Role of Compilers\n\u25a0\nA-25"
    },
    {
        "page": 676,
        "text": "in the compiler transform higher-level, more abstract representations into progres-\nsively lower-level representations. Eventually it reaches the instruction set. This\nstructure helps manage the complexity of the transformations and makes writing\na bug-free compiler easier.\nThe complexity of writing a correct compiler is a major limitation on the\namount of optimization that can be done. Although the multiple-pass structure\nhelps reduce compiler complexity, it also means that the compiler must order\nand perform some transformations before others. In the diagram of the optimizing\ncompiler in Figure A.19, we can see that certain high-level optimizations are per-\nformed long before it is known what the resulting code will look like. Once such a\ntransformation is made, the compiler can\u2019t afford to go back and revisit all steps,\npossibly undoing transformations. Such iteration would be prohibitive, both in\ncompilation time and in complexity. Thus, compilers make assumptions about\nthe ability of later steps to deal with certain problems. For example, compilers usu-\nally have to choose which procedure calls to expand inline before they know the\nexact size of the procedure being called. Compiler writers call this problem the\nphase-ordering problem.\nHow does this ordering of transformations interact with the instruction set\narchitecture? A good example occurs with the optimization called global common\nsubexpression elimination. This optimization finds two instances of an expression\nthat compute the same value and saves the value of the first computation in a tem-\nporary. It then uses the temporary value, eliminating the second computation of the\ncommon expression.\nFor this optimization to be significant, the temporary must be allocated to a\nregister. Otherwise, the cost of storing the temporary in memory and later reloading\nit may negate the savings gained by not recomputing the expression. There are, in\nfact, cases where this optimization actually slows down code when the temporary\nis not register allocated. Phase ordering complicates this problem because register\nallocation is typically done near the end of the global optimization pass, just before\ncode generation. Thus, an optimizer that performs this optimization must assume\nthat the register allocator will allocate the temporary to a register.\nOptimizations performed by modern compilers can be classified by the style of\nthe transformation, as follows:\n\u25a0\nHigh-level optimizations are often done on the source with output fed to later\noptimization passes.\n\u25a0\nLocal optimizations optimize code only within a straight-line code fragment\n(called a basic block by compiler people).\n\u25a0\nGlobal optimizations extend the local optimizations across branches and intro-\nduce a set of transformations aimed at optimizing loops.\n\u25a0\nRegister allocation associates registers with operands.\n\u25a0\nProcessor-dependent optimizations attempt to take advantage of specific archi-\ntectural knowledge.\nA-26\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 677,
        "text": "Register Allocation\nBecause of the central role that register allocation plays, both in speeding up the\ncode and in making other optimizations useful, it is one of the most important\u2014\nif not the most important\u2014of the optimizations. Register allocation algorithms\ntoday are based on a technique called graph coloring. The basic idea behind\ngraph coloring is to construct a graph representing the possible candidates\nfor allocation to a register and then to use the graph to allocate registers.\nRoughly speaking, the problem is how to use a limited set of colors so that\nno two adjacent nodes in a dependency graph have the same color. The empha-\nsis in the approach is to achieve 100% register allocation of active variables.\nThe problem of coloring a graph in general can take exponential time as a\nfunction of the size of the graph (NP-complete). There are heuristic algorithms,\nhowever, that work well in practice, yielding close allocations that run in near-\nlinear time.\nGraph coloring works best when there are at least 16 (and preferably more)\ngeneral-purpose registers available for global allocation for integer variables\nand additional registers for floating point. Unfortunately, graph coloring does\nnot work very well when the number of registers is small because the heuristic\nalgorithms for coloring the graph are likely to fail.\nImpact of Optimizations on Performance\nIt is sometimes difficult to separate some of the simpler optimizations\u2014local and\nprocessor-dependent optimizations\u2014from transformations done in the code gen-\nerator. Examples of typical optimizations are given in Figure A.20. The last col-\numn of Figure A.20 indicates the frequency with which the listed optimizing\ntransforms were applied to the source program.\nFigure A.21 shows the effect of various optimizations on instructions executed\nfor two programs. In this case, optimized programs executed roughly 25%\u201390%\nfewer instructions than unoptimized programs. The figure illustrates the impor-\ntance of looking at optimized code before suggesting new instruction set features,\nbecause a compiler might completely remove the instructions the architect was try-\ning to improve.\nThe Impact of Compiler Technology on the Architect\u2019s\nDecisions\nThe interaction of compilers and high-level languages significantly affects how\nprograms use an instruction set architecture. There are two important questions:\nhow are variables allocated and addressed? How many registers are needed to allo-\ncate variables appropriately? To address these questions, we must look at the three\nseparate areas in which current high-level languages allocate their data:\nA.8\nCross-Cutting Issues: The Role of Compilers\n\u25a0\nA-27"
    },
    {
        "page": 678,
        "text": "\u25a0\nThe stack is used to allocate local variables. The stack is grown or shrunk on\nprocedure call or return, respectively. Objects on the stack are addressed rel-\native to the stack pointer and are primarily scalars (single variables) rather\nthan arrays. The stack is used for activation records, not as a stack for eval-\nuating expressions. Hence, values are almost never pushed or popped on\nthe stack.\nOptimization name\nExplanation\nPercentage of the total number\nof optimizing transforms\nHigh-level\nAt or near the source level; processor-independent\nProcedure integration\nReplace procedure call by procedure body\nN.M.\nLocal\nWithin straight-line code\nCommon subexpression\nelimination\nReplace two instances of the same computation by\nsingle copy\n18%\nConstant propagation\nReplace all instances of a variable that is assigned a\nconstant with the constant\n22%\nStack height reduction\nRearrange expression tree to minimize resources\nneeded for expression evaluation\nN.M.\nGlobal\nAcross a branch\nGlobal common\nsubexpression elimination\nSame as local, but this version crosses branches\n13%\nCopy propagation\nReplace all instances of a variable A that has been\nassigned X (i.e., A\u00bcX) with X\n11%\nCode motion\nRemove code from a loop that computes same\nvalue each iteration of the loop\n16%\nInduction variable\nelimination\nSimplify/eliminate array addressing calculations\nwithin loops\n2%\nProcessor-dependent\nDepends on processor knowledge\nStrength reduction\nMany examples, such as replace multiply by a\nconstant with adds and shifts\nN.M.\nPipeline scheduling\nReorder instructions to improve pipeline\nperformance\nN.M.\nBranch offset\noptimization\nChoose the shortest branch displacement that\nreaches target\nN.M.\nFigure A.20 Major types of optimizations and examples in each class. These data tell us about the relative fre-\nquency of occurrence of various optimizations. The third column lists the static frequency with which some of\nthe common optimizations are applied in a set of 12 small Fortran and Pascal programs. There are nine local and\nglobal optimizations done by the compiler included in the measurement. Six of these optimizations are covered\nin the figure, and the remaining three account for 18% of the total static occurrences. The abbreviation N.M. means\nthat the number of occurrences of that optimization was not measured. Processor-dependent optimizations are usu-\nally done in a code generator, and none of those was measured in this experiment. The percentage is the portion of\nthe static optimizations that are of the specified type. Data from Chow, F.C., 1983. A Portable Machine-Independent\nGlobal Optimizer\u2014Design and Measurements (Ph.D. thesis). Stanford University, Palo Alto, CA (collected using the\nStanford UCODE compiler).\nA-28\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 679,
        "text": "\u25a0\nThe global data area is used to allocate statically declared objects, such as\nglobal variables and constants. A large percentage of these objects are arrays\nor other aggregate data structures.\n\u25a0\nThe heap is used to allocate dynamic objects that do not adhere to a stack disci-\npline. Objects in the heap are accessed with pointers and are typically not scalars.\nRegister allocation is much more effective for stack-allocated objects than for\nglobal variables, and register allocation is essentially impossible for heap-allocated\nobjects because they are accessed with pointers. Global variables and some stack\nvariables are impossible to allocate because they are aliased\u2014there are multiple\nways to refer to the address of a variable, making it illegal to put it into a register.\n(Most heap variables are effectively aliased for today\u2019s compiler technology.)\nFor example, consider the following code sequence, where & returns the\naddress of a variable and * dereferences a pointer:\np \u00bc&a\n\u2013 gets address of a in p\na \u00bc...\n\u2013 assigns to a directly\n*p \u00bc...\n\u2013 uses p to assign to a\n...a...\n\u2013 accesses a\nThe variable a could not be register allocated across the assignment to *p with-\nout generating incorrect code. Aliasing causes a substantial problem because it is\nmcf, level 0\nProgram, compiler optimization level\n100%\n0%\n20%\n40%\n60%\n80%\nBranches/calls\nFloating-point ALU ops\nLoads-stores\nInteger ALU ops\nPercentage of unoptimized instructions executed\nmcf, level 1\nmcf, level 2\nmcf, level 3\nlucas, level 0\nlucas, level 1\nlucas, level 2\nlucas, level 3\n11%\n12%\n21%\n100%\n76%\n76%\n84%\n100%\nFigure A.21 Change in instruction count for the programs lucas and mcf from the\nSPEC2000 as compiler optimization levels vary. Level 0 is the same as unoptimized\ncode. Level 1 includes local optimizations, code scheduling, and local register allocation.\nLevel 2 includes global optimizations, loop transformations (software pipelining), and\nglobal register allocation. Level 3 adds procedure integration. These experiments were\nperformed on Alpha compilers.\nA.8\nCross-Cutting Issues: The Role of Compilers\n\u25a0\nA-29"
    },
    {
        "page": 680,
        "text": "often difficult or impossible to decide what objects a pointer may refer to. A com-\npiler must be conservative; some compilers will not allocate any local variables of a\nprocedure in a register when there is a pointer that may refer to one of the local\nvariables.\nHow the Architect Can Help the Compiler Writer\nToday, the complexity of a compiler does not come from translating simple state-\nments like A\u00bcB+C. Most programs are locally simple, and simple translations\nwork fine. Rather, complexity arises because programs are large and globally com-\nplex in their interactions, and because the structure of compilers means decisions\nare made one step at a time about which code sequence is best.\nCompiler writers often are working under their own corollary of a basic prin-\nciple in architecture: make the frequent cases fast and the rare case correct. That is,\nif we know which cases are frequent and which are rare, and if generating code for\nboth is straightforward, then the quality of the code for the rare case may not be\nvery important\u2014but it must be correct!\nSome instruction set properties help the compiler writer. These properties\nshould not be thought of as hard-and-fast rules, but rather as guidelines\nthat will make it easier to write a compiler that will generate efficient and cor-\nrect code.\n\u25a0\nProvide regularity\u2014Whenever it makes sense, the three primary components\nof an instruction set\u2014the operations, the data types, and the addressing\nmodes\u2014should be orthogonal. Two aspects of an architecture are said to\nbe orthogonal if they are independent. For example, the operations and\naddressing modes are orthogonal if, for every operation to which one addres-\nsing mode can be applied, all addressing modes are applicable. This regular-\nity helps simplify code generation and is particularly important when the\ndecision about what code to generate is split into two passes in the compiler.\nA good counterexample of this property is restricting what registers can be\nused for a certain class of instructions. Compilers for special-purpose register\narchitectures typically get stuck in this dilemma. This restriction can result\nin the compiler finding itself with lots of available registers, but none of\nthe right kind!\n\u25a0\nProvide primitives, not solutions\u2014Special features that \u201cmatch\u201d a language\nconstruct or a kernel function are often unusable. Attempts to support high-\nlevel languages may work only with one language or do more or less than\nis required for a correct and efficient implementation of the language. An\nexample of how such attempts have failed is given in Section A.10.\n\u25a0\nSimplify trade-offs among alternatives\u2014One of the toughest jobs a compiler\nwriter has is figuring out what instruction sequence will be best for every seg-\nment of code that arises. In earlier days, instruction counts or total code size\nmight have been good metrics, but\u2014as we saw in Chapter 1\u2014this is no longer\nA-30\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 681,
        "text": "true. With caches and pipelining, the trade-offs have become very complex.\nAnything the designer can do to help the compiler writer understand the costs\nof alternative code sequences would help improve the code. One of the most\ndifficult instances of complex trade-offs occurs in a register-memory architec-\nture in deciding how many times a variable should be referenced before it is\ncheaper to load it into a register. This threshold is hard to compute and, in fact,\nmay vary among models of the same architecture.\n\u25a0\nProvide instructions that bind the quantities known at compile time as con-\nstants\u2014A compiler writer hates the thought of the processor interpreting at\nruntime a value that was known at compile time. Good counterexamples of this\nprinciple include instructions that interpret values that were fixed at compile\ntime. For instance, the VAX procedure call instruction (calls) dynamically\ninterprets a mask saying what registers to save on a call, but the mask is fixed at\ncompile time (see Section A.10).\nCompiler Support (or Lack Thereof) for Multimedia Instructions\nAlas, the designers of the SIMD instructions (see Section 4.3 in Chapter 4) basi-\ncally ignored the previous subsection. These instructions tend to be solutions, not\nprimitives; they are short of registers; and the data types do not match existing pro-\ngramming languages. Architects hoped to find an inexpensive solution that would\nhelp some users, but often only a few low-level graphics library routines use them.\nThe SIMD instructions are really an abbreviated version of an elegant architec-\nture style that has its own compiler technology. As explained in Section 4.2, vector\narchitectures operate on vectors of data. Invented originally for scientific codes,\nmultimedia kernels are often vectorizable as well, albeit often with shorter vectors.\nAs Section 4.3 suggests, we can think of Intel\u2019s MMX and SSE or PowerPC\u2019s Alti-\nVec, or the RISC-V P extension, as simply short vector computers: MMX with\nvectors of eight 8-bit elements, four 16-bit elements, or two 32-bit elements,\nand AltiVec with vectors twice that length. They are implemented as simply adja-\ncent, narrow elements in wide registers.\nThese microprocessor architectures build the vector register size into the archi-\ntecture: the sum of the sizes of the elements is limited to 64 bits for MMX and 128\nbits for AltiVec. When Intel decided to expand to 128-bit vectors, it added a whole\nnew set of instructions, called streaming SIMD extension (SSE).\nA major advantage of vector computers is hiding latency of memory access by\nloading many elements at once and then overlapping execution with data transfer.\nThe goal of vector addressing modes is to collect data scattered about memory,\nplace them in a compact form so that they can be operated on efficiently, and then\nplace the results back where they belong.\nVector computers include strided addressing and gather/scatter addressing\n(see Section 4.2) to increase the number of programs that can be vectorized. Strided\naddressing skips a fixed number of words between each access, so sequential\naddressing is often called unit stride addressing. Gather and scatter find their\nA.8\nCross-Cutting Issues: The Role of Compilers\n\u25a0\nA-31"
    },
    {
        "page": 682,
        "text": "addresses in another vector register: think of it as register indirect addressing for\nvector computers. From a vector perspective, in contrast, these short-vector SIMD\ncomputers support only unit strided accesses: memory accesses load or store all\nelements at once from a single wide memory location. Because the data for mul-\ntimedia applications are often streams that start and end in memory, strided and\ngather/scatter addressing modes are essential to successful vectorization (see\nSection 4.7).\nExample\nAs an example, compare a vector computer with MMX for color representation\nconversion of pixels from RGB (red, green, blue) to YUV (luminosity chromi-\nnance), with each pixel represented by 3 bytes. The conversion is just three lines\nof C code placed in a loop:\nY \u00bc(9798*R +19235*G +3736*B) / 32768;\nU \u00bc(-4784*R 9437*G +4221*B) / 32768 +128;\nV \u00bc(20218*R 16941*G 3277*B) / 32768 +128;\nA 64-bit-wide vector computer can calculate 8 pixels simultaneously. One vec-\ntor computer for media with strided addresses takes\n\u25a0\n3 vector loads (to get RGB)\n\u25a0\n3 vector multiplies (to convert R)\n\u25a0\n6 vector multiply adds (to convert G and B)\n\u25a0\n3 vector shifts (to divide by 32,768)\n\u25a0\n2 vector adds (to add 128)\n\u25a0\n3 vector stores (to store YUV)\nThe total is 20 instructions to perform the 20 operations in the previous C code to\nconvert 8 pixels (Kozyrakis, 2000). (Because a vector might have 32 64-bit ele-\nments, this code actually converts up to 328 or 256 pixels.)\nIn contrast, Intel\u2019s website shows that a library routine to perform the same\ncalculation on 8 pixels takes 116 MMX instructions plus 6 80x86 instructions\n(Intel, 2001). This six-fold increase in instructions is due to the large number of\ninstructions to load and unpack RGB pixels and to pack and store YUV pixels,\nbecause there are no strided memory accesses.\nHaving short, architecture-limited vectors with few registers and simple memory\naddressing modes makes it more difficult to use vectorizing compiler technology.\nHence, these SIMD instructions are more likely to be found in hand-coded libraries\nthan in compiled code.\nSummary: The Role of Compilers\nThis section leads to several recommendations. First, we expect a new instruction\nset architecture to have at least 16 general-purpose registers\u2014not counting\nA-32\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 683,
        "text": "separate registers for floating-point numbers\u2014to simplify allocation of registers\nusing graph coloring. The advice on orthogonality suggests that all supported\naddressing modes apply to all instructions that transfer data. Finally, the last three\npieces of advice\u2014provide primitives instead of solutions, simplify trade-offs\nbetween alternatives, don\u2019t bind constants at runtime\u2014all suggest that it is better\nto err on the side of simplicity. In other words, understand that less is more in the\ndesign of an instruction set. Alas, SIMD extensions are more an example of good\nmarketing than of outstanding achievement of hardware\u2013software co-design.\nA.9\nPutting It All Together: The RISC-V Architecture\nIn this section we describe the load-store architecture called RISC-V. RISC-V is a\nfreely licensed open standard, similar to many of the RISC architectures, and based\non observations similar to those covered in the last sections. (In Section M.3 we\ndiscuss how and why these architectures became popular.) RISC-V builds on\n30 years of experience with RISC architectures and \u201ccleans up\u201d most of the\nshort-term inclusions and omissions, leading to an architecture that is easier\nand more efficient to implement. RISC-V provides a both a 32-bit and a 64-bit\ninstruction set, as well as a variety of extensions for features like floating point;\nthese extensions can be added to either the 32-bit or 64-bit base instruction set.\nWe discuss a 64-bit version of RISC-V, RV64, which is a superset of the 32-bit\nversion RV32.\nReviewing our expectations from each section, for desktop and server\napplications:\n\u25a0\nSection A.2\u2014Use general-purpose registers with a load-store architecture.\n\u25a0\nSection A.3\u2014Support these addressing modes: displacement (with an address\noffset size of 12\u201316 bits), immediate (size 8\u201316 bits), and register indirect.\n\u25a0\nSection A.4\u2014Support these data sizes and types: 8-, 16-, 32-, and 64-bit inte-\ngers and 64-bit IEEE 754 floating-point numbers.\n\u25a0\nSection A.5\u2014Support these simple instructions, because they will dominate\nthe number of instructions executed: load, store, add, subtract, move\nregister-register, and shift.\n\u25a0\nSection A.6\u2014Compare equal, compare not equal, compare less, branch (with a\nPC-relative address at least 8 bits long), jump, call, and return.\n\u25a0\nSection A.7\u2014Use fixed instruction encoding if interested in performance,\nand use variable instruction encoding if interested in code size. In some\nlow-end, embedded applications, with small or only one-level caches, larger\ncode size may have significant performance implications. ISAs that provide\na compressed instruction set extension provide a way of addressing this\ndifference.\n\u25a0\nSection A.8\u2014Provide at least 16, and preferably 32, general-purpose registers,\nbe sure all addressing modes apply to all data transfer instructions, and aim for\nA.9\nPutting It All Together: The RISC-V Architecture\n\u25a0\nA-33"
    },
    {
        "page": 684,
        "text": "a minimalist instruction set. This section didn\u2019t cover floating-point programs,\nbut they often use separate floating-point registers. The justification is to\nincrease the total number of registers without raising problems in the instruc-\ntion format or in the speed of the general-purpose register file. This compro-\nmise, however, is not orthogonal.\nWe introduce RISC-V by showing how it follows these recommendations. Like its\nRISC predecessors, RISC-V emphasizes\n\u25a0\nA simple load-store instruction set.\n\u25a0\nDesign for pipelining efficiency (discussed in Appendix C), including a fixed\ninstruction set encoding.\n\u25a0\nEfficiency as a compiler target.\nRISC-V provides a good architectural model for study, not only because of the pop-\nularity of this type of processor, but also because it is an easy architecture to under-\nstand. We will use this architecture again in Appendix C and in Chapter 3, and it\nforms the basis for a number of exercises and programming projects.\nRISC-V Instruction Set Organization\nThe RISC-V instruction set is organized as three base instruction sets that support\n32-bitor64-bitintegers,andavarietyofoptionalextensionstooneofthebaseinstruc-\ntion sets. This allows RISC-V to be implemented for a wide range of potential appli-\ncationsfromasmallembeddedprocessorwithaminimalbudgetforlogicandmemory\nthat likely costs $1 or less, to high-end processor configurations with full support for\nfloating point, vectors, and multiprocessor configurations. Figure A.22 summarizes\nthe three base instruction sets and the instruction set extensions with their basic func-\ntionality. For purposes of this text, we use RV64IMAFD (also known as RV64G, for\nshort) in examples. RV32G is the 32-bit subset of the 64-bit architecture RV64G.\nRegisters for RISC-V\nRV64G has 32 64-bit general-purpose registers (GPRs), named x0, x1, \u2026 , x31.\nGPRs are also sometimes known as integer registers. Additionally, with the F and\nD extensions for floating point that are part of RV64G, come a set of 32 floating-\npoint registers (FPRs), named f0, f1, \u2026 , f31, which can hold 32 single-precision\n(32-bit) values or 32 double-precision (64-bit) values. (When holding one single-\nprecision number, the other half of the FPR is unused.) Both single- and double-\nprecision floating-point operations (32-bit and 64-bit) are provided.\nThe value of x0 is always 0. We shall see later how we can use this register to\nsynthesize a variety of useful operations from a simple instruction set.\nA few special registers can be transferred to and from the general-purpose reg-\nisters. An example is the floating-point status register, used to hold information\nA-34\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 685,
        "text": "about the results of floating-point operations. There are also instructions for mov-\ning between an FPR and a GPR.\nData Types for RISC-V\nThe data types are 8-bit bytes, 16-bit half words, 32-bit words, and 64-bit double-\nwords for integer data and 32-bit single precision and 64-bit double precision for\nfloating point. Half words were added because they are found in languages like C\nName of base\nor extension\nFunctionality\nRV32I\nBase 32-bit integer instruction set with 32 registers\nRV32E\nBase 32-bit instruction set but with only 16 registers; intended for\nvery low-end embedded applications\nRV64I\nBase 64-bit instruction set; all registers are 64-bits, and instructions\nto move 64-bit from/to the registers (LD and SD) are added\nM\nAdds integer multiply and divide instructions\nA\nAdds atomic instructions needed for concurrent processing; see\nChapter 5\nF\nAdds single precision (32-bit) IEEE floating point, includes 32 32-\nbit floating point registers, instructions to load and store those\nregisters and operate on them\nD\nExtends floating point to double precision, 64-bit, making the\nregisters 64-bits, adding instructions to load, store, and operate on\nthe registers\nQ\nFurther extends floating point to add support for quad precision,\nadding 128-bit operations\nL\nAdds support for 64- and 128-bit decimal floating point for the\nIEEE standard\nC\nDefines a compressed version of the instruction set intended for\nsmall-memory-sized embedded applications. Defines 16-bit\nversions of common RV32I instructions\nV\nA future extension to support vector operations (see Chapter 4)\nB\nA future extension to support operations on bit fields\nT\nA future extension to support transactional memory\nP\nAn extension to support packed SIMD instructions: see Chapter 4\nRV128I\nA future base instruction set providing a 128-bit address space\nFigure A.22 RISC-V has three base instructions sets (and a reserved spot for a future\nfourth); all the extensions extend one of the base instruction sets. An instruction set is\nthus named by the base name followed by the extensions. For example, RISC-V64IMAFD\nrefers to the base 64-bit instruction set with extensions M, A, F, and D. For consistency of\nnaming and software, this combination is given the abbreviated name: RV64G, and we\nuse RV64G through most of this text.\nA.9\nPutting It All Together: The RISC-V Architecture\n\u25a0\nA-35"
    },
    {
        "page": 686,
        "text": "and are popular in some programs, such as the operating systems, concerned about\nsize of data structures. They will also become more popular if Unicode becomes\nwidely used.\nThe RV64G operations work on 64-bit integers and 32- or 64-bit floating point.\nBytes, half words, and words are loaded into the general-purpose registers with\neither zeros or the sign bit replicated to fill the 64 bits of the GPRs. Once loaded,\nthey are operated on with the 64-bit integer operations.\nAddressing Modes for RISC-V Data Transfers\nThe only data addressing modes are immediate and displacement, both with 12-bit\nfields. Register indirect is accomplished simply by placing 0 in the 12-bit displace-\nment field, and limited absolute addressing with a 12-bit field is accomplished by\nusing register 0 as the base register. Embracing zero gives us four effective modes,\nalthough only two are supported in the architecture.\nRV64G memory is byte addressable with a 64-bit address and uses Little End-\nian byte numbering. As it is a load-store architecture, all references between mem-\nory and either GPRs or FPRs are through loads or stores. Supporting the data types\nmentioned herein, memory accesses involving GPRs can be to a byte, half word,\nword, or double word. The FPRs may be loaded and stored with single-precision or\ndouble-precision numbers. Memory accesses need not be aligned; however, it may\nbe that unaligned accesses run extremely slow. In practice, programmers and com-\npilers would be stupid to use unaligned accesses.\nRISC-V Instruction Format\nBecause RISC-V has just two addressing modes, these can be encoded into the\nopcode. Following the advice on making the processor easy to pipeline and\ndecode, all instructions are 32 bits with a 7-bit primary opcode. Figure A.23 shows\nthe instruction layout of the four major instruction types. These formats are simple\nwhile providing 12-bit fields for displacement addressing, immediate constants, or\nPC-relative branch addresses.\nR-type\nfunct7\n31\n25  24\n20  19\n15  14\n12  11\n7  6\n0\nfunct3\nrs2\nrs1\nrd\nopcode\nI-type\nfunct3\nrs1\nrd\nopcode\nS-type\nimm[11:5]\nimm[11:0]\nfunct3\nrs2\nrs1\nimm[4:0]\nopcode\nU-type\nimm[31:12]\nrd\nopcode\nFigure A.23 The RISC-V instruction layout. There are two variations on these formata,\ncalled the SB and UJ formats; they deal with a slightly different treatment for immediate\nfields.\nA-36\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 687,
        "text": "The instruction formats and the use of the instruction fields is described in\nFigure A.24. The opcode specifies the general instruction type (ALU instruction,\nALU immediate, load, store, branch, or jump), while the funct fields are used for\nspecific operations. For example, an ALU instruction is encoded with a single\nopcode with the funct field dictating the exact operation: add, subtract, and, etc.\nNotice that several formats encode multiple types of instructions, including the\nuse of the I-format for both ALU immediates and loads, and the use of the S-format\nfor stores and conditional branches.\nRISC-V Operations\nRISC-V (or more properly RV64G) supports the list of simple operations\nrecommended herein plus a few others. There are four broad classes of\ninstructions: loads and stores, ALU operations, branches and jumps, and\nfloating-point operations.\nAny of the general-purpose or floating-point registers may be loaded or stored,\nexcept that loading x0 has no effect. Figure A.25 gives examples of the load and\nstore instructions. Single-precision floating-point numbers occupy half a floating-\npoint register. Conversions between single and double precision must be done\nexplicitly. The floating-point format is IEEE 754 (see Appendix J). A list of all\nthe RV64G instructions appears in Figure A.28 (page A.42).\nInstruction\nformat\nPrimary use\nrd\nrs1\nrs2\nImmediate\nR-type\nRegister-register\nALU instructions\nDestination\nFirst source\nSecond source\nI-type\nALU immediates\nLoad\nDestination\nFirst source base\nregister\nValue\ndisplacement\nS-type\nStore\nCompare and\nbranch\nBase register first\nsource\nData source to\nstore second\nsource\nDisplacement\noffset\nU-type\nJump and link\nJump and link\nregister\nRegister\ndestination for\nreturn PC\nTarget address for\njump and link\nregister\nTarget address\nfor jump and link\nFigure A.24 The use of instruction fields for each instruction type. Primary use shows the major instructions that\nuse the format. A blank indicates that the corresponding field is not present in this instruction type. The I-format is\nused for both loads and ALU immediates, with the 12-bit immediate holding either the value for an immediate or the\ndisplacement for a load. Similarly, the S-format encodes both store instructions (where the first source register is the\nbase register and the second contains the register source for the value to store) and compare and branch instructions\n(where the register fields contain the sources to compare and the immediate field specifies the offset of the branch\ntarget). There are actually two other formats: SB and UJ that follow the same basic organization as S and J, but slightly\nmodify the interpretation of the immediate fields.\nA.9\nPutting It All Together: The RISC-V Architecture\n\u25a0\nA-37"
    },
    {
        "page": 688,
        "text": "To understand these figures we need to introduce a few additional extensions to\nour C description language used initially on page A-9:\n\u25a0\nA subscript is appended to the symbol whenever the length of the datum\nbeing transferred might not be clear. Thus,  n means transfer an n-bit quantity.\nWe use x, y z to indicate that z should be transferred to x and y.\n\u25a0\nA subscript is used to indicate selection of a bit from a field. Bits are labeled\nfrom the most-significant bit starting at 0. The subscript may be a single digit\n(e.g., Regs[x4]0 yields the sign bit of x4) or a subrange (e.g., Regs\n[x3]56..63 yields the least-significant byte of x3).\n\u25a0\nThe variable Mem, used as an array that stands for main memory, is indexed by\na byte address and may transfer any number of bytes.\n\u25a0\nA superscript is used to replicate a field (e.g., 048 yields a field of zeros of\nlength 48 bits).\n\u25a0\nThe symbol ## is used to concatenate two fields and may appear on either side\nof a data transfer, and the symbols \u226aand \u226bshift the first operand left or right\nby the amount of the second operand.\nExample instruction\nInstruction name\nMeaning\nld\nx1,80(x2)\nLoad doubleword\nRegs[x1] Mem[80+Regs[x2]]\nlw\nx1,60(x2)\nLoad word\nRegs[x1] 64 Mem[60+Regs[x2]]0)32 ##\nMem[60+Regs[x2]]\nlwu x1,60(x2)\nLoad word unsigned\nRegs[x1] 64 032 ## Mem[60+Regs[x2]]\nlb\nx1,40(x3)\nLoad byte\nRegs[x1] 64 (Mem[40+Regs[x3]]0)56 ##\nMem[40+Regs[x3]]\nlbu x1,40(x3)\nLoad byte unsigned\nRegs[x1] 64 056 ## Mem[40+Regs[x3]]\nlh\nx1,40(x3)\nLoad half word\nRegs[x1] 64 (Mem[40+Regs[x3]]0)48 ##\nMem[40+Regs[x3]]\nflw f0,50(x3)\nLoad FP single\nRegs[f0] 64 Mem[50+Regs[x3]] ## 032\nfld f0,50(x2)\nLoad FP double\nRegs[f0] 64 Mem[50+Regs[x2]]\nsd\nx2,400(x3)\nStore double\nMem[400+Regs[x3]] 64 Regs[x2]\nsw\nx3,500(x4)\nStore word\nMem[500+Regs[x4]] 32 Regs[x3]32..63\nfsw f0,40(x3)\nStore FP single\nMem[40+Regs[x3]] 32 Regs[f0]0..31\nfsd f0,40(x3)\nStore FP double\nMem[40+Regs[x3]] 64 Regs[f0]\nsh\nx3,502(x2)\nStore half\nMem[502+Regs[x2]] 16 Regs[x3]48..63\nsb\nx2,41(x3)\nStore byte\nMem[41+Regs[x3]] 8 Regs[x2]56..63\nFigure A.25 The load and store instructions in RISC-V. Loads shorter than 64 bits are available in both sign-\nextended and zero-extended forms. All memory references use a single addressing mode. Of course, both loads\nand stores are available for all the data types shown. Because RV64G supports double precision floating point, all\nsingle precision floating point loads must be aligned in the FP register, which are 64-bits wide.\nA-38\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 689,
        "text": "As an example, assuming that x8 and x10 are 32-bit registers:\nRegs[x10] 64(Mem[Regs[x8]]0)32## Mem[Regs[R8]]\nmeans that the word at the memory location addressed by the contents of register\nx8 is sign-extended to form a 64-bit quantity that is stored into register x10.\nAll ALU instructions are register-register instructions. Figure A.26 gives some\nexamples of the arithmetic/logical instructions. The operations include simple\narithmetic and logical operations: add, subtract, AND, OR, XOR, and shifts. Immediate\nforms of all these instructions are provided using a 12-bit sign-extended immedi-\nate. The operation LUI (load upper immediate) loads bits 12\u201331 of a register, sign-\nextends the immediate field to the upper 32-bits, and sets the low-order 12-bits of\nthe register to 0. LUI allows a 32-bit constant to be built in two instructions, or a\ndata transfer using any constant 32-bit address in one extra instruction.\nAs mentioned herein, x0 is used to synthesize popular operations. Loading\na constant is simply an add immediate where the source operand is x0, and a\nregister-register move is simply an add (or an or) where one of the sources is\nx0. (We sometimes use the mnemonic li, standing for load immediate, to represent\nthe former, and the mnemonic mv for the latter.)\nRISC-V Control Flow Instructions\nControl is handled through a set of jumps and a set of branches, and Figure A.27\ngives some typical branch and jump instructions. The two jump instructions (jump\nand link and jump and link register) are unconditional transfers and always store\nthe \u201clink,\u201d which is the address of the instruction sequentially following the jump\ninstruction, in the register specified by the rd field. In the event that the link address\nis not needed, the rd field can simply be set to x0, which results in a typical uncon-\nditional jump. The two jump instructions are differentiated by whether the address\nis computed by adding an immediate field to the PC or by adding the immediate\nExample\ninstrucmtion\nInstruction name\nMeaning\nadd\nx1,x2,x3\nAdd\nRegs[x1] Regs[x2]+Regs[x3]\naddi x1,x2,3\nAdd immediate\nunsigned\nRegs[x1] Regs[x2]+3\nlui\nx1,42\nLoad upper\nimmediate\nRegs[x1] 032##42##012\nsll\nx1,x2,5\nShift left logical\nRegs[x1] Regs[x2]<<5\nslt\nx1,x2,x3\nSet less than\nif (Regs[x2]<Regs[x3])\nRegs[x1] 1 else Regs[x1] 0\nFigure A.26 The basic ALU instructions in RISC-V are available both with register-\nregister operands and with one immediate operand. LUI uses the U-format that\nemploys the rs1 field as part of the immediate, yielding a 20-bit immediate.\nA.9\nPutting It All Together: The RISC-V Architecture\n\u25a0\nA-39"
    },
    {
        "page": 690,
        "text": "field to the contents of a register. The offset is interpreted as a half word offset for\ncompatibility with the compressed instruction set, R64C, which includes 16-bit\ninstructions.\nAll branches are conditional. The branch condition is specified by the instruc-\ntion, and any arithmetic comparison (equal, greater than, less than, and their\ninverses) is permitted. The branch-target address is specified with a 12-bit signed\noffset that is shifted left one place (to get 16-bit alignment) and then added to the\ncurrent program counter. Branches based on the contents of the floating point\nregisters are implemented by executing a floating point comparison (e.g., feq.d\nor fle.d), which sets an integer register to 0 or 1 based on the comparison, and then\nexecuting a beq or bne with x0 as an operand.\nThe observant reader will have noticed that there are very few 64-bit only\ninstructions in RV64G. Primarily, these are the 64-bit loads and stores and versions\nof 32-bit, 16-bit, and 8-bit loads that do not sign extend (the default is to sign-extend).\nTo support 32-bit modular arithmetic without additional instructions, there are ver-\nsions of the instructions that ignore the upper 32 bits of a 64-bit register, such as add\nand subtract word (addw, subw). Amazingly, everything else just works.\nRISC-V Floating-Point Operations\nFloating-point instructions manipulate the floating-point registers and indicate\nwhether the operation to be performed is single or double precision. The\nfloating-point operations are add, subtract, multiply, divide, square root, as well\nas fused multiply-add and multiply-subtract. All floating point instructions begin\nwith the letter f and use the suffix d for double precision and s for single precision\n(e.g., fadd.d, fadd.s, fmul.d, fmul.s, fmadd.d fmadd.s). Floating-\npoint compares set an integer register based on the comparison, similarly to the\ninteger instruction set-less-than and set-great-than.\nIn addition to floating-point loads and stores (flw, fsw, fld, fsd), instruc-\ntionsareprovidedforconvertingbetweendifferentFPprecisions,formovingbetween\ninteger and FP registers (fmv), and for converting between floating point and integer\n(fcvt, which uses the integer registers for source or destination as appropriate).\nFigure A.28 contains a list of nearly all the RV64G instructions and a summary\nof their meaning.\nExample instruction\nInstruction name\nMeaning\njal\nx1,offset\nJump and link\nRegs[x1] PC+4; PC PC + (offset<<1)\njalr x1,x2,offset\nJump and link register\nRegs[x1] PC+4; PC Regs[x2]+offset\nbeq\nx3,x4,offset\nBranch equal zero\nif (Regs[x3]==Regs[x4]) PC PC + (offset<<1)\nbgt\nx3,x4,name\nBranch not equal zero\nif (Regs[x3]>Regs[x4]) PC PC + (offset<<1)\nFigure A.27 Typical control flow instructions in RISC-V. All control instructions, except jumps to an address in a\nregister, are PC-relative.\nA-40\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 691,
        "text": "Instruction type/opcode\nInstruction meaning\nData transfers\nMove data between registers and memory, or between the integer and FP;\nonly memory address mode is 12-bit displacement+contents of a GPR\nlb, lbu, sb\nLoad byte, load byte unsigned, store byte (to/from integer registers)\nlh, lhu, sh\nLoad half word, load half word unsigned, store half word (to/from integer\nregisters)\nlw, lwu, sw\nLoad word, store word (to/from integer registers)\nld, sd\nLoad doubleword, store doubleword\nArithmetic/logical\nOperations on data in GPRs. Word versions ignore upper 32 bits\nadd, addi, addw, addiw, sub,\nsubi, subw, subiw\nAdd and subtract, with both word and immediate versions\nslt, sltu, slti, sltiu\nset-less-than with signed and unsigned, and immediate\nand, or, xor, andi, ori, xori\nand, or, xor, both register-register and register-immediate\nlui\nLoad upper immediate: loads bits 31..12 of a register with the immediate\nvalue. Upper 32 bits are set to 0\nauipc\nSums an immediate and the upper 20-bits of the PC into a register; used for\nbuilding a branch to any 32-bit address\nsll, srl, sra, slli, srli,\nsrai, sllw,slliw, srli,\nsrliw, srai, sraiw\nShifts: logical shift left and right and arithmetic shift right, both immediate\nand word versions (word versions leave the upper 32 bit untouched)\nmul, mulw, mulh, mulhsu,\nmulhu, div,divw, divu, rem,\nremu, remw, remuw\nInteger multiply, divide, and remainder, signed and unsigned with support for\n64-bit products in two instructions. Also word versions\nControl\nConditional branches and jumps; PC-relative or through register\nbeq, bne, blt, bge, bltu, bgeu\nBranch based on compare of two registers, equal, not equal, less than, greater\nor equal, signed and unsigned\njal,jalr\nJump and link address relative to a register or the PC\nFloating point\nAll FP operation appear in double precision (.d) and single (.s)\nflw, fld, fsw, fsd\nLoad, store, word (single precision), doubleword (double precision)\nfadd, fsub, fmult, fiv, fsqrt,\nfmadd, fmsub, fnmadd, fnmsub,\nfmin, fmax, fsgn, fsgnj, fsjnx\nAdd, subtract, multiply, divide, square root, multiply-add, multiply-subtract,\nnegate multiply-add, negate multiply-subtract, maximum, minimum, and\ninstructions to replace the sign bit. For single precision, the opcode is\nfollowed by: .s, for double precision: .d. Thus fadd.s, fadd.d\nfeq, flt, fle\nCompare two floating point registers; result is 0 or 1 stored into a GPR\nfmv.x.*, fmv.*.x\nMove between the FP register abd GPR, \u201c*\u201d is s or d\nfcvt.*.l, fcvt.l.*, fcvt.*.\nlu, fcvt.lu.*, fcvt.*.w, fcvt.\nw.*, fcvt.*.wu, fcvt.wu.*\nConverts between a FP register and integer register, where \u201c*\u201d is S or D for\nsingle or double precision. Signed and unsigned versions and word,\ndoubleword versions\nFigure A.28 A list of the vast majority of instructions in RV64G. This list can also be found on the back inside cover.\nThis table omits system instructions, synchronization and atomic instructions, configuration instructions, instructions\nto reset and access performance counters, about 10 instructions in total.\nA.9\nPutting It All Together: The RISC-V Architecture\n\u25a0\nA-41"
    },
    {
        "page": 692,
        "text": "RISC-V Instruction Set Usage\nTo give an idea of which instructions are popular, Figure A.29 shows the frequency\nof instructions and instruction classes for the SPECint2006 programs, using\nRV32G.\nA.10\nFallacies and Pitfalls\nArchitects have repeatedly tripped on common, but erroneous, beliefs. In this sec-\ntion we look at a few of them.\nPitfall\nDesigning a \u201chigh-level\u201d instruction set feature specifically oriented to supporting\na high-level language structure.\nAttempts to incorporate high-level language features in the instruction set have led\narchitects to provide powerful instructions with a wide range of flexibility. How-\never, often these instructions do more work than is required in the frequent case, or\nthey don\u2019t exactly match the requirements of some languages. Many such efforts\nhave been aimed at eliminating what in the 1970s was called the semantic gap.\nAlthough the idea is to supplement the instruction set with additions that bring\nProgram\nLoads\nStores\nBranches\nJumps\nALU operations\nastar\n28%\n6%\n18%\n2%\n46%\nbzip\n20%\n7%\n11%\n1%\n54%\ngcc\n17%\n23%\n20%\n4%\n36%\ngobmk\n21%\n12%\n14%\n2%\n50%\nh264ref\n33%\n14%\n5%\n2%\n45%\nhmmer\n28%\n9%\n17%\n0%\n46%\nlibquantum\n16%\n6%\n29%\n0%\n48%\nmcf\n35%\n11%\n24%\n1%\n29%\nomnetpp\n23%\n15%\n17%\n7%\n31%\nperlbench\n25%\n14%\n15%\n7%\n39%\nsjeng\n19%\n7%\n15%\n3%\n56%\nxalancbmk\n30%\n8%\n27%\n3%\n31%\nFigure A.29 RISC-V dynamic instruction mix for the SPECint2006 programs. Omnetpp includes 7% of the instruc-\ntions that are floating point loads, stores, operations, or compares; no other program includes even 1% of other\ninstruction types. A change in gcc in SPECint2006, creates an anomaly in behavior. Typical integer programs have\nload frequencies that are 1/5 to 3x the store frequency. In gcc, the store frequency is actually higher than the load\nfrequency! This arises because a large fraction of the execution time is spent in a loop that clears memory by storing\nx0 (not where a compiler like gcc would usually spend most of its execution time!). A store instruction that stores a\nregister pair, which some other RISC ISAs have included, would address this issue.\nA-42\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 693,
        "text": "the hardware up to the level of the language, the additions can generate what Wulf\net al. (1981) have called a semantic clash:\n\u2026 by giving too much semantic content to the instruction, the computer\ndesigner made it possible to use the instruction only in limited contexts. [p. 43]\nMore often the instructions are simply overkill\u2014they are too general for the most\nfrequent case, resulting in unneeded work and a slower instruction. Again, the\nVAX CALLS is a good example. CALLS uses a callee save strategy (the registers\nto be saved are specified by the callee), but the saving is done by the call instruction\nin the caller. The CALLS instruction begins with the arguments pushed on the\nstack, and then takes the following steps:\n1. Align the stack if needed.\n2. Push the argument count on the stack.\n3. Save the registers indicated by the procedure call mask on the stack (as men-\ntioned in Section A.8). The mask is kept in the called procedure\u2019s code\u2014this\npermits the callee to specify the registers to be saved by the caller even with\nseparate compilation.\n4. Push the return address on the stack, and then push the top and base of stack\npointers (for the activation record).\n5. Clear the condition codes, which sets the trap enable to a known state.\n6. Push a word for status information and a zero word on the stack.\n7. Update the two stack pointers.\n8. Branch to the first instruction of the procedure.\nThe vast majority of calls in real programs do not require this amount of over-\nhead. Most procedures know their argument counts, and a much faster linkage\nconvention can be established using registers to pass arguments rather than the\nstack in memory. Furthermore, the CALLS instruction forces two registers to\nbe used for linkage, while many languages require only one linkage register.\nMany attempts to support procedure call and activation stack management have\nfailed to be useful, either because they do not match the language needs or\nbecause they are too general and hence too expensive to use.\nThe VAX designers provided a simpler instruction, JSB, that is much faster\nbecause it only pushes the return PC on the stack and jumps to the procedure. How-\never, most VAX compilers use the more costly CALLS instructions. The call\ninstructions were included in the architecture to standardize the procedure linkage\nconvention. Other computers have standardized their calling convention by agree-\nment among compiler writers and without requiring the overhead of a complex,\nvery general procedure call instruction.\nFallacy\nThere is such a thing as a typical program.\nA.10\nFallacies and Pitfalls\n\u25a0\nA-43"
    },
    {
        "page": 694,
        "text": "Many people would like to believe that there is a single \u201ctypical\u201d program that\ncould be used to design an optimal instruction set. For example, see the synthetic\nbenchmarks discussed in Chapter 1. The data in this appendix clearly show that\nprograms can vary significantly in how they use an instruction set. For example,\nFigure A.30 shows the mix of data transfer sizes for four of the SPEC2000 pro-\ngrams: It would be hard to say what is typical from these four programs. The var-\niations are even larger on an instruction set that supports a class of applications,\nsuch as decimal instructions, that are unused by other applications.\nPitfall\nInnovating at the instruction set architecture to reduce code size without account-\ning for the compiler.\nFigure A.31 shows the relative code sizes for four compilers for the MIPS instruction\nset. Whereas architects struggle to reduce code size by 30%\u201340%, different compiler\nstrategies can change code size by much larger factors. Similar to performance opti-\nmization techniques, the architect should start with the tightest code the compilers\ncan produce before proposing hardware innovations to save space.\nFallacy\nAn architecture with flaws cannot be successful.\nThe 80x86 provides a dramatic example: the instruction set architecture is one only\nits creators could love (see Appendix K). Succeeding generations of Intel engineers\nhave tried to correct unpopular architectural decisions made in designing the\n80x86. For example, the 80x86 supports segmentation, whereas all others picked\npaging; it uses extended accumulators for integer data, but other processors use\ngeneral-purpose registers; and it uses a stack for floating-point data, when every-\none else abandoned execution stacks long before.\n0%\n20%\n40%\n60%\n80%\n100%\nByte\n(8 bits)\nHalf word\n(16 bits)\nWord\n(32 bits)\nDouble word\n(64 bits)\n18%\n22%\n0%\n0%\n3%\n19%\n0%\n0%\n18%\n28%\n6%\n40%\n62%\n31%\n94%\n60%\napplu\nequake\ngzip\nperl\nFigure A.30 Data reference size of four programs from SPEC2000. Although you can\ncalculate an average size, it would be hard to claim the average is typical of programs.\nA-44\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 695,
        "text": "Despite these major difficulties, the 80x86 architecture has been enormously\nsuccessful. The reasons are threefold: first, its selection as the microprocessor in\nthe initial IBM PC makes 80x86 binary compatibility extremely valuable. Second,\nMoore\u2019s Law provided sufficient resources for 80x86 microprocessors to translate\nto an internal RISC instruction set and then execute RISC-like instructions. This\nmix enables binary compatibility with the valuable PC software base and perfor-\nmance on par with RISC processors. Third, the very high volumes of PC micro-\nprocessors mean Intel can easily pay for the increased design cost of hardware\ntranslation. In addition, the high volumes allow the manufacturer to go up the\nlearning curve, which lowers the cost of the product.\nThe larger die size and increased power for translation may be a liability for\nembedded applications, but it makes tremendous economic sense for the desktop.\nAnd its cost-performance in the desktop also makes it attractive for servers, with its\nmain weakness for servers being 32-bit addresses, which was resolved with a 64-\nbit address extension.\nFallacy\nYou can design a flawless architecture.\nAll architecture design involves trade-offs made in the context of a set of hardware\nand software technologies. Over time those technologies are likely to change, and\ndecisions that may have been correct at the time they were made look like mistakes.\nFor example, in 1975 the VAX designers overemphasized the importance of code\nsize efficiency, underestimating how important ease of decoding and pipelining\nwould be five years later. An example in the RISC camp is delayed branch (see\nAppendix K). It was a simple matter to control pipeline hazards with five-stage\npipelines, but a challenge for processors with longer pipelines that issue multiple\ninstructions per clock cycle. In addition, almost all architectures eventually suc-\ncumb to the lack of sufficient address space. This is one reason that RISC-V\nCompiler\nApogee software\nversion 4.1\nGreen Hills\nMulti2000\nVersion 2.0\nAlgorithmics\nSDE4.0B\nIDT/c 7.2.1\nArchitecture\nMIPS IV\nMIPS IV\nMIPS 32\nMIPS 32\nProcessor\nNEC VR5432\nNEC VR5000\nIDT 32334\nIDT 79RC32364\nAutocorrelation kernel\n1.0\n2.1\n1.1\n2.7\nConvolutional encoder kernel\n1.0\n1.9\n1.2\n2.4\nFixed-point bit allocation kernel\n1.0\n2.0\n1.2\n2.3\nFixed-point complex FFT kernel\n1.0\n1.1\n2.7\n1.8\nViterbi GSM decoder kernel\n1.0\n1.7\n0.8\n1.1\nGeometric mean of five kernels\n1.0\n1.7\n1.4\n2.0\nFigure A.31 Code size relative to Apogee Software Version 4.1 C compiler for Telecom application of EEMBC\nbenchmarks. The instruction set architectures are virtually identical, yet the code sizes vary by factors of 2. These\nresults were reported February\u2013June 2000.\nA.10\nFallacies and Pitfalls\n\u25a0\nA-45"
    },
    {
        "page": 696,
        "text": "has planned for the possibility of 128-bit addresses, although it may be decades\nbefore such capability is needed.\nIn general, avoiding such flaws in the long run would probably mean\ncompromising the efficiency of the architecture in the short run, which is danger-\nous, since a new instruction set architecture must struggle to survive its first\nfew years.\nA.11\nConcluding Remarks\nThe earliest architectures were limited in their instruction sets by the hardware\ntechnology of that time. As soon as the hardware technology permitted, computer\narchitects began looking for ways to support high-level languages. This search led\nto three distinct periods of thought about how to support programs efficiently. In\nthe 1960s, stack architectures became popular. They were viewed as being a good\nmatch for high-level languages\u2014and they probably were, given the compiler tech-\nnology of the day. In the 1970s, the main concern of architects was how to reduce\nsoftware costs. This concern was met primarily by replacing software with hard-\nware, or by providing high-level architectures that could simplify the task of soft-\nware designers. The result was both the high-level language computer architecture\nmovement and powerful architectures like the VAX, which has a large number of\naddressing modes, multiple data types, and a highly orthogonal architecture. In the\n1980s, more sophisticated compiler technology and a renewed emphasis on pro-\ncessor performance saw a return to simpler architectures, based mainly on the\nload-store style of computer.\nThe following instruction set architecture changes occurred in the 1990s:\n\u25a0\nAddress size doubles\u2014The 32-bit address instruction sets for most desktop and\nserver processors were extended to 64-bit addresses, expanding the width of\nthe registers (among other things) to 64 bits. Appendix K gives three examples\nof architectures that have gone from 32 bits to 64 bits.\n\u25a0\nOptimization of conditional branches via conditional execution\u2014In Chapter 3,\nwe see that conditional branches can limit the performance of aggressive com-\nputer designs. Hence, there was interest in replacing conditional branches with\nconditional completion of operations, such as conditional move (see Appendix\nH), which was added to most instruction sets.\n\u25a0\nOptimization of cache performance via prefetch\u2014Chapter 2 explains the\nincreasing role of memory hierarchy in the performance of computers, with\na cache miss on some computers taking as many instruction times as page faults\ntook on earlier computers. Hence, prefetch instructions were added to try to\nhide the cost of cache misses by prefetching (see Chapter 2).\n\u25a0\nSupport for multimedia\u2014Most desktop and embedded instruction sets were\nextended with support for multimedia applications.\nA-46\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 697,
        "text": "\u25a0\nFaster floating-point operations\u2014Appendix J describes operations added to\nenhance floating-point performance, such as operations that perform a multiply\nand an add and paired single execution, which are part of RISC-V.\nBetween1970 and1985manythoughtthe primary jobof thecomputerarchitectwas\nthe design of instruction sets. As a result, textbooks of that era emphasize instruction\nset design, much as computer architecture textbooks of the 1950s and 1960s empha-\nsized computerarithmetic. Theeducatedarchitect was expected tohave strongopin-\nions about the strengths and especially the weaknesses of the popular computers.\nThe importance of binary compatibility in quashing innovations in instruction set\ndesign was unappreciated by many researchers and textbook writers, giving the\nimpression that many architects would get a chance to design an instruction set.\nThe definition of computer architecture today has been expanded to include\ndesign and evaluation of the full computer system\u2014not just the definition of\nthe instruction set and not just the processor\u2014and hence there are plenty of topics\nfor the architect to study. In fact, the material in this appendix was a central point of\nthe book in its first edition in 1990, but now is included in an appendix primarily as\nreference material!\nAppendix K may satisfy readers interested in instruction set architecture; it\ndescribes a variety of instruction sets, which are either important in the marketplace\ntoday or historically important, and it compares nine popular load-store computers\nwith RISC-V.\nA.12\nHistorical Perspective and References\nSection M.4 (available online) features a discussion on the evolution of instruction\nsets and includes references for further reading and exploration of related topics.\nExercises by Gregory D. Peterson\nA.1\n[10]<A.9>Compute the effective CPI for an implementation of an embedded\nRISC-V CPU using Figure A.29. Assume we have made the following measure-\nments of average CPI for each of the instruction types:\nInstruction\nClock cycles\nAll ALU operations\n1.0\nLoads\n5.0\nStores\n3.0\nBranches\nTaken\n5.0\nNot taken\n3.0\nJumps\n3.0\nAverage the instruction frequencies of astar and gcc to obtain the\ninstruction mix.\nExercises by Gregory D. Peterson\n\u25a0\nA-47"
    },
    {
        "page": 698,
        "text": "A.2\n[10]<A.9>Compute the effective CPI for RISC-V using Figure A.29 and the\ntable above. Average the instruction frequencies of bzip and hmmer to obtain\nthe instruction mix. You may assume that all other instructions (for instructions\nnot accounted for by the types in Table A.29) require 3.0 clock cycles each.\nA.3\n[10]<A.9>Compute the effective CPI for an implementation of a RISC-V CPU\nusing Figure A.29. Assume we have made the following measurements of average\nCPI for each of the instruction types:\nA.4\n[10]<A.9>Compute the effective CPI for RISC-V using Figure A.29 and the\ntable above. Average the instruction frequencies of perlbench and sjeng to obtain\nthe instruction mix.\nA.5\n[10]<A.8>Consider this high-level code sequence of three statements:\nA =B+C;\nB =A+C;\nD =A\u2013B;\nUse the technique of copy propagation (see Figure A.20) to transform the\ncode sequence to the point where no operand is a computed value. Note the\ninstances in which the transformation has reduced the computational work of a\nstatement and those cases where the work has increased. What does this suggest\nabout the technical challenge faced in trying to satisfy the desire for optimizing\ncompilers?\nA.6\n[30]<A.8>Compiler optimizations may result in improvements to code size\nand/or performance. Consider one or more of the benchmark programs from\nthe SPEC CPU2017 or the EEMBC benchmark suites. Use the RISC-V processor\nor a processor available to you along with the GNU C compiler to optimize the\nbenchmark program(s) using no optimization, \u2013O1, \u2013O2, and \u2013O3. Compare\nthe performance and size of the resulting programs. Also compare your results\nto Figure A.21.\nInstruction\nClock cycles\nAll ALU operations\n1.0\nLoads\n3.5\nStores\n2.8\nBranches\nTaken\n4.0\nNot taken\n2.0\nJumps\n2.4\nAverage the instruction frequencies of gobmk and mcf to obtain the\ninstruction mix. You may assume that all other instructions (for\ninstructions not accounted for by the types in Table A.29)\nrequire 3.0 clock cycles each.\nA-48\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 699,
        "text": "A.7\n[20/20/20/25/10]<A.2, A.9>Consider the following fragment of C code:\nfor (i=0; i<100; i++) {\nA[i]=B[i]+C;\n}\nAssume that A and B are arrays of 64-bit integers, and C and i are 64-bit inte-\ngers. Assume that all data values and their addresses are kept in memory (at\naddresses 1000, 3000, 5000, and 7000 for A, B, C, and i, respectively) except when\nthey are operated on. Assume that values in registers are lost between iterations of\nthe loop. Assume all addresses and words are 64 bits.\na. [20]<A.2, A.9>Write the code for RISC-V. How many instructions are\nrequired dynamically? How many memory-data references will be executed?\nWhat is the code size in bytes?\nb. [20]<A.2>Write the code for x86. How many instructions are required\ndynamically? How many memory-data references will be executed? What is the\ncode size in bytes?\nc. [20]<A.2>Write the code for a stack machine. Assume all operations occur\non top of the stack. Push and pop are the only instructions that access memory;\nall others remove their operands from the stack and replace them with the result.\nThe implementation uses a hardwired stack for only the top two stack entries,\nwhich keeps the processor circuit very small and low in cost. Additional stack\npositions are kept in memory locations, and accesses to these stack positions\nrequire memory references. How many instructions are required dynamically?\nHow many memory-data references will be executed?\nd. [25]<A.2, A.9>Instead of the code fragment above, write a routine for\ncomputing a matrix multiplication for dense, single precision matrices, also\nknown as SGEMM. For input matrices of size 100100, how many instruc-\ntions are required dynamically? How many memory-data references will be\nexecuted?\ne. [10]<A.2, A.9>As the matrix size increases, how does this affect the number\nof instructions executed dynamically or the number of memory-data\nreferences?\nA.8\n[25/25]<A.2, A.8, A.9>Consider the following fragment of C code:\nfor(p =0; p <8; p++) {\nY[p] =(9798*R[p] +19235*G[p] +3736*B[p])/32768;\nU[p] =(-4784*R[p] \u0003 9437*G[p] +4221*B[p])/32768 +128;\nV[p] =(20218*R[p]\u000316941*G[p]\u00033277*B[p])/32768 +128;\n}\nAssume that R, G, B, Y, U, and V are arrays of 64-bit integers. Assume that\nall data values and their addresses are kept in memory (at addresses 1000, 2000,\n3000, 4000, 5000, and 6000 for R, G, B, Y, U, and V, respectively) except when\nthey are operated on. Assume that values in registers are lost between iterations of\nthe loop. Assume all addresses and words are 64 bits.\nExercises by Gregory D. Peterson\n\u25a0\nA-49"
    },
    {
        "page": 700,
        "text": "a. [25]<A.2, A.9>Write the code for RISC-V. How many instructions are\nrequired dynamically? How many memory-data references will be executed?\nWhat is the code size in bytes?\nb. [25]<A.2>Write the code for x86. How many instructions are required\ndynamically? How many memory-data references will be executed? What is\nthe code size in bytes? Compare your results to the multimedia instructions\n(MMX) and vector implementations discussed in the A.8.\nA.9\n[10/10/10/10]<A.2, A.7>For the following, we consider instruction encoding for\ninstruction set architectures.\na. [10]<A.2, A.7>Consider the case of a processor with an instruction length of\n14 bits and with 64 general-purpose registers so the size of the address fields is\n6 bits. Is it possible to have instruction encodings for the following?\n\u25a0\n3 two-address instructions\n\u25a0\n63 one-address instructions\n\u25a0\n45 zero-address instructions\nb. [10]<A.2, A.7>Assuming the same instruction length and address field sizes\nas above, determine if it is possible to have\n\u25a0\n3 two-address instructions\n\u25a0\n65 one-address instructions\n\u25a0\n35 zero-address instructions\nExplain your answer.\nc. [10]<A.2, A.7>Assume the same instruction length and address field sizes as\nabove. Further assume there are already 3 two-address and 24 zero-address\ninstructions. What is the maximum number of one-address instructions that\ncan be encoded for this processor?\nd. [10]<A.2, A.7>Assume the same instruction length and address field sizes as\nabove. Further assume there are already 3 two-address and 65 zero-address\ninstructions. What is the maximum number of one-address instructions that\ncan be encoded for this processor?\nA.10\n[10/15]<A.2>For the following assume that integer values A, B, C, D, E, and F\nreside in memory. Also assume that instruction operation codes are represented in\n8 bits, memory addresses are 64 bits, and register addresses are 6 bits.\na. [10]<A.2>For each instruction set architecture shown in Figure A.2, how\nmany addresses, or names, appear in each instruction for the code to compute\nC=A+B, and what is the total code size?\nb. [15]<A.2>Some of the instruction set architectures in Figure A.2 destroy\noperands in the course of computation. This loss of data values from processor\ninternal storage has performance consequences. For each architecture in Figure\nA.2, write the code sequence to compute:\nC=A+B\nD=A \u2013 E\nF=C+D\nA-50\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 701,
        "text": "In your code, mark each operand that is destroyed during execution and mark\neach \u201coverhead\u201d instruction that is included just to overcome this loss of data from\nprocessor internal storage. What is the total code size, the number of bytes of\ninstructions and data moved to or from memory, the number of overhead instruc-\ntions, and the number of overhead data bytes for each of your code sequences?\nA.11\n[15]<A.2, A.7, A.9>The design of RISC-V provides for 32 general-purpose reg-\nisters and 32 floating-point registers. If registers are good, are more registers bet-\nter? List and discuss as many trade-offs as you can that should be considered by\ninstruction set architecture designers examining whether to, and how much to,\nincrease the number of RISC-V registers.\nA.12\n[5]<A.3>Consider a C struct that includes the following members:\nstruct foo {\nchar a;\nbool b;\nint c;\ndouble d;\nshort e;\nfloat f;\ndouble g;\nchar *cptr;\nfloat *fptr;\nint x;\n};\nNote that for C, the compiler must keep the elements of the struct in the same\norder as given in the struct definition. For a 32-bit machine, what is the size of the\nfoo struct? What is the minimum size required for this struct, assuming you may\narrangetheorderofthestructmembersasyouwish?Whatabout fora64-bitmachine?\nA.13\n[30]<A.7>Many computer manufacturers now include tools or simulators that\nallow you to measure the instruction set usage of a user program. Among the\nmethods in use are machine simulation, hardware-supported trapping, and tech-\nniques that instrument the object code module by inserting counters in software\nor using built-in hardware counters. Pick a processor and tools to instrument user\nprograms. (The open source RISC-V architecture supports a collection of tools.\nTools such as the Performance API (PAPI) work with x86 processors.) Use the\nprocessor and tools to measure the instruction set mix for one of the SPEC\nCPU2017 benchmarks. Compare the results to those shown in this chapter.\nA.14\n[30]<A.8>Newer processors such as Intel's i7 Kaby Lake include support for\nAVX2 vector/multimedia instructions. Write a dense matrix multiply function\nusing single-precision values and compile it with different compilers and optimi-\nzation flags. Linear algebra codes using Basic Linear Algebra Subroutine (BLAS)\nroutines such as SGEMM include optimized versions of dense matrix multiply.\nCompare the code size and performance of your code to that of BLAS SGEMM.\nExplore what happens when using double-precision values and DGEMM.\nExercises by Gregory D. Peterson\n\u25a0\nA-51"
    },
    {
        "page": 702,
        "text": "A.15\n[30]<A.8>For the SGEMM code developed above for the i7 processor, include\nthe use of AVX2 intrinsics to improve the performance. In particular, try to vec-\ntorize your code to better utilize the AVX hardware. Compare the code size and\nperformance to the original code. Compare your results to Intel's Math Kernel\nLibrary (MKL) implementation for SGEMM.\nA.16\n[30]<A.7, A.9>The RISC-V processor is open source and boasts an impressive\ncollection of implementations, simulators, compilers, and other tools. See riscv.org\nfor an overview of tools, including spike, a simulator for RISC-V processors. Use\nspike or another simulator to measure the instruction set mix for some SPEC\nCPU2017 benchmark programs.\nA.17\n[35/35/35/35]<A.2\u2013A.8>gcc targets most modern instruction set architectures\n(see www.gnu.org/software/gcc/). Create a version of gcc for several architectures\nthat you have access to, such as x86, RISC-V, PowerPC, and ARM.\na. [35]<A.2\u2013A.8>Compile a subset of SPEC CPU2017 integer benchmarks and\ncreate a table of code sizes. Which architecture is best for each program?\nb. [35]<A.2\u2013A.8>Compile\na\nsubset\nof\nSPEC\nCPU2017\nfloating-point\nbenchmarks and create a table of code sizes. Which architecture is best for each\nprogram?\nc. [35]<A.2\u2013A.8>Compile a subset of EEMBC AutoBench benchmarks (see\nwww.eembc.org/home.php) and create a table of code sizes. Which architecture\nis best for each program?\nd. [35]<A.2\u2013A.8>Compile a subset of EEMBC FPBench floating-point bench-\nmarks and create a table of code sizes. Which architecture is best for each\nprogram?\nA.18\n[40]<A.2\u2013A.8>Power efficiency has become very important for modern proces-\nsors, particularly for embedded systems. Create a version of gcc for two architec-\ntures that you have access to, such as x86, RISC-V, PowerPC, Atom, and ARM.\n(Note that the different versions of RISC-V can also be explored and compared.)\nCompile a subset of EEMBC benchmarks while using EnergyBench to measure\nenergy usage during execution. Compare code size, performance, and energy\nusage for the processors. Which is best for each program?\nA.19\n[20/15/15/20] Your task is to compare the memory efficiency of four different\nstyles of instruction set architectures. The architecture styles are:\n\u25a0\nAccumulator\u2014All operations occur between a single register and a memory\nlocation.\n\u25a0\nMemory-memory\u2014All instruction addresses reference only memory locations.\n\u25a0\nStack\u2014All operations occur on top of the stack. Push and pop are the only\ninstructions that access memory; all others remove their operands from the\nstack and replace them with the result. The implementation uses a hardwired\nstack for only the top two stack entries, which keeps the processor circuit very\nsmall and low in cost. Additional stack positions are kept in memory loca-\ntions, and accesses to these stack positions require memory references.\nA-52\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 703,
        "text": "\u25a0\nLoad-store\u2014All operations occur in registers, and register-to-register instruc-\ntions have three register names per instruction.\nTo measure memory efficiency, make the following assumptions about all four\ninstruction sets:\n\u25a0\nAll instructions are an integral number of bytes in length.\n\u25a0\nThe opcode is always one byte (8 bits).\n\u25a0\nMemory accesses use direct, or absolute, addressing.\n\u25a0\nThe variables A, B, C, and D are initially in memory.\na. [20]<A.2, A.3>Invent your own assembly language mnemonics (Figure\nA.2 provides a useful sample to generalize), and for each architecture write\nthe best equivalent assembly language code for this high-level language code\nsequence:\nA =B+C;\nB =A+C;\nD =A\u2013B;\nb. [15]<A.3>Label each instance in your assembly codes for part (a) where a\nvalue is loaded from memory after having been loaded once. Also label each\ninstance in your code where the result of one instruction is passed to another\ninstruction as an operand, and further classify these events as involving storage\nwithin the processor or storage in memory.\nc. [15]<A.7>Assume that the given code sequence is from a small, embedded\ncomputer application that uses a 16-bit memory address and data operands. If\na load-store architecture is used, assume it has 16 general-purpose registers.\nFor eacharchitecture answer the following questions: How manyinstruction bytes\nare fetched? How many bytes of data are transferred from/to memory? Which\narchitecture is most efficient as measured by total memory traffic (code+data)?\nd. [20]<A.7>Now assume a processor with 64-bit memory addresses and data\noperands. For each architecture answer the questions of part (c). How have the\nrelative merits of the architectures changed for the chosen metrics?\nA.20\n[30]<A.2, A.3>Use the four different instruction set architecture styles from\nabove, but assume that the memory operations supported include register indirect\nas well as direct addressing. Invent your own assembly language mnemonics (Fig-\nure A.2 provides a useful sample to generalize), and for each architecture, write the\nbest equivalent assembly language code for this fragment of C code:\nfor (i =0; i <= 100; i++) {\nA[i] =B[i]* C+ D ;\n}\nAssume that A and B are arrays of 64-bit integers, and C, D, and i are 64-bit\nintegers.\nExercises by Gregory D. Peterson\n\u25a0\nA-53"
    },
    {
        "page": 704,
        "text": "A.21\n[20/20]<A.3, A.6, A.9>The size of displacement values needed for the displace-\nment addressing mode or for PC-relative addressing can be extracted from com-\npiled applications. Use a disassembler with one or more of the SPEC CPU2017\nor EEMBC benchmarks compiled for the RISC-V processor.\na. [20]<A.3, A.9>For each instruction using displacement addressing, record\nthe displacement value used. Create a histogram of displacement values. Com-\npare the results to those shown in this appendix in Figure A.8.\nb. [20]<A.6, A.9>For each branch instruction using PC-relative addressing,\nrecord the offset value used. Create a histogram of offset values. Compare\nthe results to those shown in this chapter in Figure A.15.\nA.22\n[15/15/10/10/10/10]<A.3>The value represented by the hexadecimal number\n5249 5343 5643 5055 is to be stored in an aligned 64-bit double word.\na. [15]<A.3>Using the physical arrangement of the first row in Figure A.5,\nwrite the value to be stored using Big Endian byte order. Next, interpret each\nbyte as an ASCII character and below each byte write the corresponding char-\nacter, forming the character string as it would be stored in Big Endian order.\nb. [15]<A.3>Using the same physical arrangement as in part (a), write the value\nto be stored using Little Endian byte order, and below each byte write the cor-\nresponding ASCII character.\nc. [10]<A.3>What are the hexadecimal values of all misaligned 2-byte words\nthat can be read from the given 64-bit double word when stored in Big Endian\nbyte order?\nd. [10]<A.3>What are the hexadecimal values of all misaligned 2-byte words\nthat can be read from the given 64-bit double word when stored in Big Endian\nbyte order?\ne. [10]<A.3>What are the hexadecimal values of all misaligned 2-byte words\nthat can be read from the given 64-bit double word when stored in Little Endian\nbyte order?\nf. [10]<A.3>What are the hexadecimal values of all misaligned 4-byte words\nthat can be read from the given 64-bit double word when stored in Little Endian\nbyte order?\nA.23\n[25,25]<A.3, A.9>Therelativefrequency of different addressing modes impactsthe\nchoices of addressing modes support for an instruction set architecture. Figure A.7\nillustrates the relative frequency of addressing modes for three applications on the\nVAX.\na. [25]<A.3>Compile one or more programs from the SPEC CPU2017 or\nEEMBC benchmark suites to target the x86 architecture. Using a disassembler,\ninspect the instructions and the relative frequency of various addressing modes.\nCreate a histogram to illustrate the relative frequency of the addressing modes.\nHow do your results compare to Figure A.7?\nA-54\n\u25a0\nAppendix A Instruction Set Principles"
    },
    {
        "page": 705,
        "text": "b. [25]<A.3, A.9>Compile one or more programs from the SPEC CPU2017 or\nEEMBC benchmark suites to target the RISC-V architecture. Using a disassem-\nbler, inspect the instructions and the relative frequency of various addressing\nmodes. Create a histogram to illustrate the relative frequency of the addressing\nmodes. How do your results compare to Figure A.7?\nA.24\n[Discussion]<A.2\u2013A.12>Consider typical applications for desktop, server,\ncloud, and embedded computing. How would instruction set architecture be\nimpacted for machines targeting each of these markets?\nExercises by Gregory D. Peterson\n\u25a0\nA-55"
    },
    {
        "page": 706,
        "text": "B.1\nIntroduction\nB-2\nB.2\nCache Performance\nB-15\nB.3\nSix Basic Cache Optimizations\nB-22\nB.4\nVirtual Memory\nB-40\nB.5\nProtection and Examples of Virtual Memory\nB-49\nB.6\nFallacies and Pitfalls\nB-57\nB.7\nConcluding Remarks\nB-59\nB.8\nHistorical Perspective and References\nB-59\nExercises by Amr Zaky\nB-60"
    },
    {
        "page": 707,
        "text": "B\nReview of Memory Hierarchy\nCache: a safe place for hiding or storing things.\nWebster\u2019s New World Dictionary of\nthe American Language,\nSecond College Edition (1976)"
    },
    {
        "page": 708,
        "text": "B.1\nIntroduction\nThis appendix is a quick refresher of the memory hierarchy, including the basics of\ncache and virtual memory, performance equations, and simple optimizations. This\nfirst section reviews the following 36 terms:\ncache\nfully associative\nwrite allocate\nvirtual memory\ndirty bit\nunified cache\nmemory stall cycles\nblock offset\nmisses per instruction\ndirect mapped\nwrite back\nblock\nvalid bit\ndata cache\nlocality\nblock address\nhit time\naddress trace\nwrite through\ncache miss\nset\ninstruction cache\npage fault\nrandom replacement\naverage memory access time\nmiss rate\nindex field\ncache hit\nn-way set associative\nno-write allocate\npage\nleast recently used\nwrite buffer\nmiss penalty\ntag field\nwrite stall\nIf this review goes too quickly, you might want to look at Chapter 7 in Computer\nOrganization and Design, which we wrote for readers with less experience.\nCache is the name given to the highest or first level of the memory hierarchy\nencountered once the address leaves the processor. Because the principle of local-\nity applies at many levels, and taking advantage of locality to improve performance\nis popular, the term cache is now applied whenever buffering is employed to reuse\ncommonly occurring items. Examples include file caches, name caches, and so on.\nWhen the processor finds a requested data item in the cache, it is called a cache\nhit. When the processor does not find a data item it needs in the cache, a cache miss\noccurs. A fixed-size collection of data containing the requested word, called a\nblock or line run, is retrieved from the main memory and placed into the cache.\nTemporal locality tells us that we are likely to need this word again in the near\nfuture, so it is useful to place it in the cache where it can be accessed quickly.\nBecause of spatial locality, there is a high probability that the other data in the\nblock will be needed soon.\nThe time required for the cache miss depends on both the latency and band-\nwidth of the memory. Latency determines the time to retrieve the first word of\nthe block, and bandwidth determines the time to retrieve the rest of this block.\nA cache miss is handled by hardware and causes processors using in-order execu-\ntion to pause, or stall, until the data are available. With out-of-order execution, an\ninstruction using the result must still wait, but other instructions may proceed dur-\ning the miss.\nSimilarly, not all objects referenced by a program need to reside in main mem-\nory. Virtual memory means some objects may reside on disk. The address space is\nB-2\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 709,
        "text": "usually broken into fixed-size blocks, called pages. At any time, each page resides\neither in main memory or on disk. When the processor references an item within a\npage that is not present in the cache or main memory, a palt occurs, and the entire\npage is moved from the disk to main memory. Because page faults take so long,\nthey are handled in software and the processor is not stalled. The processor usually\nswitches to some other task while the disk access occurs. From a high-level per-\nspective, the reliance on locality of references and the relative relationships in size\nand relative cost per bit of cache versus main memory are similar to those of main\nmemory versus disk.\nFigure B.1 shows the range of sizes and access times of each level in the mem-\nory hierarchy for computers ranging from high-end desktops to low-end servers.\nCache Performance Review\nBecause of locality and the higher speed of smaller memories, a memory hierarchy\ncan substantially improve performance. One method to evaluate cache perfor-\nmance is to expand our processor execution time equation from Chapter 1. We\nnow account for the number of cycles during which the processor is stalled waiting\nfor a memory access, which we call the memory stall cycles. The performance is\nthen the product of the clock cycle time and the sum of the processor cycles and the\nmemory stall cycles:\nCPU execution time \u00bc CPU clock cycles + Memory stall cycles\n\u00f0\n\u00deClockcycle time\nLevel\n1\n2\n3\n4\nName\nRegisters\nCache\nMain memory\nDisk storage\nTypical size\n<4 KiB\n32 KiB to 8 MiB\n<1 TB\n>1 TB\nImplementation technology\nCustom memory with\nmultiple ports, CMOS\nOn-chip CMOS\nSRAM\nCMOS DRAM\nMagnetic disk\nor FLASH\nAccess time (ns)\n0.1\u20130.2\n0.5\u201310\n30\u2013150\n5,000,000\nBandwidth (MiB/sec)\n1,000,000\u201310,000,000\n20,000\u201350,000\n10,000\u201330,000\n100\u20131000\nManaged by\nCompiler\nHardware\nOperating system\nOperating\nsystem\nBacked by\nCache\nMain memory\nDisk or FLASH\nOther disks\nand DVD\nFigure B.1 The typical levels in the hierarchy slow down and get larger as we move away from the processor for a\nlarge workstation or small server. Embedded computers might have no disk storage and much smaller memories\nand caches. Increasingly, FLASH is replacing magnetic disks, at least for first level file storage. The access times\nincrease as we move to lower levels of the hierarchy, which makes it feasible to manage the transfer less responsively.\nThe implementation technology shows the typical technology used for these functions. The access time is given in\nnanoseconds for typical values in 2017; these times will decrease over time. Bandwidth is given in megabytes per\nsecond between levels in the memory hierarchy. Bandwidth for disk/FLASH storage includes both the media and the\nbuffered interfaces.\nB.1\nIntroduction\n\u25a0\nB-3"
    },
    {
        "page": 710,
        "text": "This equation assumes that the CPU clock cycles include the time to handle a cache\nhit and that the processor is stalled during a cache miss. Section B.2 reexamines\nthis simplifying assumption.\nThe number of memory stall cycles depends on both the number of misses and\nthe cost per miss, which is called the miss penalty:\nMemorystall cycles \u00bc Number of missesMiss penalty\n\u00bc IC\nMisses\nInstructionMiss penalty\n\u00bc ICMemory accesses\nInstruction\nMiss rateMisspenalty\nThe advantage of the last form is that the components can be easily measured. We\nalready know how to measure instruction count (IC). (For speculative processors,\nwe only count instructions that commit.) Measuring the number of memory refer-\nences per instruction can be done in the same fashion; every instruction requires an\ninstruction access, and it is easy to decide if it also requires a data access.\nNote that we calculated miss penalty as an average, but we will use it herein as\nif it were a constant. The memory behind the cache may be busy at the time of the\nmiss because of prior memory requests or memory refresh. The number of clock\ncycles also varies at interfaces between different clocks of the processor, bus, and\nmemory. Thus, please remember that using a single number for miss penalty is a\nsimplification.\nThe component miss rate is simply the fraction of cache accesses that result in a\nmiss (i.e., number of accesses that miss divided by number of accesses). Miss rates\ncan be measured with cache simulators that take an address trace of the instruction\nand data references, simulate the cache behavior to determine which references hit\nand which miss, and then report the hit and miss totals. Many microprocessors\ntoday provide hardware to count the number of misses and memory references,\nwhich is a much easier and faster way to measure miss rate.\nThe preceding formula is an approximation because the miss rates and miss\npenalties are often different for reads and writes. Memory stall clock cycles could\nthen be defined in terms of the number of memory accesses per instruction, miss\npenalty (in clock cycles) for reads and writes, and miss rate for reads and writes:\nMemory stall clock cycles \u00bc ICReadsper instructionRead missrateReadmisspenalty\n+ ICWrites per instructionWrite missrateWritemiss penalty\nWe usually simplify the complete formula by combining the reads and writes and\nfinding the average miss rates and miss penalty for reads and writes:\nMemory stall clock cycles \u00bc ICMemory accesses\nInstruction\nMissrateMiss penalty\nThe miss rate is one of the most important measures of cache design, but, as we\nwill see in later sections, not the only measure.\nB-4\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 711,
        "text": "Example\nAssume we have a computer where the cycles per instruction (CPI) is 1.0 when all\nmemory accesses hit in the cache. The only data accesses are loads and stores, and\nthese total 50% of the instructions. If the miss penalty is 50 clock cycles and the\nmiss rate is 1%, how much faster would the computer be if all instructions were\ncache hits?\nAnswer\nFirst compute the performance for the computer that always hits:\nCPU execution time \u00bc CPU clock cycles + Memory stall cycles\n\u00f0\n\u00deClockcycle\n\u00bc ICCPI + 0\n\u00f0\n\u00deClockcycle\n\u00bc IC1:0Clockcycle\nNow for the computer with the real cache, first we compute memory stall cycles:\nMemory stall cycles \u00bc ICMemory accesses\nInstruction\nMiss rateMisspenalty\n\u00bc IC 1 + 0:5\n\u00f0\n\u00de0:0150\n\u00bc IC0:75\nwhere the middle term (1+0.5) represents one instruction access and 0.5 data\naccesses per instruction. The total performance is thus\nCPU execution timecache \u00bc IC1:0 + IC0:75\n\u00f0\n\u00deClockcycle\n\u00bc 1:75ICClockcycle\nThe performance ratio is the inverse of the execution times:\nCPU execution timecache\nCPU execution time\n\u00bc 1:75ICClockcycle\n1:0ICClockcycle\n\u00bc 1:75\nThe computer with no cache misses is 1.75 times faster.\nSome designers prefer measuring miss rate as misses per instruction rather than\nmisses per memory reference. These two are related:\nMisses\nInstruction \u00bc Miss rate Memory accesses\nInstructioncount\n\u00bc MissrateMemory accesses\nInstruction\nThe latter formula is useful when you know the average number of memory\naccesses per instruction because it allows you to convert miss rate into misses\nper instruction, and vice versa. For example, we can turn the miss rate per memory\nreference in the previous example into misses per instruction:\nMisses\nInstruction \u00bc Miss rateMemory accesses\nInstruction\n\u00bc 0:02 1:5\n\u00f0\n\u00de \u00bc 0:030\nB.1\nIntroduction\n\u25a0\nB-5"
    },
    {
        "page": 712,
        "text": "By the way, misses per instruction are often reported as misses per 1000\ninstructions to show integers instead of fractions. Thus, the preceding answer could\nalso be expressed as 30 misses per 1000 instructions.\nThe advantage of misses per instruction is that it is independent of the hardware\nimplementation. For example, speculative processors fetch about twice as many\ninstructions as are actually committed, which can artificially reduce the miss rate\nif measured as misses per memory reference rather than per instruction. The draw-\nback is that misses per instruction is architecture dependent; for example, the aver-\nage number of memory accesses per instruction may be very different for an 80x86\nversus RISC V. Thus, misses per instruction are most popular with architects work-\ning with a single computer family, although the similarity of RISC architectures\nallows one to give insights into others.\nExample\nTo show equivalency between the two miss rate equations, let\u2019s redo the preceding\nexample, this time assuming a miss rate per 1000 instructions of 30. What is\nmemory stall time in terms of instruction count?\nAnswer\nRecomputing the memory stall cycles:\nMemory stall cycles \u00bc Number of missesMisspenalty\n\u00bc IC\nMisses\nInstructionMiss penalty\n\u00bc IC=1000\nMisses\nIntruction1000Misspenalty\n\u00bc IC=10003025\n\u00bc IC=1000750\n\u00bc IC0:75\nWe get the same answer as on page B-5, showing equivalence of the two equations.\nFour Memory Hierarchy Questions\nWe continue our introduction to caches by answering the four common questions\nfor the first level of the memory hierarchy:\nQ1: Where can a block be placed in the upper level? (block placement)\nQ2: How is a block found if it is in the upper level? (block identification)\nQ3: Which block should be replaced on a miss? (block replacement)\nQ4: What happens on a write? (write strategy)\nThe answers to these questions help us understand the different trade-offs of mem-\nories at different levels of a hierarchy; hence, we ask these four questions on every\nexample.\nB-6\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 713,
        "text": "Q1: Where Can a Block be Placed in a Cache?\nFigure B.2 shows that the restrictions on where a block is placed create three cat-\negories of cache organization:\n\u25a0\nIf each block has only one place it can appear in the cache, the cache is said to\nbe direct mapped. The mapping is usually\n(Block address) MOD (Number of blocks in cache)\n\u25a0\nIf a block can be placed anywhere in the cache, the cache is said to be fully\nassociative.\nFully associative:\nblock 12 can go\nanywhere\nDirect mapped:\nblock 12 can go\nonly into block 4\n(12 MOD 8)\nSet associative:\nblock 12 can go\nanywhere in set 0\n(12 MOD 4)\n0 1 2 3 4 5 6 7\n0 1 2 3 4 5 6 7\n0 1 2 3 4 5 6 7\nBlock \nno.\nBlock \nno.\nBlock \nno.\nSet\n0\nSet\n1\nSet\n2\nSet\n3\n1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\nBlock \nBlock frame address \nno.\nCache\nMemory\nFigure B.2 This example cache has eight block frames and memory has 32 blocks.\nThe three options for caches are shown left to right. In fully associative, block 12 from\nthe lower level can go into any of the eight block frames of the cache. With direct\nmapped, block 12 can only be placed into block frame 4 (12 modulo 8). Set associative,\nwhich has some of both features, allows the block to be placed anywhere in set 0 (12\nmodulo 4). With two blocks per set, this means block 12 can be placed either in block\n0 or in block 1 of the cache. Real caches contain thousands of block frames, and real\nmemories contain millions of blocks. The set associative organization has four sets with\ntwo blocks per set, called two-way set associative. Assume that there is nothing in the\ncache and that the block address in question identifies lower-level block 12.\nB.1\nIntroduction\n\u25a0\nB-7"
    },
    {
        "page": 714,
        "text": "\u25a0\nIf a block can be placed in a restricted set of places in the cache, the cache is set\nassociative. A set is a group of blocks in the cache. A block is first mapped onto\na set, and then the block can be placed anywhere within that set. The set is usu-\nally chosen by bit selection; that is,\n(Block address) MOD (Number of sets in cache)\nIf there are n blocks in a set, the cache placement is called n-way set\nassociative.\nThe range of caches from direct mapped to fully associative is really a continuum\nof levels of set associativity. Direct mapped is simply one-way set associative, and\na fully associative cache with m blocks could be called \u201cm-way set associative.\u201d\nEquivalently, direct mapped can be thought of as having m sets, and fully associa-\ntive as having one set.\nThe vast majority of processor caches today are direct mapped, two-way set\nassociative, or four-way set associative, for reasons we will see shortly.\nQ2: How Is a Block Found If It Is in the Cache?\nCaches have an address tag on each block frame that gives the block address. The\ntag of every cache block that might contain the desired information is checked to\nsee if it matches the block address from the processor. As a rule, all possible tags\nare searched in parallel because speed is critical.\nThere must be a way to know that a cache block does not have valid informa-\ntion. The most common procedure is to add a valid bit to the tag to say whether or\nnot this entry contains a valid address. If the bit is not set, there cannot be a match\non this address.\nBefore proceeding to the next question, let\u2019s explore the relationship of a pro-\ncessor address to the cache. Figure B.3 shows how an address is divided. The first\ndivision is between the block address and the block offset. The block frame address\ncan be further divided into the tag field and the index field. The block offset field\nselects the desired data from the block, the index field selects the set, and the tag\nfield is compared against it for a hit. Although the comparison could be made on\nmore of the address than the tag, there is no need because of the following:\n\u25a0\nThe offset should not be used in the comparison, because the entire block is\npresent or not, and hence all block offsets result in a match by definition.\nTag\nIndex\nBlock\noffset\nBlock address\nFigure B.3 The three portions of an address in a set associative or direct-\nmapped cache. The tag is used to check all the blocks in the set, and the index is used\nto select the set. The block offset is the address of the desired data within the block. Fully\nassociative caches have no index field.\nB-8\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 715,
        "text": "\u25a0\nChecking the index is redundant, because it was used to select the set to be\nchecked. An address stored in set 0, for example, must have 0 in the index field\nor it couldn\u2019t be stored in set 0; set 1 must have an index value of 1; and so on.\nThis optimization saves hardware and power by reducing the width of memory\nsize for the cache tag.\nIf the total cache size is kept the same, increasing associativity increases the num-\nber of blocks per set, thereby decreasing the size of the index and increasing the\nsize of the tag. That is, the tag-index boundary in Figure B.3 moves to the right with\nincreasing associativity, with the end point of fully associative caches having no\nindex field.\nQ3: Which Block Should be Replaced on a Cache Miss?\nWhen a miss occurs, the cache controller must select a block to be replaced with the\ndesired data. A benefit of direct-mapped placement is that hardware decisions are\nsimplified\u2014in fact, so simple that there is no choice: only one block frame is\nchecked for a hit, and only that block can be replaced. With fully associative or\nset associative placement, there are many blocks to choose from on a miss. There\nare three primary strategies employed for selecting which block to replace:\n\u25a0\nRandom\u2014To spread allocation uniformly, candidate blocks are randomly\nselected. Some systems generate pseudorandom block numbers to get repro-\nducible behavior, which is particularly useful when debugging hardware.\n\u25a0\nLeast recently used (LRU)\u2014To reduce the chance of throwing out information\nthat will be needed soon, accesses to blocks are recorded. Relying on the past to\npredict the future, the block replaced is the one that has been unused for the\nlongest time. LRU relies on a corollary of locality: if recently used blocks\nare likely to be used again, then a good candidate for disposal is the least\nrecently used block.\n\u25a0\nFirst in, first out (FIFO)\u2014Because LRU can be complicated to calculate, this\napproximates LRU by determining the oldest block rather than the LRU.\nA virtue of random replacement is that it is simple to build in hardware. As the\nnumber of blocks to keep track of increases, LRU becomes increasingly expensive\nand is usually only approximated. A common approximation (often called pseudo-\nLRU) has a set of bits for each set in the cache with each bit corresponding to a\nsingle way (a way is bank in a set associative cache; there are four ways in\nfour-way set associative cache) in the cache. When a set is accessed, the bit corre-\nsponding to the way containing the desired block is turned on; if all the bits asso-\nciated with a set are turned on, they are reset with the exception of the most recently\nturned on bit. When a block must be replaced, the processor chooses a block\nfrom the way whose bit is turned off, often randomly if more than one choice is\navailable. This approximates LRU, because the block that is replaced will not have\nB.1\nIntroduction\n\u25a0\nB-9"
    },
    {
        "page": 716,
        "text": "been accessed since the last time that all the blocks in the set were accessed.\nFigure B.4 shows the difference in miss rates between LRU, random, and FIFO\nreplacement.\nQ4: What Happens on a Write?\nReads dominate processor cache accesses. All instruction accesses are reads, and\nmost instructions don\u2019t write to memory. Figures A.32 and A.33 in Appendix A\nsuggest a mix of 10% stores and 26% loads for RISC V programs, making writes\n10%/(100%+26%+10%) or about 7% of the overall memory traffic. Of the data\ncache traffic, writes are 10%/(26%+10%) or about 28%. Making the common case\nfast means optimizing caches for reads, especially because processors traditionally\nwait for reads to complete but need not wait for writes. Amdahl\u2019s Law (Section 1.9)\nreminds us, however, that high-performance designs cannot neglect the speed of\nwrites.\nFortunately, the common case is also the easy case to make fast. The block can\nbe read from the cache at the same time that the tag is read and compared, so the\nblock read begins as soon as the block address is available. If the read is a hit, the\nrequested part of the block is passed on to the processor immediately. If it is a miss,\nthere is no benefit\u2014but also no harm except more power in desktop and server\ncomputers; just ignore the value read.\nSuch optimism is not allowed for writes. Modifying a block cannot begin until\nthe tag is checked to see if the address is a hit. Because tag checking cannot occur in\nparallel, writes usually take longer than reads. Another complexity is that the pro-\ncessor also specifies the size of the write, usually between 1 and 8 bytes; only that\nportion of a block can be changed. In contrast, reads can access more bytes than\nnecessary without fear.\nThe write policies often distinguish cache designs. There are two basic options\nwhen writing to the cache:\nAssociativity\nTwo-way\nFour-way\nEight-way\nSize\nLRU\nRandom\nFIFO\nLRU\nRandom\nFIFO\nLRU\nRandom\nFIFO\n16 KiB\n114.1\n117.3\n115.5\n111.7\n115.1\n113.3\n109.0\n111.8\n110.4\n64 KiB\n103.4\n104.3\n103.9\n102.4\n102.3\n103.1\n99.7\n100.5\n100.3\n256 KiB\n92.2\n92.1\n92.5\n92.1\n92.1\n92.5\n92.1\n92.1\n92.5\nFigure B.4 Data cache misses per 1000 instructions comparing least recently used, random, and first in, first out\nreplacement for several sizes and associativities. There is little difference between LRU and random for the largest\nsize cache, with LRU outperforming the others for smaller caches. FIFO generally outperforms random in the smaller\ncache sizes. These data were collected for a block size of 64 bytes for the Alpha architecture using 10 SPEC2000\nbenchmarks. Five are from SPECint2000 (gap, gcc, gzip, mcf, and perl) and five are from SPECfp2000 (applu, art,\nequake, lucas, and swim). We will use this computer and these benchmarks in most figures in this appendix.\nB-10\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 717,
        "text": "\u25a0\nWrite through\u2014The information is written to both the block in the cache and to\nthe block in the lower-level memory.\n\u25a0\nWrite back\u2014The information is written only to the block in the cache. The\nmodified cache block is written to main memory only when it is replaced.\nTo reduce the frequency of writing back blocks on replacement, a feature called\nthe dirty bit is commonly used. This status bit indicates whether the block is dirty\n(modified while in the cache) or clean (not modified). If it is clean, the block is not\nwritten back on a miss, because identical information to the cache is found in lower\nlevels.\nBoth write back and write through have their advantages. With write back,\nwrites occur at the speed of the cache memory, and multiple writes within a block\nrequire only one write to the lower-level memory. Because some writes don\u2019t go to\nmemory, write back uses less memory bandwidth, making write back attractive in\nmultiprocessors. Since write back uses the rest of the memory hierarchy and mem-\nory interconnect less than write through, it also saves power, making it attractive\nfor embedded applications.\nWrite through is easier to implement than write back. The cache is always\nclean, so unlike write back read misses never result in writes to the lower level.\nWrite through also has the advantage that the next lower level has the most current\ncopy of the data, which simplifies data coherency. Data coherency is important for\nmultiprocessors and for I/O, which we examine in Chapter 4 and Appendix D.\nMultilevel caches make write through more viable for the upper-level caches, as\nthe writes need only propagate to the next lower level rather than all the way to\nmain memory.\nAs we will see, I/O and multiprocessors are fickle: they want write back for\nprocessor caches to reduce the memory traffic and write through to keep the cache\nconsistent with lower levels of the memory hierarchy.\nWhen the processor must wait for writes to complete during write through, the\nprocessor is said to write stall. A common optimization to reduce write stalls is a\nwrite buffer, which allows the processor to continue as soon as the data are written\nto the buffer, thereby overlapping processor execution with memory updating. As\nwe will see shortly, write stalls can occur even with write buffers.\nBecause the data are not needed on a write, there are two options on a write\nmiss:\n\u25a0\nWrite allocate\u2014The block is allocated on a write miss, followed by the pre-\nceding write hit actions. In this natural option, write misses act like read misses.\n\u25a0\nNo-write allocate\u2014This apparently unusual alternative is write misses do not\naffect the cache. Instead, the block is modified only in the lower-level memory.\nThus, blocks stay out of the cache in no-write allocate until the program tries to\nread the blocks, but even blocks that are only written will still be in the cache with\nwrite allocate. Let\u2019s look at an example.\nB.1\nIntroduction\n\u25a0\nB-11"
    },
    {
        "page": 718,
        "text": "Example\nAssume a fully associative write-back cache with many cache entries that starts\nempty. Following is a sequence of five memory operations (the address is in square\nbrackets):\nWrite Mem[100];\nWrite Mem[100];\nRead\nMem[200];\nWrite Mem[200];\nWrite Mem[100].\nWhat are the number of hits and misses when using no-write allocate versus write\nallocate?\nAnswer\nFor no-write allocate, the address 100 is not in the cache, and there is no allocation\non write, so the first two writes will result in misses. Address 200 is also not in the\ncache, so the read is also a miss. The subsequent write to address 200 is a hit. The\nlast write to 100 is still a miss. The result for no-write allocate is four misses and\none hit.\nFor write allocate, the first accesses to 100 and 200 are misses, and the rest are\nhits because 100 and 200 are both found in the cache. Thus, the result for write\nallocate is two misses and three hits.\nEither write miss policy could be used with write through or write back.\nUsually, write-back caches use write allocate, hoping that subsequent writes to that\nblock will be captured by the cache. Write-through caches often use no-write allo-\ncate. The reasoning is that even if there are subsequent writes to that block, the\nwrites must still go to the lower-level memory, so what\u2019s to be gained?\nAn Example: The Opteron Data Cache\nTo give substance to these ideas, Figure B.5 shows the organization of the data\ncache in the AMD Opteron microprocessor. The cache contains 65,536 (64 K)\nbytes of data in 64-byte blocks with two-way set associative placement, least-\nrecently used replacement, write back, and write allocate on a write miss.\nLet\u2019s trace a cache hit through the steps of a hit as labeled in Figure B.5. (The\nfour steps are shown as circled numbers.) As described in Section B.5, the Opteron\npresents a 48-bit virtual address to the cache for tag comparison, which is simul-\ntaneously translated into a 40-bit physical address.\nThe reason Opteron doesn\u2019t use all 64 bits of virtual address is that its designers\ndon\u2019t think anyone needs that much virtual address space yet, and the smaller size\nsimplifies the Opteron virtual address mapping. The designers plan to grow the\nvirtual address in future microprocessors.\nThe physical address coming into the cache is divided into two fields: the 34-bit\nblock address and the 6-bit block offset (64\u00bc26 and 34+6\u00bc40). The block\nB-12\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 719,
        "text": "address is further divided into an address tag and cache index. Step 1 shows this\ndivision.\nThe cache index selects the tag to be tested to see if the desired block is in the\ncache. The size of the index depends on cache size, block size, and set associativity.\nFor the Opteron cache the set associativity is set to two, and we calculate the index\nas follows:\n2Index \u00bc\nCache size\nBlock sizeSet associativity \u00bc 65,536\n642 \u00bc 512 \u00bc 29\n<25>\nTag\n(512\nblocks)\n(512\nblocks)\nIndex\n<9>\nBlock\noffset\n<6>\nBlock address\nValid\n<1>\nData\n<64>\nCPU\naddress\nVictim\nbuffer\nData\nin\nData\nout\nTag\n<25>\n=?\n2:1 mux\nLower-level memory\n=?\n2\n2\n1\n3\n3\n4\nFigure B.5 The organization of the data cache in the Opteron microprocessor. The 64 KiB cache is two-way set\nassociative with 64-byte blocks. The 9-bit index selects among 512 sets. The four steps of a read hit, shown as circled\nnumbers in order of occurrence, label this organization. Three bits of the block offset join the index to supply the\nRAM address to select the proper 8 bytes. Thus, the cache holds two groups of 4096 64-bit words, with each group\ncontaining half of the 512 sets. Although not exercised in this example, the line from lower-level memory to the\ncache is used on a miss to load the cache. The size of address leaving the processor is 40 bits because it is a physical\naddress and not a virtual address. Figure B.24 on page B-47 explains how the Opteron maps from virtual to physical\nfor a cache access.\nB.1\nIntroduction\n\u25a0\nB-13"
    },
    {
        "page": 720,
        "text": "Hence, the index is 9 bits wide, and the tag is 34\u00039 or 25 bits wide. Although that\nis the index needed to select the proper block, 64 bytes is much more than the pro-\ncessor wants to consume at once. Hence, it makes more sense to organize the data\nportion of the cache memory 8 bytes wide, which is the natural data word of the 64-\nbit Opteron processor. Thus, in addition to 9 bits to index the proper cache block, 3\nmore bits from the block offset are used to index the proper 8 bytes. Index selection\nis step 2 in Figure B.5.\nAfter reading the two tags from the cache, they are compared with the tag por-\ntion of the block address from the processor. This comparison is step 3 in the figure.\nTo be sure the tag contains valid information, the valid bit must be set or else the\nresults of the comparison are ignored.\nAssuming one tag does match, the final step is to signal the processor to\nload the proper data from the cache by using the winning input from a 2:1 mul-\ntiplexor. The Opteron allows 2 clock cycles for these four steps, so the instruc-\ntions in the following 2 clock cycles would wait if they tried to use the result of\nthe load.\nHandling writes is more complicated than handling reads in the Opteron, as it is\nin any cache. If the word to be written is in the cache, the first three steps are the\nsame. Because the Opteron executes out of order, only after it signals that the\ninstruction has committed and the cache tag comparison indicates a hit are the data\nwritten to the cache.\nSo far we have assumed the common case of a cache hit. What happens on a\nmiss? On a read miss, the cache sends a signal to the processor telling it the data\nare not yet available, and 64 bytes are read from the next level of the hierarchy.\nThe latency is 7 clock cycles to the first 8 bytes of the block, and then 2 clock\ncycles per 8 bytes for the rest of the block. Because the data cache is set associa-\ntive, there is a choice on which block to replace. Opteron uses LRU, which selects\nthe block that was referenced longest ago, so every access must update the LRU\nbit. Replacing a block means updating the data, the address tag, the valid bit, and\nthe LRU bit.\nBecause the Opteron uses write back, the old data block could have been mod-\nified, and hence it cannot simply be discarded. The Opteron keeps 1 dirty bit per\nblock to record if the block was written. If the \u201cvictim\u201d was modified, its data and\naddress are sent to the victim buffer. (This structure is similar to a write buffer in\nother computers.) The Opteron has space for eight victim blocks. In parallel with\nother cache actions, it writes victim blocks to the next level of the hierarchy. If the\nvictim buffer is full, the cache must wait.\nA write miss is very similar to a read miss, because the Opteron allocates a\nblock on a read or a write miss.\nWe have seen how it works, but the data cache cannot supply all the memory\nneeds of the processor: the processor also needs instructions. Although a single\ncache could try to supply both, it can be a bottleneck. For example, when a load\nor store instruction is executed, the pipelined processor will simultaneously request\nboth a data word and an instruction word. Hence, a single cache would present a\nstructural hazard for loads and stores, leading to stalls. One simple way to conquer\nB-14\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 721,
        "text": "this problem is to divide it: one cache is dedicated to instructions and another to\ndata. Separate caches are found in most recent processors, including the Opteron.\nHence, it has a 64 KiB instruction cache as well as the 64 KiB data cache.\nThe processor knows whether it is issuing an instruction address or a data\naddress, so there can be separate ports for both, thereby doubling the bandwidth\nbetween the memory hierarchy and the processor. Separate caches also offer the\nopportunity of optimizing each cache separately: different capacities, block sizes,\nand associativities may lead to better performance. (In contrast to the instruction\ncaches and data caches of the Opteron, the terms unified or mixed are applied to\ncaches that can contain either instructions or data.)\nFigure B.6 shows that instruction caches have lower miss rates than data\ncaches. Separating instructions and data removes misses due to conflicts between\ninstruction blocks and data blocks, but the split also fixes the cache space devoted\nto each type. Which is more important to miss rates? A fair comparison of separate\ninstruction and data caches to unified caches requires the total cache size to be the\nsame. For example, a separate 16 KiB instruction cache and 16 KiB data cache\nshould be compared with a 32 KiB unified cache. Calculating the average miss rate\nwith separate instruction and data caches necessitates knowing the percentage of\nmemory references to each cache. From the data in Appendix A we find the split is\n100%/(100%+26%+10%) or about 74% instruction references to (26%+10%)/\n(100%+26%+10%) or about 26% data references. Splitting affects performance\nbeyond what is indicated by the change in miss rates, as we will see shortly.\nB.2\nCache Performance\nBecause instruction count is independent of the hardware, it is tempting to evaluate\nprocessor performance using that number. Such indirect performance measures\nhave waylaid many a computer designer. The corresponding temptation for eval-\nuating memory hierarchy performance is to concentrate on miss rate because it,\nSize (KiB)\nInstruction\ncache\nData cache\nUnified\ncache\n8\n8.16\n44.0\n63.0\n16\n3.82\n40.9\n51.0\n32\n1.36\n38.4\n43.3\n64\n0.61\n36.9\n39.4\n128\n0.30\n35.3\n36.2\n256\n0.02\n32.6\n32.9\nFigure B.6 Miss per 1000 instructions for instruction, data, and unified caches of\ndifferent sizes. The percentage of instruction references is about 74%. The data are\nfor two-way associative caches with 64-byte blocks for the same computer and bench-\nmarks as Figure B.4.\nB.2\nCache Performance\n\u25a0\nB-15"
    },
    {
        "page": 722,
        "text": "too, is independent of the speed of the hardware. As we will see, miss rate can be\njust as misleading as instruction count. A better measure of memory hierarchy per-\nformance is the average memory access time:\nAverage memory access time \u00bc Hit time + Miss rateMiss penalty\nwhere hit time is the time to hit in the cache; we have seen the other two terms\nbefore. The components of average access time can be measured either in absolute\ntime\u2014say, 0.25\u20131.0 ns on a hit\u2014or in the number of clock cycles that the proces-\nsor waits for the memory\u2014such as a miss penalty of 150\u2013200 clock cycles.\nRemember that average memory access time is still an indirect measure of perfor-\nmance; although it is a better measure than miss rate, it is not a substitute for\nexecution time.\nThis formula can help us decide between split caches and a unified cache.\nExample\nWhich has the lower miss rate: a 16 KiB instruction cache with a 16 KiB data cache\nor a 32 KiB unified cache? Use the miss rates in Figure B.6 to help calculate the\ncorrect answer, assuming 36% of the instructions are data transfer instructions.\nAssume a hit takes 1 clock cycle and the miss penalty is 100 clock cycles. A load\nor store hit takes 1 extra clock cycle on a unified cache if there is only one cache\nport to satisfy two simultaneous requests. Using the pipelining terminology of\nChapter 3, the unified cache leads to a structural hazard. What is the average mem-\nory access time in each case? Assume write-through caches with a write buffer and\nignore stalls due to the write buffer.\nAnswer\nFirst let\u2019s convert misses per 1000 instructions into miss rates. Solving the preced-\ning general formula, the miss rate is\nMissrate \u00bc\nMisses\n1000Instructions=1000\nMemory accesses\nInstruction\nBecause every instruction access has exactly one memory access to fetch the\ninstruction, the instruction miss rate is\nMissrate16KB instruction \u00bc 3:82=1000\n1:00\n\u00bc 0:004\nBecause 36% of the instructions are data transfers, the data miss rate is\nMissrate16KB data \u00bc 40:9=1000\n0:36\n\u00bc 0:114\nThe unified miss rate needs to account for instruction and data accesses:\nMiss rate32KB unified \u00bc 43:3=1000\n1:00 + 0:36 \u00bc 0:0318\nB-16\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 723,
        "text": "As stated herein, about 74% of the memory accesses are instruction references.\nThus, the overall miss rate for the split caches is\n74%0:004\n\u00f0\n\u00de + 26%0:114\n\u00f0\n\u00de \u00bc 0:0326\nThus, a 32 KiB unified cache has a slightly lower effective miss rate than two\n16 KiB caches.\nThe average memory access time formula can be divided into instruction and\ndata accesses:\nAverage memory access time\n\u00bc % instructions Hit time + Instruction missrateMiss penalty\n\u00f0\n\u00de\n+ % data Hit time + Datamiss rateMisspenalty\n\u00f0\n\u00de\nTherefore, the time for each organization is\nAverage memory access timesplit\n\u00bc 74% 1 + 0:004200\n\u00f0\n\u00de + 26% 1 + 0:114200\n\u00f0\n\u00de\n\u00bc 74%1:80\n\u00f0\n\u00de + 26%23:80\n\u00f0\n\u00de \u00bc 1:332 + 6:188 \u00bc 7:52\nAverage memory access timeunified\n\u00bc 74% 1 + 0:0318200\n\u00f0\n\u00de + 26% 1 + 1 + 0:0318200\n\u00f0\n\u00de\n\u00bc 74%7:36\n\u00f0\n\u00de + 26%8:36\n\u00f0\n\u00de \u00bc 5:446 + 2:174 \u00bc 7:62\nHence, the split caches in this example\u2014which offer two memory ports per clock\ncycle, thereby avoiding the structural hazard\u2014have a better average memory\naccess time than the single-ported unified cache despite having a worse effective\nmiss rate.\nAverage Memory Access Time and Processor Performance\nAn obvious question is whether average memory access time due to cache misses\npredicts processor performance.\nFirst, there are other reasons for stalls, such as contention due to I/O devices\nusing memory. Designers often assume that all memory stalls are due to cache mis-\nses, because the memory hierarchy typically dominates other reasons for stalls. We\nuse this simplifying assumption here, but be sure to account for all memory stalls\nwhen calculating final performance.\nSecond, the answer also depends on the processor. If we have an in-order exe-\ncution processor (see Chapter 3), then the answer is basically yes. The processor\nstalls during misses, and the memory stall time is strongly correlated to average\nmemory access time. Let\u2019s make that assumption for now, but we\u2019ll return to\nout-of-order processors in the next subsection.\nB.2\nCache Performance\n\u25a0\nB-17"
    },
    {
        "page": 724,
        "text": "As stated in the previous section, we can model CPU time as:\nCPU time \u00bc CPU execution clock cycles + Memory stall clock cycles\n\u00f0\n\u00deClockcycle time\nThis formula raises the question of whether the clock cycles for a cache hit should\nbe considered part of CPU execution clock cycles or part of memory stall clock\ncycles. Although either convention is defensible, the most widely accepted is to\ninclude hit clock cycles in CPU execution clock cycles.\nWe can now explore the impact of caches on performance.\nExample\nLet\u2019s use an in-order execution computer for the first example. Assume that the\ncache miss penalty is 200 clock cycles, and all instructions usually take 1.0 clock\ncycles (ignoring memory stalls). Assume that the average miss rate is 2%, there is\nan average of 1.5 memory references per instruction, and the average number of\ncache misses per 1000 instructions is 30. What is the impact on performance when\nbehavior of the cache is included? Calculate the impact using both misses per\ninstruction and miss rate.\nAnswer\nCPU time \u00bc IC\nCPIexecution + Memory stall clock cycles\nInstruction\n\u0001\n\u0003\nClockcycle time\nThe performance, including cache misses, is\nCPU timewith cache \u00bc IC 1:0 + 30=1000200\n\u00f0\n\u00de\n\u00bd\n\u0004Clockcycle time\n\u00bc IC7:00Clock cycle time\nNow calculating performance using miss rate:\nCPU time \u00bc IC\nCPIexecution + MissrateMemory accesses\nInstruction\nMiss penalty\n\u0001\n\u0003\nClock cycle time\nCPU timewith cache \u00bc IC 1:0 + 1:52%200\n\u00f0\n\u00de\n\u00bd\n\u0004Clock cycle time\n\u00bc IC7:00Clockcycle time\nThe clock cycle time and instruction count are the same, with or without a\ncache. Thus, CPU time increases sevenfold, with CPI from 1.00 for a \u201cperfect\ncache\u201d to 7.00 with a cache that can miss. Without any memory hierarchy at all\nthe CPI would increase again to 1.0+2001.5 or 301\u2014a factor of more than\n40 times longer than a system with a cache!\nAs this example illustrates, cache behavior can have enormous impact on per-\nformance. Furthermore, cache misses have a double-barreled impact on a proces-\nsor with a low CPI and a fast clock:\n1. The lower the CPIexecution, the higher the relative impact of a fixed number of\ncache miss clock cycles.\n2. When calculating CPI, the cache miss penalty is measured in processor clock\ncycles for a miss. Therefore, even if memory hierarchies for two computers are\nB-18\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 725,
        "text": "identical, the processor with the higher clock rate has a larger number of clock\ncycles per miss and hence a higher memory portion of CPI.\nThe importance of the cache for processors with low CPI and high clock rates is\nthus greater, and, consequently, greater is the danger of neglecting cache behavior\nin assessing performance of such computers. Amdahl\u2019s Law strikes again!\nAlthough minimizing average memory access time is a reasonable goal\u2014and\nwe will use it in much of this appendix\u2014keep in mind that the final goal is to\nreduce processor execution time. The next example shows how these two can\ndiffer.\nExample\nWhat is the impact of two different cache organizations on the performance of a\nprocessor? Assume that the CPI with a perfect cache is 1.0, the clock cycle time\nis 0.35 ns, there are 1.4 memory references per instruction, the size of both caches\nis 128 KiB, and both have a block size of 64 bytes. One cache is direct mapped and\nthe other is two-way set associative. Figure B.5 shows that for set associative\ncaches we must add a multiplexor to select between the blocks in the set depending\non the tag match. Because the speed of the processor can be tied directly to the\nspeed of a cache hit, assume the processor clock cycle time must be stretched\n1.35 times to accommodate the selection multiplexor of the set associative cache.\nTo the first approximation, the cache miss penalty is 65 ns for either cache orga-\nnization. (In practice, it is normally rounded up or down to an integer number of\nclock cycles.) First, calculate the average memory access time and then processor\nperformance. Assume the hit time is 1 clock cycle, the miss rate of a direct-mapped\n128 KiB cache is 2.1%, and the miss rate for a two-way set associative cache of the\nsame size is 1.9%.\nAnswer\nAverage memory access time is\nAverage memory access time \u00bc Hit time + Miss rateMisspenalty\nThus, the time for each organization is\nAverage memory access time1-way \u00bc 0:35 + :02165\n\u00f0\n\u00de \u00bc 1:72ns\nAverage memory access time2-way \u00bc 0:351:35 + :01965\n\u00f0\n\u00de \u00bc 1:71ns\nThe average memory access time is better for the two-way set-associative cache.\nThe processor performance is\nCPU time \u00bc IC\nCPIexecution +\nMisses\nInstructionMisspenalty\n\u0001\n\u0003\nClockcycle time\n\u00bc IC CPIexecution Clockcycle time\n\u00f0\n\u00de\n\u00bd\n+ MissrateMemory accesses\nInstruction\nMiss penaltyClockcycle time\n\u0001\n\u0003\u0004\nB.2\nCache Performance\n\u25a0\nB-19"
    },
    {
        "page": 726,
        "text": "Substituting 65 ns for (Miss penaltyClock cycle time), the performance of each\ncache organization is\nCPU time1-way \u00bc IC 1:00:35 + 0:0211:465\n\u00f0\n\u00de\n\u00bd\n\u0004 \u00bc 2:26IC\nCPU time2-way \u00bc IC 1:00:351:35 + 0:0191:465\n\u00f0\n\u00de\n\u00bd\n\u0004 \u00bc 2:20IC\nand relative performance is\nCPU time2-way\nCPU time1-way\n\u00bc 2:26Instruction count\n2:20Instruction count \u00bc 1:03\nIn contrast to the results of average memory access time comparison, the direct-\nmapped cache leads to slightly better average performance because the clock cycle\nis stretched for all instructions for the two-way set associative case, even if there\nare fewer misses. Because CPU time is our bottom-line evaluation and because\ndirect mapped is simpler to build, the preferred cache is direct mapped in this\nexample.\nMiss Penalty and Out-of-Order Execution Processors\nFor an out-of-order execution processor, how do you define \u201cmiss penalty\u201d? Is it\nthe full latency of the miss to memory, or is it just the \u201cexposed\u201d or nonoverlapped\nlatency when the processor must stall? This question does not arise in processors\nthat stall until the data miss completes.\nLet\u2019s redefine memory stalls to lead to a new definition of miss penalty as non-\noverlapped latency:\nMemory stall cycles\nInstruction\n\u00bc\nMisses\nInstruction Total misslatency\u0003Overlapped miss latency\n\u00f0\n\u00de\nSimilarly, as some out-of-order processors stretch the hit time, that portion of the\nperformance equation could be divided by total hit latency less overlapped hit\nlatency. This equation could be further expanded to account for contention for\nmemory resources in an out-of-order processor by dividing total miss latency into\nlatency without contention and latency due to contention. Let\u2019s just concentrate on\nmiss latency.\nWe now have to decide the following:\n\u25a0\nLength of memory latency\u2014What to consider as the start and the end of a mem-\nory operation in an out-of-order processor.\n\u25a0\nLength of latency overlap\u2014What is the start of overlap with the processor\n(or, equivalently, when do we say a memory operation is stalling the\nprocessor)?\nB-20\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 727,
        "text": "Given the complexity of out-of-order execution processors, there is no single cor-\nrect definition.\nBecause only committed operations are seen at the retirement pipeline stage,\nwe say a processor is stalled in a clock cycle if it does not retire the maximum pos-\nsible number of instructions in that cycle. We attribute that stall to the first instruc-\ntion that could not be retired. This definition is by no means foolproof. For\nexample, applying an optimization to improve a certain stall time may not always\nimprove execution time because another type of stall\u2014hidden behind the targeted\nstall\u2014may now be exposed.\nFor latency, we could start measuring from the time the memory instruction is\nqueued in the instruction window, or when the address is generated, or when the\ninstruction is actually sent to the memory system. Any option works as long as it is\nused in a consistent fashion.\nExample\nLet\u2019s redo the preceding example, but this time we assume the processor with the\nlonger clock cycle time supports out-of-order execution yet still has a direct-\nmapped cache. Assume 30% of the 65 ns miss penalty can be overlapped; that\nis, the average CPU memory stall time is now 45.5 ns.\nAnswer\nAverage memory access time for the out-of-order (OOO) computer is\nAverage memoryaccess time1-way,OOO \u00bc 0:351:35 + 0:02145:5\n\u00f0\n\u00de \u00bc 1:43ns\nThe performance of the OOO cache is\nCUP time1-way,OOO \u00bc IC 1:60:351:35 + 0:0211:445:5\n\u00f0\n\u00de\n\u00bd\n\u0004 \u00bc 2:09IC\nHence, despite a much slower clock cycle time and the higher miss rate of a direct-\nmapped cache, the out-of-order computer can be slightly faster if it can hide 30% of\nthe miss penalty.\nIn summary, although the state of the art in defining and measuring memory stalls\nfor out-of-order processors is complex, be aware of the issues because they signif-\nicantly affect performance. The complexity arises because out-of-order processors\ntolerate some latency due to cache misses without hurting performance. Conse-\nquently, designers usually use simulators of the out-of-order processor and mem-\nory when evaluating trade-offs in the memory hierarchy to be sure that an\nimprovement that helps the average memory latency actually helps program\nperformance.\nTo help summarize this section and to act as a handy reference, Figure B.7 lists\nthe cache equations in this appendix.\nB.2\nCache Performance\n\u25a0\nB-21"
    },
    {
        "page": 728,
        "text": "B.3\nSix Basic Cache Optimizations\nThe average memory access time formula gave us a framework to present cache\noptimizations for improving cache performance:\nAverage memory access time \u00bc Hit time + Miss rateMiss penalty\nHence, we organize six cache optimizations into three categories:\n\u25a0\nReducing the miss rate\u2014larger block size, larger cache size, and higher\nassociativity\n\u25a0\nReducing the miss penalty\u2014multilevel caches and giving reads priority over\nwrites\n\u25a0\nReducing the time to hit in the cache\u2014avoiding address translation when\nindexing the cache\nFigure B.18 on page B-40 concludes this section with a summary of the implemen-\ntation complexity and the performance benefits of these six techniques.\n2index \u00bc\nCache size\nBlocksizeSet associativity\nCPU execution time \u00bc CPU clock cycles + Memory stall cycles\n\u00f0\n\u00deClock cycle time\nMemory stall cycles \u00bc Number of missesMisspenalty\nMemory stall cycles \u00bc IC\nMisses\nInstructionMiss penalty\nMisses\nInstruction \u00bc MissrateMemory accesses\nInstruction\nAverage memory access time \u00bc Hit time + MissrateMisspenalty\nCPU execution time \u00bc IC\nCPIexecution + Memory stall clock cycles\nInstruction\n\u0001\n\u0003\nClockcycle time\nCPU execution time \u00bc IC\nCPIexecution +\nMisses\nInstructionMiss penalty\n\u0001\n\u0003\nClockcycle time\nCPU execution time \u00bc IC\nCPIexecution + Miss rateMemory accesses\nInstruction\nMisspenalty\n\u0001\n\u0003\nClockcycle time\nMemory stall cycles\nInstruction\n\u00bc\nMisses\nInstruction Total misslatency\u0003Overlapped miss latency\n\u00f0\n\u00de\nAverage memory access time \u00bc Hit timeL1 + MissrateL1  Hit timeL2 + MissrateL2 MisspenaltyL2\n\u00f0\n\u00de\nMemory stall cycles\nInstruction\n\u00bc MissesL1\nInstructionHit timeL2 + MissesL2\nInstructionMisspenaltyL2\nFigure B.7 Summary of performance equations in this appendix. The first equation calculates the cache index size,\nand the rest help evaluate performance. The final two equations deal with multilevel caches, which are explained\nearly in the next section. They are included here to help make the figure a useful reference.\nB-22\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 729,
        "text": "The classical approach to improving cache behavior is to reduce miss rates, and\nwe present three techniques to do so. To gain better insights into the causes of mis-\nses, we first start with a model that sorts all misses into three simple categories:\n\u25a0\nCompulsory\u2014The very first access to a block cannot be in the cache, so the\nblock must be brought into the cache. These are also called cold-start misses\nor first-reference misses.\n\u25a0\nCapacity\u2014If the cache cannot contain all the blocks needed during execution\nof a program, capacity misses (in addition to compulsory misses) will occur\nbecause of blocks being discarded and later retrieved.\n\u25a0\nConflict\u2014If the block placement strategy is set associative or direct mapped,\nconflict misses (in addition to compulsory and capacity misses) will occur\nbecause a block may be discarded and later retrieved if too many blocks\nmap to its set. These misses are also called collision misses. The idea is that\nhits in a fully associative cache that become misses in an n-way set-associative\ncache are due to more than n requests on some popular sets.\n(Chapter 5 adds a fourth C, for coherency misses due to cache flushes to keep mul-\ntiple caches coherent in a multiprocessor; we won\u2019t consider those here.)\nFigure B.8 shows the relative frequency of cache misses, broken down by the\nthree C\u2019s. Compulsory misses are those that occur in an infinite cache. Capacity\nmisses are those that occur in a fully associative cache. Conflict misses are those\nthat occur going from fully associative to eight-way associative, four-way associa-\ntive, and so on. Figure B.9 presents the same data graphically. The top graph shows\nabsolute miss rates; the bottom graph plots the percentage of all the misses by type\nof miss as a function of cache size.\nTo show the benefit of associativity, conflict misses are divided into misses\ncaused by each decrease in associativity. Here are the four divisions of conflict\nmisses and how they are calculated:\n\u25a0\nEight-way\u2014Conflict misses due to going from fully associative (no conflicts)\nto eight-way associative\n\u25a0\nFour-way\u2014Conflict misses due to going from eight-way associative to four-\nway associative\n\u25a0\nTwo-way\u2014Conflict misses due to going from four-way associative to two-way\nassociative\n\u25a0\nOne-way\u2014Conflict misses due to going from two-way associative to one-way\nassociative (direct mapped)\nAs we can see from the figures, the compulsory miss rate of the SPEC2000\nprograms is very small, as it is for many long-running programs.\nHaving identified the three C\u2019s, what can a computer designer do about them?\nConceptually, conflicts are the easiest: Fully associative placement avoids all\nB.3\nSix Basic Cache Optimizations\n\u25a0\nB-23"
    },
    {
        "page": 730,
        "text": "Cache size (KiB)\nDegree\nassociative\nTotal miss\nrate\nMiss rate components (relative percent)\n(sum5100% of total miss rate)\nCompulsory\nCapacity\nConflict\n4\n1-way\n0.098\n0.0001\n0.1%\n0.070\n72%\n0.027\n28%\n4\n2-way\n0.076\n0.0001\n0.1%\n0.070\n93%\n0.005\n7%\n4\n4-way\n0.071\n0.0001\n0.1%\n0.070\n99%\n0.001\n1%\n4\n8-way\n0.071\n0.0001\n0.1%\n0.070\n100%\n0.000\n0%\n8\n1-way\n0.068\n0.0001\n0.1%\n0.044\n65%\n0.024\n35%\n8\n2-way\n0.049\n0.0001\n0.1%\n0.044\n90%\n0.005\n10%\n8\n4-way\n0.044\n0.0001\n0.1%\n0.044\n99%\n0.000\n1%\n8\n8-way\n0.044\n0.0001\n0.1%\n0.044\n100%\n0.000\n0%\n16\n1-way\n0.049\n0.0001\n0.1%\n0.040\n82%\n0.009\n17%\n16\n2-way\n0.041\n0.0001\n0.2%\n0.040\n98%\n0.001\n2%\n16\n4-way\n0.041\n0.0001\n0.2%\n0.040\n99%\n0.000\n0%\n16\n8-way\n0.041\n0.0001\n0.2%\n0.040\n100%\n0.000\n0%\n32\n1-way\n0.042\n0.0001\n0.2%\n0.037\n89%\n0.005\n11%\n32\n2-way\n0.038\n0.0001\n0.2%\n0.037\n99%\n0.000\n0%\n32\n4-way\n0.037\n0.0001\n0.2%\n0.037\n100%\n0.000\n0%\n32\n8-way\n0.037\n0.0001\n0.2%\n0.037\n100%\n0.000\n0%\n64\n1-way\n0.037\n0.0001\n0.2%\n0.028\n77%\n0.008\n23%\n64\n2-way\n0.031\n0.0001\n0.2%\n0.028\n91%\n0.003\n9%\n64\n4-way\n0.030\n0.0001\n0.2%\n0.028\n95%\n0.001\n4%\n64\n8-way\n0.029\n0.0001\n0.2%\n0.028\n97%\n0.001\n2%\n128\n1-way\n0.021\n0.0001\n0.3%\n0.019\n91%\n0.002\n8%\n128\n2-way\n0.019\n0.0001\n0.3%\n0.019\n100%\n0.000\n0%\n128\n4-way\n0.019\n0.0001\n0.3%\n0.019\n100%\n0.000\n0%\n128\n8-way\n0.019\n0.0001\n0.3%\n0.019\n100%\n0.000\n0%\n256\n1-way\n0.013\n0.0001\n0.5%\n0.012\n94%\n0.001\n6%\n256\n2-way\n0.012\n0.0001\n0.5%\n0.012\n99%\n0.000\n0%\n256\n4-way\n0.012\n0.0001\n0.5%\n0.012\n99%\n0.000\n0%\n256\n8-way\n0.012\n0.0001\n0.5%\n0.012\n99%\n0.000\n0%\n512\n1-way\n0.008\n0.0001\n0.8%\n0.005\n66%\n0.003\n33%\n512\n2-way\n0.007\n0.0001\n0.9%\n0.005\n71%\n0.002\n28%\n512\n4-way\n0.006\n0.0001\n1.1%\n0.005\n91%\n0.000\n8%\n512\n8-way\n0.006\n0.0001\n1.1%\n0.005\n95%\n0.000\n4%\nFigure B.8 Total miss rate for each size cache and percentage of each according to the three C\u2019s. Compulsory\nmisses are independent of cache size, while capacity misses decrease as capacity increases, and conflict misses\ndecrease as associativity increases. Figure B.9 shows the same information graphically. Note that a direct-mapped\ncache of size N has about the same miss rate as a two-way set-associative cache of size N/2 up through 128 K. Caches\nlarger than 128 KiB do not prove that rule. Note that the Capacity column is also the fully associative miss rate. Data\nwere collected as in Figure B.4 using LRU replacement.\nB-24\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 731,
        "text": "conflict misses. Full associativity is expensive in hardware, however, and may\nslow the processor clock rate (see the example on page B-29), leading to lower\noverall performance.\nThere is little to be done about capacity except to enlarge the cache. If the\nupper-level memory is much smaller than what is needed for a program, and a sig-\nnificant percentage of the time is spent moving data between two levels in the\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.10\ne\np\nyt\nr\ne\np\ne\nt\na\nr\ns\nsi\nM\ne\np\nyt\nr\ne\np\ne\nt\na\nr\ns\nsi\nM\n1024\n4\n8\n16\n32\n64\n128\n256\n512\n1-way\n2-way\n4-way\n8-way\nCapacity\nCompulsory\nCache size (KB)\n0%\n100%\n80%\n60%\n40%\n20%\nCache size (KB)\n4\n8\n16\n32\n64\n128\n256\n512\n1024\n1-way\n2-way\n4-way\n8-way\nCapacity\nCompulsory\nFigure B.9 Total miss rate (top) and distribution of miss rate (bottom) for each size\ncache according to the three C\u2019s for the data in Figure B.8. The top diagram shows the\nactual data cache miss rates, while the bottom diagram shows the percentage in each\ncategory. (Space allows the graphs to show one extra cache size than can fit in\nFigure B.8.)\nB.3\nSix Basic Cache Optimizations\n\u25a0\nB-25"
    },
    {
        "page": 732,
        "text": "hierarchy, the memory hierarchy is said to thrash. Because so many replacements\nare required, thrashing means the computer runs close to the speed of the lower-\nlevel memory, or maybe even slower because of the miss overhead.\nAnother approach to improving the three C\u2019s is to make blocks larger to reduce\nthe number of compulsory misses, but, as we will see shortly, large blocks can\nincrease other kinds of misses.\nThe three C\u2019s give insight into the cause of misses, but this simple model has its\nlimits; it gives you insight into average behavior but may not explain an individual\nmiss. For example, changing cache size changes conflict misses as well as capacity\nmisses, because a larger cache spreads out references to more blocks. Thus, a miss\nmight move from a capacity miss to a conflict miss as cache size changes. Simi-\nlarly, changing the block size can sometimes reduce capacity misses (in addition to\nthe expected reduction in compusolory misses), as Gupta et al. (2013) show.\nNote also that the three C\u2019s also ignore replacement policy, because it is dif-\nficult to model and because, in general, it is less significant. In specific circum-\nstances the replacement policy can actually lead to anomalous behavior, such as\npoorer miss rates for larger associativity, which contradicts the three C\u2019s model.\n(Some have proposed using an address trace to determine optimal placement in\nmemory to avoid placement misses from the three C\u2019s model; we\u2019ve not followed\nthat advice here.)\nAlas, many of the techniques that reduce miss rates also increase hit time or\nmiss penalty. The desirability of reducing miss rates using the three optimizations\nmust be balanced against the goal of making the whole system fast. This first exam-\nple shows the importance of a balanced perspective.\nFirst Optimization: Larger Block Size to Reduce Miss Rate\nThe simplest way to reduce miss rate is to increase the block size. Figure B.10\nshows the trade-off of block size versus miss rate for a set of programs and cache\nsizes. Larger block sizes will reduce also compulsory misses. This reduction occurs\nbecause the principle of locality has two components: temporal locality and spatial\nlocality. Larger blocks take advantage of spatial locality.\nAt the same time, larger blocks increase the miss penalty. Because they reduce\nthe number of blocks in the cache, larger blocks may increase conflict misses and\neven capacity misses if the cache is small. Clearly, there is little reason to increase\nthe block size to such a size that it increases the miss rate. There is also no benefit to\nreducing miss rate if it increases the average memory access time. The increase in\nmiss penalty may outweigh the decrease in miss rate.\nExample\nFigure B.11 shows the actual miss rates plotted in Figure B.10. Assume the mem-\nory system takes 80 clock cycles of overhead and then delivers 16 bytes every 2\nclock cycles. Thus, it can supply 16 bytes in 82 clock cycles, 32 bytes in 84 clock\ncycles, and so on. Which block size has the smallest average memory access time\nfor each cache size in Figure B.11?\nB-26\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 733,
        "text": "Answer\nAverage memory access time is\nAverage memory access time \u00bc Hit time + Miss rateMisspenalty\nIf we assume the hit time is 1 clock cycle independent of block size, then the access\ntime for a 16-byte block in a 4 KiB cache is\nAverage memory access time \u00bc 1 + 8:57%82\n\u00f0\n\u00de \u00bc 8:027 clock cycles\nand for a 256-byte block in a 256 KiB cache the average memory access time is\nAverage memory access time \u00bc 1 + 0:49%112\n\u00f0\n\u00de \u00bc 1:549 clock cycles\nBlock size\n16\n32\n64\n128\n256\nMiss rate\n5%\n10%\n0%\n64K\n16K\n4K\n256K\nFigure B.10 Miss rate versus block size for five different-sized caches. Note that miss\nrate actually goes up if the block size is too large relative to the cache size. Each line\nrepresents a cache of different size. Figure B.11 shows the data used to plot these lines.\nUnfortunately, SPEC2000 traces would take too long if block size were included, so these\ndata are based on SPEC92 on a DECstation 5000 (Gee et al. 1993).\nCache size\nBlock size\n4K\n16K\n64K\n256K\n16\n8.57%\n3.94%\n2.04%\n1.09%\n32\n7.24%\n2.87%\n1.35%\n0.70%\n64\n7.00%\n2.64%\n1.06%\n0.51%\n128\n7.78%\n2.77%\n1.02%\n0.49%\n256\n9.51%\n3.29%\n1.15%\n0.49%\nFigure B.11 Actual miss rate versus block size for the five different-sized caches in\nFigure B.10. Note that for a 4 KiB cache, 256-byte blocks have a higher miss rate than 32-\nbyte blocks. In this example, the cache would have to be 256 KiB in order for a 256-byte\nblock to decrease misses.\nB.3\nSix Basic Cache Optimizations\n\u25a0\nB-27"
    },
    {
        "page": 734,
        "text": "Figure B.12 shows the average memory access time for all block and cache sizes\nbetween those two extremes. The boldfaced entries show the fastest block size for a\ngiven cache size: 32 bytes for 4 KiB and 64 bytes for the larger caches. These sizes\nare, in fact, popular block sizes for processor caches today.\nAs in all of these techniques, the cache designer is trying to minimize both the\nmiss rate and the miss penalty. The selection of block size depends on both the\nlatency and bandwidth of the lower-level memory. High latency and high bandwidth\nencourage large block size because the cache gets many more bytes per miss for a\nsmall increase in miss penalty. Conversely, low latency and low bandwidth encour-\nage smaller block sizes because there is little time saved from a larger block. For\nexample, twice the miss penalty of a small block may be close to the penalty of a\nblock twice thesize.Thelargernumber of small blocksmayalsoreduce conflictmis-\nses. Note that Figures B.10 and B.12 show the difference between selecting a block\nsize based on minimizing miss rate versus minimizing average memory access time.\nAfter seeing the positive and negative impact of larger block size on compul-\nsory and capacity misses, the next two subsections look at the potential of higher\ncapacity and higher associativity.\nSecond Optimization: Larger Caches to Reduce Miss Rate\nThe obvious way to reduce capacity misses in Figures B.8 and B.9 is to increase\ncapacityofthecache.Theobviousdrawbackispotentiallylonger hittimeandhigher\ncost and power. This technique has been especially popular in off-chip caches.\nThird Optimization: Higher Associativity to Reduce Miss Rate\nFigures B.8 and B.9 show how miss rates improve with higher associativity. There\nare two general rules of thumb that can be gleaned from these figures. The first is\nCache size\nBlock size\nMiss penalty\n4K\n16K\n64K\n256K\n16\n82\n8.027\n4.231\n2.673\n1.894\n32\n84\n7.082\n3.411\n2.134\n1.588\n64\n88\n7.160\n3.323\n1.933\n1.449\n128\n96\n8.469\n3.659\n1.979\n1.470\n256\n112\n11.651\n4.685\n2.288\n1.549\nFigure B.12 Average memory access time versus block size for five different-sized\ncaches in Figure B.10. Block sizes of 32 and 64 bytes dominate. The smallest average\ntime per cache size is boldfaced.\nB-28\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 735,
        "text": "that eight-way set associative is for practical purposes as effective in reducing mis-\nses for these sized caches as fully associative. You can see the difference by com-\nparing the eight-way entries to the capacity miss column in Figure B.8, because\ncapacity misses are calculated using fully associative caches.\nThe second observation, called the 2:1 cache rule of thumb, is that a direct-\nmapped cache of size N has about the same miss rate as a two-way set associative\ncache of size N/2. This held in three C\u2019s figures for cache sizes less than 128 KiB.\nLike many of these examples, improving one aspect of the average memory\naccess time comes at the expense of another. Increasing block size reduces miss\nrate while increasing miss penalty, and greater associativity can come at the cost\nof increased hit time. Hence, the pressure of a fast processor clock cycle encour-\nages simple cache designs, but the increasing miss penalty rewards associativity, as\nthe following example suggests.\nExample\nAssume that higher associativity would increase the clock cycle time as listed as\nfollows:\nClock cycle time2-way \u00bc 1:36Clock cycle time1-way\nClock cycle time4-way \u00bc 1:44Clock cycle time1-way\nClock cycle time8-way \u00bc 1:52Clock cycle time1-way\nAssume that the hit time is 1 clock cycle, that the miss penalty for the direct-\nmapped case is 25 clock cycles to a level 2 cache (see next subsection) that never\nmisses, and that the miss penalty need not be rounded to an integral number of\nclock cycles. Using Figure B.8 for miss rates, for which cache sizes are each of\nthese three statements true?\nAverage memoryaccess time8-way < Average memory access time4-way\nAverage memoryaccess time4-way < Average memory access time2-way\nAverage memoryaccess time2-way < Average memory access time1-way\nAnswer\nAverage memory access time for each associativity is\nAverage memory access time8-way \u00bc Hit time8-way + Miss rate8-way Miss penalty8-way\n\u00bc 1:52 + Miss rate8-way 25\nAverage memory access time4-way \u00bc 1:44 + Miss rate4-way 25\nAverage memory access time2-way \u00bc 1:36 + Miss rate2-way 25\nAverage memory access time1-way \u00bc 1:00 + Miss rate1-way 25\nThe miss penalty is the same time in each case, so we leave it as 25 clock cycles.\nFor example, the average memory access time for a 4 KiB direct-mapped cache is\nAverage memory access time1-way \u00bc 1:00 + 0:09825\n\u00f0\n\u00de \u00bc 3:44\nand the time for a 512 KiB, eight-way set associative cache is\nAverage memory access time8-way \u00bc 1:52 + 0:00625\n\u00f0\n\u00de \u00bc 1:66\nUsing these formulas and the miss rates from Figure B.8, Figure B.13 shows the\naverage memory access time for each cache and associativity. The figure shows\nB.3\nSix Basic Cache Optimizations\n\u25a0\nB-29"
    },
    {
        "page": 736,
        "text": "that the formulas in this example hold for caches less than or equal to 8 KiB for up\nto four-way associativity. Starting with 16 KiB, the greater hit time of larger asso-\nciativity outweighs the time saved due to the reduction in misses.\nNote that we did not account for the slower clock rate on the rest of the program\nin this example, thereby understating the advantage of direct-mapped cache.\nFourth Optimization: Multilevel Caches to Reduce\nMiss Penalty\nReducing cache misses had been the traditional focus of cache research, but the\ncache performance formula assures us that improvements in miss penalty can\nbe just as beneficial as improvements in miss rate. Moreover, Figure 2.2 on page 80\nshows that technology trends have improved the speed of processors faster than\nDRAMs, making the relative cost of miss penalties increase over time.\nThis performance gap between processors and memory leads the architect to\nthis question: Should I make the cache faster to keep pace with the speed of pro-\ncessors, or make the cache larger to overcome the widening gap between the pro-\ncessor and main memory?\nOne answer is, do both. Adding another level of cache between the original\ncache and memory simplifies the decision. The first-level cache can be small\nenough to match the clock cycle time of the fast processor. Yet, the second-level\ncache can be large enough to capture many accesses that would go to main mem-\nory, thereby lessening the effective miss penalty.\nAlthough the concept of adding another level in the hierarchy is straightfor-\nward, it complicates performance analysis. Definitions for a second level of cache\nare not always straightforward. Let\u2019s start with the definition of average memory\naccess time for a two-level cache. Using the subscripts L1 and L2 to refer, respec-\ntively, to a first-level and a second-level cache, the original formula is\nAssociativity\nCache size (KiB)\n1-way\n2-way\n4-way\n8-way\n4\n3.44\n3.25\n3.22\n3.28\n8\n2.69\n2.58\n2.55\n2.62\n16\n2.23\n2.40\n2.46\n2.53\n32\n2.06\n2.30\n2.37\n2.45\n64\n1.92\n2.14\n2.18\n2.25\n128\n1.52\n1.84\n1.92\n2.00\n256\n1.32\n1.66\n1.74\n1.82\n512\n1.20\n1.55\n1.59\n1.66\nFigure B.13 Average memory access time using miss rates in Figure B.8 for param-\neters in the example. Boldface type means that this time is higher than the number to\nthe left, that is, higher associativity increases average memory access time.\nB-30\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 737,
        "text": "Average memory access time \u00bc Hit timeL1 + MissrateL1 MisspenaltyL1\nand\nMisspenaltyL1 \u00bc Hit timeL2 + MissrateL2 Miss penaltyL2\nso\nAverage memory access time \u00bc Hit timeL1 + MissrateL1\n Hit timeL2 + MissrateL2 Miss penaltyL2\n\u00f0\n\u00de\nIn this formula, the second-level miss rate is measured on the leftovers from the\nfirst-level cache. To avoid ambiguity, these terms are adopted here for a two-level\ncache system:\n\u25a0\nLocal miss rate\u2014This rate is simply the number of misses in a cache divided\nby the total number of memory accesses to this cache. As you would expect, for\nthe first-level cache it is equal to Miss rateL1, and for the second-level cache it is\nMiss rateL2.\n\u25a0\nGlobal miss rate\u2014The number of misses in the cache divided by the total num-\nber of memory accesses generated by the processor. Using the terms above, the\nglobal miss rate for the first-level cache is still just Miss rateL1, but for the\nsecond-level cache it is Miss rateL1Miss rateL2.\nThis local miss rate is large for second-level caches because the first-level\ncache skims the cream of the memory accesses. This is why the global miss rate\nis the more useful measure: It indicates what fraction of the memory accesses that\nleave the processor go all the way to memory.\nHere is a place where the misses per instruction metric shines. Instead of con-\nfusion about local or global miss rates, we just expand memory stalls per instruc-\ntion to add the impact of a second-level cache.\nAverage memory stalls per instruction \u00bc Misses per instructionL1 Hit timeL2\n+ Misses per instructionL2 Miss penaltyL2\nExample\nSuppose that in 1000 memory references there are 40 misses in the first-level cache\nand 20 misses in the second-level cache. What are the various miss rates? Assume\nthe miss penalty from the L2 cache to memory is 200 clock cycles, the hit time of\nthe L2 cache is 10 clock cycles, the hit time of L1 is 1 clock cycle, and there are 1.5\nmemory references per instruction. What is the average memory access time and\naverage stall cycles per instruction? Ignore the impact of writes.\nAnswer\nThe miss rate (either local or global) for the first-level cache is 40/1000 or 4%. The\nlocal miss rate for the second-level cache is 20/40 or 50%. The global miss rate of\nthe second-level cache is 20/1000 or 2%. Then\nAverage memory access time \u00bc Hit timeL1 + Miss rateL1  Hit timeL2 + Miss rateL2 Miss penaltyL2\n\u00f0\n\u00de\n\u00bc 1 + 4% 10 + 50%200\n\u00f0\n\u00de \u00bc 1 + 4%110 \u00bc 5:4 clock cycles\nB.3\nSix Basic Cache Optimizations\n\u25a0\nB-31"
    },
    {
        "page": 738,
        "text": "To see how many misses we get per instruction, we divide 1000 memory refer-\nences by 1.5 memory references per instruction, which yields 667 instructions.\nThus, we need to multiply the misses by 1.5 to get the number of misses per\n1000 instructions. We have 401.5 or 60 L1 misses, and 201.5 or 30 L2 mis-\nses, per 1000 instructions. For average memory stalls per instruction, assuming the\nmisses are distributed uniformly between instructions and data:\nAverage memorystalls per instruction \u00bc Misses per instructionL1 Hit timeL2 + Misses per instructionL2\nMisspenaltyL2\n\u00bc 60=1000\n\u00f0\n\u00de10 + 30=1000\n\u00f0\n\u00de200\n\u00bc 0:06010 + 0:030200 \u00bc 6:6 clock cycles\nIf we subtract the L1 hit time from the average memory access time (AMAT) and\nthen multiply by the average number of memory references per instruction, we get\nthe same average memory stalls per instruction:\n5:4\u00031:0\n\u00f0\n\u00de1:5 \u00bc 4:41:5 \u00bc 6:6 clock cycles\nAs this example shows, there may be less confusion with multilevel caches when\ncalculating using misses per instruction versus miss rates.\nNote that these formulas are for combined reads and writes, assuming a write-\nback first-level cache. Obviously, a write-through first-level cache will send all\nwrites to the second level, not just the misses, and a write buffer might be used.\nFigures B.14 and B.15 show how miss rates and relative execution time change\nwith the size of a second-level cache for one design. From these figures we can gain\ntwo insights. The first is that the global cache miss rate is very similar to the single\ncache miss rate of the second-level cache, provided that the second-level cache is\nmuch larger than the first-level cache. Hence, our intuition and knowledge about\nthe first-level caches apply. The second insight is that the local cache miss rate is\nnot a good measure of secondary caches; it is a function of the miss rate of the first-\nlevel cache, and hence can vary by changing the first-level cache. Thus, the global\ncache miss rate should be used when evaluating second-level caches.\nWith these definitions in place, we can consider the parameters of second-level\ncaches. The foremost difference between the two levels is that the speed of the first-\nlevel cache affects the clock rate of the processor, while the speed of the second-\nlevel cache only affects the miss penalty of the first-level cache. Thus, we can con-\nsider many alternatives in the second-level cache that would be ill chosen for the\nfirst-level cache. There are two major questions for the design of the second-level\ncache: Will it lower the average memory access time portion of the CPI, and how\nmuch does it cost?\nThe initial decision is the size of a second-level cache. Since everything in the\nfirst-level cache is likely to be in the second-level cache, the second-level cache\nshould be much bigger than the first. If second-level caches are just a little bigger,\nthe local miss rate will be high. This observation inspires the design of huge\nsecond-level caches\u2014the size of main memory in older computers!\nB-32\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 739,
        "text": "One question is whether set associativity makes more sense for second-level\ncaches.\nExample\nGiven the following data, what is the impact of second-level cache associativity on\nits miss penalty?\n\u25a0\nHit timeL2 for direct mapped\u00bc10 clock cycles.\n\u25a0\nTwo-way set associativity increases hit time by 0.1 clock cycle to 10.1 clock\ncycles.\n\u25a0\nLocal miss rateL2 for direct mapped\u00bc25%.\n\u25a0\nLocal miss rateL2 for two-way set associative\u00bc20%.\n\u25a0\nMiss penaltyL2\u00bc200 clock cycles.\nAnswer\nFor a direct-mapped second-level cache, the first-level cache miss penalty is\nMisspenalty1-way L2 \u00bc 10 + 25%200 \u00bc 60:0 clock cycles\n100%\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\nMiss rate\n99%\n99%\n98%\n4\n8\n16\n32\n64\n128\n256\n512\n1024 2048 4096\nCache size (KB)\n96%\n55%\n6%\n5%\n4%\n4%\n4%\n3%\n3%\n3%\n2%\n2%\n2%\n1%\n1%\n4%\n4%\n46%\n39%\n34%\n51%\n88%\n67%\nLocal miss rate\nGlobal miss rate\nSingle cache miss rate\nFigure B.14 Miss rates versus cache size for multilevel caches. Second-level caches\nsmaller than the sum of the two 64 KiB first-level caches make little sense, as reflected\nin the high miss rates. After 256 KiB the single cache is within 10% of the global miss\nrates. The miss rate of a single-level cache versus size is plotted against the local miss\nrate and global miss rate of a second-level cache using a 32 KiB first-level cache. The L2\ncaches (unified) were two-way set associative with replacement. Each had split L1\ninstruction and data caches that were 64 KiB two-way set associative with LRU replace-\nment. The block size for both L1 and L2 caches was 64 bytes. Data were collected as in\nFigure B.4.\nB.3\nSix Basic Cache Optimizations\n\u25a0\nB-33"
    },
    {
        "page": 740,
        "text": "Adding the cost of associativity increases the hit cost only 0.1 clock cycle, making\nthe new first-level cache miss penalty:\nMisspenalty2-way L2 \u00bc 10:1 + 20%200 \u00bc 50:1 clock cycles\nIn reality, second-level caches are almost always synchronized with the first-level\ncache and processor. Accordingly, the second-level hit time must be an integral\nnumber of clock cycles. If we are lucky, we shave the second-level hit time to\n10 cycles; if not, we round up to 11 cycles. Either choice is an improvement over\nthe direct-mapped second-level cache:\nMisspenalty2-way L2 \u00bc 10 + 20%200 \u00bc 50:0 clock cycles\nMisspenalty2-way L2 \u00bc 11 + 20%200 \u00bc 51:0 clock cycles\nNow we can reduce the miss penalty by reducing the miss rate of the second-level\ncaches.\nAnother consideration concerns whether data in the first-level cache are in the\nsecond-level cache. Multilevel inclusion is the natural policy for memory hierar-\nchies: L1 data are always present in L2. Inclusion is desirable because consistency\nbetween I/O and caches (or among caches in a multiprocessor) can be determined\njust by checking the second-level cache.\n8192\nSecond-level cache size (KB)\n4096\n2048\n1024\n512\n256\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\nRelative execution time\n1.60\n1.65\n1.10\n1.14\n1.02\n1.06\n2.34\n2.39\n1.94\n1.99\n1.76\n1.82\nL2 hit=8 clock cycles\nL2 hit=16 clock cycles\nFigure B.15 Relative execution time by second-level cache size. The two bars are for\ndifferent clock cycles for an L2 cache hit. The reference execution time of 1.00 is for an\n8192 KiB second-level cache with a 1-clock-cycle latency on a second-level hit. These\ndata were collected the same way as in Figure B.14, using a simulator to imitate the\nAlpha 21264.\nB-34\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 741,
        "text": "One drawback to inclusion is that measurements can suggest smaller blocks for\nthe smaller first-level cache and larger blocks for the larger second-level cache. For\nexample, the Pentium 4 has 64-byte blocks in its L1 caches and 128-byte blocks in\nits L2 cache. Inclusion can still be maintained with more work on a second-level\nmiss. The second-level cache must invalidate all first-level blocks that map onto\nthe second-level block to be replaced, causing a slightly higher first-level miss rate.\nTo avoid such problems, many cache designers keep the block size the same in all\nlevels of caches.\nHowever, what if the designer can only afford an L2 cache that is slightly big-\nger than the L1 cache? Should a significant portion of its space be used as a redun-\ndant copy of the L1 cache? In such cases a sensible opposite policy is multilevel\nexclusion: L1 data are never found in an L2 cache. Typically, with exclusion a\ncache miss in L1 results in a swap of blocks between L1 and L2 instead of a\nreplacement of an L1 block with an L2 block. This policy prevents wasting space\nin the L2 cache. For example, the AMD Opteron chip obeys the exclusion property\nusing two 64 KiB L1 caches and 1 MiB L2 cache.\nAs these issues illustrate, although a novice might design the first- and second-\nlevel caches independently, the designer of the first-level cache has a simpler job\ngiven a compatible second-level cache. It is less of a gamble to use a write through,\nfor example, if there is a write-back cache at the next level to act as a backstop for\nrepeated writes and it uses multilevel inclusion.\nThe essence of all cache designs is balancing fast hits and few misses. For\nsecond-level caches, there are far fewer hits than in the first-level cache, so the\nemphasis shifts to fewer misses. This insight leads to much larger caches and tech-\nniques to lower the miss rate, such as higher associativity and larger blocks.\nFifth Optimization: Giving Priority to Read Misses over Writes to\nReduce Miss Penalty\nThis optimization serves reads before writes have been completed. We start with\nlooking at the complexities of a write buffer.\nWith a write-through cache the most important improvement is a write buffer\nof the proper size. Write buffers, however, do complicate memory accesses\nbecause they might hold the updated value of a location needed on a read miss.\nExample\nLook at this code sequence:\nsd x3, 512(x0);M[512] \u00ac R3 (cache index 0)\nld x1, 1024(x0);x1 \u00ac M[1024](cache index 0)\nld x2, 512(x0);x2 \u00ac M[512] (cache index 0)\nAssume a direct-mapped, write-through cache that maps 512 and 1024 to the same\nblock, and a four-word write buffer that is not checked on a read miss. Will the\nvalue in x2 always be equal to the value in x3?\nB.3\nSix Basic Cache Optimizations\n\u25a0\nB-35"
    },
    {
        "page": 742,
        "text": "Answer\nUsing the terminology from Chapter 2, this is a read-after-write data hazard in\nmemory. Let\u2019s follow a cache access to see the danger. The data in x3 are placed\ninto the write buffer after the store. The following load uses the same cache index\nand is therefore a miss. The second load instruction tries to put the value in location\n512 into register x2; this also results in a miss. If the write buffer hasn\u2019t completed\nwriting to location 512 in memory, the read of location 512 will put the old, wrong\nvalue into the cache block, and then into x2. Without proper precautions, x3x1\nwould not be equal to x2!\nThe simplest way out of this dilemma is for the read miss to wait until the write\nbuffer is empty. The alternative is to check the contents of the write buffer on a read\nmiss, and if there are no conflicts and the memory system is available, let the read\nmiss continue. Virtually all desktop and server processors use the latter approach,\ngiving reads priority over writes.\nThe cost of writes by the processor in a write-back cache can also be reduced.\nSuppose a read miss will replace a dirty memory block. Instead of writing the dirty\nblock to memory, and then reading memory, we could copy the dirty block to a\nbuffer, then read memory, and then write memory. This way the processor read,\nfor which the processor is probably waiting, will finish sooner. Similar to the pre-\nvious situation, if a read miss occurs, the processor can either stall until the buffer is\nempty or check the addresses of the words in the buffer for conflicts.\nNow that we have five optimizations that reduce cache miss penalties or miss\nrates, it is time to look at reducing the final component of average memory access\ntime. Hit time is critical because it can affect the clock rate of the processor; in\nmany processors today the cache access time limits the clock cycle rate, even\nfor processors that take multiple clock cycles to access the cache. Hence, a fast\nhit time is multiplied in importance beyond the average memory access time for-\nmula because it helps everything.\nSixth Optimization: Avoiding Address Translation During\nIndexing of the Cache to Reduce Hit Time\nEven a small and simple cache must cope with the translation of a virtual address\nfrom the processor to a physical address to access memory. As described in\nSection B.4, processors treat main memory as just another level of the memory\nhierarchy, and thus the address of the virtual memory that exists on disk must\nbe mapped onto the main memory.\nThe guideline of making the common case fast suggests that we use virtual\naddresses for the cache, because hits are much more common than misses. Such\ncaches are termed virtual caches, with physical cache used to identify the tradi-\ntional cache that uses physical addresses. As we will shortly see, it is important\nto distinguish two tasks: indexing the cache and comparing addresses. Thus, the\nissues are whether a virtual or physical address is used to index the cache and\nB-36\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 743,
        "text": "whether a virtual or physical address is used in the tag comparison. Full virtual\naddressing for both indices and tags eliminates address translation time from a\ncache hit. Then why doesn\u2019t everyone build virtually addressed caches?\nOne reason is protection. Page-level protection is checked as part of the virtual\nto physical address translation, and it must be enforced no matter what. One solu-\ntion is to copy the protection information from the TLB on a miss, add a field to\nhold it, and check it on every access to the virtually addressed cache.\nAnother reason is that every time a process is switched, the virtual addresses\nrefer to different physical addresses, requiring the cache to be flushed. Figure B.16\nshows the impact on miss rates of this flushing. One solution is to increase the\nwidth of the cache address tag with a process-identifier tag (PID). If the operating\nsystem assigns these tags to processes, it only need flush the cache when a PID is\nMiss rate\n20%\n18%\n16%\n14%\n12%\n10%\n8%\n6%\n4%\n2%\n0%\n0.6%\n0.4%\n18.8%\n1.1%\n0.5%\n13.0%\n1.8%\n0.6%\n8.7%\n2.7%\n0.6%\n3.9%\n3.4%\n0.4%\n2.7%\n3.9%\n0.4%\n0.9%\n4.1%\n0.3%\n0.4%\n4.3%\n0.3%\n0.3%\n4.3%\n0.3%\n0.3%\n4.3%\n0.3%\n0.3%\nCache size\n2K\n4K\n8K\n16K\n32K\n64K\n128K\n256K\n512K\n1024K\nPurge\nPIDs\nUniprocess\nFigure B.16 Miss rate versus virtually addressed cache size of a program measured\nthree ways: without process switches (uniprocess), with process switches using a\nprocess-identifier tag (PID), and with process switches but without PIDs (purge). PIDs\nincrease the uniprocess absolute miss rate by 0.3%\u20130.6% and save 0.6%\u20134.3% over\npurging. Agarwal (1987) collected these statistics for the Ultrix operating system run-\nning on a VAX, assuming direct-mapped caches with a block size of 16 bytes. Note that\nthe miss rate goes up from 128 to 256 K. Such nonintuitive behavior can occur in caches\nbecause changing size changes the mapping of memory blocks onto cache blocks,\nwhich can change the conflict miss rate.\nB.3\nSix Basic Cache Optimizations\n\u25a0\nB-37"
    },
    {
        "page": 744,
        "text": "recycled; that is, the PID distinguishes whether or not the data in the cache are for\nthis program. Figure B.16 shows the improvement in miss rates by using PIDs to\navoid cache flushes.\nA third reason why virtual caches are not more popular is that operating sys-\ntems and user programs may use two different virtual addresses for the same phys-\nical address. These duplicate addresses, called synonyms or aliases, could result in\ntwo copies of the same data in a virtual cache; if one is modified, the other will have\nthe wrong value. With a physical cache this wouldn\u2019t happen, because the accesses\nwould first be translated to the same physical cache block.\nHardware solutions to the synonym problem, called antialiasing, guarantee\nevery cache block a unique physical address. For example, the AMD Opteron uses\na 64 KiB instruction cache with a 4 KiB page and two-way set associativity; hence,\nthe hardware must handle aliases involved with the three virtual address bits in\nthe set index. It avoids aliases by simply checking all eight possible locations\non a miss\u2014two blocks in each of four sets\u2014to be sure that none matches the phys-\nical address of the data being fetched. If one is found, it is invalidated, so when\nthe new data are loaded into the cache their physical address is guaranteed to be\nunique.\nSoftware can make this problem much easier by forcing aliases to share some\naddress bits. An older version of UNIX from Sun Microsystems, for example,\nrequired all aliases to be identical in the last 18 bits of their addresses; this restric-\ntion is called page coloring. Note that page coloring is simply set associative map-\nping applied to virtual memory: the 4 KiB (212) pages are mapped using 64 (26)\nsets to ensure that the physical and virtual addresses match in the last 18 bits. This\nrestriction means a direct-mapped cache that is 218 (256 K) bytes or smaller can\nnever have duplicate physical addresses for blocks. From the perspective of the\ncache, page coloring effectively increases the page offset, as software guarantees\nthat the last few bits of the virtual and physical page address are identical.\nThe final area of concern with virtual addresses is I/O. I/O typically uses phys-\nical addresses and thus would require mapping to virtual addresses to interact with\na virtual cache. (The impact of I/O on caches is further discussed in Appendix D.)\nOne alternative to get the best of both virtual and physical caches is to use part\nof the page offset\u2014the part that is identical in both virtual and physical\naddresses\u2014to index the cache. At the same time as the cache is being read using\nthat index, the virtual part of the address is translated, and the tag match uses phys-\nical addresses.\nThis alternative allows the cache read to begin immediately, and yet the tag\ncomparison is still with physical addresses. The limitation of this virtually indexed,\nphysically tagged alternative is that a direct-mapped cache can be no bigger than\nthe page size. For example, in the data cache in Figure B.5 on page B-13, the index\nis 9 bits and the cache block offset is 6 bits. To use this trick, the virtual page size\nwould have to be at least 2(9+6) bytes or 32 KiB. If not, a portion of the index must\nbe translated from virtual to physical address. Figure B.17 shows the organization\nof the caches, translation lookaside buffers (TLBs), and virtual memory when this\ntechnique is used.\nB-38\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 745,
        "text": "Associativity can keep the index in the physical part of the address and yet still\nsupport a large cache. Recall that the size of the index is controlled by this formula:\n2Index \u00bc\nCache size\nBlocksizeSet associativity\nFor example, doubling associativity and doubling the cache size does not change\nthe size of the index. The IBM 3033 cache, as an extreme example, is 16-way set\nassociative, even though studies show there is little benefit to miss rates above\nL1 tag compare address <26>\nL2 cache tag <21>\nL2 data <512>\n=?\n=?\n=?\nTLB tag compare address <43> TLB index <7>\nVirtual address <64>\nPhysical address <40>\nVirtual page number <50>\nL2 tag compare address <21> L2 cache index <14> Block offset  <6>\nPage offset  <14>\nL1 cache tag <26>\nL1 data <512>\nTo CPU\nTo CPU\nTo CPU\nTo L1 cache or CPU\nL1 cache index <8> Block offset <6>\nTLB tag <43>\nTLB data <26>\nFigure B.17 The overall picture of a hypothetical memory hierarchy going from virtual address to L2 cache\naccess. The page size is 16 KiB. The TLB is two-way set associative with 256 entries. The L1 cache is a direct-mapped\n16 KiB, and the L2 cache is a four-way set associative with a total of 4 MiB. Both use 64-byte blocks. The virtual address\nis 64 bits and the physical address is 40 bits.\nB.3\nSix Basic Cache Optimizations\n\u25a0\nB-39"
    },
    {
        "page": 746,
        "text": "8-way set associativity. This high associativity allows a 64 KiB cache to be\naddressed with a physical index, despite the handicap of 4 KiB pages in the\nIBM architecture.\nSummary of Basic Cache Optimization\nThe techniques in this section to improve miss rate, miss penalty, and hit time gen-\nerally impact the other components of the average memory access equation as well\nas the complexity of the memory hierarchy. Figure B.18 summarizes these tech-\nniques and estimates the impact on complexity, with + meaning that the technique\nimproves the factor, \u2013 meaning it hurts that factor, and blank meaning it has no\nimpact. No optimization in this figure helps more than one category.\nB.4\nVirtual Memory\n\u2026 a system has been devised to make the core drum combination appear to\nthe programmer as a single level store, the requisite transfers taking place\nautomatically.\nKilburn et al. (1962)\nAt any instant in time computers are running multiple processes, each with its own\naddress space. (Processes are described in the next section.) It would be too expen-\nsive to dedicate a full address space worth of memory for each process, especially\nbecause many processes use only a small part of their address space. Hence, there\nmust be a means of sharing a smaller amount of physical memory among many\nprocesses.\nTechnique\nHit\ntime\nMiss\npenalty\nMiss\nrate\nHardware\ncomplexity\nComment\nLarger block size\n\u2013\n+\n0\nTrivial; Pentium 4L2 uses 128 bytes\nLarger cache size\n\u2013\n+\n1\nWidely used, especially for L2\ncaches\nHigher associativity\n\u2013\n+\n1\nWidely used\nMultilevel caches\n+\n2\nCostly hardware; harder if L1 block\nsize6\u00bcL2 block size; widely used\nRead priority over writes\n+\n1\nWidely used\nAvoiding address translation during\ncache indexing\n+\n1\nWidely used\nFigure B.18 Summary of basic cache optimizations showing impact on cache performance and complexity for\nthe techniques in this appendix. Generally a technique helps only one factor. + means that the technique improves\nthe factor, \u2013 means it hurts that factor, and blank means it has no impact. The complexity measure is subjective, with\n0 being the easiest and 3 being a challenge.\nB-40\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 747,
        "text": "One way to do this, virtual memory, divides physical memory into blocks and\nallocates them to different processes. Inherent in such an approach must be a pro-\ntection scheme that restricts a process to the blocks belonging only to that process.\nMost forms of virtual memory also reduce the time to start a program, because not\nall code and data need be in physical memory before a program can begin.\nAlthough protection provided by virtual memory is essential for current com-\nputers, sharing is not the reason that virtual memory was invented. If a program\nbecame too large for physical memory, it was the programmer\u2019s job to make it\nfit. Programmers divided programs into pieces, then identified the pieces that were\nmutually exclusive, and loaded or unloaded these overlays under user program\ncontrol during execution. The programmer ensured that the program never tried\nto access more physical main memory than was in the computer, and that the\nproper overlay was loaded at the proper time. As you can well imagine, this respon-\nsibility eroded programmer productivity.\nVirtual memory was invented to relieve programmers of this burden; it auto-\nmatically manages the two levels of the memory hierarchy represented by main\nmemory and secondary storage. Figure B.19 shows the mapping of virtual memory\nto physical memory for a program with four pages.\nIn addition to sharing protected memory space and automatically managing the\nmemory hierarchy, virtual memory also simplifies loading the program for execu-\ntion. Called relocation, this mechanism allows the same program to run in any\nlocation in physical memory. The program in Figure B.19 can be placed anywhere\n0\n4K\n8K\n12K\n16K\n20K\n24K\n28K\nPhysical\naddress\nPhysical\nmain memory\nDisk\nD\n0\n4K\n8K\n12K\nVirtual\naddress\nVirtual memory\nA\nB\nC\nD\nC\nA\nB\nFigure B.19 The logical program in its contiguous virtual address space is shown on\nthe left. It consists of four pages, A, B, C, and D. The actual location of three of the blocks\nis in physical main memory and the other is located on the disk.\nB.4\nVirtual Memory\n\u25a0\nB-41"
    },
    {
        "page": 748,
        "text": "in physical memory or disk just by changing the mapping between them. (Prior to\nthe popularity of virtual memory, processors would include a relocation register\njust for that purpose.) An alternative to a hardware solution would be software that\nchanged all addresses in a program each time it was run.\nSeveral general memory hierarchy ideas from Chapter 1 about caches are anal-\nogous to virtual memory, although many of the terms are different. Page or seg-\nment is used for block, and page fault or address fault is used for miss. With virtual\nmemory, the processor produces virtual addresses that are translated by a combi-\nnation of hardware and software to physical addresses, which access main mem-\nory. This process is called memory mapping or address translation. Today, the two\nmemory hierarchy levels controlled by virtual memory are DRAMs and magnetic\ndisks. Figure B.20 shows a typical range of memory hierarchy parameters for vir-\ntual memory.\nThere are further differences between caches and virtual memory beyond those\nquantitative ones mentioned in Figure B.20:\n\u25a0\nReplacement on cache misses is primarily controlled by hardware, while vir-\ntual memory replacement is primarily controlled by the operating system. The\nlonger miss penalty means it\u2019s more important to make a good decision, so\nthe operating system can be involved and take time deciding what to replace.\n\u25a0\nThe size of the processor address determines the size of virtual memory, but the\ncache size is independent of the processor address size.\n\u25a0\nIn addition to acting as the lower-level backing store for main memory in\nthe hierarchy, secondary storage is also used for the file system. In fact, the\nfile system occupies most of secondary storage. It is not usually in the\naddress space.\nParameter\nFirst-level cache\nVirtual memory\nBlock (page) size\n16\u2013128 bytes\n4096\u201365,536 bytes\nHit time\n1\u20133 clock cycles\n100\u2013200 clock cycles\nMiss penalty\n8\u2013200 clock cycles\n1,000,000\u201310,000,000 clock cycles\n(access time)\n(6\u2013160 clock cycles)\n(800,000\u20138,000,000 clock cycles)\n(transfer time)\n(2\u201340 clock cycles)\n(200,000\u20132,000,000 clock cycles)\nMiss rate\n0.1%\u201310%\n0.00001%\u20130.001%\nAddress mapping\n25\u201345-bit physical address\nto 14\u201320-bit cache address\n32\u201364-bit virtual address to 25\u201345-bit\nphysical address\nFigure B.20 Typical ranges of parameters for caches and virtual memory. Virtual\nmemory parameters represent increases of 10\u20131,000,000 times over cache parame-\nters. Usually, first-level caches contain at most 1 MiB of data, whereas physical memory\ncontains 256 MiB to 1 TB.\nB-42\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 749,
        "text": "Virtual memory also encompasses several related techniques. Virtual memory\nsystems can be categorized into two classes: those with fixed-size blocks, called\npages, and those with variable-size blocks, called segments. Pages are typically\nfixed at 4096\u20138192 bytes, while segment size varies. The largest segment sup-\nported on any processor ranges from 216 bytes up to 232 bytes; the smallest segment\nis 1 byte. Figure B.21 shows how the two approaches might divide code and data.\nThe decision to use paged virtual memory versus segmented virtual memory\naffects the processor. Paged addressing has a single fixed-size address divided\ninto page number and offset within a page, analogous to cache addressing. A single\naddress does not work for segmented addresses; the variable size of segments\nrequires 1 word for a segment number and 1 word for an offset within a segment,\nfor a total of 2 words. An unsegmented address space is simpler for the compiler.\nThe pros and cons of these two approaches have been well documented in oper-\nating systems textbooks; Figure B.22 summarizes the arguments. Because of the\na\nt\na\nD\ne\nd\no\nC\nPaging\nSegmentation\nFigure B.21 Example of how paging and segmentation divide a program.\nPage\nSegment\nWords per address\nOne\nTwo (segment and offset)\nProgrammer visible?\nInvisible to application\nprogrammer\nMay be visible to application\nprogrammer\nReplacing a block\nTrivial (all blocks are the\nsame size)\nDifficult (must find contiguous,\nvariable-size, unused portion of\nmain memory)\nMemory use\ninefficiency\nInternal fragmentation\n(unused portion of page)\nExternal fragmentation (unused\npieces of main memory)\nEfficient disk traffic\nYes (adjust page size to\nbalance access time and\ntransfer time)\nNot always (small segments may\ntransfer just a few bytes)\nFigure B.22 Paging versus segmentation. Both can waste memory, depending on the\nblock size and how well the segments fit together in main memory. Programming lan-\nguages with unrestricted pointers require both the segment and the address to be\npassed. A hybrid approach, called paged segments, shoots for the best of both worlds:\nsegments are composed of pages, so replacing a block is easy, yet a segment may be\ntreated as a logical unit.\nB.4\nVirtual Memory\n\u25a0\nB-43"
    },
    {
        "page": 750,
        "text": "replacement problem (the third line of the figure), few computers today use pure\nsegmentation. Some computers use a hybrid approach, called paged segments, in\nwhich a segment is an integral number of pages. This simplifies replacement\nbecause memory need not be contiguous, and the full segments need not be in main\nmemory. A more recent hybrid is for a computer to offer multiple page sizes,\nwith the larger sizes being powers of 2 times the smallest page size. The IBM\n405CR embedded processor, for example, allows 1 KiB, 4 KiB (221 KiB),\n16 KiB (241 KiB), 64 KiB (261 KiB), 256 KiB (281 KiB), 1024 KiB\n(2101 KiB), and 4096 KiB (2121 KiB) to act as a single page.\nFour Memory Hierarchy Questions Revisited\nWe are now ready to answer the four memory hierarchy questions for virtual\nmemory.\nQ1: Where Can a Block be Placed in Main Memory?\nThe miss penalty for virtual memory involves access to a rotating magnetic storage\ndevice and is therefore quite high. Given the choice of lower miss rates or a simpler\nplacement algorithm, operating systems designers usually pick lower miss rates\nbecause of the exorbitant miss penalty. Thus, operating systems allow blocks to\nbe placed anywhere in main memory. According to the terminology in\nFigure B.2 on page B-8, this strategy would be labeled fully associative.\nQ2: How Is a Block Found If It Is in Main Memory?\nBoth paging and segmentation rely on a data structure that is indexed by the page or\nsegment number. This data structure contains the physical address of the block. For\nsegmentation, the offset is added to the segment\u2019s physical address to obtain the\nfinal physical address. For paging, the offset is simply concatenated to this physical\npage address (see Figure B.23).\nThis data structure, containing the physical page addresses, usually takes the\nform of a page table. Indexed by the virtual page number, the size of the table\nis the number of pages in the virtual address space. Given a 32-bit virtual address,\n4 KiB pages, and 4 bytes per page table entry (PTE), the size of the page table\nwould be (232/212)22\u00bc222 or 4 MiB.\nTo reduce the size of this data structure, some computers apply a hashing func-\ntion to the virtual address. The hash allows the data structure to be the length of the\nnumber of physical pages in main memory. This number could be much smaller\nthan the number of virtual pages. Such a structure is called an inverted page table.\nUsing the previous example, a 512 MiB physical memory would only need 1 MiB\n(8512 MiB/4 KiB) for an inverted page table; the extra 4 bytes per page table\nentry are for the virtual address. The HP/Intel IA-64 covers both bases by offering\nB-44\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 751,
        "text": "both traditional pages tables and inverted page tables, leaving the choice of mech-\nanism to the operating system programmer.\nTo reduce address translation time, computers use a cache dedicated to these\naddress translations, called a translation lookaside buffer, or simply translation\nbuffer, described in more detail shortly.\nQ3: Which Block Should be Replaced on a Virtual Memory Miss?\nAs mentioned earlier, the overriding operating system guideline is minimizing\npage faults. Consistent with this guideline, almost all operating systems try to\nreplace the least recently used (LRU) block because if the past predicts the future,\nthat is the one less likely to be needed.\nTo help the operating system estimate LRU, many processors provide a use bit\nor reference bit, which is logically set whenever a page is accessed. (To reduce\nwork, it is actually set only on a translation buffer miss, which is described shortly.)\nThe operating system periodically clears the use bits and later records them so it\ncan determine which pages were touched during a particular time period. By keep-\ning track in this way, the operating system can select a page that is among the least\nrecently referenced.\nQ4: What Happens on a Write?\nThe level below main memory contains rotating magnetic disks that take millions\nof clock cycles to access. Because of the great discrepancy in access time, no one\nhas yet built a virtual memory operating system that writes through main memory\nto disk on every store by the processor. (This remark should not be interpreted as an\nopportunity to become famous by being the first to build one!) Thus, the write strat-\negy is always write-back.\nMain \nmemory\nPage\ntable\nVirtual address\nVirtual page number\nPage offset\nPhysical address\nFigure B.23 The mapping of a virtual address to a physical address via a page table.\nB.4\nVirtual Memory\n\u25a0\nB-45"
    },
    {
        "page": 752,
        "text": "Because the cost of an unnecessary access to the next-lower level is so high,\nvirtual memory systems usually include a dirty bit. It allows blocks to be written to\ndisk only if they have been altered since being read from the disk.\nTechniques for Fast Address Translation\nPage tables are usually so large that they are stored in main memory and are some-\ntimes paged themselves. Paging means that every memory access logically takes at\nleast twice as long, with one memory access to obtain the physical address and a\nsecond access to get the data. As mentioned in Chapter 2, we use locality to avoid\nthe extra memory access. By keeping address translations in a special cache, a\nmemory access rarely requires a second access to translate the data. This special\naddress translation cache is referred to as a translation look aside buffer (TLB),\nalso called a translation buffer (TB).\nA TLB entry is like a cache entry where the tag holds portions of the virtual\naddress and the data portion holds a physical page frame number, protection field,\nvalid bit, and usually a use bit and dirty bit. To change the physical page frame\nnumber or protection of an entry in the page table, the operating system must make\nsure the old entry is not in the TLB; otherwise, the system won\u2019t behave properly.\nNote that this dirty bit means the corresponding page is dirty, not that the address\ntranslation in the TLB is dirty nor that a particular block in the data cache is dirty.\nThe operating system resets these bits by changing the value in the page table and\nthen invalidates the corresponding TLB entry. When the entry is reloaded from the\npage table, the TLB gets an accurate copy of the bits.\nFigure B.24 shows the Opteron data TLB organization, with each step of the\ntranslation labeled. This TLB uses fully associative placement; thus, the translation\nbegins (steps 1 and 2) by sending the virtual address to all tags. Of course, the tag\nmust be marked valid to allow a match. At the same time, the type of memory\naccess is checked for a violation (also in step 2) against protection information\nin the TLB.\nFor reasons similar to those in the cache case, there is no need to include the 12\nbits of the page offset in the TLB. The matching tag sends the corresponding phys-\nical address through effectively a 40:1 multiplexor (step 3). The page offset is then\ncombined with the physical page frame to form a full physical address (step 4). The\naddress size is 40 bits.\nAddress translation can easily be on the critical path determining the clock\ncycle of the processor, so the Opteron uses virtually addressed, physically tagged\nL1 caches.\nSelecting a Page Size\nThe most obvious architectural parameter is the page size. Choosing the page is a\nquestion of balancing forces that favor a larger page size versus those favoring a\nsmaller size. The following favor a larger size:\nB-46\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 753,
        "text": "\u25a0\nThe size of the page table is inversely proportional to the page size; memory (or\nother resources used for the memory map) can therefore be saved by making\nthe pages bigger.\n\u25a0\nAs mentioned in Section B.3, a larger page size can allow larger caches with\nfast cache hit times.\n\u25a0\nTransferring larger pages to or from secondary storage, possibly over a net-\nwork, is more efficient than transferring smaller pages.\n\u25a0\nThe number of TLB entries is restricted, so a larger page size means that more\nmemory can be mapped efficiently, thereby reducing the number of TLB\nmisses.\nIt is for this final reason that recent microprocessors have decided to support mul-\ntiple page sizes; for some programs, TLB misses can be as significant on CPI as the\ncache misses.\nThe main motivation for a smaller page size is conserving storage. A small\npage size will result in less wasted storage when a contiguous region of virtual\nmemory is not equal in size to a multiple of the page size. The term for this unused\nmemory in a page is internal fragmentation. Assuming that each process has three\nprimary segments (text, heap, and stack), the average wasted storage per process\nwill be 1.5 times the page size. This amount is negligible for computers with hun-\ndreds of megabytes of memory and page sizes of 4\u20138 KiB. Of course, when the\npage sizes become very large (more than 32 KiB), storage (both main and second-\nary) could be wasted, as well as I/O bandwidth. A final concern is process start-up\ntime; many processes are small, so a large page size would lengthen the time to\ninvoke a process.\n<28>\nVirtual page\nnumber\n<36>\nPage\noffset\n<12>\n<1>\nV\n<1>\nD\n<1>\nA\n<36>\nTag\n<28>\nPhysical address\n(Low-order 12 bits\nof address)\n(High-order 28 bits of address)\n40-bit\nphysical\naddress\nR/W\nU/S\n40:1 mux\n2\n1\n4\n<12>\n3\nFigure B.24 Operation of the Opteron data TLB during address translation. The four\nsteps of a TLB hit are shown as circled numbers. This TLB has 40 entries. Section B.5\ndescribes the various protection and access fields of an Opteron page table entry.\nB.4\nVirtual Memory\n\u25a0\nB-47"
    },
    {
        "page": 754,
        "text": "Summary of Virtual Memory and Caches\nWith virtual memory, TLBs, first-level caches, and second-level caches all map-\nping portions of the virtual and physical address space, it can get confusing what\nbits go where. Figure B.25 gives a hypothetical example going from a 64-bit virtual\naddress to a 41-bit physical address with two levels of cache. This L1 cache is vir-\ntually indexed, and physically tagged because both the cache size and the page size\nare 8 KiB. The L2 cache is 4 MiB. The block size for both is 64 bytes.\nL1 tag compare address <28>\nL2 cache tag <19>\nL2 data <512>\n=?\n=?\n=?\nTLB tag compare address <43> TLB index <8>\nVirtual address <64>\nPhysical address <41>\nVirtual page number <51>\nL2 tag compare address <19> L2 cache index <16> Block offset  <6>\nPage offset  <13>\nL1 cache tag <43>\nL1 data <512>\nTLB tag <43>\nTLB data <28>\nTo CPU\nTo CPU\nTo CPU\nTo L1 cache or CPU\nL1 cache index <7> Block offset  <6>\nFigure B.25 The overall picture of a hypothetical memory hierarchy going from virtual address to L2 cache\naccess. The page size is 8 KiB. The TLB is direct mapped with 256 entries. The L1 cache is a direct-mapped 8 KiB,\nandtheL2cacheisadirect-mapped4 MiB.Bothuse64-byteblocks.Thevirtualaddressis64bitsandthephysicaladdress\nis 41 bits. The primary difference between this simple figure and a real cache is replication of pieces of this figure.\nB-48\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 755,
        "text": "First, the 64-bit virtual address is logically divided into a virtual page number\nand page offset. The former is sent to the TLB to be translated into a physical\naddress, and the high bit of the latter is sent to the L1 cache to act as an index.\nIf the TLB match is a hit, then the physical page number is sent to the L1 cache\ntag to check for a match. If it matches, it\u2019s an L1 cache hit. The block offset then\nselects the word for the processor.\nIf the L1 cache check results in a miss, the physical address is then used to try\nthe L2 cache. The middle portion of the physical address is used as an index to the 4\nMiB L2 cache. The resulting L2 cache tag is compared with the upper part of the\nphysical address to check for a match. If it matches, we have an L2 cache hit, and\nthe data are sent to the processor, which uses the block offset to select the desired\nword. On an L2 miss, the physical address is then used to get the block from\nmemory.\nAlthough this is a simple example, the major difference between this drawing\nand a real cache is replication. First, there is only one L1 cache. When there are\ntwo L1 caches, the top half of the diagram is duplicated. Note that this would\nlead to two TLBs, which is typical. Hence, one cache and TLB is for instructions,\ndriven from the PC, and one cache and TLB is for data, driven from the effective\naddress.\nThe second simplification is that all the caches and TLBs are direct mapped. If\nany were n-way set associative, then we would replicate each set of tag memory,\ncomparators, and data memory n times and connect data memories with an n:1\nmultiplexor to select a hit. Of course, if the total cache size remained the same,\nthe cache index would also shrink by log 2n bits according to the formula in\nFigure B.7 on page B-22.\nB.5\nProtection and Examples of Virtual Memory\nThe invention of multiprogramming, where a computer would be shared by\nseveral programs running concurrently, led to new demands for protection and\nsharing among programs. These demands are closely tied to virtual memory in\ncomputers today, and so we cover the topic here along with two examples of virtual\nmemory.\nMultiprogramming leads to the concept of a process. Metaphorically, a process\nis a program\u2019s breathing air and living space\u2014that is, a running program plus any\nstate needed to continue running it. Time-sharing is a variation of multiprogram-\nming that shares the processor and memory with several interactive users at the\nsame time, giving the illusion that all users have their own computers. Thus, at\nany instant it must be possible to switch from one process to another. This\nexchange is called a process switch or context switch.\nA process must operate correctly whether it executes continuously from start to\nfinish, or it is interrupted repeatedly and switched with other processes. The\nresponsibility for maintaining correct process behavior is shared by designers of\nthe computer and the operating system. The computer designer must ensure that\nB.5\nProtection and Examples of Virtual Memory\n\u25a0\nB-49"
    },
    {
        "page": 756,
        "text": "the processor portion of the process state can be saved and restored. The operating\nsystem designer must guarantee that processes do not interfere with each others\u2019\ncomputations.\nThe safest way to protect the state of one process from another would be to\ncopy the current information to disk. However, a process switch would then take\nseconds\u2014far too long for a time-sharing environment.\nThis problem is solved by operating systems partitioning main memory so that\nseveral different processes have their state in memory at the same time. This divi-\nsion means that the operating system designer needs help from the computer\ndesigner to provide protection so that one process cannot modify another. Besides\nprotection, the computers also provide for sharing of code and data between pro-\ncesses, to allow communication between processes or to save memory by reducing\nthe number of copies of identical information.\nProtecting Processes\nProcesses can be protected from one another by having their own page tables, each\npointing to distinct pages of memory. Obviously, user programs must be prevented\nfrom modifying their page tables or protection would be circumvented.\nProtection can be escalated, depending on the apprehension of the computer\ndesigner or the purchaser. Rings added to the processor protection structure expand\nmemory access protection from two levels (user and kernel) to many more. Like a\nmilitary classification system of top secret, secret, confidential, and unclassified,\nconcentric rings of security levels allow the most trusted to access anything, the\nsecond most trusted to access everything except the innermost level, and so on.\nThe \u201ccivilian\u201d programs are the least trusted and, hence, have the most limited\nrange of accesses. There may also be restrictions on what pieces of memory can\ncontain code\u2014execute protection\u2014and even on the entrance point between the\nlevels. The Intel 80x86 protection structure, which uses rings, is described later\nin this section. It is not clear whether rings are an improvement in practice over\nthe simple system of user and kernel modes.\nAs the designer\u2019s apprehension escalates to trepidation, these simple rings may\nnot suffice. Restricting the freedom given a program in the inner sanctum requires a\nnew classification system. Instead of a military model, the analogy of this system is\nto keys and locks: a program can\u2019t unlock access to the data unless it has the key.\nFor these keys, or capabilities, to be useful, the hardware and operating system\nmust be able to explicitly pass them from one program to another without allowing\na program itself to forge them. Such checking requires a great deal of hardware\nsupport if time for checking keys is to be kept low.\nThe 80x86 architecture has tried several of these alternatives over the years.\nBecause backward compatibility is one of the guidelines of this architecture, the\nmost recent versions of the architecture include all of its experiments in virtual\nmemory. We\u2019ll go over two of the options here: first the older segmented address\nspace and then the newer flat, 64-bit address space.\nB-50\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 757,
        "text": "A Segmented Virtual Memory Example: Protection\nin the Intel Pentium\nThe second system is the most dangerous system a man ever designs\u2026 . The\ngeneral tendency is to over-design the second system, using all the ideas and\nfrills that were cautiously sidetracked on the first one.\nF. P. Brooks, Jr.\nThe Mythical Man-Month (1975)\nThe original 8086 used segments for addressing, yet it provided nothing for virtual\nmemory or for protection. Segments had base registers but no bound registers and\nno access checks, and before a segment register could be loaded the corresponding\nsegment had to be in physical memory. Intel\u2019s dedication to virtual memory and\nprotection is evident in the successors to the 8086, with a few fields extended to\nsupport larger addresses. This protection scheme is elaborate, with many details\ncarefully designed to try to avoid security loopholes. We\u2019ll refer to it as IA-32.\nThe next few pages highlight a few of the Intel safeguards; if you find the reading\ndifficult, imagine the difficulty of implementing them!\nThe first enhancement is to double the traditional two-level protection model:\nthe IA-32 has four levels of protection. The innermost level (0) corresponds to the\ntraditional kernel mode, and the outermost level (3) is the least privileged mode.\nThe IA-32 has separate stacks for each level to avoid security breaches between the\nlevels. There are also data structures analogous to traditional page tables that con-\ntain the physical addresses for segments, as well as a list of checks to be made on\ntranslated addresses.\nThe Intel designers did not stop there. The IA-32 divides the address space,\nallowing both the operating system and the user access to the full space. The\nIA-32 user can call an operating system routine in this space and even pass param-\neters to it while retaining full protection. This safe call is not a trivial action,\nbecause the stack for the operating system is different from the user\u2019s stack. More-\nover, the IA-32 allows the operating system to maintain the protection level of the\ncalled routine for the parameters that are passed to it. This potential loophole in\nprotection is prevented by not allowing the user process to ask the operating system\nto access something indirectly that it would not have been able to access itself.\n(Such security loopholes are called Trojan horses.)\nThe Intel designers were guided by the principle of trusting the operating\nsystem as little as possible, while supporting sharing and protection. As an exam-\nple of the use of such protected sharing, suppose a payroll program writes checks\nand also updates the year-to-date information on total salary and benefits pay-\nments. Thus, we want to give the program the ability to read the salary and\nyear-to-date information and modify the year-to-date information but not the sal-\nary. We will see the mechanism to support such features shortly. In the rest of\nthis subsection, we will look at the big picture of the IA-32 protection and exam-\nine its motivation.\nB.5\nProtection and Examples of Virtual Memory\n\u25a0\nB-51"
    },
    {
        "page": 758,
        "text": "Adding Bounds Checking and Memory Mapping\nThe first step in enhancing the Intel processor was getting the segmented addres-\nsing to check bounds as well as supply a base. Rather than a base address, the seg-\nment registers in the IA-32 contain an index to a virtual memory data structure\ncalled a descriptor table. Descriptor tables play the role of traditional page tables.\nOn the IA-32 the equivalent of a page table entry is a segment descriptor. It con-\ntains fields found in PTEs:\n\u25a0\nPresent bit\u2014Equivalent to the PTE valid bit, used to indicate this is a valid\ntranslation\n\u25a0\nBase field\u2014Equivalent to a page frame address, containing the physical\naddress of the first byte of the segment\n\u25a0\nAccess bit\u2014Like the reference bit or use bit in some architectures that is helpful\nfor replacement algorithms\n\u25a0\nAttributes field\u2014Specifies the valid operations and protection levels for oper-\nations that use this segment\nThere is also a limit field, not found in paged systems, which establishes the upper\nbound of valid offsets for this segment. Figure B.26 shows examples of IA-32 seg-\nment descriptors.\nIA-32 provides an optional paging system in addition to this segmented addres-\nsing. The upper portion of the 32-bit address selects the segment descriptor, and the\nmiddle portion is an index into the page table selected by the descriptor. The fol-\nlowing section describes the protection system that does not rely on paging.\nAdding Sharing and Protection\nTo provide for protected sharing, half of the address space is shared by all processes\nand half is unique to each process, called global address space and local address\nspace, respectively. Each half is given a descriptor table with the appropriate name.\nA descriptor pointing to a shared segment is placed in the global descriptor table,\nwhile a descriptor for a private segment is placed in the local descriptor table.\nA program loads an IA-32 segment register with an index to the table and a bit\nsaying which table it desires. The operation is checked according to the attributes in\nthe descriptor, the physical address being formed by adding the offset in the pro-\ncessor to the base in the descriptor, provided the offset is less than the limit field.\nEvery segment descriptor has a separate 2-bit field to give the legal access level of\nthis segment. A violation occurs only if the program tries to use a segment with a\nlower protection level in the segment descriptor.\nWe can now show how to invoke the payroll program mentioned herein to\nupdate the year-to-date information without allowing it to update salaries. The pro-\ngram could be given a descriptor to the information that has the writable field clear,\nmeaning it can read but not write the data. A trusted program can then be supplied\nthat will only write the year-to-date information. It is given a descriptor with the\nB-52\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 759,
        "text": "writable field set (Figure B.26). The payroll program invokes the trusted code\nusing a code segment descriptor with the conforming field set. This setting means\nthe called program takes on the privilege level of the code being called rather than\nthe privilege level of the caller. Hence, the payroll program can read the salaries\nand call a trusted program to update the year-to-date totals, yet the payroll program\ncannot modify the salaries. If a Trojan horse exists in this system, to be effective it\nmust be located in the trusted code whose only job is to update the year-to-date\ninformation. The argument for this style of protection is that limiting the scope\nof the vulnerability enhances security.\nAttributes\nBase\nLimit\nstib\n \n4\n2\nstib\n \n2\n3\nstib\n \n4\nstib\n \n8\nPresent\nCode segment\nDPL 11\nConforming\nReadable\nAccessed\nPresent\nData segment\nDPL 10\nExpand down\nWritable\nAccessed\nAttributes\nDestination selector\nDestination offset\nstib\n \n6\n1\nstib\n \n6\n1\nstib\n \n8\nWord\ncount\n8 bits\nPresent\nCall gate\nDPL\n0\n00100\nGD\nFigure B.26 The IA-32 segment descriptors are distinguished by bits in the\nattributes field. Base, limit, present, readable, and writable are all self-explanatory. D gives\nthe default addressing size of the instructions: 16 bits or 32 bits. G gives the granularity of\nthe segment limit: 0 means in bytes and 1 means in 4 KiB pages. G is set to 1 when paging\nis turned on to set the size of the page tables. DPL means descriptor privilege level\u2014this is\nchecked against the code privilege level to see if the access will be allowed. Conforming\nsays the code takes on the privilege level of the code being called rather than the priv-\nilege level of the caller; it is used for library routines. The expand-down field flips the check\nto let the base field be the high-water mark and the limit field be the low-water mark. As\nyou might expect, this is used for stack segments that grow down. Word count controls\nthe number of words copied from the current stack to the new stack on a call gate. The\nother two fields of the call gate descriptor, destination selector and destination offset,\nselect the descriptor of the destination of the call and the offset into it, respectively. There\nare many more than these three segment descriptors in the IA-32 protection model.\nB.5\nProtection and Examples of Virtual Memory\n\u25a0\nB-53"
    },
    {
        "page": 760,
        "text": "Adding Safe Calls from User to OS Gates and Inheriting Protection\nLevel for Parameters\nAllowing the user to jump into the operating system is a bold step. How, then, can\na hardware designer increase the chances of a safe system without trusting the\noperating system or any other piece of code? The IA-32 approach is to restrict\nwhere the user can enter a piece of code, to safely place parameters on the proper\nstack, and to make sure the user parameters don\u2019t get the protection level of the\ncalled code.\nTo restrict entry into others\u2019 code, the IA-32 provides a special segment\ndescriptor, or call gate, identified by a bit in the attributes field. Unlike other\ndescriptors, call gates are full physical addresses of an object in memory; the offset\nsupplied by the processor is ignored. As stated previously, their purpose is to pre-\nvent the user from randomly jumping anywhere into a protected or more privileged\ncode segment. In our programming example, this means the only place the payroll\nprogram can invoke the trusted code is at the proper boundary. This restriction is\nneeded to make conforming segments work as intended.\nWhat happens if caller and callee are \u201cmutually suspicious,\u201d so that neither trusts\nthe other? The solution is found in the word count field in the bottom descriptor in\nFigureB.26.Whenacallinstructioninvokesacallgatedescriptor,thedescriptorcop-\nies the number of words specified in the descriptor from the local stack onto the stack\ncorresponding to the level of this segment. This copying allows the user to pass\nparametersby first pushing them onto the local stack. The hardware thensafely trans-\nfers them onto the correct stack. A return from a call gate will pop the parameters off\nboth stacks and copy any return values to the proper stack. Note that this model is\nincompatible with the current practice of passing parameters in registers.\nThis scheme still leaves open the potential loophole of having the operating\nsystem use the user\u2019s address, passed as parameters, with the operating system\u2019s\nsecurity level, instead of with the user\u2019s level. The IA-32 solves this problem by\ndedicating 2 bits in every processor segment register to the requested protection\nlevel. When an operating system routine is invoked, it can execute an instruction\nthat sets this 2-bit field in all address parameters with the protection level of the\nuser that called the routine. Thus, when these address parameters are loaded into\nthe segment registers, they will set the requested protection level to the proper\nvalue. The IA-32 hardware then uses the requested protection level to prevent\nany foolishness: no segment can be accessed from the system routine using those\nparameters if it has a more privileged protection level than requested.\nA Paged Virtual Memory Example: The 64-Bit\nOpteron Memory Management\nAMD engineers found few uses of the elaborate protection model described in the\nprevious section. The popular model is a flat, 32-bit address space, introduced by\nthe 80386, which sets all the base values of the segment registers to zero. Hence,\nB-54\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 761,
        "text": "AMD dispensed with the multiple segments in the 64-bit mode. It assumes that the\nsegment base is zero and ignores the limit field. The page sizes are 4 KiB, 2 MiB,\nand 4 MiB.\nThe 64-bit virtual address of the AMD64 architecture is mapped onto 52-bit\nphysical addresses, although implementations can implement fewer bits to sim-\nplify hardware. The Opteron, for example, uses 48-bit virtual addresses and 40-\nbit physical addresses. AMD64 requires that the upper 16 bits of the virtual address\nbe just the sign extension of the lower 48 bits, which it calls canonical form.\nThe size of page tables for the 64-bit address space is alarming. Hence, AMD64\nuses a multilevel hierarchical page table to map the address space to keep the size\nreasonable. The number of levels depends on the size of the virtual address space.\nFigure B.27 shows the four-level translation of the 48-bit virtual addresses of the\nOpteron.\nThe offsets for each of these page tables come from four 9-bit fields. Address\ntranslation starts with adding the first offset to the page-map level 4 base register\nand then reading memory from this location to get the base of the next-level page\ntable. The next address offset is in turn added to this newly fetched address, and\n63\n48\n47\n39 38\n30 29\n21 20\n12 11\n0\n000 . . . 0 or\n111 . . . 1\nPage-map L4\nPage-dir-ptr\nPage-directory\nPage-table\nPage offset\nPage-map L4\nbase addr (CR3)\nPhysical page frame number\nPage offset\nPage-mp entry\nPage-map L4 table\n+\n+\nPage-dir-ptr entry\nPage-directory\npointer table\n+\nPage-dir entry\nPage-directory\ntable\n+\nPage-table entry\nPage table\nPhysical address\nMain memory\nFigure B.27 The mapping of an Opteron virtual address. The Opteron virtual memory implementation with four\npage table levels supports an effective physical address size of 40 bits. Each page table has 512 entries, so each level\nfield is 9 bits wide. The AMD64 architecture document allows the virtual address size to grow from the current 48 bits\nto 64 bits, and the physical address size to grow from the current 40 bits to 52 bits.\nB.5\nProtection and Examples of Virtual Memory\n\u25a0\nB-55"
    },
    {
        "page": 762,
        "text": "memory is accessed again to determine the base of the third page table. It happens\nagain in the same fashion. The last address field is added to this final base address,\nand memory is read using this sum to (finally) get the physical address of the page\nbeing referenced. This address is concatenated with the 12-bit page offset to get the\nfull physical address. Note that the page table in the Opteron architecture fits within\na single 4 KiB page.\nThe Opteron uses a 64-bit entry in each of these page tables. The first 12 bits are\nreserved for future use, the next 52 bits contain the physical page frame number,\nand the last 12 bits give the protection and use information. Although the fields\nvary some between the page table levels, here are the basic ones:\n\u25a0\nPresence\u2014Says that page is present in memory.\n\u25a0\nRead/write\u2014Says whether page is read-only or read-write.\n\u25a0\nUser/supervisor\u2014Says whether a user can access the page or if it is limited to\nthe upper three privilege levels.\n\u25a0\nDirty\u2014Says if page has been modified.\n\u25a0\nAccessed\u2014Says if page has been read or written since the bit was last cleared.\n\u25a0\nPage size\u2014Says whether the last level is for 4 KiB pages or 4 MiB pages; if it\u2019s\nthe latter, then the Opteron only uses three instead of four levels of pages.\n\u25a0\nNo execute\u2014Not found in the 80386 protection scheme, this bit was added to\nprevent code from executing in some pages.\n\u25a0\nPage level cache disable\u2014Says whether the page can be cached or not.\n\u25a0\nPage level write through\u2014Says whether the page allows write back or write\nthrough for data caches.\nBecause the Opteron usually goes through four levels of tables on a TLB\nmiss, there are three potential places to check protection restrictions. The Opteron\nobeys only the bottom-level PTE, checking the others only to be sure the valid bit\nis set.\nAs the entry is 8 bytes long, each page table has 512 entries, and the Opteron\nhas 4 KiB pages, the page tables are exactly one page long. Each of the four level\nfields are 9 bits long, and the page offset is 12 bits. This derivation leaves\n64\u0003(49+12) or 16 bits to be sign extended to ensure canonical addresses.\nAlthough we have explained translation of legal addresses, what prevents the\nuser from creating illegal address translations and getting into mischief? The page\ntables themselves are protected from being written by user programs. Thus, the\nuser can try any virtual address, but by controlling the page table entries the oper-\nating system controls what physical memory is accessed. Sharing of memory\nbetween processes is accomplished by having a page table entry in each address\nspace point to the same physical memory page.\nThe Opteron employs four TLBs to reduce address translation time, two for\ninstruction accesses and two for data accesses. Like multilevel caches, the Opteron\nB-56\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 763,
        "text": "reduces TLB misses by having two larger L2 TLBs: one for instructions and one\nfor data. Figure B.28 describes the data TLB.\nSummary: Protection on the 32-Bit Intel Pentium Versus\nthe 64-Bit AMD Opteron\nMemory management in the Opteron is typical of most desktop or server com-\nputers today, relying on page-level address translation and correct operation of\nthe operating system to provide safety to multiple processes sharing the computer.\nAlthough presented as alternatives, Intel has followed AMD\u2019s lead and embraced\nthe AMD64 architecture. Hence, both AMD and Intel support the 64-bit extension\nof 80x86; yet, for compatibility reasons, both support the elaborate segmented pro-\ntection scheme.\nIf the segmented protection model looks harder to build than the AMD64\nmodel, that\u2019s because it is. This effort must be especially frustrating for the engi-\nneers, because few customers use the elaborate protection mechanism. In addition,\nthe fact that the protection model is a mismatch to the simple paging protection of\nUNIX-like systems means it will be used only by someone writing an operating\nsystem especially for this computer, which hasn\u2019t happened yet.\nB.6\nFallacies and Pitfalls\nEven a review of memory hierarchy has fallacies and pitfalls!\nPitfall\nToo small an address space.\nJust five years after DEC and Carnegie Mellon University collaborated to design\nthe new PDP-11 computer family, it was apparent that their creation had a fatal\nParameter\nDescription\nBlock size\n1 PTE (8 bytes)\nL1 hit time\n1 clock cycle\nL2 hit time\n7 clock cycles\nL1 TLB size\nSame for instruction and data TLBs: 40 PTEs per TLBs, with\n32 4 KiB pages and 8 for 2 MiB or 4 MiB pages\nL2 TLB size\nSame for instruction and data TLBs: 512 PTEs of 4 KiB pages\nBlock selection\nLRU\nWrite strategy\n(Not applicable)\nL1 block placement\nFully associative\nL2 block placement\n4-way set associative\nFigure B.28 Memory hierarchy parameters of the Opteron L1 and L2 instruction and\ndata TLBs.\nB.6\nFallacies and Pitfalls\n\u25a0\nB-57"
    },
    {
        "page": 764,
        "text": "flaw. An architecture announced by IBM six years before the PDP-11 was still\nthriving, with minor modifications, 25 years later. And the DEC VAX, criticized\nfor including unnecessary functions, sold millions of units after the PDP-11 went\nout of production. Why?\nThe fatal flaw of the PDP-11 was the size of its addresses (16 bits) as compared\nwith the address sizes of the IBM 360 (24\u201331 bits) and the VAX (32 bits). Address\nsize limits the program length, because the size of a program and the amount of\ndata needed by the program must be less than 2Address size. The reason the address\nsize is so hard to change is that it determines the minimum width of anything that\ncan contain an address: PC, register, memory word, and effective-address arith-\nmetic. If there is no plan to expand the address from the start, then the chances\nof successfully changing address size are so slim that it usually means the end\nof that computer family. Bell and Strecker (1976) put it like this:\nThere is only one mistake that can be made in computer design that is difficult\nto recover from\u2014not having enough address bits for memory addressing and\nmemory management. The PDP-11 followed the unbroken tradition of nearly\nevery known computer. [p. 2]\nA partial list of successful computers that eventually starved to death for lack of\naddress bits includes the PDP-8, PDP-10, PDP-11, Intel 8080, Intel 8086, Intel\n80186, Intel 80286, Motorola 6800, AMI 6502, Zilog Z80, CRAY-1, and CRAY\nX-MP.\nThe venerable 80x86 line bears the distinction of having been extended twice,\nfirst to 32 bits with the Intel 80386 in 1985 and recently to 64 bits with the AMD\nOpteron.\nPitfall\nIgnoring the impact of the operating system on the performance of the memory\nhierarchy.\nFigure B.29 shows the memory stall time due to the operating system spent on\nthree large workloads. About 25% of the stall time is either spent in misses in\nthe operating system or results from misses in the application programs because\nof interference with the operating system.\nPitfall\nRelying on the operating systems to change the page size over time.\nThe Alpha architects had an elaborate plan to grow the architecture over time by\ngrowing its page size, even building it into the size of its virtual address. When it\ncame time to grow page sizes with later Alphas, the operating system designers\nbalked and the virtual memory system was revised to grow the address space while\nmaintaining the 8 KiB page.\nArchitects of other computers noticed very high TLB miss rates, and so added\nmultiple, larger page sizes to the TLB. The hope was that operating systems pro-\ngrammers would allocate an object to the largest page that made sense, thereby\npreserving TLB entries. After a decade of trying, most operating systems use these\n\u201csuperpages\u201d only for handpicked functions: mapping the display memory or other\nI/O devices, or using very large pages for the database code.\nB-58\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 765,
        "text": "B.7\nConcluding Remarks\nThe difficulty of building a memory system to keep pace with faster processors is\nunderscored by the fact that the raw material for main memory is the same as that\nfound in the cheapest computer. It is the principle of locality that helps us here\u2014its\nsoundness is demonstrated at all levels of the memory hierarchy in current com-\nputers, from disks to TLBs.\nHowever, the increasing relative latency to memory, taking hundreds of clock\ncycles in 2016, means that programmers and compiler writers must be aware of the\nparameters of the caches and TLBs if they want their programs to perform well.\nB.8\nHistorical Perspective and References\nIn Section M.3 (available online) we examine the history of caches, virtual mem-\nory, and virtual machines. (The historical section covers both this appendix and\nChapter 3.) IBM plays a prominent role in the history of all three. References\nfor further reading are included.\nAdditional reference: Gupta, S. Xiang, P., Yang, Y., Zhou, H., Locality prin-\nciple revisited: a probability-based quantitative approach. J. Parallel Distrib. Com-\nput. 73 (7), 1011\u20131027.\nTime\nMisses\n% Time due to\napplication misses\n% Time due directly to OS misses\n% Time OS\nmisses and\napplication\nconflicts\nWorkload\n% in\napplications\n%\nin\nOS\nInherent\napplication\nmisses\nOS conflicts\nwith\napplications\nOS\ninstruction\nmisses\nData\nmisses\nfor\nmigration\nData\nmisses in\nblock\noperations\nRest\nof OS\nmisses\nPmake\n47%\n53%\n14.1%\n4.8%\n10.9%\n1.0%\n6.2%\n2.9%\n25.8%\nMultipgm\n53%\n47%\n21.6%\n3.4%\n9.2%\n4.2%\n4.7%\n3.4%\n24.9%\nOracle\n73%\n27%\n25.7%\n10.2%\n10.6%\n2.6%\n0.6%\n2.8%\n26.8%\nFigure B.29 Misses and time spent in misses for applications and operating system. The operating system adds\nabout 25% to the execution time of the application. Each processor has a 64 KiB instruction cache and a two-level\ndata cache with 64 KiB in the first level and 256 KiB in the second level; all caches are direct mapped with 16-byte\nblocks. Collected on Silicon Graphics POWER station 4D/340, a multiprocessor with four 33 MHz R3000 processors\nrunning three application workloads under a UNIX System V\u2014Pmake, a parallel compile of 56 files; Multipgm,\nthe parallel numeric program MP3D running concurrently with Pmake and a five-screen edit session; and Oracle,\nrunning a restricted version of the TP-1 benchmark using the Oracle database. Data from Torrellas, J., Gupta, A., Hen-\nnessy, J., 1992. Characterizing the caching and synchronization performance of a multiprocessor operating system. In:\nProceedings of the Fifth International Conference on Architectural Support for Programming Languages and Oper-\nating Systems (ASPLOS), October 12\u201315, 1992, Boston (SIGPLAN Notices 27:9 (September)), pp. 162\u2013174.\nB.8\nHistorical Perspective and References\n\u25a0\nB-59"
    },
    {
        "page": 766,
        "text": "Exercises by Amr Zaky\nB.1\n[10/10/10/15] <B.1> You are trying to appreciate how important the principle of\nlocality is in justifying the use of a cache memory, so you experiment with a com-\nputer having an L1 data cache and a main memory (you exclusively focus on data\naccesses). The latencies (in CPU cycles) of the different kinds of accesses are as\nfollows: cache hit, 1 cycle; cache miss, 110 cycles; main memory access with cache\ndisabled, 105 cycles.\na. [10] <B.1> When you run a program with an overall miss rate of 3%, what\nwill the average memory access time (in CPU cycles) be?\nb. [10] <B.1> Next, you run a program specifically designed to produce\ncompletely random data addresses with no locality. Toward that end, you use an\narray of size 1 GB (all of which fits in the main memory). Accesses to random\nelements of this array are continuously made (using a uniform random number\ngenerator to generate the elements indices). If your data cache size is 64 KB,\nwhat will the average memory access time be?\nc. [10] <B.1> If you compare the result obtained in part (b) with the main\nmemory access time when the cache is disabled, what can you conclude about\nthe role of the principle of locality in justifying the use of cache memory?\nd. [15] <B.1> You observed that a cache hit produces a gain of 104 cycles\n(1 cycle vs. 105), but it produces a loss of 5 cycles in the case of a\nmiss (110 cycles vs. 105). In the general case, we can express these two\nquantities as G (gain) and L (loss). Using these two quantities (G and L),\nidentify the highest miss rate after which the cache use would be disadvanta-\ngeous.\nB.2\n[15/15] <B.1> For the purpose of this exercise, we assume that we have a 512-\nbyte cache with 64-byte blocks. We will also assume that the main memory is 2 KB\nlarge. We can regard the memory as an array of 64-byte blocks: M0, M1, \u2026, M31.\nFigure B.30 sketches the memory blocks that can reside in different cache blocks if\nthe cache was direct-mapped.\na. [15] <B.1> Show the contents of the table if the cache is organized as a fully-\nassociative cache.\nb. [15] <B.1> Repeat part (a) with the cache organized as a four-way set\nassociative cache.\nB.3\n[10/10/10/10/15/10/15/20] <B.1> Cache organization is often influenced by the\ndesire to reduce the cache's power consumption. For that purpose we assume that\nthe cache is physically distributed into a data array (holding the data), a tag array\n(holding the tags), and replacement array (holding information needed by replace-\nment policy). Furthermore, every one of these arrays is physically distributed into\nmultiple subarrays (one per way) that can be individually accessed; for example, a\nfour-way set associative least recently used (LRU) cache would have four data sub-\narrays, four tag subarrays, and four replacement subarrays. We assume that the\nB-60\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 767,
        "text": "replacement subarrays are accessed once per access when the LRU replacement\npolicy is used, and once per miss if the first-in, first-out (FIFO) replacement policy\nis used. It is not needed when a random replacement policy is used. For a specific\ncache, it was determined that the accesses to the different arrays have the following\npower consumption weights (Figure B.31):\na. [10] <B.1> A cache read hit. All arrays are read simultaneously.\nb. [10] <B.1> Repeat part (a) for a cache read miss.\nc. [10] <B.1> Repeat part (a) assuming that the cache access is split across two\ncycles. In the first cycle, all the tag subarrays are accessed. In the second cycle,\nonly the subarray whose tag matched will be accessed.\nd. [10] <B.1> Repeat part (c) for a cache read miss (no data array accesses in the\nsecond cycle).\ne. [15] <B.1> Repeat part (c) assuming that logic is added to predict the cache\nway to be accessed. Only the tag subarray for the predicted way is accessed in\ncycle one. A way hit (address match in predicted way) implies a cache hit. A\nway miss dictates examining all the tag subarrays in the second cycle. In case of\na way hit, only one data subarray (the one whose tag matched) is accessed in\ncycle two. Assume the way predictor hits.\nCache block\nSet\nWay\nPossible memory blocks\n0\n0\n0\nM0, M8, M16, M24\n1\n1\n0\nM1, M9, M17, M25\n2\n2\n0\nM2, M10, M18, M26\n3\n3\n0\n\u2026.\n4\n4\n0\n\u2026.\n5\n5\n0\n\u2026.\n6\n6\n0\n\u2026.\n7\n7\n0\nM7, M15, M23, M31\nFigure B.30 Memory blocks distributed to direct-mapped cache.\nArray\nPower consumption weight\n(per way accessed)\nData array\n20 units\nTag\nArray 5 units\nMiscellaneous array\n1 unit\nMemory access\n200 units\nFigure B.31 Power consumption costs of different operations.\nExercises by Amr Zaky\n\u25a0\nB-61"
    },
    {
        "page": 768,
        "text": "f. [10] <B.1> Repeat part (e) assuming that the way predictor misses (the way it\nchoses is wrong). When it fails, the way predictor adds an extra cycle in which it\naccesses all the tag subarrays. Assume the way predictor miss is followed by a\ncache read hit.\ng. [15] <B.1> Repeat part (f) assuming a cache read miss.\nh. [20] <B.1> Use parts (e), (f), and (g) for the general case where the workload\nhas the following statistics: way predictor miss rate=5% and cache miss\nrate=3%. (Consider different replacement policies.)\nEstimate the memory system (cache+memory) power usage (in power units) for\nthe following configurations. We assume the cache is four-way set associative.\nProvide answers for the LRU, FIFO, and random replacement policies.\nB.4\n[10/10/15/15/15/20] <B.1> We compare the write bandwidth requirements of\nwrite-through versus write-back caches using a concrete example. Let us assume\nthat we have a 64 KB cache with a line size of 32 bytes. The cache will allocate a\nline on a write miss. If configured as a write-back cache, it will write back all of the\ndirty line if it needs to be replaced. We will also assume that the cache is connected\nto the lower level in the hierarchy through a 64-bit-wide (8-byte-wide) bus. The\nnumber of CPU cycles for a B-bytes write access on this bus is\n10 + 5 B\n8 \u00031\n\u0005\n\u0006\n, where the square brackets represent the \u201cceiling\u201d function. For\nexample, an 8-byte write would take\n10 + 5 8\n8\u00031\n\u0005\n\u0006\n\u00bc 10 cycles, whereas using the same formula a 12-byte write\nwould take 15 cycles.\nAnswer the following questions while referring to the C code snippet below:\n...\n#define PORTION 1\n...\nbase = 8*i;\nfor (unsigned int j = base; j < base + PORTION; j++)\n//assume j is stored in a register\n{\ndata[j] = j;\n}\na. [10] <B.1> For a write-through cache, how many CPU cycles are spent on\nwrite transfers to the memory for all the combined iterations of the j loop?\nb. [10] <B.1> If the cache is configured as a write-back cache, how many CPU\ncycles are spent on writing back a cache line?\nc. [15] <B.1> Change PORTION to 8 and repeat part (a).\nd. [15] <B.1> What is the minimum number of array updates to the same cache\nline (before replacing it) that would render the write-back cache superior?\ne. [15] <B.1> Think of a scenario where all the words of the cache line will be\nwritten (not necessarily using the above code) and a write-through cache will\nrequire fewer total CPU cycles than the write-back cache.\nB-62\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 769,
        "text": "B.5\n[10/10/10/10/] <B.2> You are building a system around a processor with in-order\nexecution that runs at 1.1 GHz and has a CPI of 1.35 excluding memory accesses.\nThe only instructions that read or write data from memory are loads (20% of all\ninstructions) and stores (10% of all instructions). The memory system for this com-\nputer is composed of a split L1 cache that imposes no penalty on hits. Both the I-\ncache and D-cache are direct-mapped and hold 32 KB each. The I-cache has a 2%\nmiss rate and 32-byte blocks, and the D-cache is write-through with a 5% miss rate\nand 16-byte blocks. There is a write buffer on the D-cache that eliminates stalls for\n95% of all writes. The 512 KB write-back, unified L2 cache has 64-byte blocks and\nan access time of 15 ns. It is connected to the L1 cache by a 128-bit data bus that\nruns at 266 MHz and can transfer one 128-bit word per bus cycle. Of all memory\nreferences sent to the L2 cache in this system, 80% are satisfied without going to\nmain memory. Also, 50% of all blocks replaced are dirty. The 128-bit-wide main\nmemory has an access latency of 60 ns, after which any number of bus words may\nbe transferred at the rate of one per cycle on the 128-bit-wide 133 MHz main mem-\nory bus.\na. [10] <B.2> What is the average memory access time for instruction accesses?\nb. [10] <B.2> What is the average memory access time for data reads?\nc. [10] <B.2> What is the average memory access time for data writes?\nd. [10] <B.2> What is the overall CPI, including memory accesses?\nB.6\n[10/15/15] <B.2> Converting miss rate (misses per reference) into misses per\ninstruction relies upon two factors: references per instruction fetched and the frac-\ntion of fetched instructions that actually commits.\na. [10] <B.2> The formula for misses per instruction on page B-5 is written first\nin terms of three factors: miss rate, memory accesses, and instruction count.\nEach of these factors represents actual events. What is different about writing\nmisses per instruction as miss rate times the factor memory accesses per\ninstruction?\nb. [15] <B.2> Speculative processors will fetch instructions that do not commit.\nThe formula for misses per instruction on page B-5 refers to misses per instruc-\ntion on the execution path; that is, only the instructions that must actually be\nexecuted to carry out the program. Convert the formula for misses per instruc-\ntion on page B-5 into one that uses only miss rate, references per instruction\nfetched, and fraction of fetched instructions that commit. Why rely upon these\nfactors rather than those in the formula on page B-5?\nc. [15] <B.2> The conversion in part (b) could yield an incorrect value to the\nextent that the value of the factor references per instruction fetched is not equal\nto the number of references for any particular instruction. Rewrite the formula\nof part (b) to correct this deficiency.\nB.7\n[20] <B.1, B.3> In systems with a write-through L1 cache backed by a write-back\nL2 cache instead of main memory, a merging write buffer can be simplified.\nExplain how this can be done. Are there situations where having a full write buffer\n(instead of the simple version you have just proposed) could be helpful?\nExercises by Amr Zaky\n\u25a0\nB-63"
    },
    {
        "page": 770,
        "text": "B.8\n[5/5/5] <B.3> We want to observe the following calculation\ndi \u00bc ai + bi \u2217ci ,\ni : 0 : 511\n\u00f0\n\u00de\nArrays a, b, c, and d memory layout is displayed below (each has 512 4-byte-wide\ninteger elements).\nThe above calculation employs a for loop that runs through 512 iterations.\nAssume a 32 Kbyte 4-way set associative cache with a single cycle access time.\nThe miss penalty is 100 CPU cycles/access, and so is the cost of a write-back. The\ncache is a write-back on hits write-allocate on misses cache (Figure B.32).\na. [5]<B3>How many cycles will an iteration take if all three loads and single\nstore miss in the data cache?\nb. [5]<B3>If the cache line size is 16 bytes, what is the average number of\ncycles an average iteration will take? (Hint: Spatial locality!)\nc. [5]<B3>If the cache line size is 64 bytes, what is the average number of\ncycles an average iteration will take?\nd. If the cache is direct-mapped and its size is reduced to 2048 bytes, what is the\naverage number of cycles an average iteration will take?\nB.9\n[20]<B.3> Increasing a cache's associativity (with all other parameters kept con-\nstant) statistically reduces the miss rate. However, there can be pathological cases\nwhere increasing a cache's associativity would increase the miss rate for a partic-\nular workload.\nConsider the case of direct-mapped compared to a two-way set associative\ncache of equal size. Assume that the set associative cache uses the LRU replace-\nment policy. To simplify, assume that the block size is one word. Now, construct a\ntrace of word accesses that would produce more misses in the two-way associative\ncache.\n(Hint: Focus on constructing a trace of accesses that are exclusively directed to\na single set of the two-way set associative cache, such that the same trace would\nexclusively access two blocks in the direct-mapped cache.)\nB.10\n[10/10/15] <B.3> Consider a two-level memory hierarchy made of L1 and L2\ndata caches. Assume that both caches use write-back policy on write hit and both\nhave the same block size. List the actions taken in response to the following events:\na. [10] <B.3> An L1 cache miss when the caches are organized in an inclusive\nhierarchy.\nMem. address in bytes\nContents\n0\u20132047\nArray a\n2048\u20134095\nArray b\n4096\u20136143\nArray c\n6144\u20138191\nArray d\nFigure B.32 Arrays layout in memory.\nB-64\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 771,
        "text": "b. [10] <B.3> An L1 cache miss when the caches are organized in an exclusive\nhierarchy.\nc. [15] <B.3> In both parts (a) and (b), consider the possibility that the evicted\nline might be clean or dirty.\nB.11\n[15/20] <B.2, B.3> excluding some instructions from entering the cache can\nreduce conflict misses.\na. [15] <B.3> Sketch a program hierarchy where parts of the program would be\nbetter excluded from entering the instruction cache. (Hint: Consider a program\nwith code blocks that are placed in deeper loop nests than other blocks.)\nb. [20] <B.2, B.3> Suggest software or hardware techniques to enforce exclu-\nsion of certain blocks from the instruction cache.\nB.12\n[5/15] <B.3>Whereas larger caches have lower miss rates, they also tend to have\nlonger hit times.\nAssume a direct-mapped 8 KB cache has 0.22 ns hit time and miss rate m1; also\nassume a 4-way associative 64 KB cache has 0.52 ns hit time and a miss rate m2.\na. [5]<B.3>If the miss penalty is 100 ns, when would it be advantageous to use\nthe smaller cache to reduce the overall memory access time?\nb. [15]<B.3>Repeat part (a) for miss penalties of 10 and 1000 cycles. Conclude\nwhen it might be advantageous to use a smaller cache.\nB.13\n[15] <B.4> A program is running on a computer with a four-entry fully associa-\ntive (micro) translation lookaside buffer (TLB) (Figure B.33):\nThe following is a trace of virtual page numbers accessed by a program. For\neach access indicate whether it produces a TLB hit/miss and, if it accesses the page\ntable, whether it produces a page hit or fault. Put an X under the page table column\nif it is not accessed (Figures B.34 and B.35).\nB.14\n[15/15/15/15/] <B.4> Some memory systems handle TLB misses in software (as\nan exception), while others use hardware for TLB misses.\na. [15] <B.4> What are the trade-offs between these two methods for handling\nTLB misses?\nb. [15] <B.4> Will TLB miss handling in software always be slower than TLB\nmiss handling in hardware? Explain.\nVP#\nPP#\nEntry valid\n5\n30\n1\n7\n1\n0\n10\n10\n1\n15\n25\n1\nFigure B.33 TLB contents (problem B.12).\nExercises by Amr Zaky\n\u25a0\nB-65"
    },
    {
        "page": 772,
        "text": "c. [15] <B.4> Are there page table structures that would be difficult to handle in\nhardware but possible in software? Are there any such structures that would be\ndifficult for software to handle but easy for hardware to manage?\nd. [15] <B.4> Why are TLB miss rates for floating-point programs generally\nhigher than those for integer programs?\nVirtual page index\nPhysical page #\nPresent\n0\n3\nY\n1\n7\nN\n2\n6\nN\n3\n5\nY\n4\n14\nY\n5\n30\nY\n6\n26\nY\n7\n11\nY\n8\n13\nN\n9\n18\nN\n10\n10\nY\n11\n56\nY\n12\n110\nY\n13\n33\nY\n14\n12\nN\n15\n25\nY\nFigure B.34 Page table contents.\nVirtual page accessed\nTLB (hit or miss)\nPage table (hit or fault)\n1\n5\n9\n14\n10\n6\n15\n12\n7\n2\nFigure B.35 Page access trace.\nB-66\n\u25a0\nAppendix B Review of Memory Hierarchy"
    },
    {
        "page": 773,
        "text": "B.15\n[20/20] <B.5> It is possible to provide more flexible protection than that in the\nIntel Pentium architecture by using a protection scheme similar to that used in the\nHewlett-Packard Precision Architecture (HP/PA). In such a scheme, each page\ntable entry contains a \u201cprotection ID\u201d (key) along with access rights for the page.\nOn each reference, the CPU compares the protection ID in the page table entry with\nthose stored in each of four protection ID registers (access to these registers\nrequires that the CPU be in supervisor mode). If there is no match for the protection\nID in the page table entry or if the access is not a permitted access (writing to a read-\nonly page, for example), an exception is generated.\na. [20] <B.5> Explain how this model could be used to facilitate the construction\nof operating systems from relatively small pieces of code that cannot overwrite\neach other (microkernels). What advantages might such an operating system\nhave over a monolithic operating system in which any code in the OS can write\nto any memory location?\nb. [20] <B.5> A simple design change to this system would allow two protection\nIDs for each page table entry, one for read access and the other for either write or\nexecute access (the field is unused if neither the writable nor executable bit is\nset). What advantages might there be from having different protection IDs for\nread and write capabilities? (Hint: Could this make it easier to share data and\ncode between processes?)\nExercises by Amr Zaky\n\u25a0\nB-67"
    },
    {
        "page": 774,
        "text": "C.1\nIntroduction\nC-2\nC.2\nThe Major Hurdle of Pipelining\u2014Pipeline Hazards\nC-10\nC.3\nHow Is Pipelining Implemented?\nC-26\nC.4\nWhat Makes Pipelining Hard to Implement?\nC-37\nC.5\nExtending the RISC V Integer Pipeline to Handle Multicycle\nOperations\nC-45\nC.6\nPutting It All Together: The MIPS R4000 Pipeline\nC-55\nC.7\nCross-Cutting Issues\nC-65\nC.8\nFallacies and Pitfalls\nC-70\nC.9\nConcluding Remarks\nC-71\nC.10\nHistorical Perspective and References\nC-71\nUpdated Exercises by Diana Franklin\nC-71"
    },
    {
        "page": 775,
        "text": "C\nPipelining: Basic and\nIntermediate Concepts\nIt is quite a three-pipe problem.\nSir Arthur Conan Doyle,\nThe Adventures of Sherlock Holmes"
    },
    {
        "page": 776,
        "text": "C.1\nIntroduction\nMany readers of this text will have covered the basics of pipelining in another text\n(such as our more basic text Computer Organization and Design) or in another\ncourse. Because Chapter 3 builds heavily on this material, readers should ensure\nthat they are familiar with the concepts discussed in this appendix before proceed-\ning. As you read Chapter 3, you may find it helpful to turn to this material for a\nquick review.\nWe begin the appendix with the basics of pipelining, including discussing the\ndata path implications, introducing hazards, and examining the performance of\npipelines. This section describes the basic five-stage RISC pipeline that is the basis\nfor the rest of the appendix. Section C.2 describes the issue of hazards, why they\ncause performance problems, and how they can be dealt with. Section C.3 dis-\ncusses how the simple five-stage pipeline is actually implemented, focusing on\ncontrol and how hazards are dealt with.\nSection C.4 discusses the interaction between pipelining and various aspects of\ninstruction set design, including discussing the important topic of exceptions and\ntheir interaction with pipelining. Readers unfamiliar with the concepts of precise\nand imprecise interrupts and resumption after exceptions will find this material\nuseful, because they are key to understanding the more advanced approaches in\nChapter 3.\nSection C.5 discusses how the five-stage pipeline can be extended to handle\nlonger-running floating-point instructions. Section C.6 puts these concepts\ntogether in a case study of a deeply pipelined processor, the MIPS R4000/4400,\nincluding both the eight-stage integer pipeline and the floating-point pipeline.\nThe MIPS R40000 is similar to a single-issue embedded processor, such as the\nARM Cortex-A5, which became available in 2010, and was used in several smart\nphones and tablets.\nSection C.7 introduces the concept of dynamic scheduling and the use of\nscoreboards to implement dynamic scheduling. It is introduced as a cross-cutting\nissue, because it can be used to serve as an introduction to the core concepts in\nChapter 3, which focused on dynamically scheduled approaches. Section C.7 is\nalso a gentle introduction to the more complex Tomasulo\u2019s algorithm covered\nin Chapter 3. Although Tomasulo\u2019s algorithm can be covered and understood with-\nout introducing scoreboarding, the scoreboarding approach is simpler and easier to\ncomprehend.\nWhat Is Pipelining?\nPipelining is an implementation technique whereby multiple instructions are over-\nlapped in execution; it takes advantage of parallelism that exists among the actions\nneeded to execute an instruction. Today, pipelining is the key implementation tech-\nnique used to make fast processors, and even processors that cost less than a dollar\nare pipelined.\nC-2\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 777,
        "text": "A pipeline is like an assembly line. In an automobile assembly line, there are\nmany steps, each contributing something to the construction of the car. Each step\noperates in parallel with the other steps, although on a different car. In a computer\npipeline, each step in the pipeline completes a part of an instruction. Like the\nassembly line, different steps are completing different parts of different instruc-\ntions in parallel. Each of these steps is called a pipe stage or a pipe segment.\nThe stages are connected one to the next to form a pipe\u2014instructions enter at\none end, progress through the stages, and exit at the other end, just as cars would\nin an assembly line.\nIn an automobile assembly line, throughput is defined as the number of cars per\nhour and is determined by how often a completed car exits the assembly line. Like-\nwise, the throughput of an instruction pipeline is determined by how often an\ninstruction exits the pipeline. Because the pipe stages are hooked together, all\nthe stages must be ready to proceed at the same time, just as we would require\nin an assembly line. The time required between moving an instruction one step\ndown the pipeline is a processor cycle. Because all stages proceed at the same time,\nthe length of a processor cycle is determined by the time required for the slowest\npipe stage, just as in an auto assembly line the longest step would determine the\ntime between advancing cars in the line. In a computer, this processor cycle is\nalmost always 1 clock cycle.\nThe pipeline designer\u2019s goal is to balance the length of each pipeline stage, just\nas the designer of the assembly line tries to balance the time for each step in the\nprocess. If the stages are perfectly balanced, then the time per instruction on the\npipelined processor\u2014assuming ideal conditions\u2014is equal to\nTime per instruction on unpipelined machine\nNumber of pipe stages\nUnder these conditions, the speedup from pipelining equals the number of pipe\nstages, just as an assembly line with n stages can ideally produce cars n times\nas fast. Usually, however, the stages will not be perfectly balanced; furthermore,\npipelining does involve some overhead. Thus, the time per instruction on the pipe-\nlined processor will not have its minimum possible value, yet it can be close.\nPipelining yields a reduction in the average execution time per instruction. If\nthe starting point is a processor that takes multiple clock cycles per instruction, then\npipelining reduces the CPI. This is the primary view we will take.\nPipelining is an implementation technique that exploits parallelism among\nthe instructions in a sequential instruction stream. It has the substantial advantage\nthat, unlike some speedup techniques (see Chapter 4), it is not visible to the\nprogrammer.\nThe Basics of the RISC V Instruction Set\nThroughout this book we use RISC V, a load-store architecture, to illustrate the\nbasic concepts. Nearly all the ideas we introduce in this book are applicable to other\nC.1\nIntroduction\n\u25a0\nC-3"
    },
    {
        "page": 778,
        "text": "processors, but the implementation may be much more complicated with complex\ninstructions. In this section, we make use of the core of the RISC V architecture;\nsee Chapter 1 for a full description. Although we use RISC V, the concepts are\nsignificantly similar in that they will apply to any RISC, including the core archi-\ntectures of ARM and MIPS. All RISC architectures are characterized by a few key\nproperties:\n\u25a0\nAll operations on data apply to data in registers and typically change the entire\nregister (32 or 64 bits per register).\n\u25a0\nThe only operations that affect memory are load and store operations that move\ndata from memory to a register or to memory from a register, respectively.\nLoad and store operations that load or store less than a full register (e.g., a byte,\n16 bits, or 32 bits) are often available.\n\u25a0\nThe instruction formats are few in number, with all instructions typically being\none size. In RISC V, the register specifiers: rs1, rs2, and rd are always in the\nsame place simplifying the control.\nThese simple properties lead to dramatic simplifications in the implementation of\npipelining, which is why these instruction sets were designed this way. Chapter 1\ncontains a full description of the RISC V ISA, and we assume the reader has read\nChapter 1.\nA Simple Implementation of a RISC Instruction Set\nTo understand how a RISC instruction set can be implemented in a pipelined\nfashion, we need to understand how it is implemented without pipelining. This\nsection shows a simple implementation where every instruction takes at most 5\nclock cycles. We will extend this basic implementation to a pipelined version,\nresulting in a much lower CPI. Our unpipelined implementation is not the most eco-\nnomical or the highest-performance implementation without pipelining. Instead, it\nis designed to lead naturally to a pipelined implementation. Implementing the\ninstruction set requires the introduction of several temporary registers that are\nnot part of the architecture; these are introduced in this section to simplify pipelin-\ning. Our implementation will focus only on a pipeline for an integer subset of a\nRISC architecture that consists of load-store word, branch, and integer ALU\noperations.\nEvery instruction in this RISC subset can be implemented in, at most, 5 clock\ncycles. The 5 clock cycles are as follows.\n1. Instruction fetch cycle (IF):\nSend the program counter (PC) to memory and fetch the current instruction from\nmemory. Update the PC to the next sequential instruction by adding 4 (because\neach instruction is 4 bytes) to the PC.\nC-4\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 779,
        "text": "2. Instruction decode/register fetch cycle (ID):\nDecode the instruction and read the registers corresponding to register source\nspecifiers from the register file. Do the equality test on the registers as they\nare read, for a possible branch. Sign-extend the offset field of the instruction\nin case it is needed. Compute the possible branch target address by adding\nthe sign-extended offset to the incremented PC.\nDecoding is done in parallel with reading registers, which is possible because\nthe register specifiers are at a fixed location in a RISC architecture. This tech-\nnique is known as fixed-field decoding. Note that we may read a register we don\u2019t\nuse, which doesn\u2019t help but also doesn\u2019t hurt performance. (It does waste energy\nto read an unneeded register, and power-sensitive designs might avoid this.) For\nloads and ALU immediate operations, the immediate field is always in the same\nplace, so we can easily sign extend it. (For a more complete implementation of\nRISC V, we would need to compute two different sign-extended values, because\nthe immediate field for store is in a different location.)\n3. Execution/effective address cycle (EX):\nThe ALU operates on the operands prepared in the prior cycle, performing one\nof three functions, depending on the instruction type.\n\u25a0\nMemory reference\u2014The ALU adds the base register and the offset to form\nthe effective address.\n\u25a0\nRegister-Register ALU instruction\u2014The ALU performs the operation spec-\nified by the ALU opcode on the values read from the register file.\n\u25a0\nRegister-Immediate ALU instruction\u2014The ALU performs the operation\nspecified by the ALU opcode on the first value read from the register file\nand the sign-extended immediate.\n\u25a0\nConditional branch\u2014Determine whether the condition is true.\nIn a load-store architecture the effective address and execution cycles can be\ncombined into a single clock cycle, because no instruction needs to simulta-\nneously calculate a data address and perform an operation on the data.\n4. Memory access (MEM):\nIf the instruction is a load, the memory does a read using the effective address\ncomputed in the previous cycle. If it is a store, then the memory writes the data\nfrom the second register read from the register file using the effective address.\n5. Write-back cycle (WB):\n\u25a0\nRegister-Register ALU instruction or load instruction:\nWrite the result into the register file, whether it comes from the memory system\n(for a load) or from the ALU (for an ALU instruction).\nIn this implementation, branch instructions require three cycles, store instruc-\ntions require four cycles, and all other instructions require five cycles. Assuming a\nC.1\nIntroduction\n\u25a0\nC-5"
    },
    {
        "page": 780,
        "text": "branch frequency of 12% and a store frequency of 10%, a typical instruction dis-\ntribution leads to an overall CPI of 4.66. This implementation, however, is not opti-\nmal either in achieving the best performance or in using the minimal amount of\nhardware given the performance level; we leave the improvement of this design\nas an exercise for you and instead focus on pipelining this version.\nThe Classic Five-Stage Pipeline for a RISC Processor\nWe can pipeline the execution described in the previous section with almost no\nchanges by simply starting a new instruction on each clock cycle. (See why we\nchose this design?) Each of the clock cycles from the previous section becomes\na pipe stage\u2014a cycle in the pipeline. This results in the execution pattern shown\nin Figure C.1, which is the typical way a pipeline structure is drawn. Although each\ninstruction takes 5 clock cycles to complete, during each clock cycle the hardware\nwill initiate a new instruction and will be executing some part of the five different\ninstructions.\nYou may find it hard to believe that pipelining is as simple as this; it\u2019s not. In\nthis and the following sections, we will make our RISC pipeline \u201creal\u201d by dealing\nwith problems that pipelining introduces.\nTo start with, we have to determine what happens on every clock cycle of the\nprocessor and make sure we don\u2019t try to perform two different operations with the\nsame data path resource on the same clock cycle. For example, a single ALU can-\nnot be asked to compute an effective address and perform a subtract operation at\nthe same time. Thus, we must ensure that the overlap of instructions in the pipeline\ncannot cause such a conflict. Fortunately, the simplicity of a RISC instruction set\nmakes resource evaluation relatively easy. Figure C.2 shows a simplified version\nof a RISC data path drawn in pipeline fashion. As you can see, the major functional\nunits are used in different cycles, and hence overlapping the execution of multiple\nInstruction number\nClock number\n1\n2\n3\n4\n5\n6\n7\n8\n9\nInstruction i\nIF\nID\nEX\nMEM\nWB\nInstruction i+1\nIF\nID\nEX\nMEM\nWB\nInstruction i+2\nIF\nID\nEX\nMEM\nWB\nInstruction i+3\nIF\nID\nEX\nMEM\nWB\nInstruction i+4\nIF\nID\nEX\nMEM\nWB\nFigure C.1 Simple RISC pipeline. On each clock cycle, another instruction is fetched and begins its five-cycle\nexecution. If an instruction is started every clock cycle, the performance will be up to five times that of a processor\nthat is not pipelined. The names for the stages in the pipeline are the same as those used for the cycles in the unpi-\npelined implementation: IF\u00bcinstruction fetch, ID\u00bcinstruction decode, EX\u00bcexecution, MEM\u00bcmemory access, and\nWB\u00bcwrite-back.\nC-6\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 781,
        "text": "instructions introduces relatively few conflicts. There are three observations on\nwhich this fact rests.\nFirst, we use separate instruction and data memories, which we would typically\nimplement with separate instruction and data caches (discussed in Chapter 2). The\nuse of separate caches eliminates a conflict for a single memory that would arise\nbetween instruction fetch and data memory access. Notice that if our pipelined pro-\ncessor has a clock cycle that is equal to that of the unpipelined version, the memory\nsystem must deliver five times the bandwidth. This increased demand is one cost of\nhigher performance.\nSecond, the register file is used in the two stages: one for reading in ID and one\nfor writing in WB. These uses are distinct, so we simply show the register file in\ntwo places. Hence, we need to perform two reads and one write every clock cycle.\nALU\nALU\nReg\nReg\nIM\nDM\nReg\nIM\nDM\nTime (in clock cycles)\nCC 1\nCC 2\nCC 3\nCC 4\nCC 5\nCC 6\nCC 7\nProgram execution order (in instructions) \nReg\nCC 8\nCC 9\nReg\nIM\nDM\nReg\nALU\nReg\nIM\nDM\nReg\nALU\nReg\nIM\nDM\nReg\nALU\nFigure C.2 The pipeline can be thought of as a series of data paths shifted in time. This figure shows the overlap\namong the parts of the data path, with clock cycle 5 (CC 5) showing the steady-state situation. Because the reg-\nister file is used as a source in the ID stage and as a destination in the WB stage, it appears twice. We show that it\nis read in one part of the stage and written in another by using a solid line, on the right or left, respectively, and a\ndashed line on the other side. The abbreviation IM is used for instruction memory, DM for data memory, and CC\nfor clock cycle.\nC.1\nIntroduction\n\u25a0\nC-7"
    },
    {
        "page": 782,
        "text": "To handle reads and a write to the same register (and for another reason, which will\nbecome obvious shortly), we perform the register write in the first half of the clock\ncycle and the read in the second half.\nThird, Figure C.2 does not deal with the PC. To start a new instruction every\nclock, we must increment and store the PC every clock, and this must be done dur-\ning the IF stage in preparation for the next instruction. Furthermore, we must also\nhave an adder to compute the potential branch target address during ID. One further\nproblem is that we need the ALU in the ALU stage to evaluate the branch condi-\ntion. Actually, we don\u2019t really need a full ALU to evaluate the comparison between\ntwo registers, but we need enough of the function that it has to occur in this\npipestage.\nAlthough it is critical to ensure that instructions in the pipeline do not attempt to\nuse the hardware resources at the same time, we must also ensure that instructions\nin different stages of the pipeline do not interfere with one another. This separation\nis done by introducing pipeline registers between successive stages of the pipeline,\nso that at the end of a clock cycle all the results from a given stage are stored into a\nregister that is used as the input to the next stage on the next clock cycle. Figure C.3\nshows the pipeline drawn with these pipeline registers.\nAlthough many figures will omit such registers for simplicity, they are required\nto make the pipeline operate properly and must be present. Of course, similar reg-\nisters would be needed even in a multicycle data path that had no pipelining\n(because only values in registers are preserved across clock boundaries). In the case\nof a pipelined processor, the pipeline registers also play the key role of carrying\nintermediate results from one stage to another where the source and destination\nmay not be directly adjacent. For example, the register value to be stored during\na store instruction is read during ID, but not actually used until MEM; it is passed\nthrough two pipeline registers to reach the data memory during the MEM stage.\nLikewise, the result of an ALU instruction is computed during EX, but not actually\nstored until WB; it arrives there by passing through two pipeline registers. It is\nsometimes useful to name the pipeline registers, and we follow the convention\nof naming them by the pipeline stages they connect, so the registers are called\nIF/ID, ID/EX, EX/MEM, and MEM/WB.\nBasic Performance Issues in Pipelining\nPipelining increases the processor instruction throughput\u2014the number of instruc-\ntions completed per unit of time\u2014but it does not reduce the execution time of an\nindividual instruction. In fact, it usually slightly increases the execution time\nof each instruction due to overhead in the control of the pipeline. The increase\nin instruction throughput means that a program runs faster and has lower total exe-\ncution time, even though no single instruction runs faster!\nThe fact that the execution time of each instruction does not decrease puts\nlimits on the practical depth of a pipeline, as we will see in the next section. In\naddition to limitations arising from pipeline latency, limits arise from imbalance\nC-8\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 783,
        "text": "among the pipe stages and from pipelining overhead. Imbalance among the pipe\nstages reduces performance because the clock can run no faster than the time\nneeded for the slowest pipeline stage. Pipeline overhead arises from the combina-\ntion of pipeline register delay and clock skew. The pipeline registers add setup\ntime, which is the time that a register input must be stable before the clock signal\nthat triggers a write occurs, plus propagation delay to the clock cycle. Clock skew,\nwhich is the maximum delay between when the clock arrives at any two registers,\nCC 1\nCC 2\nCC 3\nCC 4\nCC 5\nCC 6\nTime (in clock cycles)\nDM\nIM\nALU\nDM\nIM\nALU\nDM\nIM\nALU\nIM\nALU\nIM\nReg\nReg\nReg\nReg\nReg\nReg\nReg\nFigure C.3 A pipeline showing the pipeline registers between successive pipeline stages. Notice that the registers\nprevent interference between two different instructions in adjacent stages in the pipeline. The registers also play the\ncritical role of carrying data for a given instruction from one stage to the other. The edge-triggered property of reg-\nisters\u2014that is, that the values change instantaneously on a clock edge\u2014is critical. Otherwise, the data from one\ninstruction could interfere with the execution of another!\nC.1\nIntroduction\n\u25a0\nC-9"
    },
    {
        "page": 784,
        "text": "also contributes to the lower limit on the clock cycle. Once the clock cycle is as\nsmall as the sum of the clock skew and latch overhead, no further pipelining is\nuseful, because there is no time left in the cycle for useful work. The interested\nreader should see Kunkel and Smith (1986).\nExample\nConsider the unpipelined processor in the previous section. Assume that it has a\n4 GHz clock (or a 0.5 ns clock cycle) and that it uses four cycles for ALU oper-\nations and branches and five cycles for memory operations. Assume that the rel-\native frequencies of these operations are 40%, 20%, and 40%, respectively.\nSuppose that due to clock skew and setup, pipelining the processor adds 0.1 ns\nof overhead to the clock. Ignoring any latency impact, how much speedup in\nthe instruction execution rate will we gain from a pipeline?\nAnswer\nThe average instruction execution time on the unpipelined processor is\nAverage instruction execution time \u00bc Clock cycleAverage CPI\n\u00bc 0:5 ns 40% + 20%\n\u00f0\n\u00de4 + 40%5\n\u00bd\n\u0003\n\u00bc 0:5 ns4:4\n\u00bc 2:2 ns\nIn the pipelined implementation, the clock must run at the speed of the slowest\nstage plus overhead, which will be 0.5+0.1 or 0.6 ns; this is the average instruction\nexecution time. Thus, the speedup from pipelining is\nSpeedup from pipelining \u00bc Average instruction time unpipelined\nAverage instruction time pipelined\n\u00bc 2:2 ns\n0:6 ns \u00bc 3:7 times\nThe 0.1 ns overhead essentially establishes a limit on the effectiveness of pipelin-\ning. If the overhead is not affected by changes in the clock cycle, Amdahl\u2019s Law\ntells us that the overhead limits the speedup.\nThis simple RISC pipeline would function just fine for integer instructions if\nevery instruction were independent of every other instruction in the pipeline. In\nreality, instructions in the pipeline can depend on one another; this is the topic\nof the next section.\nC.2\nThe Major Hurdle of Pipelining\u2014Pipeline Hazards\nThere are situations, called hazards, that prevent the next instruction in the instruc-\ntion stream from executing during its designated clock cycle. Hazards reduce the\nperformance from the ideal speedup gained by pipelining. There are three classes\nof hazards:\nC-10\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 785,
        "text": "1. Structural hazards arise from resource conflicts when the hardware cannot sup-\nport all possible combinations of instructions simultaneously in overlapped exe-\ncution. In modern processors, structural hazards occur primarily in special\npurpose functional units that are less frequently used (such as floating point\ndivide or other complex long running instructions). They are not a major per-\nformance factor, assuming programmers and compiler writers are aware of the\nlower throughput of these instructions. Instead of spending more time on this\ninfrequent case, we focus on the two other hazards that are much more frequent.\n2. Data hazards arise when an instruction depends on the results of a previous\ninstruction in a way that is exposed by the overlapping of instructions in the\npipeline.\n3. Control hazards arise from the pipelining of branches and other instructions\nthat change the PC.\nHazards in pipelines can make it necessary to stall the pipeline. Avoiding a haz-\nard often requires that some instructions in the pipeline be allowed to proceed while\nothers are delayed. For the pipelines we discuss in this appendix, when an instruction\nis stalled, all instructions issued later than the stalled instruction\u2014and hence not as\nfar along in the pipeline\u2014are also stalled. Instructions issued earlier than the stalled\ninstruction\u2014and hence farther along in the pipeline\u2014must continue, because oth-\nerwise the hazard will never clear. As a result, no new instructions are fetched during\nthe stall. We will see several examples of how pipeline stalls operate in this section\u2014\ndon\u2019t worry, they aren\u2019t as complex as they might sound!\nPerformance of Pipelines With Stalls\nA stall causes the pipeline performance to degrade from the ideal performance.\nLet\u2019s look at a simple equation for finding the actual speedup from pipelining,\nstarting with the formula from the previous section:\nSpeedup from pipelining \u00bc Average instruction time unpipelined\nAverage instruction time pipelined\n\u00bc CPI unpipelinedClock cycle unpipelined\nCPI pipelinedClock cycle pipelined\n\u00bc CPI unpipelinedClock cycle unpipelined\nCPI pipelinedClock cycle pipelined\nPipelining can be thought of as decreasing the CPI or the clock cycle time. Because\nit is traditional to use the CPI to compare pipelines, let\u2019s start with that assumption.\nThe ideal CPI on a pipelined processor is almost always 1. Hence, we can compute\nthe pipelined CPI:\nCPI pipelined \u00bc Ideal CPI + Pipeline stall clock cycles per instruction\n\u00bc 1 + Pipelines stall clock cycles per instruction\nC.2\nThe Major Hurdle of Pipelining\u2014Pipeline Hazards\n\u25a0\nC-11"
    },
    {
        "page": 786,
        "text": "If we ignore the cycle time overhead of pipelining and assume that the stages\nare perfectly balanced, then the cycle time of the two processors can be equal,\nleading to\nSpeedup \u00bc\nCPI unpiplined\n1 + Pipeline stall cycles per instruction\nOne important simple case is where all instructions take the same number of cycles,\nwhich must also equal the number of pipeline stages (also called the depth of the\npipeline). In this case, the unpipelined CPI is equal to the depth of the pipeline,\nleading to\nSpeedup \u00bc\nPipeline depth\n1 + Pipeline stall cycles per instruction\nIf there are no pipeline stalls, this leads to the intuitive result that pipelining can\nimprove performance by the depth of the pipeline.\nData Hazards\nA major effect of pipelining is to change the relative timing of instructions by over-\nlapping their execution. This overlap introduces data and control hazards. Data\nhazards occur when the pipeline changes the order of read/write accesses to\noperands so that the order differs from the order seen by sequentially executing\ninstructions on an unpipelined processor. Assume instruction i occurs in program\norder before instruction j and both instructions use register x, then there are three\ndifferent types of hazards that can occur between i and j:\n1. Read After Write (RAW) hazard: the most common, these occur when a\nread of register x by instruction j occurs before the write of register x by instruc-\ntion i. If this hazard were not prevented instruction j would use the wrong value\nof x.\n2. Write After Read (WAR) hazard: this hazard occurs when read of register x by\ninstruction i occurs after a write of register x by instruction j. In this case,\ninstruction i would use the wrong value of x. WAR hazards are impossible\nin the simple five stage, integrer pipeline, but they occur when instructions\nare reordered, as we will see when we discuss dynamically scheduled pipelines\nbeginning on page C.65.\n3. Write After Write (WAW) hazard: this hazard occurs when write of register x by\ninstruction i occurs after a write of register x by instruction j. When this occurs,\nregister x will have the wrong value going forward. WAR hazards are also\nimpossible in the simple five stage, integrer pipeline, but they occur when\ninstructions are reordered or when running times vary, as we will see later.\nChapter 3 explores the issues of data dependence and hazards in much more detail.\nFor now, we focus only on RAW hazards.\nC-12\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 787,
        "text": "Consider the pipelined execution of these instructions:\nadd\nx1,x2,x3\nsub\nx4,x1,x5\nand\nx6,x1,x7\nor\nx8,x1,x9\nxor\nx10,x1,x11\nAll the instructions after the add use the result of the add instruction. As shown in\nFigure C.4, the add instruction writes the value of x1 in the WB pipe stage, but the\nsub instruction reads the value during its ID stage, which results in a RAW hazard.\nUnless precautions are taken to prevent it, the sub instruction will read the wrong\nvalue and try to use it. In fact, the value used by the sub instruction is not even\ndeterministic: though we might think it logical to assume that sub would always\nuse the value of x1 that was assigned by an instruction prior to add, this is not\nFigure C.4 The use of the result of the add instruction in the next three instructions causes a hazard, because the\nregister is not written until after those instructions read it.\nC.2\nThe Major Hurdle of Pipelining\u2014Pipeline Hazards\n\u25a0\nC-13"
    },
    {
        "page": 788,
        "text": "always the case. If an interrupt should occur between the add and sub\ninstructions, the WB stage of the add will complete, and the value of x1 at that\npoint will be the result of the add. This unpredictable behavior is obviously\nunacceptable.\nThe and instruction also creates a possible RAW hazard. As we can see from\nFigure C.4, the write of x1 does not complete until the end of clock cycle 5. Thus,\nthe and instruction that reads the registers during clock cycle 4 will receive the\nwrong results.\nThe xor instruction operates properly because its register read occurs in\nclock cycle 6, after the register write. The or instruction also operates without\nincurring a hazard because we perform the register file reads in the second half\nof the cycle and the writes in the first half. Note that the xor instruction still\ndepends on the add, but it no longer creates a hazard; a topic we explore in more\ndetail in Chapter 3.\nThe next subsection discusses a technique to eliminate the stalls for the hazard\ninvolving the sub and and instructions.\nMinimizing Data Hazard Stalls by Forwarding\nThe problem posed in Figure C.4 can be solved with a simple hardware technique\ncalled forwarding (also called bypassing and sometimes short-circuiting). The key\ninsight in forwarding is that the result is not really needed by the sub until after the\nadd actually produces it. If the result can be moved from the pipeline register\nwhere the add stores it to where the sub needs it, then the need for a stall can\nbe avoided. Using this observation, forwarding works as follows:\n1. The ALU result from both the EX/MEM and MEM/WB pipeline registers is\nalways fed back to the ALU inputs.\n2. If the forwarding hardware detects that the previous ALU operation has written\nthe register corresponding to a source for the current ALU operation, control\nlogic selects the forwarded result as the ALU input rather than the value read\nfrom the register file.\nNotice that with forwarding, if the sub is stalled, the add will be completed and\nthe bypass will not be activated. This relationship is also true for the case of an\ninterrupt between the two instructions.\nAs the example in Figure C.4 shows, we need to forward results not only from\nthe immediately previous instruction but also possibly from an instruction that\nstarted two cycles earlier. Figure C.5 shows our example with the bypass paths\nin place and highlighting the timing of the register read and writes. This code\nsequence can be executed without stalls.\nForwarding can be generalized to include passing a result directly to the func-\ntional unit that requires it: a result is forwarded from the pipeline register corre-\nsponding to the output of one unit to the input of another, rather than just from\nC-14\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 789,
        "text": "the result of a unit to the input of the same unit. Take, for example, the following\nsequence:\nadd\nx1,x2,x3\nld\nx4,0(x1)\nsd\nx4,12(x1)\nTo prevent a stall in this sequence, we would need to forward the values of\nthe ALU output and memory unit output from the pipeline registers to the\nALU and data memory inputs. Figure C.6 shows all the forwarding paths for this\nexample.\nFigure C.5 A set of instructions that depends on the add result uses forwarding paths to avoid the data hazard.\nThe inputs for the sub and and instructions forward from the pipeline registers to the first ALU input. The or\nreceives its result by forwarding through the register file, which is easily accomplished by reading the registers\nin the second half of the cycle and writing in the first half, as the dashed lines on the registers indicate. Notice\nthat the forwarded result can go to either ALU input; in fact, both ALU inputs could use forwarded inputs from\neither the same pipeline register or from different pipeline registers. This would occur, for example, if the and\ninstruction was and x6,x1,x4.\nC.2\nThe Major Hurdle of Pipelining\u2014Pipeline Hazards\n\u25a0\nC-15"
    },
    {
        "page": 790,
        "text": "Data Hazards Requiring Stalls\nUnfortunately, not all potential data hazards can be handled by bypassing. Con-\nsider the following sequence of instructions:\nld\nx1,0(x2)\nsub\nx4,x1,x5\nand\nx6,x1,x7\nor\nx8,x1,x9\nThe pipelined data path with the bypass paths for this example is shown in\nFigure C.7. This case is different from the situation with back-to-back ALU oper-\nations. The ld instruction does not have the data until the end of clock cycle 4 (its\nMEM cycle), while the sub instruction needs to have the data by the beginning of\nthat clock cycle. Thus, the data hazard from using the result of a load instruction\ncannot be completely eliminated with simple hardware. As Figure C.7 shows, such\na forwarding path would have to operate backward in time\u2014a capability not yet\navailable to computer designers! We can forward the result immediately to the\nALU from the pipeline registers for use in the and operation, which begins 2 clock\ncycles after the load. Likewise, the or instruction has no problem, because it\nreceives the value through the register file. For the sub instruction, the forwarded\nFigure C.6 Forwarding of operand required by stores during MEM. The result of the load is forwarded from the\nmemory output to the memory input to be stored. In addition, the ALU output is forwarded to the ALU input for the\naddress calculation of both the load and the store (this is no different than forwarding to another ALU operation). If\nthe store depended on an immediately preceding ALU operation (not shown herein), the result would need to be\nforwarded to prevent a stall.\nC-16\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 791,
        "text": "result arrives too late\u2014at the end of a clock cycle, when it is needed at the\nbeginning.\nThe load instruction has a delay or latency that cannot be eliminated by for-\nwarding alone. Instead, we need to add hardware, called a pipeline interlock, to\npreserve the correct execution pattern. In general, a pipeline interlock detects a haz-\nard and stalls the pipeline until the hazard is cleared. In this case, the interlock stalls\nthe pipeline, beginning with the instruction that wants to use the data until the\nsource instruction produces it. This pipeline interlock introduces a stall or bubble,\njust as it did for the structural hazard. The CPI for the stalled instruction increases\nby the length of the stall (1 clock cycle in this case).\nFigure C.8 shows the pipeline before and after the stall using the names of the\npipeline stages. Because the stall causes the instructions starting with the sub to\nmove one cycle later in time, the forwarding to the and instruction now goes\nthrough the register file, and no forwarding at all is needed for the or instruction.\nThe insertion of the bubble causes the number of cycles to complete this sequence\nto grow by one. No instruction is started during clock cycle 4 (and none finishes\nduring cycle 6).\nFigure C.7 The load instruction can bypass its results to the and and or instructions, but not to the sub, because\nthat would mean forwarding the result in \u201cnegative time.\u201d\nC.2\nThe Major Hurdle of Pipelining\u2014Pipeline Hazards\n\u25a0\nC-17"
    },
    {
        "page": 792,
        "text": "Branch Hazards\nControl hazards can cause a greater performance loss for our RISC V pipeline than\ndo data hazards. When a branch is executed, it may or may not change the PC\nto something other than its current value plus 4. Recall that if a branch changes\nthe PC to its target address, it is a taken branch; if it falls through, it is not\ntaken, or untaken. If instruction i is a taken branch, then the PC is usually not chan-\nged until the end of ID, after the completion of the address calculation and\ncomparison.\nFigure C.9 shows that the simplest method of dealing with branches is to redo\nthe fetch of the instruction following a branch, once we detect the branch during ID\n(when instructions are decoded). The first IF cycle is essentially a stall, because it\nnever performs useful work. You may have noticed that if the branch is untaken,\nthen the repetition of the IF stage is unnecessary because the correct instruction was\nindeed fetched. We will develop several schemes to take advantage of this fact\nshortly.\nOne stall cycle for every branch will yield a performance loss of 10% to 30%\ndepending on the branch frequency, so we will examine some techniques to deal\nwith this loss.\nld\nx1,0(x2)\nIF\nID\nEX\nMEM\nWB\nsub x4,x1,x5\nIF\nID\nEX\nMEM\nWB\nand x6,x1,x7\nIF\nID\nEX\nMEM\nWB\nor\nx8,x1,x9\nIF\nID\nEX\nMEM\nWB\nld\nx1,0(x2)\nIF\nID\nEX\nMEM\nWB\nsub x4,x1,x5\nIF\nID\nStall\nEX\nMEM\nWB\nand x6,x1,x7\nIF\nStall\nID\nEX\nMEM\nWB\nor\nx8,x1,x9\nStall\nIF\nID\nEX\nMEM\nWB\nFigure C.8 In the top half, we can see why a stall is needed: the MEM cycle of the load produces a value that is\nneeded in the EX cycle of the sub, which occurs at the same time. This problem is solved by inserting a stall, as\nshown in the bottom half.\nBranch instruction\nIF\nID\nEX\nMEM\nWB\nBranch successor\nIF\nIF\nID\nEX\nMEM\nWB\nBranch successor+1\nIF\nID\nEX\nMEM\nBranch successor+2\nIF\nID\nEX\nFigure C.9 A branch causes a one-cycle stall in the five-stage pipeline. The instruction\nafter the branch is fetched, but the instruction is ignored, and the fetch is restarted once\nthe branch target is known. It is probably obvious that if the branch is not taken, the\nsecond IF for branch successor is redundant. This will be addressed shortly.\nC-18\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 793,
        "text": "Reducing Pipeline Branch Penalties\nThere are many methods for dealing with the pipeline stalls caused by branch delay;\nwe discuss four simple compile time schemes in this subsection. In these four\nschemes the actions for a branch are static\u2014they are fixed for each branch during\nthe entire execution. The software can try to minimize the branch penalty using\nknowledge of the hardware scheme and of branch behavior. We will then look at\nhardware-based schemes that dynamically predict branch behavior, and Chapter 3\nlooks at more powerful hardware techniques for dynamic branch prediction.\nThe simplest scheme to handle branches is to freeze or flush the pipeline, holding\nor deleting any instructions after the branch until the branch destination is known.\nThe attractiveness of this solution lies primarily in its simplicity both for hardware\nand software. It is the solution used earlier in the pipeline shown in Figure C.9. In this\ncase, the branch penalty is fixed and cannot be reduced by software.\nA higher-performance, and only slightly more complex, scheme is to treat\nevery branch as not taken, simply allowing the hardware to continue as if the\nbranch were not executed. Here, care must be taken not to change the processor\nstate until the branch outcome is definitely known. The complexity of this scheme\narises from having to know when the state might be changed by an instruction and\nhow to \u201cback out\u201d such a change.\nIn the simple five-stage pipeline, this predicted-not-taken or predicted-untaken\nscheme is implemented by continuing to fetch instructions as if the branch were a\nnormal instruction. The pipeline looks as if nothing out of the ordinary is happen-\ning. If the branch is taken, however, we need to turn the fetched instruction into a\nno-op and restart the fetch at the target address. Figure C.10 shows both situations.\nAn alternative scheme is to treat every branch as taken. As soon as the branch is\ndecoded and the target address is computed, we assume the branch to be taken and\nUntaken branch instruction\nIF\nID\nEX\nMEM\nWB\nInstruction i+1\nIF\nID\nEX\nMEM\nWB\nInstruction i+2\nIF\nID\nEX\nMEM\nWB\nInstruction i+3\nIF\nID\nEX\nMEM\nWB\nInstruction i+4\nIF\nID\nEX\nMEM\nWB\nTaken branch instruction\nIF\nID\nEX\nMEM\nWB\nInstruction i+1\nIF\nidle\nidle\nidle\nidle\nBranch target\nIF\nID\nEX\nMEM\nWB\nBranch target+1\nIF\nID\nEX\nMEM\nWB\nBranch target+2\nIF\nID\nEX\nMEM\nWB\nFigure C.10 The predicted-not-taken scheme and the pipeline sequence when the branch is untaken (top) and\ntaken (bottom). When the branch is untaken, determined during ID, we fetch the fall-through and just continue. If the\nbranch is taken during ID, we restart the fetch at the branch target. This causes all instructions following the branch to\nstall 1 clock cycle.\nC.2\nThe Major Hurdle of Pipelining\u2014Pipeline Hazards\n\u25a0\nC-19"
    },
    {
        "page": 794,
        "text": "begin fetching and executing at the target. This buys us a one-cycle improvement\nwhen the branch is actually taken, because we know the target address at the end\nof ID, one cycle before we know whether the branch condition is satisfied in the\nALU stage. In either a predicted-taken or predicted-not-taken scheme, the compiler\ncan improve performance by organizing the code so that the most frequent path\nmatches the hardware\u2019s choice.\nA fourth scheme, which was heavily used in early RISC processors is called\ndelayed branch. In a delayed branch, the execution cycle with a branch delay\nof one is\nbranch instruction\nsequential successor1\nbranch target if taken\nThe sequential successor is in the branch delay slot. This instruction is executed\nwhether or not the branch is taken. The pipeline behavior of the five-stage pipeline\nwith a branch delay is shown in Figure C.11. Although it is possible to have a\nbranch delay longer than one, in practice almost all processors with delayed branch\nhave a single instruction delay; other techniques are used if the pipeline has a lon-\nger potential branch penalty.The job of the compiler is to make the successor\ninstructions valid and useful.\nAlthough the delayed branch was useful for short simple pipelines at a time\nwhen hardware prediction was too expensive, the technique complicates imple-\nmentation when there is dynamic branch prediction. For this reason, RISC V\nappropriately omitted delayed branches.\nUntaken branch instruction\nIF\nID\nEX\nMEM\nWB\nBranch delay instruction (i+1)\nIF\nID\nEX\nMEM\nWB\nInstruction i+2\nIF\nID\nEX\nMEM\nWB\nInstruction i+3\nIF\nID\nEX\nMEM\nWB\nInstruction i+4\nIF\nID\nEX\nMEM\nWB\nTaken branch instruction\nIF\nID\nEX\nMEM\nWB\nBranch delay instruction (i+1)\nIF\nID\nEX\nMEM\nWB\nBranch target\nIF\nID\nEX\nMEM\nWB\nBranch target+1\nIF\nID\nEX\nMEM\nWB\nBranch target+2\nIF\nID\nEX\nMEM\nWB\nFigure C.11 The behavior of a delayed branch is the same whether or not the branch is taken. The instructions in\nthe delay slot (there was only one delay slot for most RISC architectures that incorporated them) are executed. If the\nbranch is untaken, execution continues with the instruction after the branch delay instruction; if the branch is taken,\nexecution continues at the branch target. When the instruction in the branch delay slot is also a branch, the meaning\nis unclear: if the branch is not taken, what should happen to the branch in the branch delay slot? Because of this\nconfusion, architectures with delay branches often disallow putting a branch in the delay slot.\nC-20\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 795,
        "text": "Performance of Branch Schemes\nWhat is the effective performance of each of these schemes? The effective pipeline\nspeedup with branch penalties, assuming an ideal CPI of 1, is\nPipeline speedup \u00bc\nPipeline depth\n1 + Pipeline stall cycles from branches\nBecause of the following:\nPipeline stall cycles from branches \u00bc Branch frequencyBranch penalty\nwe obtain:\nPipeline speedup \u00bc\nPipeline depth\n1 + Branch frequencyBranch penalty\nThe branch frequency and branch penalty can have a component from both uncon-\nditional and conditional branches. However, the latter dominate because they are\nmore frequent.\nExample\nFor a deeper pipeline, such as that in a MIPS R4000 and later RISC processors, it\ntakes at least three pipeline stages before the branch-target address is known\nand an additional cycle before the branch condition is evaluated, assuming no\nstalls on the registers in the conditional comparison. A three-stage delay leads\nto the branch penalties for the three simplest prediction schemes listed in\nFigure C.12.\nFind the effective addition to the CPI arising from branches for this pipeline,\nassuming the following frequencies:\nUnconditional branch\n4%\nConditional branch, untaken\n6%\nConditional branch, taken\n10%\nAnswer\nWe find the CPIs by multiplying the relative frequency of unconditional, condi-\ntional untaken, and conditional taken branches by the respective penalties. The\nresults are shown in Figure C.13.\nBranch scheme\nPenalty unconditional\nPenalty untaken\nPenalty taken\nFlush pipeline\n2\n3\n3\nPredicted taken\n2\n3\n2\nPredicted untaken\n2\n0\n3\nFigure C.12 Branch penalties for the three simplest prediction schemes for a deeper pipeline.\nC.2\nThe Major Hurdle of Pipelining\u2014Pipeline Hazards\n\u25a0\nC-21"
    },
    {
        "page": 796,
        "text": "The differences among the schemes are substantially increased with this longer\ndelay. If the base CPI were 1 and branches were the only source of stalls, the ideal\npipeline would be 1.56 times faster than a pipeline that used the stall-pipeline\nscheme. The predicted-untaken scheme would be 1.13 times better than the\nstall-pipeline scheme under the same assumptions.\nReducing the Cost of Branches Through Prediction\nAs pipelines get deeper and the potential penalty of branches increases, using\ndelayed branches and similar schemes becomes insufficient. Instead, we need to\nturn to more aggressive means for predicting branches. Such schemes fall into\ntwo classes: low-cost static schemes that rely on information available at compile\ntime and strategies that predict branches dynamically based on program behavior.\nWe discuss both approaches here.\nStatic Branch Prediction\nA key way to improve compile-time branch prediction is to use profile information\ncollected from earlier runs. The key observation that makes this worthwhile is that\nthe behavior of branches is often bimodally distributed; that is, an individual\nbranch is often highly biased toward taken or untaken. Figure C.14 shows the suc-\ncess of branch prediction using this strategy. The same input data were used for\nruns and for collecting the profile; other studies have shown that changing the input\nso that the profile is for a different run leads to only a small change in the accuracy\nof profile-based prediction.\nThe effectiveness of any branch prediction scheme depends both on the accu-\nracy of the scheme and the frequency of conditional branches, which vary in SPEC\nfrom 3% to 24%. The fact that the misprediction rate for the integer programs is\nhigher and such programs typically have a higher branch frequency is a major lim-\nitation for static branch prediction. In the next section, we consider dynamic branch\npredictors, which most recent processors have employed.\nAdditions to the CPI from branch costs\nBranch scheme\nUnconditional\nbranches\nUntaken conditional\nbranches\nTaken conditional\nbranches\nAll\nbranches\nFrequency of\nevent\n4%\n6%\n10%\n20%\nStall pipeline\n0.08\n0.18\n0.30\n0.56\nPredicted taken\n0.08\n0.18\n0.20\n0.46\nPredicted untaken\n0.08\n0.00\n0.30\n0.38\nFigure C.13 CPI penalties for three branch-prediction schemes and a deeper pipeline.\nC-22\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 797,
        "text": "Dynamic Branch Prediction and Branch-Prediction Buffers\nThe simplest dynamic branch-prediction scheme is a branch-prediction buffer or\nbranch history table. A branch-prediction buffer is a small memory indexed by the\nlower portion of the address of the branch instruction. The memory contains a bit\nthat says whether the branch was recently taken or not. This scheme is the simplest\nsort of buffer; it has no tags and is useful only to reduce the branch delay when it is\nlonger than the time to compute the possible target PCs.\nWith such a buffer, we don\u2019t know, in fact, if the prediction is correct\u2014it may\nhave been put there by another branch that has the same low-order address bits. But\nthis doesn\u2019t matter. The prediction is a hint that is assumed to be correct, and fetch-\ning begins in the predicted direction. If the hint turns out to be wrong, the predic-\ntion bit is inverted and stored back.\nThis buffer is effectively a cache where every access is a hit, and, as we will see,\nthe performance of the buffer depends on both how often the prediction is for the\nbranch of interest and how accurate the prediction is when it matches. Before we\nanalyze the performance, it is useful to make a small, but important, improvement\nin the accuracy of the branch-prediction scheme.\nMisprediction rate\n0%\n25%\n5%\n10%\n20%\n15%\nBenchmark\nInteger\nFloating-point\nh\ncompress\neqntott\nespresso\ngcc\nli\ndoduc\near\nydro2d\nmdljdp\nsu2cor\n12%\n22%\n18%\n11% 12%\n5% 6%\n9% 10%\n15%\nFigure C.14 Misprediction rate on SPEC92 for a profile-based predictor varies widely\nbut is generally better for the floating-point programs, which have an average mis-\nprediction rate of 9% with a standard deviation of 4%, than for the integer programs,\nwhich have an average misprediction rate of 15% with a standard deviation of 5%.\nThe actual performance depends on both the prediction accuracy and the branch fre-\nquency, which vary from 3% to 24%.\nC.2\nThe Major Hurdle of Pipelining\u2014Pipeline Hazards\n\u25a0\nC-23"
    },
    {
        "page": 798,
        "text": "This simple 1-bit prediction scheme has a performance shortcoming: even if a\nbranch is almost always taken, we will likely predict incorrectly twice, rather than\nonce, when it is not taken, because the misprediction causes the prediction bit to be\nflipped.\nTo remedy this weakness, 2-bit prediction schemes are often used. In a 2-bit\nscheme, a prediction must miss twice before it is changed. Figure C.15 shows\nthe finite-state processor for a 2-bit prediction scheme.\nA branch-prediction buffer can be implemented as a small, special \u201ccache\u201d\naccessed with the instruction address during the IF pipe stage, or as a pair of bits\nattached to each block in the instruction cache and fetched with the instruction. If\nthe instruction is decoded as a branch and if the branch is predicted as taken, fetch-\ning begins from the target as soon as the PC is known. Otherwise, sequential fetch-\ning and executing continue. As Figure C.15 shows, if the prediction turns out to be\nwrong, the prediction bits are changed.\nWhat kind of accuracy can be expected from a branch-prediction buffer using 2\nbits per entry on real applications? Figure C.16 shows that for the SPEC89 bench-\nmarks a branch-prediction buffer with 4096 entries results in a prediction accuracy\nTaken\nTaken\nTaken\nTaken\nNot taken\nNot taken\nNot taken\nNot taken\nPredict taken\n11\nPredict taken\n10\nPredict not taken\n01\nPredict not taken\n00\nFigure C.15 The states in a 2-bit prediction scheme. By using 2 bits rather than 1, a\nbranch that strongly favors taken or not taken\u2014as many branches do\u2014will be mispre-\ndicted less often than with a 1-bit predictor. The 2 bits are used to encode the four states\nin the system. The 2-bit scheme is actually a specialization of a more general scheme\nthat has an n-bit saturating counter for each entry in the prediction buffer. With an\nn-bit counter, the counter can take on values between 0 and 2n\u00041: when the counter\nis greater than or equal to one-half of its maximum value (2n\u00041), the branch is pre-\ndicted as taken; otherwise, it is predicted as untaken. Studies of n-bit predictors have\nshown that the 2-bit predictors do almost as well, thus most systems rely on 2-bit branch\npredictors rather than the more general n-bit predictors.\nC-24\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 799,
        "text": "ranging from over 99% to 82%, or a misprediction rate of 1%\u201318%. A 4K entry\nbuffer, like that used for these results, is considered small in 2017, and a larger\nbuffer could produce somewhat better results.\nAs we try to exploit more ILP, the accuracy of our branch prediction becomes\ncritical. As we can see in Figure C.16, the accuracy of the predictors for integer\nprograms, which typically also have higher branch frequencies, is lower than\nfor the loop-intensive scientific programs. We can attack this problem in two ways:\nby increasing the size of the buffer and by increasing the accuracy of the scheme we\nuse for each prediction. A buffer with 4K entries, however, as Figure C.17 shows,\nperforms quite comparably to an infinite buffer, at least for benchmarks like those\nin SPEC. The data in Figure C.17 make it clear that the hit rate of the buffer is not\nthe major limiting factor. As we mentioned, simply increasing the number of bits\nper predictor without changing the predictor structure also has little impact.\nInstead, we need to look at how we might increase the accuracy of each predictor,\nas we will in Chapter 3.\n18%\ntomcatv\nspice\nSPEC89 benchmarks\ngcc\nli\n2%\n4%\n6%\n8%\n10%\n12%\n14%\n16%\n0%\n1%\n5%\n9%\n9%\n12%\n5%\n10%\n18%\nnasa7\nmatrix300\ndoduc\nfpppp\nespresso\neqntott\n1%\n0%\nFrequency of mispredictions\nFigure C.16 Prediction accuracy of a 4096-entry 2-bit prediction buffer for the\nSPEC89 benchmarks. The misprediction rate for the integer benchmarks (gcc, espresso,\neqntott, and li) is substantially higher (average of 11%) than that for the floating-point\nprograms (average of 4%). Omitting the floating-point kernels (nasa7, matrix300, and\ntomcatv) still yields a higher accuracy for the FP benchmarks than for the integer bench-\nmarks. These data, as well as the rest of the data in this section, are taken from a branch-\nprediction study done using the IBM Power architecture and optimized code for that\nsystem. See Pan et al. (1992). Although these data are for an older version of a subset\nof the SPEC benchmarks, the newer benchmarks are larger and would show slightly\nworse behavior, especially for the integer benchmarks.\nC.2\nThe Major Hurdle of Pipelining\u2014Pipeline Hazards\n\u25a0\nC-25"
    },
    {
        "page": 800,
        "text": "C.3\nHow Is Pipelining Implemented?\nBefore we proceed to basic pipelining, we need to review a simple implementation\nof an unpipelined version of RISC V.\nA Simple Implementation of RISC V\nIn this section we follow the style of Section C.1, showing first a simple unpipe-\nlined implementation and then the pipelined implementation. This time, however,\nour example is specific to the RISC V architecture.\nnasa7\n1%\n0%\nmatrix300\n0%\n0%\ntomcatv\n1%\n0%\ndoduc\nspice\nSPEC89 benchmarks\nfpppp\ngcc\nespresso \neqntott\nli\n0%\n2%\n4%\n6%\n8%\n10%\n12%\n14%\n16%\n18%\n4096 entries:\n2 bits per entry\nUnlimited entries:\n2 bits per entry\nFrequency of mispredictions\n5%\n5%\n9%\n9%\n9%\n9%\n12%\n11%\n5%\n5%\n18%\n18%\n10%\n10%\nFigure C.17 Prediction accuracy of a 4096-entry 2-bit prediction buffer versus an\ninfinite buffer for the SPEC89 benchmarks. Although these data are for an older\nversion of a subset of the SPEC benchmarks, the results would be comparable for\nnewerversionswithperhapsasmanyas8Kentriesneededtomatchaninfinite2-bitpredictor.\nC-26\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 801,
        "text": "Inthissubsection, we focus ona pipelinefor aninteger subsetofRISC V that con-\nsists of load-store word, branch equal, and integer ALU operations. Later in this\nappendixwewillincorporatethebasicfloating-pointoperations.Althoughwediscuss\nonly a subset of RISC V, the basic principles can be extended to handle all the instruc-\ntions;forexample,addingstoreinvolvessomeadditionalcomputingoftheimmediate\nfield. We initially used a less aggressive implementation of a branch instruction. We\nshow how to implement the more aggressive version at the end of this section.\nEvery RISC V instruction can be implemented in, at most, 5 clock cycles. The 5\nclock cycles are as follows:\n1. Instruction fetch cycle (IF):\nIR  Mem[PC];\nNPC  PC + 4;\nOperation\u2014Send out the PC and fetch the instruction from memory into the\ninstruction register (IR); increment the PC by 4 to address the next sequential\ninstruction. TheIR isusedtoholdthe instructionthat will beneededonsubsequent\nclock cycles; likewise, the register NPC is used to hold the next sequential PC.\n2. Instruction decode/register fetch cycle (ID):\nA  Regs[rs1];\nB  Regs[rs2];\nImm  sign-extended immediate field of IR;\nOperation\u2014Decode the instruction and access the register file to read the reg-\nisters (rs1 and rs2 are the register specifiers). The outputs of the general-purpose\nregisters are read into two temporary registers (A and B) for use in later clock\ncycles. The lower 16 bits of the IR are also sign extended and stored into the\ntemporary register Imm, for use in the next cycle.\nDecoding is done in parallel with reading registers, which is possible\nbecause these fields are at a fixed location in the RISC V instruction format.\nBecause the immediate portion of a load and an ALU immediate is located\nin an identical place in every RISC V instruction, the sign-extended immediate\nis also calculated during this cycle in case it is needed in the next cycle. For\nstores, a separate sign-extension is needed, because the immediate field is split\nin two pieces.\n3. Execution/effective address cycle (EX):\nThe ALU operates on the operands prepared in the prior cycle, performing one\nof four functions depending on the RISC V instruction type:\n\u25a0\nMemory reference:\nALUOutput  A + Imm;\nOperation\u2014The ALU adds the operands to form the effective address and\nplaces the result into the register ALUOutput.\nC.3\nHow Is Pipelining Implemented?\n\u25a0\nC-27"
    },
    {
        "page": 802,
        "text": "\u25a0\nRegister-register ALU instruction:\nALUOutput  A func B;\nOperation\u2014The ALU performs the operation specified by the function code\n(a combination of the func3 and func7 fields) on the value in register A\nand on the value in register B. The result is placed in the temporary register\nALUOutput.\n\u25a0\nRegister-Immediate ALU instruction:\nALUOutput  A op Imm;\nOperation\u2014The ALU performs the operation specified by the opcode on the\nvalue in register A and on the value in register Imm. The result is placed in\nthe temporary register ALUOutput.\n\u25a0\nBranch:\nALUOutput  NPC + (Imm << 2);\nCond  (A == B)\nOperation\u2014The ALU adds the NPC to the sign-extended immediate value in\nImm, which is shifted left by 2 bits to create a word offset, to compute the\naddress of the branch target. Register A, which has been read in the prior cycle,\nis checked to determine whether the branch is taken, by comparison with Reg-\nister B, because we consider only branch equal.\nThe load-store architecture of RISC V means that effective address and\nexecution cycles can be combined into a single clock cycle, because no instruc-\ntion needs to simultaneously calculate a data address, calculate an instruction\ntarget address, and perform an operation on the data. The other integer instruc-\ntions not included herein are jumps of various forms, which are similar to\nbranches.\n4. Memory access/branch completion cycle (MEM):\nThe PC is updated for all instructions: PC NPC;\n\u25a0\nMemory reference:\nLMD  Mem[ALUOutput] or\nMem[ALUOutput]  B;\nOperation\u2014Access memory if needed. If the instruction is a load, data return\nfrom memory and are placed in the LMD (load memory data) register; if it is a\nstore, then the data from the B register are written into memory. In either case,\nthe address used is the one computed during the prior cycle and stored in the\nregister ALUOutput.\nC-28\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 803,
        "text": "\u25a0\nBranch:\nif (cond) PC  ALUOutput\nOperation\u2014If the instruction branches, the PC is replaced with the branch des-\ntination address in the register ALUOutput.\n5. Write-back cycle (WB):\n\u25a0\nRegister-register or Register-immediate ALU instruction:\nRegs[rd]  ALUOutput;\n\u25a0\nLoad instruction:\nRegs[rd]  LMD;\nOperation\u2014Write the result into the register file, whether it comes from the\nmemory system (which is in LMD) or from the ALU (which is in ALUOutput)\nwith rd designating the register.\nFigure C.18 shows how an instruction flows through the data path. At the end\nof each clock cycle, every value computed during that clock cycle and required on\na later clock cycle (whether for this instruction or the next) is written into a storage\ndevice, which may be memory, a general-purpose register, the PC, or a temporary\nregister (i.e., LMD, Imm, A, B, IR, NPC, ALUOutput, or Cond). The temporary\nregisters hold values between clock cycles for one instruction, while the other stor-\nage elements are visible parts of the state and hold values between successive\ninstructions.\nAlthough all processors today are pipelined, this multicycle implementation is a\nreasonable approximation of how most processors would have been implemented in\nearlier times. A simple finite-state machine could be used to implement the control\nfollowing the five-cycle structure shown herein. For a much more complex proces-\nsor,microcodecontrolcouldbeused.Ineitherevent,aninstructionsequencelikethe\none described in this section would determine the structure of the control.\nThere are some hardware redundancies that could be eliminated in this multi-\ncycle implementation. For example, there are two ALUs: one to increment the PC\nand one used for effective address and ALU computation. Because they are not\nneeded on the same clock cycle, we could merge them by adding additional mul-\ntiplexers and sharing the same ALU. Likewise, instructions and data could be\nstored in the same memory, because the data and instruction accesses happen\non different clock cycles.\nRather than optimize this simple implementation, we will leave the design as it\nis in Figure C.18, because this provides us with a better base for the pipelined\nimplementation.\nC.3\nHow Is Pipelining Implemented?\n\u25a0\nC-29"
    },
    {
        "page": 804,
        "text": "A Basic Pipeline for RISC V\nAs before, we can pipeline the data path of Figure C.18 with almost no changes by\nstarting a new instruction on each clock cycle. Because every pipe stage is active\non every clock cycle, all operations in a pipe stage must complete in 1 clock cycle\nand any combination of operations must be able to occur at once. Furthermore,\npipelining the data path requires that values passed from one pipe stage to the next\nmust be placed in registers. Figure C.19 shows the RISC V pipeline with the appro-\npriate registers, called pipeline registers or pipeline latches, between each pipeline\nstage. The registers are labeled with the names of the stages they connect.\nInstruction fetch\nInstruction decode/\nregister fetch\nExecute/\naddress\ncalculation\nMemory\naccess\nWrite-\nback \nB\nPC\n4\nALU\n12\n32\nAdd\nData\nmemory\nRegisters\nSign-\nextend\nInstruction\nmemory\nM\nu\nx\nM\nu\nx\nM\nu\nx\nM\nu\nx\n=?\nBranch\ntaken Cond\nNPC\nlmm\nALU\noutput\nIR\nA\nLMD\nFigure C.18 The implementation of the RISC V data path allows every instruction to be executed in 4 or 5 clock\ncycles. Although the PC is shown in the portion of the data path that is used in instruction fetch and the registers are\nshown in the portion of the data path that is used in instruction decode/register fetch, both of these functional units\nare read as well as written by an instruction. Although we show these functional units in the cycle corresponding to\nwhere they are read, the PC is written during the memory access clock cycle and the registers are written during the\nwrite-back clock cycle. In both cases, the writes in later pipe stages are indicated by the multiplexer output (in mem-\nory access or write-back), which carries a value back to the PC or registers. These backward-flowing signals introduce\nmuch of the complexity of pipelining, because they indicate the possibility of hazards.\nC-30\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 805,
        "text": "Figure C.19 is drawn so that connections through the pipeline registers from one\nstage to another are clear.\nAll of the registers needed to hold values temporarily between clock cycles\nwithin one instruction are subsumed into these pipeline registers. The fields of\nthe instruction register (IR), which is part of the IF/ID register, are labeled when\nthey are used to supply register names. The pipeline registers carry both data and\ncontrol from one pipeline stage to the next. Any value needed on a later pipeline\nstage must be placed in such a register and copied from one pipeline register to\nthe next, until it is no longer needed. If we tried to just use the temporary registers\nwe had in our earlier unpipelined data path, values could be overwritten before all\nuses were completed. For example, the field of a register operand used for a write\non a load or ALU operation is supplied from the MEM/WB pipeline register rather\nthan from the IF/ID register. This is because we want a load or ALU operation to\nwrite the register designated by that operation, not the register field of the\nData\nmemory\nALU\nSign-\nextend\nPC\nInstruction\nmemory\nADD\nIF/ID\n4\nID/EX\nEX/MEM\nMEM/WB\nIR19..15\nMEM/WB.IR\nM\nu\nx\nM\nu\nx\nM\nu\nx\nIR24..20\nRegisters\nBranch\ntaken\nIR\n12\n32\nM\nu\nx\n=?\nFigure C.19 The data path is pipelined by adding a set of registers, one between each pair of pipe stages. The\nregisters serve to convey values and control information from one stage to the next. We can also think of the PC as a\npipeline register, which sits before the IF stage of the pipeline, leading to one pipeline register for each pipe stage.\nRecall that the PC is an edge-triggered register written at the end of the clock cycle; hence, there is no race condition\nin writing the PC. The selection multiplexer for the PC has been moved so that the PC is written in exactly one stage\n(IF). If we didn\u2019t move it, there would be a conflict when a branch occurred, because two instructions would try to\nwrite different values into the PC. Most of the data paths flow from left to right, which is from earlier in time to later.\nThe paths flowing from right to left (which carry the register write-back information and PC information on a branch)\nintroduce complications into our pipeline.\nC.3\nHow Is Pipelining Implemented?\n\u25a0\nC-31"
    },
    {
        "page": 806,
        "text": "instruction currently transitioning from IF to ID! This destination register field is\nsimply copied from one pipeline register to the next, until it is needed during the\nWB stage.\nAny instruction is active in exactly one stage of the pipeline at a time; therefore,\nany actions taken on behalf of an instruction occur between a pair of pipeline reg-\nisters. Thus, we can also look at the activities of the pipeline by examining what has\nto happen on any pipeline stage depending on the instruction type. Figure C.20\nshows this view. Fields of the pipeline registers are named so as to show the flow\nStage\nAny instruction\nIF\nIF/ID.IR Mem[PC]\nIF/ID.NPC,PC (if ((EX/MEM.opcode == branch) & EX/MEM.cond){EX/MEM.\nALUOutput} else {PC+4});\nID\nID/EX.A Regs[IF/ID.IR[rs1]]; ID/EX.B Regs[IF/ID.IR[rs2]];\nID/EX.NPC IF/ID.NPC; ID/EX.IR IF/ID.IR;\nID/EX.Imm sign-extend(IF/ID.IR[immediate field]);\nALU instruction\nLoad instruction\nBranch instruction\nEX\nEX/MEM.IR ID/EX.IR;\nEX/MEM.ALUOutput \nID/EX.A func ID/EX.B;\nor\nEX/MEM.ALUOutput \nID/EX.A op ID/EX.Imm;\nEX/MEM.IR to ID/EX.IR\nEX/MEM.ALUOutput \nID/EX.A+ID/EX.Imm;\nEX/MEM.B ID/EX.B;\nEX/MEM.ALUOutput \nID/EX.NPC +\n(ID/EX.Imm<< 2);\nEX/MEM.cond \n(ID/EX.A == ID/EX.B);\nMEM\nMEM/WB.IR EX/MEM.IR;\nMEM/WB.ALUOutput \nEX/MEM.ALUOutput;\nMEM/WB.IR EX/MEM.IR;\nMEM/WB.LMD \nMem[EX/MEM.ALUOutput];\nor\nMem[EX/MEM.ALUOutput] \nEX/MEM.B;\nWB\nRegs[MEM/WB.IR[rd]] \nMEM/WB.ALUOutput;\nFor load only:\nRegs[MEM/WB.IR[rd]] \nMEM/WB.LMD;\nFigure C.20 Events on every pipe stage of the RISC V pipeline. Let\u2019s review the actions in the stages that are specific\nto the pipeline organization. In IF, in addition to fetching the instruction and computing the new PC, we store the\nincremented PC both into the PC and into a pipeline register (NPC) for later use in computing the branch-target\naddress. This structure is the same as the organization in Figure C.19, where the PC is updated in IF from one of\ntwo sources. In ID, we fetch the registers, extend the sign of the 12 bits of the IR (the immediate field), and pass along\nthe IR and NPC. During EX, we perform an ALU operation or an address calculation; we pass along the IR and the B\nregister (if the instruction is a store). We also set the value of cond to 1 if the instruction is a taken branch. During the\nMEM phase, we cycle the memory, write the PC if needed, and pass along values needed in the final pipe stage.\nFinally, during WB, we update the register field from either the ALU output or the loaded value. For simplicity we\nalways pass the entire IR from one stage to the next, although as an instruction proceeds down the pipeline, less\nand less of the IR is needed.\nC-32\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 807,
        "text": "of data from one stage to the next. Notice that the actions in the first two stages are\nindependent of the current instruction type; they must be independent because the\ninstruction is not decoded until the end of the ID stage. The IF activity depends on\nwhether the instruction in EX/MEM is a taken branch. If so, then the branch-target\naddress of the branch instruction in EX/MEM is written into the PC at the end of IF;\notherwise, the incremented PC will be written back. (As we said earlier, this effect\nof branches leads to complications in the pipeline that we deal with in the next few\nsections.) The fixed-position encoding of the register source operands is critical to\nallowing the registers to be fetched during ID.\nTo control this simple pipeline we need only determine how to set the con-\ntrol for the four multiplexers in the data path of Figure C.19. The two multi-\nplexers in the ALU stage are set depending on the instruction type, which is\ndictated by the IR field of the ID/EX register. The top ALU input multiplexer\nis set by whether the instruction is a branch or not, and the bottom multiplexer is\nset by whether the instruction is a register-register ALU operation or any other\ntype of operation. The multiplexer in the IF stage chooses whether to use the\nvalue of the incremented PC or the value of the EX/MEM.ALUOutput (the\nbranch target) to write into the PC. This multiplexer is controlled by the field\nEX/MEM.cond. The fourth multiplexer is controlled by whether the instruction\nin the WB stage is a load or an ALU operation. In addition to these four mul-\ntiplexers, there is one additional multiplexer needed that is not drawn in\nFigure C.19, but whose existence is clear from looking at the WB stage of\nan ALU operation. The destination register field is in one of two different places\ndepending on the instruction type (register-register ALU versus either ALU\nimmediate or load). Thus, we will need a multiplexer to choose the correct por-\ntion of the IR in the MEM/WB register to specify the register destination field,\nassuming the instruction writes a register.\nImplementing the Control for the RISC V Pipeline\nThe process of letting an instruction move from the instruction decode stage (ID)\ninto the execution stage (EX) of this pipeline is usually called instruction issue; an\ninstruction that has made this step is said to have issued. For the RISC V integer\npipeline, all the data hazards can be checked during the ID phase of the pipeline. If\na data hazard exists, the instruction is stalled before it is issued. Likewise, we can\ndetermine what forwarding will be needed during ID and set the appropriate con-\ntrols then. Detecting interlocks early in the pipeline reduces the hardware complex-\nity because the hardware never has to suspend an instruction that has updated the\nstate of the processor, unless the entire processor is stalled. Alternatively, we can\ndetect the hazard or forwarding at the beginning of a clock cycle that uses an oper-\nand (EX and MEM for this pipeline). To show the differences in these two\napproaches, we will show how the interlock for a read after write (RAW) hazard\nwith the source coming from a load instruction (called a load interlock) can be\nimplemented by a check in ID, while the implementation of forwarding paths to\nC.3\nHow Is Pipelining Implemented?\n\u25a0\nC-33"
    },
    {
        "page": 808,
        "text": "the ALU inputs can be done during EX. Figure C.21 lists the variety of circum-\nstances that we must handle.\nLet\u2019s start with implementing the load interlock. If there is a RAW hazard with\nthe source instruction being a load, the load instruction will be in the EX stage\nwhen an instruction that needs the load data will be in the ID stage. Thus, we\ncan describe all the possible hazard situations with a small table, which can be\ndirectly translated to an implementation. Figure C.22 shows a table that detects\nall load interlocks when the instruction using the load result is in the ID stage.\nOnce a hazard has been detected, the control unit must insert the pipeline stall\nand prevent the instructions in the IF and ID stages from advancing. As we said\nearlier, all the control information is carried in the pipeline registers. (Carrying\nthe instruction along is enough, because all control is derived from it.) Thus, when\nwe detect a hazard we need only change the control portion of the ID/EX pipeline\nregister to all 0s, which happens to be a no-op (an instruction that does nothing,\nsuch as add x0,x0,x0). In addition, we simply recirculate the contents of the\nIF/ID registers to hold the stalled instruction. In a pipeline with more complex haz-\nards, the same ideas would apply: we can detect the hazard by comparing some set\nof pipeline registers and shift in no-ops to prevent erroneous execution.\nSituation\nExample code\nsequence\nAction\nNo dependence\nld\nx1,45(x2)\nadd\nx5,x6,x7\nsub\nx8,x6,x7\nor\nx9,x6,x7\nNo hazard possible because no dependence\nexists on x1 in the immediately following\nthree instructions\nDependence\nrequiring stall\nld\nx1,45(x2)\nadd\nx5,x1,x7\nsub\nx8,x6,x7\nor\nx9,x6,x7\nComparators detect the use of x1 in the add\nand stall the add (and sub and or) before\nthe add begins EX\nDependence\novercome by\nforwarding\nld\nx1,45(x2)\nadd\nx5,x6,x7\nsub\nx8,x1,x7\nor\nx9,x6,x7\nComparators detect use of x1 in sub and\nforward result of load to ALU in time for\nsub to begin EX\nDependence with\naccesses in order\nld\nx1,45(x2)\nadd\nx5,x6,x7\nsub\nx8,x6,x7\nor\nx9,x1,x7\nNo action required because the read of x1 by\nor occurs in the second half of the ID phase,\nwhile the write of the loaded data occurred in\nthe first half\nFigure C.21 Situations that the pipeline hazard detection hardware can see by com-\nparing the destination and sources of adjacent instructions. This table indicates that\nthe only comparison needed is between the destination and the sources on the two\ninstructions following the instruction that wrote the destination. In the case of a stall,\nthe pipeline dependences will look like the third case once execution continues (depen-\ndence overcome by forwarding). Of course, hazards that involve x0 can be ignored\nbecause the register always contains 0, and the preceding test could be extended to\ndo this.\nC-34\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 809,
        "text": "Implementing the forwarding logic is similar, although there are more cases to\nconsider. The key observation needed to implement the forwarding logic is that\nthe pipeline registers contain both the data to be forwarded as well as the source\nand destination register fields. All forwarding logically happens from the ALU or\ndata memory output to the ALU input, the data memory input, or the zero detec-\ntion unit. Thus, we can implement the forwarding by a comparison of the desti-\nnation registers of the IR contained in the EX/MEM and MEM/WB stages against\nthe source registers of the IR contained in the ID/EX and EX/MEM registers.\nFigure C.23 shows the comparisons and possible forwarding operations where\nthe destination of the forwarded result is an ALU input for the instruction cur-\nrently in EX.\nIn addition to the comparators and combinational logic that we must determine\nwhen a forwarding path needs to be enabled, we also must enlarge the multiplexers\nat the ALU inputs and add the connections from the pipeline registers that are used\nto forward the results. Figure C.24 shows the relevant segments of the pipelined\ndata path with the additional multiplexers and connections in place.\nFor RISC V, the hazard detection and forwarding hardware is reasonably sim-\nple; we will see that things become somewhat more complicated when we extend\nthis pipeline to deal with floating point. Before we do that, we need to handle\nbranches.\nDealing With Branches in the Pipeline\nIn RISC V, conditional branches depend on comparing two register values, which\nwe assume occurs during the EX cycle, and uses the ALU for this function. We will\nneed to also compute the branch target address. Because testing the branch condi-\ntion and determining the next PC will determine what the branch penalty is, we\nwould like to compute both the possible PCs and choose the correct PC before\nthe end of the EX cycle. We can do this by adding a separate adder that computes\nthe branch target address during ID. Because the instruction is not yet decoded, we\nwill be computing a possible target as if every instruction were a branch. This is\nOpcode field of ID/EX\n(ID/EX.IR0..5)\nOpcode field of IF/ID\n(IF/ID.IR0..6)\nMatching operand fields\nLoad\nRegister-register ALU, load,\nstore,\nALU immediate, or branch\nID/EX.IR[rd] \u00bc\u00bc IF/\nID.IR[rs1]\nLoad\nRegister-register ALU, or\nbranch\nID/EX.IR[rd] \u00bc\u00bc IF/\nID.IR[rs2]\nFigure C.22 The logic to detect the need for load interlocks during the ID stage of an\ninstruction requires two comparisons, one for each possible source. Remember that\nthe IF/ID register holds the state of the instruction in ID, which potentially uses the load\nresult, while ID/EX holds the state of the instruction in EX, which is the load instruction.\nC.3\nHow Is Pipelining Implemented?\n\u25a0\nC-35"
    },
    {
        "page": 810,
        "text": "likely faster than computing the target and evaluating the condition both in EX, but\ndoes use slightly more energy.\nFigure C.25 shows a pipelined data path assuming the adder in ID and the eval-\nuation of the branch condition in EX, a minor change of the pipeline structure. This\npipeline will incur a two-cycle penalty on branches. In some early RISC proces-\nsors, such as MIPS, the condition test on branches was restricted to allow the test to\noccur in ID, reducing the branch delay to one cycle. Of course, that meant that an\nALU operation to a register followed by a conditional branch based on that register\nincurred a data hazard, which does not occur if the branch condition is evaluated\nin EX.\nAs pipeline depths increased, the branch delay increased, which made dynamic\nbranch prediction necessary. For example, a processor with separate decode and\nPipeline\nregister of\nsource\ninstruction\nOpcode of\nsource\ninstruction\nPipeline\nregister of\ndestination\ninstruction\nOpcode of\ndestination\ninstruction\nDestination\nof the\nforwarded\nresult\nComparison (if\nequal then\nforward)\nEX/MEM\nRegister-\nregister ALU,\nALU immediate\nID/EX\nRegister-register\nALU, ALU\nimmediate, load, store,\nbranch\nTop ALU\ninput\nEX/MEM.IR[rd] ==\nID/EX.IR[rs1]\nEX/MEM\nRegister-\nregister ALU,\nALU immediate\nID/EX\nRegister-register ALU\nBottom ALU\ninput\nEX/MEM.IR[rd] ==\nID/EX.IR[rs2]\nMEM/WB\nRegister-\nregister ALU,\nALU\nimmediate,\nLoad\nID/EX\nRegister-register\nALU, ALU\nimmediate, load, store,\nbranch\nTop ALU\ninput\nMEM/WB.IR[rd] ==\nID/EX.IR[rs1]\nMEM/WB\nRegister-\nregister ALU,\nALU\nimmediate,\nLoad\nID/EX\nRegister-register ALU\nBottom ALU\ninput\nMEM/WB.IR[rd] ==\nID/EX.IR[rs2]\nFigure C.23 Forwarding of data to the two ALU inputs (for the instruction in EX) can occur from the ALU result (in\nEX/MEM or in MEM/WB) or from the load result in MEM/WB. There are 10 separate comparisons needed to tell\nwhether a forwarding operation should occur. The top and bottom ALU inputs refer to the inputs corresponding\nto the first and second ALU source operands, respectively, and are shown explicitly in Figure C.18 on page C.30\nand in Figure C.24 on page C.36. Remember that the pipeline latch for destination instruction in EX is ID/EX, while\nthe source values come from the ALUOutput portion of EX/MEM or MEM/WB or the LMD portion of MEM/WB. There is\none complication not addressed by this logic: dealing with multiple instructions that write the same register. For\nexample, during the code sequence add x1, x2, x3; addi x1, x1, 2; sub x4, x3, x1, the logic must ensure\nthat the sub instruction uses the result of the addi instruction rather than the result of the add instruction. The logic\nshown here can be extended to handle this case by simply testing that forwarding from MEM/WB is enabled only\nwhen forwarding from EX/MEM is not enabled for the same input. Because the addi result will be in EX/MEM, it will\nbe forwarded, rather than the add result in MEM/WB.\nC-36\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 811,
        "text": "register fetch stages will probably have a branch delay that is at least 1 clock cycle\nlonger. The branch delay, unless it is dealt with, turns into a branch penalty. Many\nolder processors that implement more complex instruction sets have branch delays\nof 4 clock cycles or more, and large, deeply pipelined processors often have branch\npenalties of 6 or 7. Aggressive high-end superscalars, such as the Intel i7 discussed\nin Chapter 3, may have branch penalties of 10\u201315 cycles! In general, the deeper the\npipeline, the worse the branch penalty in clock cycles, and the more critical that\nbranches be accurately predicted.\nC.4\nWhat Makes Pipelining Hard to Implement?\nNow that we understand how to detect and resolve hazards, we can deal with some\ncomplications that we have avoided so far. The first part of this section considers\nthe challenges of exceptional situations where the instruction execution order is\nchanged in unexpected ways. In the second part of this section, we discuss some\nof the challenges raised by different instruction sets.\nData\nmemory\nALU\n=?\nID/EX\nEX/MEM\nMEM/WB\nM\nu\nx\nM\nu\nx\nFigure C.24 Forwarding of results to the ALU requires the addition of three extra\ninputs on each ALU multiplexer and the addition of three paths to the new inputs.\nThe paths correspond to a bypass of: (1) the ALU output at the end of the EX, (2) the ALU\noutput at the end of the MEM stage, and (3) the memory output at the end of the\nMEM stage.\nC.4\nWhat Makes Pipelining Hard to Implement?\n\u25a0\nC-37"
    },
    {
        "page": 812,
        "text": "Dealing With Exceptions\nExceptional situations areharder to handle in a pipelined processor becausethe over-\nlapping of instructions makes it more difficult to know whether an instruction can\nsafely change the state of the processor. In a pipelined processor, an instruction is\nexecuted piece bypiece andis notcompleted forseveralclock cycles. Unfortunately,\nother instructions in the pipeline can raise exceptions that may force the processor to\nabort the instructions in the pipeline before they complete. Before we discuss these\nproblems and their solutions in detail, we need to understand what types of situations\ncan arise and what architectural requirements exist for supporting them.\nTypes of Exceptions and Requirements\nThe terminology used to describe exceptional situations where the normal execu-\ntion order of instruction is changed varies among processors. The terms interrupt,\nfault, and exception are used, although not in a consistent fashion. We use the term\nexception to cover all these mechanisms, including the following:\n\u25a0\nI/O device request\n\u25a0\nInvoking an operating system service from a user program\nData\nmemory\nALU\nSign-\nextend\n16\n32\nPC\nInstruction\nmemory\nADD\nADD\nIF/ID\n4\nEX/MEM\nMEM/WB\nIR6..10\nMEM/WB.IR\nIR11..15\nRegisters\n=?\nM\nu\nx\nM\nu\nx\nM\nu\nx\nIR\nID/EX\nFigure C.25 To minimize the impact of deciding whether a conditional branch is taken, we compute the branch\ntarget address in ID while doing the conditional test and final selection of next PC in EX. As mentioned in\nFigure C.19, the PC can be thought of as a pipeline register (e.g., as part of ID/IF), which is written with the address\nof the next instruction at the end of each IF cycle.\nC-38\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 813,
        "text": "\u25a0\nTracing instruction execution\n\u25a0\nBreakpoint (programmer-requested interrupt)\n\u25a0\nInteger arithmetic overflow\n\u25a0\nFP arithmetic anomaly\n\u25a0\nPage fault (not in main memory)\n\u25a0\nMisaligned memory accesses (if alignment is required)\n\u25a0\nMemory protection violation\n\u25a0\nUsing an undefined or unimplemented instruction\n\u25a0\nHardware malfunctions\n\u25a0\nPower failure\nWhen we wish to refer to some particular class of such exceptions, we will use a\nlonger name, such as I/O interrupt, floating-point exception, or page fault.\nAlthough we use the term exception to cover all of these events, individual\nevents have important characteristics that determine what action is needed in the\nhardware. The requirements on exceptions can be characterized on five semi-\nindependent axes:\n1. Synchronous versus asynchronous\u2014If the event occurs at the same place every\ntime the program is executed with the same data and memory allocation, the\nevent is synchronous. With the exception of hardware malfunctions, asynchro-\nnous events are caused by devices external to the processor and memory. Asyn-\nchronous events usually can be handled after the completion of the current\ninstruction, which makes them easier to handle.\n2. User requested versus coerced\u2014If the user task directly asks for it, it is a user-\nrequested event. In some sense, user-requested exceptions are not really excep-\ntions, because they are predictable. They are treated as exceptions, however,\nbecause the same mechanisms that are used to save and restore the state are used\nfor these user-requested events. Because the only function of an instruction that\ntriggers this exception is to cause the exception, user-requested exceptions can\nalways be handled after the instruction has completed. Coerced exceptions are\ncaused by some hardware event that is not under the control of the user program.\nCoerced exceptions are harder to implement because they are not predictable.\n3. User maskable versus user nonmaskable\u2014If an event can be masked or dis-\nabled by a user task, it is user maskable. This mask simply controls whether\nthe hardware responds to the exception or not.\n4. Within versus between instructions\u2014This classification depends on whether\nthe event prevents instruction completion by occurring in the middle of execu-\ntion\u2014no matter how short\u2014or whether it is recognized between instructions.\nExceptions that occur within instructions are usually synchronous, because\nthe instruction triggers the exception. It\u2019s harder to implement exceptions that\nC.4\nWhat Makes Pipelining Hard to Implement?\n\u25a0\nC-39"
    },
    {
        "page": 814,
        "text": "occur within instructions than those between instructions, because the instruc-\ntion must be stopped and restarted. Asynchronous exceptions that occur within\ninstructions arise from catastrophic situations (e.g., hardware malfunction) and\nalways cause program termination.\n5. Resume versus terminate\u2014If the program\u2019s execution always stops after the\ninterrupt, it is a terminating event. If the program\u2019s execution continues after\nthe interrupt, it is a resuming event. It is easier to implement exceptions that\nterminate execution, because the processor need not be able to restart execution\nof the same program after handling the exception.\nFigure C.26 classifies the preceding examples according to these five categories.\nThe difficult task is implementing interrupts occurring within instructions where\nthe instruction must be resumed. Implementing such exceptions requires that\nanother program must be invoked to save the state of the executing program, cor-\nrect the cause of the exception, and then restore the state of the program before the\nException type\nSynchronous vs.\nasynchronous\nUser request\nvs. coerced\nUser\nmaskable vs.\nnonmaskable\nWithin vs.\nbetween\ninstructions\nResume vs.\nterminate\nI/O device request\nAsynchronous\nCoerced\nNonmaskable\nBetween\nResume\nInvoke operating system\nSynchronous\nUser request\nNonmaskable\nBetween\nResume\nTracing instruction\nexecution\nSynchronous\nUser request\nUser maskable\nBetween\nResume\nBreakpoint\nSynchronous\nUser request\nUser maskable\nBetween\nResume\nInteger arithmetic\noverflow\nSynchronous\nCoerced\nUser maskable\nWithin\nResume\nFloating-point arithmetic\noverflow or underflow\nSynchronous\nCoerced\nUser maskable\nWithin\nResume\nPage fault\nSynchronous\nCoerced\nNonmaskable\nWithin\nResume\nMisaligned memory\naccesses\nSynchronous\nCoerced\nUser maskable\nWithin\nResume\nMemory protection\nviolations\nSynchronous\nCoerced\nNonmaskable\nWithin\nResume\nUsing undefined\ninstructions\nSynchronous\nCoerced\nNonmaskable\nWithin\nTerminate\nHardware malfunctions\nAsynchronous\nCoerced\nNonmaskable\nWithin\nTerminate\nPower failure\nAsynchronous\nCoerced\nNonmaskable\nWithin\nTerminate\nFigure C.26 Five categories are used to define what actions are needed for the different exception types. Excep-\ntions that must allow resumption are marked as resume, although the software may often choose to terminate the\nprogram. Synchronous, coerced exceptions occurring within instructions that can be resumed are the most difficult\nto implement. We might expect that memory protection access violations would always result in termination; how-\never, modern operating systems use memory protection to detect events such as the first attempt to use a page or\nthe first write to a page. Thus, processors should be able to resume after such exceptions.\nC-40\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 815,
        "text": "instruction that caused the exception can be tried again. This process must be effec-\ntively invisible to the executing program. If a pipeline provides the ability for the\nprocessor to handle the exception, save the state, and restart without affecting the\nexecution of the program, the pipeline or processor is said to be restartable. While\nearly supercomputers and microprocessors often lacked this property, almost all\nprocessors today support it, at least for the integer pipeline, because it is needed\nto implement virtual memory (see Chapter 2).\nStopping and Restarting Execution\nAs in unpipelined implementations, the most difficult exceptions have two prop-\nerties: (1) they occur within instructions (that is, in the middle of the instruction\nexecution corresponding to EX or MEM pipe stages), and (2) they must be restart-\nable. In our RISC V pipeline, for example, a virtual memory page fault resulting\nfrom a data fetch cannot occur until sometime in the MEM stage of the instruction.\nBy the time that fault is seen, several other instructions will be in execution. A page\nfault must be restartable and requires the intervention of another process, such as\nthe operating system. Thus, the pipeline must be safely shut down and the state\nsaved so that the instruction can be restarted in the correct state. Restarting is usu-\nally implemented by saving the PC of the instruction at which to restart. If the\nrestarted instruction is not a branch, then we will continue to fetch the sequential\nsuccessors and begin their execution in the normal fashion. If the restarted instruc-\ntion is a branch, then we will reevaluate the branch condition and begin fetching\nfrom either the target or the fall-through. When an exception occurs, the pipeline\ncontrol can take the following steps to save the pipeline state safely:\n1. Force a trap instruction into the pipeline on the next IF.\n2. Until the trap is taken, turn off all writes for the faulting instruction and for all\ninstructions that follow in the pipeline; this can be done by placing zeros into the\npipeline latches of all instructions in the pipeline, starting with the instruction\nthat generates the exception, but not those that precede that instruction. This\nprevents any state changes for instructions that will not be completed before\nthe exception is handled.\n3. After the exception-handling routine in the operating system receives control, it\nimmediately saves the PC of the faulting instruction. This value will be used to\nreturn from the exception later.\nAfter the exception has been handled, special instructions return the processor from\nthe exception by reloading the PCs and restarting the instruction stream (using the\nexception return in RISC V). If the pipeline can be stopped so that the instructions\njust before the faulting instruction are completed and those after it can be restarted\nfrom scratch, the pipeline is said to have precise exceptions. Ideally, the faulting\ninstruction would not have changed the state, and correctly handling some excep-\ntions requires that the faulting instruction have no effects. For other exceptions,\nC.4\nWhat Makes Pipelining Hard to Implement?\n\u25a0\nC-41"
    },
    {
        "page": 816,
        "text": "such as floating-point exceptions, the faulting instruction on some processors\nwrites its result before the exception can be handled. In such cases, the hardware\nmust be prepared to retrieve the source operands, even if the destination is identical\nto one of the source operands. Because floating-point operations may run for many\ncycles, it is highly likely that some other instruction may have written the source\noperands (as we will see in the next section, floating-point operations often com-\nplete out of order). To overcome this, many recent high-performance processors\nhave introduced two modes of operation. One mode has precise exceptions and\nthe other (fast or performance mode) does not. Of course, the precise exception\nmode is slower, since it allows less overlap among floating-point instructions.\nSupporting precise exceptions is a requirement in many systems, while in\nothers it is \u201cjust\u201d valuable because it simplifies the operating system interface.\nAt a minimum, any processor with demand paging or IEEE arithmetic trap han-\ndlers must make its exceptions precise, either in the hardware or with some soft-\nware support. For integer pipelines, the task of creating precise exceptions is easier,\nand accommodating virtual memory strongly motivates the support of precise\nexceptions for memory references. In practice, these reasons have led designers\nand architects to always provide precise exceptions for the integer pipeline. In this\nsection we describe how to implement precise exceptions for the RISC V integer\npipeline. We will describe techniques for handling the more complex challenges\narising in the floating-point pipeline in Section C.5.\nExceptions in RISC V\nFigure C.27 shows the RISC V pipeline stages and which problem exceptions\nmight occur in each stage. With pipelining, multiple exceptions may occur in\nthe same clock cycle because there are multiple instructions in execution. For\nexample, consider this instruction sequence:\nld\nIF\nID\nEX\nMEM\nWB\nadd\nIF\nID\nEX\nMEM\nWB\nPipeline stage\nProblem exceptions occurring\nIF\nPage fault on instruction fetch; misaligned memory access; memory\nprotection violation\nID\nUndefined or illegal opcode\nEX\nArithmetic exception\nMEM\nPage fault on data fetch; misaligned memory access; memory\nprotection violation\nWB\nNone\nFigure C.27 Exceptions that may occur in the RISC V pipeline. Exceptions raised from\ninstruction or data memory access account for six out of eight cases.\nC-42\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 817,
        "text": "This pair of instructions can cause a data page fault and an arithmetic exception at\nthe same time, because the ld is in the MEM stage while the add is in the EX\nstage. This case can be handled by dealing with only the data page fault and then\nrestarting the execution. The second exception will reoccur (but not the first, if\nthe software is correct), and when the second exception occurs it can be handled\nindependently.\nIn reality, the situation is not as straightforward as this simple example. Excep-\ntions may occur out of order; that is, an instruction may cause an exception before\nan earlier instruction causes one. Consider again the preceding sequence of instruc-\ntions, ld followed by add. The ld can get a data page fault, seen when the instruc-\ntion is in MEM, and the add can get an instruction page fault, seen when the add\ninstruction is in IF. The instruction page fault will actually occur first, even though\nit is caused by a later instruction!\nBecause we are implementing precise exceptions, the pipeline is required to\nhandle the exception caused by the ld instruction first. To explain how this works,\nlet\u2019s call the instruction in the position of the ld instruction i, and the instruction in\nthe position of the add instruction i+1. The pipeline cannot simply handle an\nexception when it occurs in time, because that will lead to exceptions occurring\nout of the unpipelined order. Instead, the hardware posts all exceptions caused\nby a given instruction in a status vector associated with that instruction. The excep-\ntion status vector is carried along as the instruction goes down the pipeline. Once an\nexception indication is set in the exception status vector, any control signal that may\ncause a data value to be written is turned off (this includes both register writes and\nmemory writes). Because a store can cause an exception during MEM, the hardware\nmust be prepared to prevent the store from completing if it raises an exception.\nWhen an instruction enters WB (or is about to leave MEM), the exception sta-\ntus vector is checked. If any exceptions are posted, they are handled in the order in\nwhich they would occur in time on an unpipelined processor\u2014the exception cor-\nresponding to the earliest instruction (and usually the earliest pipe stage for that\ninstruction) is handled first. This guarantees that all exceptions will be seen on\ninstruction i before any are seen on i+1. Of course, any action taken in earlier pipe\nstages on behalf of instruction i may be invalid, but because writes to the register\nfile and memory were disabled, no state could have been changed. As we will see\nin Section C.5, maintaining this precise model for FP operations is much harder.\nIn the next subsection we describe problems that arise in implementing excep-\ntions in the pipelines of processors with more powerful, longer-running instructions.\nInstruction Set Complications\nNo RISC V instruction has more than one result, and our RISC V pipeline writes that\nresult only at the end of an instruction\u2019s execution. When an instruction is guaran-\nteed to complete, it is called committed. In the RISC V integer pipeline, all instruc-\ntions are committed when they reach the end of the MEM stage (or beginning of\nWB) and no instruction updates the state before that stage. Thus, precise exceptions\nC.4\nWhat Makes Pipelining Hard to Implement?\n\u25a0\nC-43"
    },
    {
        "page": 818,
        "text": "are straightforward. Some processors have instructions that change the state in the\nmiddle of the instruction execution, before the instruction and its predecessors are\nguaranteed to complete. For example, autoincrement addressing modes in the IA-32\narchitecture cause the update of registers in the middle of an instruction execution. In\nsuch a case, if the instruction is aborted because of an exception, it will leave the\nprocessor state altered. Although we know which instruction caused the exception,\nwithout additional hardware support the exception will be imprecise because the\ninstruction will be half finished. Restarting the instruction stream after such an\nimprecise exception is difficult. Alternatively, we could avoid updating the state\nbefore the instruction commits, but this may be difficult or costly, because there\nmay be dependences on the updated state: consider a VAX instruction that autoin-\ncrements the same register multiple times. Thus, to maintain a precise exception\nmodel, most processors with such instructions have the ability to back out any state\nchanges made before the instruction is committed. If an exception occurs, the pro-\ncessor uses this ability to reset the state of the processor to its value before the inter-\nrupted instruction started. In the next section, we will see that a more powerful RISC\nV floating-point pipeline can introduce similar problems, and Section C.7 introduces\ntechniques that substantially complicate exception handling.\nA related source of difficulties arises from instructions that update memory\nstate during execution, such as the string copy operations on the Intel architecture\nor IBM 360 (see Appendix K). To make it possible to interrupt and restart these\ninstructions, the instructions are defined to use the general-purpose registers as\nworking registers. Thus, the state of the partially completed instruction is always\nin the registers, which are saved on an exception and restored after the exception,\nallowing the instruction to continue.\nA different set of difficulties arises from odd bits of state that may create addi-\ntional pipeline hazards or may require extra hardware to save and restore. Condition\ncodes are a good example of this. Many processors set the condition codes implic-\nitly as part of the instruction. This approach has advantages, because condition\ncodes decouple the evaluation of the condition from the actual branch. However,\nimplicitly set condition codes can cause difficulties in scheduling any pipeline\ndelays between setting the condition code and the branch, because most instructions\nset the condition code and cannot be used in the delay slots between the condition\nevaluation and the branch.\nAdditionally, in processors with condition codes, the processor must decide\nwhen the branch condition is fixed. This involves finding out when the condition\ncode has been set for the last time before the branch. In most processors with\nimplicitly set condition codes, this is done by delaying the branch condition eval-\nuation until all previous instructions have had a chance to set the condition code.\nOf course, architectures with explicitly set condition codes allow the delay\nbetween condition test and the branch to be scheduled; however, pipeline control\nmust still track the last instruction that sets the condition code to know when the\nbranch condition is decided. In effect, the condition code must be treated as an\noperand that requires hazard detection for RAW hazards with branches, just as\nRISC V must do on the registers.\nC-44\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 819,
        "text": "A final thorny area in pipelining is multicycle operations. Imagine trying to\npipeline a sequence of x86 instructions such as this:\nmov\nBX, AX\n; moves between registers\nadd\n42(BX+SI),BX;adds memory contents and register\n; to same memory location\nsub\nBX,AX\n;subtracts registers\nrep movsb\n;moves a character string of\n; length given by register CX\nAlthough none of these instructions is particularly long (an x86 instruction\ncan be up to 15 bytes), they do differ radically in the number of clock cycles they\nwill require, from as low as one up to hundreds of clock cycles. These instructions\nalso require different numbers of data memory accesses, from zero to possibly\nhundreds. The data hazards are very complex and occur both between and within\ninstructions (nothing prevents the movsb from having an overlapping source and\ndestination!). The simple solution of making all instructions execute for the\nsame number of clock cycles is unacceptable because it introduces an enormous\nnumber of hazards and bypass conditions and makes an immensely long pipeline.\nPipelining the x86 at the instruction level is difficult, but a clever solution\nwas found, similar to one used for the VAX. They pipeline the microinstruction\nexecution; a microinstruction is a simple instruction used in sequences to imple-\nment a more complex instruction set. Because the microinstructions are simple\n(they look a lot like RISC V), the pipeline control is much easier. Since 1995,\nall Intel IA-32 microprocessors have used this strategy of converting the IA-32\ninstructions into microoperations, and then pipelining the microoperations. In\nfact, this approach is even used for some of the more complex instructions in\nthe ARM architecture.\nIn comparison, load-store processors have simple operations with similar\namounts of work and pipeline more easily. If architects realize the relationship\nbetween instruction set design and pipelining, they can design architectures for\nmore efficient pipelining. In the next section, we will see how the RISC V pipeline\ndeals with long-running instructions, specifically floating-point operations.\nFor many years, the interaction between instruction sets and implementations\nwas believed to be small, and implementation issues were not a major focus in\ndesigning instruction sets. In the 1980s, it became clear that the difficulty and inef-\nficiency of pipelining could both be increased by instruction set complications. In\nthe 1990s, all companies moved to simpler instructions sets with the goal of reduc-\ning the complexity of aggressive implementations.\nC.5\nExtending the RISC V Integer Pipeline to Handle\nMulticycle Operations\nWe now want to explore how our RISC V pipeline can be extended to handle\nfloating-point operations. This section concentrates on the basic approach and\nC.5\nExtending the RISC V Integer Pipeline to Handle Multicycle Operations\n\u25a0\nC-45"
    },
    {
        "page": 820,
        "text": "the design alternatives, closing with some performance measurements of a RISC V\nfloating-point pipeline.\nIt is impractical to require that all RISC V FP operations complete in 1 clock\ncycle, or even in 2. Doing so would mean accepting a slow clock or using enor-\nmous amounts of logic in the FP units, or both. Instead, the FP pipeline will allow\nfor a longer latency for operations. This is easier to grasp if we imagine the FP\ninstructions as having the same pipeline as the integer instructions, with two impor-\ntant changes. First, the EX cycle may be repeated as many times as needed to com-\nplete the operation\u2014the number of repetitions can vary for different operations.\nSecond, there may be multiple FP functional units. A stall will occur if the instruc-\ntion to be issued will cause either a structural hazard for the functional unit it uses\nor a data hazard.\nFor this section, let\u2019s assume that there are four separate functional units in our\nRISC V implementation:\n1. The main integer unit that handles loads and stores, integer ALU operations,\nand branches\n2. FP and integer multiplier\n3. FP adder that handles FP add, subtract, and conversion\n4. FP and integer divider\nIf we also assume that the execution stages of these functional units are not pipe-\nlined, then Figure C.28 shows the resulting pipeline structure. Because EX is not\npipelined, no other instruction using that functional unit may issue until the previ-\nous instruction leaves EX. Moreover, if an instruction cannot proceed to the EX\nstage, the entire pipeline behind that instruction will be stalled.\nIn reality, the intermediate results are probably not cycled around the EX unit\nas Figure C.28 suggests; instead, the EX pipeline stage has some number of clock\ndelays larger than 1. We can generalize the structure of the FP pipeline shown in\nFigure C.28 to allow pipelining of some stages and multiple ongoing operations.\nTo describe such a pipeline, we must define both the latency of the functional units\nand also the initiation interval or repeat interval. We define latency the same way\nwe defined it earlier: the number of intervening cycles between an instruction that\nproduces a result and an instruction that uses the result. The initiation or repeat\ninterval is the number of cycles that must elapse between issuing two operations\nof a given type. For example, we will use the latencies and initiation intervals\nshown in Figure C.29.\nWith this definition of latency, integer ALU operations have a latency of 0,\nbecause the results can be used on the next clock cycle, and loads have a latency\nof 1, because their results can be used after one intervening cycle. Because most\noperations consume their operands at the beginning of EX, the latency is usually\nthe number of stages after EX that an instruction produces a result\u2014for example,\nzero stages for ALU operations and one stage for loads. The primary exception is\nstores, which consume the value being stored one cycle later. Hence, the latency to\nC-46\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 821,
        "text": "a store for the value being stored, but not for the base address register, will be one\ncycle less. Pipeline latency is essentially equal to one cycle less than the depth of\nthe execution pipeline, which is the number of stages from the EX stage to the stage\nthat produces the result. Thus, for the preceding example pipeline, the number\nof stages in an FP add is four, while the number of stages in an FP multiply is\nseven. To achieve a higher clock rate, designers need to put fewer logic levels\nEX\nFP/integer\nmultiply\nEX\nInteger unit\nEX\nFP adder\nEX\nFP/integer\ndivider\nB\nW\nM\nE\nM\nD\nI\nF\nI\nFigure C.28 The RISC V pipeline with three additional unpipelined, floating-point,\nfunctional units. Because only one instruction issues on every clock cycle, all instruc-\ntions go through the standard pipeline for integer operations. The FP operations simply\nloop when they reach the EX stage. After they have finished the EX stage, they proceed\nto MEM and WB to complete execution.\nFunctional unit\nLatency\nInitiation interval\nInteger ALU\n0\n1\nData memory (integer and FP loads)\n1\n1\nFP add\n3\n1\nFP multiply (also integer multiply)\n6\n1\nFP divide (also integer divide)\n24\n25\nFigure C.29 Latencies and initiation intervals for functional units.\nC.5\nExtending the RISC V Integer Pipeline to Handle Multicycle Operations\n\u25a0\nC-47"
    },
    {
        "page": 822,
        "text": "in each pipe stage, which makes the number of pipe stages required for more\ncomplex operations larger. The penalty for the faster clock rate is thus longer\nlatency for operations.\nThe example pipeline structure in Figure C.29 allows up to four outstanding FP\nadds, seven outstanding FP/integer multiplies, and one FP divide. Figure C.30\nshows how this pipeline can be drawn by extending Figure C.28. The repeat inter-\nval is implemented in Figure C.30 by adding additional pipeline stages, which will\nbe separated by additional pipeline registers. Because the units are independent, we\nname the stages differently. The pipeline stages that take multiple clock cycles,\nsuch as the divide unit, are further subdivided to show the latency of those stages.\nBecause they are not complete stages, only one operation may be active. The pipe-\nline structure can also be shown using the familiar diagrams from earlier in the\nappendix, as Figure C.31 shows for a set of independent FP operations and FP\nloads and stores. Naturally, the longer latency of the FP operations increases the\nfrequency of RAW hazards and resultant stalls, as we will see later in this section.\nThe structure of the pipeline in Figure C.30 requires the introduction of the\nadditional pipeline registers (e.g.,A1/A2, A2/A3, A3/A4) and the modification\nof the connections to those registers. The ID/EX register must be expanded to\nEX\nM1\nFP/integer multiply\nInteger unit\nFP adder\nFP/integer divider\nIF\nID\nMEM\nWB\nM2\nM3\nM4\nM5\nM6\nA1\nA2\nA3\nA4\nM7\nDIV\nFigure C.30 A pipeline that supports multiple outstanding FP operations. The FP multiplier and adder are fully\npipelined and have a depth of seven and four stages, respectively. The FP divider is not pipelined, but requires\n24 clock cycles to complete. The latency in instructions between the issue of an FP operation and the use of the result\nof that operation without incurring a RAW stall is determined by the number of cycles spent in the execution stages.\nFor example, the fourth instruction after an FP add can use the result of the FP add. For integer ALU operations, the\ndepth of the execution pipeline is always one and the next instruction can use the results.\nC-48\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 823,
        "text": "connect ID to EX, DIV, M1, and A1; we can refer to the portion of the register\nassociated with one of the next stages with the notation ID/EX, ID/DIV, ID/\nM1, or ID/A1. The pipeline register between ID and all the other stages may be\nthought of as logically separate registers and may, in fact, be implemented as sep-\narate registers. Because only one operation can be in a pipe stage at a time, the\ncontrol information can be associated with the register at the head of the stage.\nHazards and Forwarding in Longer Latency Pipelines\nThere are a number of different aspects to the hazard detection and forwarding for a\npipeline like that shown in Figure C.30.\n1. Because the divide unit is not fully pipelined, structural hazards can occur.\nThese will need to be detected and issuing instructions will need to be stalled.\n2. Because the instructions have varying running times, the number of register\nwrites required in a cycle can be larger than 1.\n3. Write after write (WAW) hazards are possible, because instructions no longer\nreach WB in order. Note that write after read (WAR) hazards are not possible,\nbecause the register reads always occur in ID.\n4. Instructions can complete in a different order than they were issued, causing\nproblems with exceptions; we deal with this in the next subsection.\n5. Because of longer latency of operations, stalls for RAW hazards will be more\nfrequent.\nThe increase in stalls arising from longer operation latencies is fundamentally the\nsame as that for the integer pipeline. Before describing the new problems that arise\nin this FP pipeline and looking at solutions, let\u2019s examine the potential impact of\nRAW hazards. Figure C.32 shows a typical FP code sequence and the resultant\nstalls. At the end of this section, we\u2019ll examine the performance of this FP pipeline\nfor our SPEC subset.\nNow look at the problems arising from writes, described as (2) and (3) in the\nearlier list. If we assume that the FP register file has one write port, sequences of FP\noperations, as well as an FP load together with FP operations, can cause conflicts\nfmul.d\nIF\nID\nM1\nM2\nM3\nM4\nM5\nM6\nM7\nMEM\nWB\nfadd.d\nIF\nID\nA1\nA2\nA3\nA4\nMEM\nWB\nfadd.d\nIF\nID\nEX\nMEM\nWB\nfsd\nIF\nID\nEX\nMEM\nWB\nFigure C.31 The pipeline timing of a set of independent FP operations. The stages in italics show where data are\nneeded, while the stages in bold show where a result is available. FP loads and stores use a 64-bit path to memory so\nthat the pipelining timing is just like an integer load or store.\nC.5\nExtending the RISC V Integer Pipeline to Handle Multicycle Operations\n\u25a0\nC-49"
    },
    {
        "page": 824,
        "text": "for the register write port. Consider the pipeline sequence shown in Figure C.33. In\nclock cycle 11, all three instructions will reach WB and want to write the register\nfile. With only a single register file write port, the processor must serialize the\ninstruction completion. This single register port represents a structural hazard.\nWe could increase the number of write ports to solve this, but that solution may\nbe unattractive because the additional write ports would be used only rarely. This\nis because the maximum steady-state number of write ports needed is 1. Instead,\nwe choose to detect and enforce access to the write port as a structural hazard.\nThere are two different ways to implement this interlock. The first is to track the\nuse of the write port in the ID stage and to stall an instruction before it issues, just as\nwe would for any other structural hazard. Tracking the use of the write port can be\ndone with a shift register that indicates when already-issued instructions will use\nthe register file. If the instruction in ID needs to use the register file at the same time\nClock cycle number\nInstruction\n1 2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\nfld\nf4,0(x2)\nIF ID EX MEM WB\nfmul.d f0,f4,f6\nIF ID Stall\nM1 M2\nM3\nM4\nM5\nM6\nM7\nMEM WB\nfadd.d f2,f0,f8\nIF\nStall\nID\nStall Stall Stall Stall Stall Stall A1\nA2\nA3\nA4\nMEM WB\nfsd\nf2,0(x2)\nIF\nStall Stall Stall Stall Stall Stall ID\nEX Stall Stall Stall\nMEM\nFigure C.32 A typical FP code sequence showing the stalls arising from RAW hazards. The longer pipeline sub-\nstantially raises the frequency of stalls versus the shallower integer pipeline. Each instruction in this sequence is\ndependent on the previous and proceeds as soon as data are available, which assumes the pipeline has full bypassing\nand forwarding. The fsd must be stalled an extra cycle so that its MEM does not conflict with the fadd.d. Extra\nhardware could easily handle this case.\nClock cycle number\nInstruction\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nfmul.d f0,f4,f6\nIF\nID\nM1\nM2\nM3\nM4\nM5\nM6\nM7\nMEM\nWB\n...\nIF\nID\nEX\nMEM\nWB\n...\nIF\nID\nEX\nMEM\nWB\nfadd.d f2,f4,f6\nIF\nID\nA1\nA2\nA3\nA4\nMEM\nWB\n...\nIF\nID\nEX\nMEM\nWB\n...\nIF\nID\nEX\nMEM\nWB\nfld\nf2,0(x2)\nIF\nID\nEX\nMEM\nWB\nFigure C.33 Three instructions want to perform a write-back to the FP register file simultaneously, as shown in\nclock cycle 11. This is not the worst case, because an earlier divide in the FP unit could also finish on the same clock.\nNote that although the fmul.d, fadd.d, and fld are in the MEM stage in clock cycle 10, only the fld actually uses\nthe memory, so no structural hazard exists for MEM.\nC-50\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 825,
        "text": "as an instruction already issued, the instruction in ID is stalled for a cycle. On each\nclock the reservation register is shifted 1 bit. This implementation has an advan-\ntage: It maintains the property that all interlock detection and stall insertion occurs\nin the ID stage. The cost is the addition of the shift register and write conflict logic.\nWe will assume this scheme throughout this section.\nAn alternative scheme is to stall a conflicting instruction when it tries to enter\neither the MEM or WB stage. If we wait to stall the conflicting instructions until\nthey want to enter the MEM or WB stage, we can choose to stall either instruction.\nA simple, though sometimes suboptimal, heuristic is to give priority to the unit\nwith the longest latency, because that is the one most likely to have caused another\ninstruction to be stalled for a RAW hazard. The advantage of this scheme is that it\ndoes not require us to detect the conflict until the entrance of the MEM or WB\nstage, where it is easy to see. The disadvantage is that it complicates pipeline con-\ntrol, as stalls can now arise from two places. Notice that stalling before entering\nMEM will cause the EX, A4, or M7 stage to be occupied, possibly forcing the stall\nto trickle back in the pipeline. Likewise, stalling before WB would cause MEM to\nback up.\nOur other problem is the possibility of WAW hazards. To see that these exist,\nconsider the example in Figure C.33. If the fadd.d instruction were issued one\ncycle earlier and had a destination of f2, then it would create a WAW hazard,\nbecause it would write f2 one cycle earlier than the fadd.d. Note that this hazard\nonly occurs when the result of the fadd.d is overwritten without any instruction\never using it! If there were a use of f2 between the fadd.d and the fadd.d, the\npipeline would need to be stalled for a RAW hazard, and the fadd.d would not\nissue until the fadd.d was completed. We could argue that, for our pipeline,\nWAW hazards only occur when a useless instruction is executed, but we must still\ndetect them and make sure that the result of the fadd.d appears in f2 when we\nare done. (As we will see in Section C.8, such sequences sometimes do occur in\nreasonable code.)\nThere are two possible ways to handle this WAW hazard. The first approach is\nto delay the issue of the load instruction until the fadd.d enters MEM. The sec-\nond approach is to stamp out the result of the fadd.d by detecting the hazard and\nchanging the control so that the fadd.d does not write its result. Then the\nfadd.d can issue right away. Because this hazard is rare, either scheme will\nwork fine\u2014you can pick whatever is simpler to implement. In either case, the\nhazard can be detected during ID when the fadd.d is issuing, and stalling\nthe fadd.d or making the fadd.d a no-op is easy. The difficult situation is\nto detect that the fadd.d might finish before the fadd.d, because that requires\nknowing the length of the pipeline and the current position of the fadd.d. Luck-\nily, this code sequence (two writes with no intervening read) will be very rare, so\nwe can use a simple solution: if an instruction in ID wants to write the same reg-\nister as an instruction already issued, do not issue the instruction to EX. In\nSection C.7, we will see how additional hardware can eliminate stalls for such\nhazards. First, let\u2019s put together the pieces for implementing the hazard and issue\nlogic in our FP pipeline.\nC.5\nExtending the RISC V Integer Pipeline to Handle Multicycle Operations\n\u25a0\nC-51"
    },
    {
        "page": 826,
        "text": "In detecting the possible hazards, we must consider hazards among FP instruc-\ntions, as well as hazards between an FP instruction and an integer instruction.\nExcept for FP loads-stores and FP-integer register moves, the FP and integer\nregisters are distinct. All integer instructions operate on the integer registers, while\nthe FP operations operate only on their own registers. Thus, we need only consider\nFP loads-stores and FP register moves in detecting hazards between FP and integer\ninstructions. This simplification of pipeline control is an additional advantage of\nhaving separate register files for integer and floating-point data. (The main advan-\ntages are a doubling of the number of registers, without making either set larger,\nand an increase in bandwidth without adding more ports to either set. The main\ndisadvantage, beyond the need for an extra register file, is the small cost of occa-\nsional moves needed between the two register sets.) Assuming that the pipeline\ndoes all hazard detection in ID, there are three checks that must be performed\nbefore an instruction can issue:\n1. Check for structural hazards\u2014Wait until the required functional unit is not\nbusy (this is only needed for divides in this pipeline) and make sure the register\nwrite port is available when it will be needed.\n2. Check for a RAW data hazard\u2014Wait until the source registers are not listed as\npending destinations in a pipeline register that will not be available when this\ninstruction needs the result. A number of checks must be made here, depending\non both the source instruction, which determines when the result will be avail-\nable, and the destination instruction, which determines when the value is\nneeded. For example, if the instruction in ID is an FP operation with source reg-\nister f2, then f2 cannot be listed as a destination in ID/A1, A1/A2, or A2/A3,\nwhich correspond to FP add instructions that will not be finished when the\ninstruction in ID needs a result. (ID/A1 is the portion of the output register\nof ID that is sent to A1.) Divide is somewhat more tricky, if we want to allow\nthe last few cycles of a divide to be overlapped, because we need to handle the\ncase when a divide is close to finishing as special. In practice, designers might\nignore this optimization in favor of a simpler issue test.\n3. Check for a WAW data hazard\u2014Determine if any instruction in A1, \u2026 , A4, D,\nM1, \u2026 , M7 has the same register destination as this instruction. If so, stall the\nissue of the instruction in ID.\nAlthough the hazard detection is more complex with the multicycle FP operations,\nthe concepts are the same as for the RISC V integer pipeline. The same is true for\nthe forwarding logic. The forwarding can be implemented by checking if the\ndestination register in any of the EX/MEM, A4/MEM, M7/MEM, D/MEM, or\nMEM/WB registers is one of the source registers of a floating-point instruction.\nIf so, the appropriate input multiplexer will have to be enabled so as to choose\nthe forwarded data. In the exercises, you will have the opportunity to specify\nthe logic for the RAW and WAW hazard detection as well as for forwarding.\nMulticycle FP operations also introduce problems for our exception mecha-\nnisms, which we deal with next.\nC-52\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 827,
        "text": "Maintaining Precise Exceptions\nAnother problem caused by these long-running instructions can be illustrated with\nthe following sequence of code:\nfdiv.d\nf0,f2,f4\nfadd.d\nf10,f10,f8\nfsub.d\nf12,f12,f14\nThis code sequence looks straightforward; there are no dependences. A problem\narises, however, because an instruction issued early may complete after an instruc-\ntion issued later. In this example, we can expect fadd.d and fsub.d to complete\nbefore the fdiv.d completes. This is called out-of-order completion and is com-\nmon in pipelines with long-running operations (see Section C.7). Because hazard\ndetection will prevent any dependence among instructions from being violated,\nwhy is out-of-order completion a problem? Suppose that the fsub.d causes a\nfloating-point arithmetic exception at a point where the fadd.d has completed\nbut the fdiv.d has not. The result will be an imprecise exception, something\nwe are trying to avoid. It may appear that this could be handled by letting the\nfloating-point pipeline drain, as we do for the integer pipeline. But the exception\nmay be in a position where this is not possible. For example, if the fdiv.d decided to\ntake a floating-point-arithmetic exception after the add completed, we could not\nhave a precise exception at the hardware level. In fact, because the fadd.d destroys\none of its operands, we could not restore the state to what it was before the fdiv.d,\neven with software help.\nThis problem arises because instructions are completing in a different order\nthan they were issued. There are four possible approaches to dealing with out-\nof-order completion. The first is to ignore the problem and settle for imprecise\nexceptions. This approach was used in the 1960s and early 1970s. It was still used\nin some supercomputers in thepast fifteen years, where certain classes of excep-\ntions were not allowed or were handled by the hardware without stopping the pipe-\nline. It is difficult to use this approach in most processors built today because of\nfeatures such as virtual memory and the IEEE floating-point standard that essen-\ntially require precise exceptions through a combination of hardware and software.\nAs mentioned earlier, some recent processors have solved this problem by intro-\nducing two modes of execution: a fast, but possibly imprecise mode and a slower,\nprecise mode. The slower precise mode is implemented either with a mode switch\nor by insertion of explicit instructions that test for FP exceptions. In either case, the\namount of overlap and reordering permitted in the FP pipeline is significantly\nrestricted so that effectively only one FP instruction is active at a time. This solu-\ntion was used in the DEC Alpha 21064 and 21164, in the IBM Power1 and Power2,\nand in the MIPS R8000.\nA second approach is to buffer the results of an operation until all the operations\nthat were issued earlier are complete. Some processors actually use this solution,\nbut it becomes expensive when the difference in running times among operations\nis large, because the number of results to buffer can become large. Furthermore,\nC.5\nExtending the RISC V Integer Pipeline to Handle Multicycle Operations\n\u25a0\nC-53"
    },
    {
        "page": 828,
        "text": "results from the queue must be bypassed to continue issuing instructions while wait-\ning for the longer instruction. This requires a large number of comparators and a\nvery large multiplexer.\nThere are two viable variations on this basic approach. The first is a history file,\nused in the CYBER 180/990. The history file keeps track of the original values of\nregisters. When an exception occurs and the state must be rolled back earlier than\nsome instruction that completed out of order, the original value of the register can\nbe restored from the history file. A similar technique is used for autoincrement and\nautodecrement addressing on processors such as VAXes. Another approach, the\nfuture file, proposed by Smith and Pleszkun (1988), keeps the newer value of a\nregister; when all earlier instructions have completed, the main register file is\nupdated from the future file. On an exception, the main register file has the precise\nvalues for the interrupted state. In Chapter 3, we will see another approach that is\nneeded to support speculation, a method of executing instructions before we know\nthe outcome of previous branches.\nA third technique in use is to allow the exceptions to become somewhat impre-\ncise, but to keep enough information so that the trap-handling routines can create a\nprecise sequence for the exception. This means knowing what operations were in\nthe pipeline and their PCs. Then, after handling the exception, the software finishes\nany instructions that precede the latest instruction completed, and the sequence can\nrestart. Consider the following worst-case code sequence:\nInstruction1\u2014A long-running instruction that eventually interrupts execution.\nInstruction2, \u2026 , Instructionn\u00041\u2014A series of instructions that are not completed.\nInstructionn\u2014An instruction that is finished.\nGiven the PCs of all the instructions in the pipeline and the exception return PC, the\nsoftware can find the state of instruction1 and instructionn. Because instructionn has\ncompleted, we will want to restart execution at instructionn+1. After handling the\nexception, the software must simulate the execution of instruction1, \u2026 ,\ninstructionn\u00041. Then we can return from the exception and restart at instructionn+1.\nThe complexity of executing these instructions properly by the handler is the major\ndifficulty of this scheme.\nThere is an important simplification for simple RISC V-like pipelines:\nIf instruction2, \u2026 , instructionn are all integer instructions, we know that if\ninstructionn has completed then all of instruction2, \u2026 , instructionn\u00041 have also\ncompleted. Thus, only FP operations need to be handled. To make this scheme\ntractable, the number of floating-point instructions that can be overlapped in\nexecution can be limited. For example, if we only overlap two instructions,\nthen only the interrupting instruction need be completed by software. This\nrestriction may reduce the potential throughput if the FP pipelines are deep or\nif there are a significant number of FP functional units. This approach is used\nin some SPARC implementations to allow overlap of floating-point and integer\noperations.\nC-54\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 829,
        "text": "The final technique is a hybrid scheme that allows the instruction issue to con-\ntinue only if it is certain that all the instructions before the issuing instruction will\ncomplete without causing an exception. This guarantees that when an exception\noccurs, no instructions after the interrupting one will be completed and all of\nthe instructions before the interrupting one can be completed. This sometimes\nmeans stalling the processor to maintain precise exceptions. To make this scheme\nwork, the floating-point functional units must determine if an exception is possible\nearly in the EX stage (in the first 3 clock cycles in the RISC V pipeline), so as\nto prevent further instructions from completing. This scheme is used in the\nMIPS R2000/3000, the R4000, and the Intel Pentium. It is discussed further in\nAppendix J.\nPerformance of a Simple RISC V FP Pipeline\nThe RISC V FP pipeline of Figure C.30 on page C.48 can generate both structural\nstalls for the divide unit and stalls for RAW hazards (it also can have WAW haz-\nards, but this rarely occurs in practice). Figure C.34 shows the number of stall\ncycles for each type of floating-point operation on a per-instance basis (i.e., the\nfirst bar for each FP benchmark shows the number of FP result stalls for each\nFP add, subtract, or convert). As we might expect, the stall cycles per operation\ntrack the latency of the FP operations, varying from 46% to 59% of the latency\nof the functional unit.\nFigure C.35 gives the complete breakdown of integer and FP stalls for five\nSPECfp benchmarks. There are four classes of stalls shown: FP result stalls, FP\ncompare stalls, load and branch delays, and FP structural delays. Branch delay\nstalls, which would be small with a one cycle delay and even a modest branch\npredictor, are not included. The total number of stalls per instruction varies from\n0.65 to 1.21.\nC.6\nPutting It All Together: The MIPS R4000 Pipeline\nIn this section, we look at the pipeline structure and performance of the MIPS\nR4000 processor family, which includes the 4400. The MIPS architecture and\nRISC V are very similar, differing only in a few instructions, including a delayed\nbranch in the MIPS ISA. The R4000 implements MIPS64 but uses a deeper pipe-\nline than that of our five-stage design both for integer and FP programs. This dee-\nper pipeline allows it to achieve higher clock rates by decomposing the five-stage\ninteger pipeline into eight stages. Because cache access is particularly time critical,\nthe extra pipeline stages come from decomposing the memory access. This type of\ndeeper pipelining is sometimes called superpipelining.\nFigure C.36 shows the eight-stage pipeline structure using an abstracted version\nof the data path. Figure C.37 shows the overlap of successive instructions in the\npipeline. Notice that, although the instruction and data memory occupy multiple\ncycles, they are fully pipelined, so that a new instruction can start on every clock.\nC.6\nPutting It All Together: The MIPS R4000 Pipeline\n\u25a0\nC-55"
    },
    {
        "page": 830,
        "text": "In fact, the pipeline uses the data before the cache hit detection is complete; Chapter 3\ndiscusses how this can be done in more detail.\nThe function of each stage is as follows:\n\u25a0\nIF\u2014First half of instruction fetch; PC selection actually happens here, together\nwith initiation of instruction cache access.\n\u25a0\nIS\u2014Second half of instruction fetch, complete instruction cache access.\n\u25a0\nRF\u2014Instruction decode and register fetch, hazard checking, and instruction\ncache hit detection.\nNumber of stalls\n0.0\n25.0\n5.0\n10.0\n20.0\n15.0\nFP SPEC benchmarks\ndoduc\near\nhydro2d\nmdljdp\nsu2cor\n0.6\n18.6\n1.6\n1.5\n0.7\n0.0\n24.5\n2.9\n1.2\n2.1\n0.0\n0.4\n3.2\n2.5\n2.3\n0.0\n12.4\n2.5\n2.0\n1.6\n2.0\n15.4\n3.7\n1.7\n1.7\nAdd/subtract/convert \nCompares\nMultiply\nDivide\nDivide structural\nFigure C.34 Stalls per FP operation for each major type of FP operation for the\nSPEC89 FP benchmarks. Except for the divide structural hazards, these data do not\ndepend on the frequency of an operation, only on its latency and the number of cycles\nbefore the result is used. The number of stalls from RAW hazards roughly tracks the\nlatency of the FP unit. For example, the average number of stalls per FP add, subtract,\nor convert is 1.7 cycles, or 56% of the latency (three cycles). Likewise, the average num-\nber of stalls for multiplies and divides are 2.8 and 14.2, respectively, or 46% and 59% of\nthe corresponding latency. Structural hazards for divides are rare, because the divide\nfrequency is low.\nC-56\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 831,
        "text": "Number of stalls\n0.00\n1.00\n0.20\n0.10\n0.40\n0.80\n0.90\n0.60\n0.70\n0.30\n0.50\nFP SPEC benchmarks\ndoduc\near\nhydro2d\nmdljdp\nsu2cor\n0.01\n0.01\n0.02\n0.61\n0.00\n0.03\n0.10\n0.88\n0.00\n0.04\n0.22\n0.54\n0.00\n0.07\n0.09\n0.52\n0.08\n0.08\n0.07\n0.98\nFP result stalls\nFP compare stalls\nBranch/load stalls\nFP structural\nFigure C.35 The stalls occurring for the a simple RISC V FP pipeline for five of the\nSPEC89 FP benchmarks. The total number of stalls per instruction ranges from 0.65\nfor su2cor to 1.21 for doduc, with an average of 0.87. FP result stalls dominate in all\ncases, with an average of 0.71 stalls per instruction, or 82% of the stalled cycles. Com-\npares generate an average of 0.1 stalls per instruction and are the second largest source.\nThe divide structural hazard is only significant for doduc. Branch stalls are not accounted\nfor, but would be small.\nIF\nIS\nInstruction memory\nReg\nALU\nData memory\nReg\nRF\nEX\nDF\nDS\nTC\nWB\nFigure C.36 The eight-stage pipeline structure of the R4000 uses pipelined instruction and data caches. The pipe\nstages are labeled and their detailed function is described in the text. The vertical dashed lines represent the stage\nboundaries as well as the location of pipeline latches. The instruction is actually available at the end of IS, but the tag\ncheck is done in RF, while the registers are fetched. Thus, we show the instruction memory as operating through RF.\nThe TC stage is needed for data memory access, because we cannot write the data into the register until we know\nwhether the cache access was a hit or not.\nC.6\nPutting It All Together: The MIPS R4000 Pipeline\n\u25a0\nC-57"
    },
    {
        "page": 832,
        "text": "\u25a0\nEX\u2014Execution, which includes effective address calculation, ALU operation,\nand branch-target computation and condition evaluation.\n\u25a0\nDF\u2014Data fetch, first half of data cache access.\n\u25a0\nDS\u2014Second half of data fetch, completion of data cache access.\n\u25a0\nTC\u2014Tag check, to determine whether the data cache access hit.\n\u25a0\nWB\u2014Write-back for loads and register-register operations.\nIn addition to substantially increasing the amount of forwarding required, this\nlonger-latency pipeline increases both the load and branch delays. Figure C.37\nshows that load delays are two cycles, because the data value is available at the\nend of DS. Figure C.38 shows the shorthand pipeline schedule when a use imme-\ndiately follows a load. It shows that forwarding is required for the result of a load\ninstruction to a destination that is three or four cycles later.\nFigure C.39 shows that the basic branch delay is three cycles, because the\nbranch condition is computed during EX. The MIPS architecture has a single-cycle\ndelayed branch. The R4000 uses a predicted-not-taken strategy for the remaining\ntwo cycles of the branch delay. As Figure C.40 shows, untaken branches are simply\none-cycle delayed branches, while taken branches have a one-cycle delay slot fol-\nlowed by two idle cycles. The instruction set provides a branch-likely instruction,\nwhich we described earlier and which helps in filling the branch delay slot.\nCC 1\nTime (in clock cycles)\nCC 2\nInstruction memory\nReg\nALU\nData memory\nReg\nInstruction memory\nReg\nALU\nData memory\nReg\nInstruction memory\nReg\nALU\nData memory\nReg\nInstruction memory\nInstruction 1\nInstruction 2\nReg\nALU\nData memory\nReg\nCC 3\nCC 4\nCC 5\nCC 6\nCC 7\nCC 8\nCC 9\nCC 10\nCC 11\nFigure C.37 The structure of the R4000 integer pipeline leads to a x1 load delay. A x1 delay is possible because the\ndata value is available at the end of DS and can be bypassed. If the tag check in TC indicates a miss, the pipeline is\nbacked up a cycle, when the correct data are available.\nC-58\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 833,
        "text": "Pipeline interlocks enforce both the x1 branch stall penalty on a taken branch and\nany data hazard stall that arises from use of a load result. After the R4000, all\nimplementations of MIPS processor made use of dynamic branch prediction.\nIn addition to the increase in stalls for loads and branches, the deeper pipeline\nincreases the number of levels of forwarding for ALU operations. In our RISC V\nfive-stage pipeline, forwarding between two register-register ALU instructions\ncould happen from the ALU/MEM or the MEM/WB registers. In the R4000 pipe-\nline, there are four possible sources for an ALU bypass: EX/DF, DF/DS, DS/TC,\nand TC/WB.\nClock number\nInstruction number\n1\n2\n3\n4\n5\n6\n7\n8\n9\nld\nx1,...\nIF\nIS\nRF\nEX\nDF\nDS\nTC\nWB\nadd x2,x1,...\nIF\nIS\nRF\nStall\nStall\nEX\nDF\nDS\nsub x3,x1,...\nIF\nIS\nStall\nStall\nRF\nEX\nDF\nor\nx4,x1,...\nIF\nStall\nStall\nIS\nRF\nEX\nFigure C.38 A load instruction followed by an immediate use results in a x1 stall. Normal forwarding paths can be\nused after two cycles, so the add and sub get the value by forwarding after the stall. The or instruction gets the value\nfrom the register file. Because the two instructions after the load could be independent and hence not stall, the\nbypass can be to instructions that are three or four cycles after the load.\nCC 1\nTime (in clock cycles)\nCC 2\nInstruction memory\nReg\nALU\nData memory\nReg\nInstruction memory\nReg\nALU\nData memory\nReg\nInstruction memory\nReg\nALU\nData memory\nReg\nInstruction memory\nBEQZ\nInstruction 1\nInstruction 2\nInstruction 3\nTarget\nReg\nALU\nData memory\nReg\nInstruction memory\nReg\nALU\nData memory\nCC 3\nCC 4\nCC 5\nCC 6\nCC 7\nCC 8\nCC 9\nCC 10\nCC 11\nFigure C.39 The basic branch delay is three cycles, because the condition evaluation is performed during EX.\nC.6\nPutting It All Together: The MIPS R4000 Pipeline\n\u25a0\nC-59"
    },
    {
        "page": 834,
        "text": "The Floating-Point Pipeline\nThe R4000 floating-point unit consists of three functional units: a floating-point\ndivider, a floating-point multiplier, and a floating-point adder. The adder logic\nis used on the final step of a multiply or divide. Double-precision FP operations\ncan take from 2 cycles (for a negate) up to 112 cycles (for a square root). In addi-\ntion, the various units have different initiation rates. The FP functional unit can be\nthought of as having eight different stages, listed in Figure C.41; these stages are\ncombined in different orders to execute various FP operations.\nThere is a single copy of each of these stages, and various instructions may use a\nstagezeroormoretimesandindifferentorders.FigureC.42showsthelatency,initiation\nrate, and pipeline stages used by the most common double-precision FP operations.\nClock number\nInstruction number\n1\n2\n3\n4\n5\n6\n7\n8\n9\nBranch instruction\nIF\nIS\nRF\nEX\nDF\nDS\nTC\nWB\nDelay slot\nIF\nIS\nRF\nEX\nDF\nDS\nTC\nWB\nStall\nStall\nStall\nStall\nStall\nStall\nStall\nStall\nStall\nStall\nStall\nStall\nStall\nStall\nStall\nBranch target\nIF\nIS\nRF\nEX\nDF\nBranch instruction\nIF\nIS\nRF\nEX\nDF\nDS\nTC\nWB\nDelay slot\nIF\nIS\nRF\nEX\nDF\nDS\nTC\nWB\nBranch instruction+2\nIF\nIS\nRF\nEX\nDF\nDS\nTC\nBranch instruction+3\nIF\nIS\nRF\nEX\nDF\nDS\nFigure C.40 A taken branch, shown in the top portion of the figure, has a one-cycle delay slot followed by a x1\nstall, while an untaken branch, shown in the bottom portion, has simply a one-cycle delay slot. The branch instruc-\ntion can be an ordinary delayed branch or a branch-likely, which cancels the effect of the instruction in the delay slot if\nthe branch is untaken.\nStage\nFunctional unit\nDescription\nA\nFP adder\nMantissa add stage\nD\nFP divider\nDivide pipeline stage\nE\nFP multiplier\nException test stage\nM\nFP multiplier\nFirst stage of multiplier\nN\nFP multiplier\nSecond stage of multiplier\nR\nFP adder\nRounding stage\nS\nFP adder\nOperand shift stage\nU\nUnpack FP numbers\nFigure C.41 The eight stages used in the R4000 floating-point pipelines.\nC-60\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 835,
        "text": "From the information in Figure C.42, we can determine whether a sequence of\ndifferent, independent FP operations can issue without stalling. If the timing of the\nsequence is such that a conflict occurs for a shared pipeline stage, then a stall will\nbe needed. Figures C.43\u2013C.46 show four common possible two-instruction\nsequences: a multiply followed by an add, an add followed by a multiply, a divide\nfollowed by an add, and an add followed by a divide. The figures show all the inter-\nesting starting positions for the second instruction and whether that second instruc-\ntion will issue or stall for each position. Of course, there could be three instructions\nactive, in which case the possibilities for stalls are much higher and the figures\nmore complex.\nPerformance of the R4000 Pipeline\nIn this section, we examine the stalls that occur for the SPEC92 benchmarks when\nrunning on the R4000 pipeline structure. There are four major causes of pipeline\nstalls or losses:\n1. Load stalls\u2014Delays arising from the use of a load result one or two cycles after\nthe load\n2. Branch stalls\u2014Two-cycle stalls on every taken branch plus unfilled or canceled\nbranch delay slots. The version of the MIPS instruction set implemented in the\nR4000 supports instructions that predict a branch at compile time and cause the\ninstruction in the branch delay slot to be canceled when the branch behavior\ndiffers from the prediction. This makes it easier to fill branch delay slots.\n3. FP result stalls\u2014Stalls because of RAW hazards for an FP operand\n4. FP structural stalls\u2014Delays because of issue restrictions arising from conflicts\nfor functional units in the FP pipeline\nFP instruction\nLatency\nInitiation interval\nPipe stages\nAdd, subtract\n4\n3\nU, S+A, A+R, R+S\nMultiply\n8\n4\nU, E+M, M, M, M, N, N+A, R\nDivide\n36\n35\nU, A, R, D28, D+A, D+R, D+A, D+R, A, R\nSquare root\n112\n111\nU, E, (A+R)108, A, R\nNegate\n2\n1\nU, S\nAbsolute value\n2\n1\nU, S\nFP compare\n3\n2\nU, A, R\nFigure C.42 The latencies and initiation intervals for the FP operations initiation intervals for the FP operations\nboth depend on the FP unit stages that a given operation must use. The latency values assume that the destination\ninstruction is an FP operation; the latencies are one cycle less when the destination is a store. The pipe stages are\nshown in the order in which they are used for any operation. The notation S+A indicates a clock cycle in which both\nthe S and A stages are used. The notation D28 indicates that the D stage is used 28 times in a row.\nC.6\nPutting It All Together: The MIPS R4000 Pipeline\n\u25a0\nC-61"
    },
    {
        "page": 836,
        "text": "Figure C.47 shows the pipeline CPI breakdown for the R4000 pipeline for the 10\nSPEC92 benchmarks. Figure C.48 shows the same data but in tabular form.\nFrom the data in Figures C.47 and C.48, we can see the penalty of the deeper\npipelining. The R4000\u2019s pipeline has much longer branch delays than the classic\nfive-stage pipeline. The longer branch delay substantially increases the cycles\nspent on branches, especially for the integer programs with a higher branch\nfrequency. This is the reason that almost all subsequent processors with moderate\nto deep pipelines (8\u201316 stages are typical today) employ dynamic branch\npredictors.\nClock cycle\nOperation\nIssue/stall\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nMultiply\nIssue\nU\nE+M\nM\nM\nM\nN\nN+A\nR\nAdd\nIssue\nU\nS+A\nA+R\nR+S\nIssue\nU\nS+A\nA+R\nR+S\nIssue\nU\nS+A\nA+R\nR+S\nStall\nU\nS+A\nA+R\nR+S\nStall\nU\nS+A\nA+R\nR+S\nIssue\nU\nS+A\nA+R\nR+S\nIssue\nU\nS+A\nA+R\nR+S\nFigure C.43 An FP multiply issued at clock 0 is followed by a single FP add issued between clocks 1 and 7. The\nsecond column indicates whether an instruction of the specified type stalls when it is issued n cycles later, where n is\nthe clock cycle number in which the U stage of the second instruction occurs. The stage or stages that cause a stall are\nin bold. Note that this table deals with only the interaction between the multiply and one add issued between clocks 1\nand 7. In this case, the add will stall if it is issued four or five cycles after the multiply; otherwise, it issues without\nstalling. Notice that the add will be stalled for two cycles if it issues in cycle 4 because on the next clock cycle it will\nstill conflict with the multiply; if, however, the add issues in cycle 5, it will stall for only 1 clock cycle, because that will\neliminate the conflicts.\nClock cycle\nOperation\nIssue/stall\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nAdd\nIssue\nU\nS+A\nA+R\nR+S\nMultiply\nIssue\nU\nE+M\nM\nM\nM\nN\nN+A\nR\nIssue\nU\nM\nM\nM\nM\nN\nN+A\nR\nFigure C.44 A multiply issuing after an add can always proceed without stalling, because the shorter instruction\nclears the shared pipeline stages before the longer instruction reaches them.\nC-62\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 837,
        "text": "An interesting effect observed in the FP programs is that the latency of the FP\nfunctional units leads to more result stalls than the structural hazards, which arise\nboth from the initiation interval limitations and from conflicts for functional units\nfrom different FP instructions. Thus, reducing the latency of FP operations should\nbe the first target, rather than more pipelining or replication of the functional units.\nOf course, reducing the latency would probably increase the structural stalls,\nbecause many potential structural stalls are hidden behind data hazards.\nClock cycle\nOperation\nIssue/stall\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\nDivide\nIssued in\ncycle 0\u2026\nD\nD\nD\nD\nD\nD+A\nD+R\nD+A\nD+R\nA\nR\nAdd\nIssue\nU\nS+A\nA+R\nR+S\nIssue\nU\nS+A\nA+R\nR+S\nStall\nU\nS+A\nA+R\nR+S\nStall\nU\nS+A\nA+R\nR+S\nStall\nU\nS+A\nA+R\nR+S\nStall\nU\nS+A\nA+R\nR+S\nStall\nU\nS+A\nA+R\nR+S\nStall\nU\nS+A\nA+R\nR+S\nIssue\nU\nS+A\nA+R\nIssue\nU\nS+A\nIssue\nU\nFigure C.45 An FP divide can cause a stall for an add that starts near the end of the divide. The divide starts at cycle\n0 and completes at cycle 35; the last 10 cycles of the divide are shown. Because the divide makes heavy use of the\nrounding hardware needed by the add, it stalls an add that starts in any of cycles 28\u201333. Notice that the add starting in\ncycle 28 will be stalled until cycle 36. If the add started right after the divide, it would not conflict, because the add\ncould complete before the divide needed the shared stages, just as we saw in Figure C.44 for a multiply and add. As in\nthe earlier figure, this example assumes exactly one add that reaches the U stage between clock cycles 26 and 35.\nClock cycle\nOperation\nIssue/stall\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nAdd\nIssue\nU\nS+A\nA+R\nR+S\nDivide\nStall\nU\nA\nR\nD\nD\nD\nD\nD\nD\nD\nD\nD\nIssue\nU\nA\nR\nD\nD\nD\nD\nD\nD\nD\nD\nIssue\nU\nA\nR\nD\nD\nD\nD\nD\nD\nD\nFigure C.46 A double-precision add is followed by a double-precision divide. If the divide starts one cycle after the\nadd, the divide stalls, but after that there is no conflict.\nC.6\nPutting It All Together: The MIPS R4000 Pipeline\n\u25a0\nC-63"
    },
    {
        "page": 838,
        "text": "Pipeline CPI\n0.00\n3.00\n0.50\n1.00\n2.00\n1.50\n2.50\nSPEC92 benchmark\ncompress\neqntott\nespresso\ngcc\nli\ndoduc\near\nhydro2d\nmdljdp\nsu2cor\nBase\nLoad stalls\nBranch stalls\nFP result stalls\nFP structural stalls\nFigure C.47 The pipeline CPI for 10 of the SPEC92 benchmarks, assuming a\nperfect cache. The pipeline CPI varies from 1.2 to 2.8. The left-most five programs\nare integer programs, and branch delays are the major CPI contributor for these. The\nright-most five programs are FP, and FP result stalls are the major contributor for these.\nFigure C.48 shows the numbers used to construct this plot.\nBenchmark\nPipeline CPI\nLoad stalls\nBranch stalls\nFP result stalls\nFP structural stalls\nCompress\n1.20\n0.14\n0.06\n0.00\n0.00\nEqntott\n1.88\n0.27\n0.61\n0.00\n0.00\nEspresso\n1.42\n0.07\n0.35\n0.00\n0.00\nGcc\n1.56\n0.13\n0.43\n0.00\n0.00\nLi\n1.64\n0.18\n0.46\n0.00\n0.00\nInteger average\n1.54\n0.16\n0.38\n0.00\n0.00\nDoduc\n2.84\n0.01\n0.22\n1.39\n0.22\nMdljdp2\n2.66\n0.01\n0.31\n1.20\n0.15\nEar\n2.17\n0.00\n0.46\n0.59\n0.12\nHydro2d\n2.53\n0.00\n0.62\n0.75\n0.17\nSu2cor\n2.18\n0.02\n0.07\n0.84\n0.26\nFP average\n2.48\n0.01\n0.33\n0.95\n0.18\nOverall average\n2.00\n0.10\n0.36\n0.46\n0.09\nFigure C.48 The total pipeline CPI and the contributions of the four major sources of stalls are shown. The major\ncontributors are FP result stalls (both for branches and for FP inputs) and branch stalls, with loads and FP structural\nstalls adding less.\nC-64\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 839,
        "text": "C.7\nCross-Cutting Issues\nRISC Instruction Sets and Efficiency of Pipelining\nWe have already discussed the advantages of instruction set simplicity in building\npipelines. Simple instruction sets offer another advantage: they make it easier to\nschedule code to achieve efficiency of execution in a pipeline. To see this, con-\nsider a simple example: suppose we need to add two values in memory and store\nthe result back to memory. In some sophisticated instruction sets this will take\nonly a single instruction; in others, it will take two or three. A typical RISC archi-\ntecture would require four instructions (two loads, an add, and a store). These\ninstructions cannot be scheduled sequentially in most pipelines without interven-\ning stalls.\nWith a RISC instruction set, the individual operations are separate instructions\nand may be individually scheduled either by the compiler (using the techniques we\ndiscussed earlier and more powerful techniques discussed in Chapter 3) or using\ndynamic hardware scheduling techniques (which we discuss next and in further\ndetail in Chapter 3). These efficiency advantages, coupled with the greater ease\nof implementation, appear to be so significant that almost all recent pipelined\nimplementations of complex instruction sets actually translate their complex\ninstructions into simple RISC-like operations, and then schedule and pipeline those\noperations. All recent Intel processors use this approach, and it is also used in ARM\nprocessors for some of the more complex instructions.\nDynamically Scheduled Pipelines\nSimple pipelines fetch an instruction and issue it, unless there is a data dependence\nbetween an instruction already in the pipeline and the fetched instruction that can-\nnot be hidden with bypassing or forwarding. Forwarding logic reduces the effec-\ntive pipeline latency so that certain dependences do not result in hazards. If there is\nan unavoidable hazard, then the hazard detection hardware stalls the pipeline (start-\ning with the instruction that uses the result). No new instructions are fetched or\nissued until the dependence is cleared. To overcome these performance losses,\nthe compiler can attempt to schedule instructions to avoid the hazard; this approach\nis called compiler or static scheduling.\nSeveral early processors used another approach, called dynamic scheduling,\nwhereby the hardware rearranges the instruction execution to reduce the stalls. This\nsection offers a simpler introduction to dynamic scheduling by explaining the scor-\neboarding technique of the CDC 6600. Some readers will find it easier to read this\nmaterial before plunging into the more complicated Tomasulo scheme, and the\nspeculation approaches that extend it, both of which are covered in Chapter 3.\nAll the techniques discussed in this appendix so far use in-order instruction\nissue, which means that if an instruction is stalled in the pipeline, no later instruc-\ntions can proceed. With in-order issue, if two instructions have a hazard between\nC.7\nCross-Cutting Issues\n\u25a0\nC-65"
    },
    {
        "page": 840,
        "text": "them, the pipeline will stall, even if there are later instructions that are independent\nand would not stall.\nIn the RISC V pipeline developed earlier, both structural and data hazards were\nchecked during instruction decode (ID): when an instruction could execute properly,\nit was issued from ID. To allow an instruction to begin execution as soon as its oper-\nands are available, even if a predecessor is stalled, we must separate the issue process\ninto two parts: checking the structural hazards and waiting for the absence of a data\nhazard.We decode andissue instructionsin order;however, we want theinstructions\ntobeginexecutionassoonastheirdataoperandsareavailable.Thus,thepipelinewill\ndo out-of-order execution, which implies out-of-order completion. To implement\nout-of-order execution, we must split the ID pipe stage into two stages:\n1. Issue\u2014Decode instructions, check for structural hazards.\n2. Read operands\u2014Wait until no data hazards, then read operands.\nThe IF stage proceeds the issue stage, and the EX stage follows the read oper-\nands stage, just as in the RISC V pipeline. As in the RISC V floating-point pipeline,\nexecution may take multiple cycles, depending on the operation. Thus, we may\nneed to distinguish when an instruction begins execution and when it completes\nexecution; between the two times, the instruction is in execution. This allows mul-\ntiple instructions to be in execution at the same time. In addition to these changes to\nthe pipeline structure, we will also change the functional unit design by varying the\nnumber of units, the latency of operations, and the functional unit pipelining so as\nto better explore these more advanced pipelining techniques.\nDynamic Scheduling With a Scoreboard\nIn a dynamically scheduled pipeline, all instructions pass through the issue stage in\norder (in-order issue); however, they can be stalled or bypass each other in the sec-\nond stage (read operands) and thus enter execution out of order. Scoreboarding is a\ntechnique for allowing instructions to execute out of order when there are sufficient\nresources and no data dependences; it is named after the CDC 6600 scoreboard,\nwhich developed this capability.\nBefore we see how scoreboarding could be used in the RISC V pipeline, it\nis important to observe that WAR hazards, which did not exist in the RISC V\nfloating-point or integer pipelines, may arise when instructions execute out of\norder. For example, consider the following code sequence:\nfdiv.d\nf0,f2,f4\nfadd.d\nf10,f0,f8\nfsub.d\nf8,f8,f14\nThere is an potential WAR hazard between the fadd.d and the fsub.d: If\nthe pipeline executes the fsub.d before the fadd.d, it will violate yield incor-\nrect execution. Likewise, the pipeline must avoid WAW hazards (e.g.,as would\nC-66\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 841,
        "text": "occur if the destination of the fsub.d were f10). As we will see, both these haz-\nards are avoided in a scoreboard by stalling the later instruction involved in\nthe hazard.\nThe goal of a scoreboard is to maintain an execution rate of one instruction per\nclock cycle (when there are no structural hazards) by executing an instruction as\nearly as possible. Thus, when the next instruction to execute is stalled, other\ninstructions can be issued and executed if they do not depend on any active or\nstalled instruction. The scoreboard takes full responsibility for instruction issue\nand execution, including all hazard detection. Taking advantage of out-of-order\nexecution requires multiple instructions to be in their EX stage simultaneously.\nThis can be achieved with multiple functional units, with pipelined functional\nunits, or with both. Because these two capabilities\u2014pipelined functional units\nand multiple functional units\u2014are essentially equivalent for the purposes of pipe-\nline control, we will assume the processor has multiple functional units.\nThe CDC 6600 had 16 separate functional units, including 4 floating-point units,\n5 units for memory references, and 7 units for integer operations. On a processor for\nthe RISCV architecture,scoreboards make sense primarily onthe floating-point unit\nbecause the latency of the other functional units is very small. Let\u2019s assume that there\nare two multipliers, one adder, one divide unit, and a single integer unit for all mem-\nory references, branches, and integer operations. Although this example is simpler\nthan the CDC 6600, it is sufficiently powerful to demonstrate the principles without\nhaving a mass of detailor needing very long examples. Because both RISCV and the\nCDC6600areload-store architectures, thetechniquesarenearlyidenticalforthetwo\nprocessors. Figure C.49 shows what the processor looks like.\nEvery instruction goes through the scoreboard, where a record of the data\ndependences is constructed; this step corresponds to instruction issue and replaces\npart of the ID step in the RISC V pipeline. The scoreboard then determines when\nthe instruction can read its operands and begin execution. If the scoreboard decides\nthe instruction cannot execute immediately, it monitors every change in the hard-\nware and decides when the instruction can execute. The scoreboard also controls\nwhen an instruction can write its result into the destination register. Thus, all hazard\ndetection and resolution are centralized in the scoreboard. We will see a picture of\nthe scoreboard later (Figure C.49 on page C.68), but first we need to understand the\nsteps in the issue and execution segment of the pipeline.\nEach instruction undergoes four steps in executing. (Because we are concen-\ntrating on the FP operations, we will not consider a step for memory access.) Let\u2019s\nfirst examine the steps informally and then look in detail at how the scoreboard\nkeeps the necessary information that determines when to progress from one step\nto the next. The four steps, which replace the ID, EX, and WB steps in the standard\nRISC V pipeline, are as follows:\n1. Issue\u2014If a functional unit for the instruction is free and no other active instruc-\ntion has the same destination register, the scoreboard issues the instruction to\nthe functional unit and updates its internal data structure. This step replaces a\nportion of the ID step in the RISC V pipeline. By ensuring that no other active\nC.7\nCross-Cutting Issues\n\u25a0\nC-67"
    },
    {
        "page": 842,
        "text": "functional unit wants to write its result into the destination register, we guaran-\ntee that WAW hazards cannot be present. If a structural or WAW hazard exists,\nthen the instruction issue stalls, and no further instructions will issue until these\nhazards are cleared. When the issue stage stalls, it causes the buffer between\ninstruction fetch and issue to fill; if the buffer is a single entry, instruction fetch\nstalls immediately. If the buffer is a queue with multiple instructions, it stalls\nwhen the queue fills.\n2. Read operands\u2014The scoreboard monitors the availability of the source oper-\nands. A source operand is available if no earlier issued active instruction is\ngoing to write it. When the source operands are available, the scoreboard tells\nControl/ \nstatus\nScoreboard\nControl/\nstatus \nInteger unit\nFP add\nFP divide\nFP mult\nFP mult\nData buses\nRegisters\nFigure C.49 The basic structure of a RISC V processor with a scoreboard. The score-\nboard\u2019s function is to control instruction execution (vertical control lines). All of the data\nflow between the register file and the functional units over the buses (the horizontal\nlines, called trunks in the CDC 6600). There are two FP multipliers, an FP divider, an\nFP adder, and an integer unit. One set of buses (two inputs and one output) serves a\ngroup of functional units. We will explore scoreboarding and its extensions in more\ndetail in Chapter 3.\nC-68\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 843,
        "text": "the functional unit to proceed to read the operands from the registers and begin\nexecution. The scoreboard resolves RAW hazards dynamically in this step, and\ninstructions may be sent into execution out of order. This step, together with\nissue, completes the function of the ID step in the simple RISC V pipeline.\n3. Execution\u2014The functional unit begins execution upon receiving operands.\nWhen the result is ready, it notifies the scoreboard that it has completed execu-\ntion. This step replaces the EX step in the RISC V pipeline and takes multiple\ncycles in the RISC V FP pipeline.\n4. Write result\u2014Once the scoreboard is aware that the functional unit has\ncompleted execution, the scoreboard checks for WAR hazards and stalls the\ncompleting instruction, if necessary.\nA WAR hazard exists if there is a code sequence like our earlier example with\nfadd.d and fsub.d that both use f8. In that example, we had the code\nfdiv.d\nf0,f2,f4\nfadd.d\nf10,f0,f8\nfsub.d\nf8,f8,f14\nfadd.d has a source operand f8, which is the same register as the destination of\nfsub.d. But fadd.d actually depends on an earlier instruction. The scoreboard\nwill still stall the fsub.d in its write result stage until fadd.d reads its operands.\nIn general, then, a completing instruction cannot be allowed to write its results\nwhen:\n\u25a0\nThere is an instruction that has not read its operands that precedes (i.e., in order\nof issue) the completing instruction, and\n\u25a0\nOne of the operands is the same register as the result of the completing\ninstruction.\nIf this WAR hazard does not exist, or when it clears, the scoreboard tells the func-\ntional unit to store its result to the destination register. This step replaces the WB\nstep in the simple RISC V pipeline.\nAt first glance, it might appear that the scoreboard will have difficulty separat-\ning RAW and WAR hazards.\nBecause the operands for an instruction are read only when both operands are\navailable in the register file, this scoreboard does not take advantage of forwarding.\nInstead, registers are only read when they are both available. This is not as large a\npenalty as you might initially think. Unlike our simple pipeline of earlier, instruc-\ntions will write their result into the register file as soon as they complete execution\n(assuming no WAR hazards), rather than wait for a statically assigned write slot\nthat may be several cycles away. The effect reduces the pipeline latency and the\nbenefits of forwarding. There is still one additional cycle of latency that arises\nbecause the write result and read operand stages cannot overlap. We would need\nadditional buffering to eliminate this overhead.\nC.7\nCross-Cutting Issues\n\u25a0\nC-69"
    },
    {
        "page": 844,
        "text": "Based on its own data structure, the scoreboard controls the instruction pro-\ngression from one step to the next by communicating with the functional units.\nThere is a small complication, however. There are only a limited number of\nsource operand buses and result buses to the register file, which represents\na structural hazard. The scoreboard must guarantee that the number of func-\ntional units allowed to proceed into steps 2 and 4 does not exceed the number\nof buses available. We will not go into further detail on this, other than to men-\ntion that the CDC 6600 solved this problem by grouping the 16 functional\nunits together into four groups and supplying a set of buses, called data trunks,\nfor each group. Only one unit in a group could read its operands or write its\nresult during a clock.\nC.8\nFallacies and Pitfalls\nPitfall\nUnexpected execution sequences may cause unexpected hazards.\nAt first glance, WAW hazards look like they should never occur in a code sequence\nbecause no compiler would ever generate two writes to the same register without an\nintervening read, but they can occur when the sequence is unexpected. For exam-\nple, consider a long running floating point divide that causes a trap. If the trap rou-\ntine writes the same register as the divide early on, it may cause a WAW hazard, if\nit writes the register before the divide completes. Hardware or software must avoid\nthis possibility.\nPitfall\nExtensive pipelining can impact other aspects of a design, leading to overall worse\ncost-performance.\nThe best example of this phenomenon comes from two implementations of the\nVAX, the 8600 and the 8700. When the 8600 was initially delivered, it had a cycle\ntime of 80 ns. Subsequently, a redesigned version, called the 8650, with a 55 ns\nclock was introduced. The 8700 has a much simpler pipeline that operates at\nthe microinstruction level, yielding a smaller processor with a faster clock cycle\nof 45 ns. The overall outcome is that the 8650 has a CPI advantage of about\n20%, but the 8700 has a clock rate that is about 20% faster. Thus, the 8700\nachieved the same performance with much less hardware.\nPitfall\nEvaluating dynamic or static scheduling on the basis of unoptimized code.\nUnoptimized code\u2014containing redundant loads, stores, and other operations that\nmight be eliminated by an optimizer\u2014is much easier to schedule than \u201ctight\u201d\noptimized code. This holds for scheduling both control delays (with delayed\nbranches) and delays arising from RAW hazards. In gcc running on an\nR3000, which has a pipeline almost identical to that of Section C.1, the fre-\nquency of idle clock cycles increases by 18% from the unoptimized and sched-\nuled code to the optimized and scheduled code. Of course, the optimized\nprogram is much faster, because it has fewer instructions. To fairly evaluate a\ncompile-time scheduler or runtime dynamic scheduling, you must use optimized\nC-70\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 845,
        "text": "code, because in the real system you will derive good performance from other\noptimizations in addition to scheduling.\nC.9\nConcluding Remarks\nAt the beginning of the 1980s, pipelining was a technique reserved primarily for\nsupercomputers and large multimillion-dollar mainframes. By the mid-1980s, the\nfirst pipelined microprocessors appeared and helped transform the world of com-\nputing, allowing microprocessors to bypass minicomputers in performance and\neventually to take on and outperform mainframes. By the early 1990s, high-end\nembedded microprocessors embraced pipelining, and desktops were headed\ntoward the use of the sophisticated dynamically scheduled, multiple-issue\napproaches discussed in Chapter 3. The material in this appendix, which was con-\nsidered reasonably advanced for graduate students when this text first appeared in\n1990, is now considered basic undergraduate material and can be found in proces-\nsors that cost less than $1!\nC.10\nHistorical Perspective and References\nSection M.5 (available online) features a discussion on the development of\npipelining and instruction-level parallelism covering both this appendix and the\nmaterial in Chapter 3. We provide numerous references for further reading and\nexploration of these topics.\nUpdated Exercises by Diana Franklin\nC.1\n[15/15/15/15/25/10/15]<A.2>Use the following code fragment:\nLoop:\nld\nx1,0(x2)\n;load x1 from address 0+x2\naddi\nx1,x1,1\n;x1=x1+1\nsd\nx1,0,(x2)\n;store x1 at address 0+x2\naddi\nx2,x2,4\n;x2=x2+4\nsub\nx4,x3,x2\n;x4=x3-x2\nbnez\nx4,Loop\n;branch to Loop if x4!= 0\nAssume that the initial value of x3 is x2+396.\na. [15]<C.2>Data hazards are caused by data dependences in the code. Whether\na dependency causes a hazard depends on the machine implementation (i.e.,\nnumber of pipeline stages). List all of the data dependences in the code above.\nRecord the register, source instruction, and destination instruction; for example,\nthere is a data dependency for register x1 from the ld to the addi.\nb. [15]<C.2>Show the timing of this instruction sequence for the 5-stage RISC\npipeline without any forwarding or bypassing hardware but assuming that a reg-\nister read and a write in the same clock cycle \u201cforwards\u201d through the register\nUpdated Exercises by Diana Franklin\n\u25a0\nC-71"
    },
    {
        "page": 846,
        "text": "file, as between the add and or shown in Figure C.5. Use a pipeline timing\nchart like that in Figure C.8. Assume that the branch is handled by flushing\nthe pipeline. If all memory references take 1 cycle, how many cycles does this\nloop take to execute?\nc. [15]<C.2>Show the timing of this instruction sequence for the 5-stage RISC\npipeline with full forwarding and bypassing hardware. Use a pipeline timing\nchart like that shown in Figure C.8. Assume that the branch is handled by pre-\ndicting it as not taken. If all memory references take 1 cycle, how many cycles\ndoes this loop take to execute?\nd. [15]<C.2>Show the timing of this instruction sequence for the 5-stage RISC\npipeline with full forwarding and bypassing hardware, as shown in Figure C.6.\nUse a pipeline timing chart like that shown in Figure C.8. Assume that the\nbranch is handled by predicting it as taken. If all memory references take 1\ncycle, how many cycles does this loop take to execute?\ne. [25]<C.2>High-performance processors have very deep pipelines\u2014more\nthan 15 stages. Imagine that you have a 10-stage pipeline in which every stage\nof the 5-stage pipeline has been split in two. The only catch is that, for data for-\nwarding, data are forwarded from the end of a pair of stages to the beginning of\nthe two stages where they are needed. For example, data are forwarded from the\noutput of the second execute stage to the input of the first execute stage, still\ncausing a 1-cycle delay. Show the timing of this instruction sequence for the\n10-stage RISC pipeline with full forwarding and bypassing hardware. Use a\npipeline timing chart like that shown in Figure C.8 (but with stages labeled\nIF1, IF2, ID1, etc.). Assume that the branch is handled by predicting it as taken.\nIf all memory references take 1 cycle, how many cycles does this loop take to\nexecute?\nf. [10]<C.2>Assume that in the 5-stage pipeline, the longest stage requires\n0.8 ns, and the pipeline register delay is 0.1 ns. What is the clock cycle time\nof the 5-stage pipeline? If the 10-stage pipeline splits all stages in half, what\nis the cycle time of the 10-stage machine?\ng. [15]<C.2>Using your answers from parts (d) and (e), determine the cycles per\ninstruction (CPI) for the loop on a 5-stage pipeline and a 10-stage pipeline.\nMake sure you count only from when the first instruction reaches the write-back\nstage to the end. Do not count the start-up of the first instruction. Using the clock\ncycle time calculated in part (f), calculate the average instruction execute time\nfor each machine.\nC.2\n[15/15]<C.2>Suppose the branch frequencies (as percentages of all instructions)\nare as follows:\nConditional branches\n15%\nJumps and calls\n1%\nTaken conditional branches\n60% are taken\nC-72\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 847,
        "text": "a. [15]<C.2>We are examining a four-stage pipeline where the branch is\nresolved at the end of the second cycle for unconditional branches and at the\nend of the third cycle for conditional branches. Assuming that only the first pipe\nstage can always be completed independent of whether the branch is taken and\nignoring other pipeline stalls, how much faster would the machine be without\nany branch hazards?\nb. [15]<C.2>Now assume a high-performance processor in which we have a 15-\ndeep pipeline where the branch is resolved at the end of the fifth cycle for\nunconditional branches and at the end of the tenth cycle for conditional\nbranches. Assuming that only the first pipe stage can always be completed inde-\npendent of whether the branch is taken and ignoring other pipeline stalls, how\nmuch faster would the machine be without any branch hazards?\nC.3\n[5/15/10/10]<C.2>We begin with a computer implemented in single-cycle\nimplementation. When the stages are split by functionality, the stages do not\nrequire exactly the same amount of time. The original machine had a clock\ncycle time of 7 ns. After the stages were split, the measured times were IF,\n1 ns; ID, 1.5 ns; EX, 1 ns; MEM, 2 ns; and WB, 1.5 ns. The pipeline register delay\nis 0.1 ns.\na. [5]<C.2>What is the clock cycle time of the 5-stage pipelined machine?\nb. [15]<C.2>If there is a stall every four instructions, what is the CPI of the new\nmachine?\nc. [10]<C.2>What is the speedup of the pipelined machine over the single-cycle\nmachine?\nd. [10]<C.2>If the pipelined machine had an infinite number of stages, what\nwould its speedup be over the single-cycle machine?\nC.4\n[15]<C.1, C.2>A reduced hardware implementation of the classic five-stage\nRISC pipeline might use the EX stage hardware to perform a branch instruction\ncomparison and then not actually deliver the branch target PC to the IF stage until\nthe clock cycle in which the branch instruction reaches the MEM stage. Control\nhazard stalls can be reduced by resolving branch instructions in ID, but improving\nperformance in one respect may reduce performance in other circumstances. Write\na small snippet of code in which calculating the branch in the ID stage causes a data\nhazard, even with data forwarding.\nC.5\n[12/13/20/20/15/15]<C.2, C.3>For these problems, we will explore a pipeline\nfor a register-memory architecture. The architecture has two instruction formats:\na register-register format and a register-memory format. There is a single-memory\naddressing mode (offset+base register). There is a set of ALU operations with the\nformat:\nALUop Rdest, Rsrc1, Rsrc2\nor\nALUop Rdest, Rsrc1, MEM\nUpdated Exercises by Diana Franklin\n\u25a0\nC-73"
    },
    {
        "page": 848,
        "text": "where the ALUop is one of the following: add, subtract, AND, OR, load (Rsrc1\nignored), or store. Rsrc or Rdest are registers. MEM is a base register and offset\npair. Branches use a full compare of two registers and are PC relative. Assume that\nthis machine is pipelined so that a new instruction is started every clock cycle. The\npipeline structure, similar to that used in the VAX 8700 micropipeline (Clark,\n1987), is\nIF\nRF\nALU1\nMEM\nALU2\nWB\nIF\nRF\nALU1\nMEM\nALU2\nWB\nIF\nRF\nALU1\nMEM\nALU2\nWB\nIF\nRF\nALU1\nMEM\nALU2\nWB\nIF\nRF\nALU1\nMEM\nALU2\nWB\nIF\nRF\nALU1\nMEM\nALU2\nWB\nThe first ALU stage is used for effective address calculation for memory references\nand branches. The second ALU cycle is used for operations and branch compa-\nrison. RF is both a decode and register-fetch cycle. Assume that when a register\nread and a register write of the same register occur in the same clock, the write data\nare forwarded.\na. [12]<C.2>Find the number of adders needed, counting any adder or incre-\nmenter; show a combination of instructions and pipe stages that justify this\nanswer. You need only give one combination that maximizes the adder count.\nb. [13]<C.2>Find the number of register read and write ports and memory read\nand write ports required. Show that your answer is correct by showing a com-\nbination of instructions and pipeline stage indicating the instruction and the\nnumber of read ports and write ports required for that instruction.\nc. [20]<C.3>Determine any data forwarding for any ALUs that will be needed.\nAssume that there are separate ALUs for the ALU1 and ALU2 pipe stages. Put\nin all forwarding among ALUs necessary to avoid or reduce stalls. Show the\nrelationship between the two instructions involved in forwarding using the for-\nmat of the table in Figure C.23 but ignoring the last two columns. Be careful to\nconsider forwarding across an intervening instruction\u2014for example,\nadd\nx1, ...\nany instruction\nadd\n..., x1, ...\nd. [20]<C.3>Show all of the data forwarding requirements necessary to avoid or\nreduce stalls when either the source or destination unit is not an ALU. Use the\nsame format as in Figure C.23, again ignoring the last two columns. Remember\nto forward to and from memory references.\nC-74\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 849,
        "text": "e. [15]<C.3>Show all the remaining hazards that involve at least one unit other\nthan an ALU as the source or destination unit. Use a table like that shown in\nFigure C.25, but replace the last column with the lengths of the hazards.\nf. [15]<C.2>Show all control hazards by example and state the length of the\nstall. Use a format like that shown in Figure C.11, labeling each example.\nC.6\n[12/13/13/15/15]<C.1, C.2, C.3>We will now add support for register-memory\nALU operations to the classic five-stage RISC pipeline. To offset this increase in\ncomplexity, all memory addressing will be restricted to register indirect (i.e., all\naddresses are simply a value held in a register; no offset or displacement may\nbe added to the register value). For example, the register-memory instruction\nadd x4, x5, (x1) means add the contents of register x5 to the contents of\nthe memory location with address equal to the value in register x1 and put the\nsum in register x4. Register-register ALU operations are unchanged. The following\nitems apply to the integer RISC pipeline:\na. [12]<C.1>List a rearranged order of the five traditional stages of the RISC\npipeline that will support register-memory operations implemented exclusively\nby register indirect addressing.\nb. [13]<C.2, C.3>Describe what new forwarding paths are needed for the rear-\nranged pipeline by stating the source, destination, and information transferred\non each needed new path.\nc. [13]<C.2, C.3>For the reordered stages of the RISC pipeline, what new data\nhazards are created by this addressing mode? Give an instruction sequence illus-\ntrating each new hazard.\nd. [15]<C.3>List all of the ways that the RISC pipeline with register-memory\nALU operations can have a different instruction count for a given program than\nthe original RISC pipeline. Give a pair of specific instruction sequences, one for\nthe original pipeline and one for the rearranged pipeline, to illustrate each way.\ne. [15]<C.3>Assume that all instructions take 1 clock cycle per stage. List all of\nthe ways that the register-memory RISC V can have a different CPI for a given\nprogram as compared to the original RISC V pipeline.\nC.7\n[10/10]<C.3>In this problem, we will explore how deepening the pipeline\naffects performance in two ways: faster clock cycle and increased stalls due to data\nand control hazards. Assume that the original machine is a 5-stage pipeline with a\n1 ns clock cycle. The second machine is a 12-stage pipeline with a 0.6 ns clock\ncycle. The 5-stage pipeline experiences a stall due to a data hazard every five\ninstructions, whereas the 12-stage pipeline experiences three stalls every eight\ninstructions. In addition, branches constitute 20% of the instructions, and the mis-\nprediction rate for both machines is 5%.\na. [10]<C.3>What is the speedup of the 12-stage pipeline over the 5-stage pipe-\nline, taking into account only data hazards?\nUpdated Exercises by Diana Franklin\n\u25a0\nC-75"
    },
    {
        "page": 850,
        "text": "b. [10]<C.3>If the branch mispredict penalty for the first machine is 2 cycles but\nthe second machine is 5 cycles, what are the CPIs of each, taking into account\nthe stalls due to branch mispredictions?\nC.8\n[15]<C.5>Construct a table like that shown in Figure C.21 to check for WAW\nstalls in the RISC V FP pipeline of Figure C.30. Do not consider FP divides.\nC.9\n[20/22/22]<C.4, C.6>In this exercise, we will look at how a common vector\nloop runs on statically and dynamically scheduled versions of the RISC V pipe-\nline. The loop is the so-called DAXPY loop (discussed extensively in Appendix\nG) and the central operation in Gaussian elimination. The loop implements the\nvector operation Y=a*X+Y for a vector of length 100. Here is the MIPS code\nfor the loop:\nfoo:\nfld\nf2, 0(x1)\n; load X(i)\nfmul.d\nf4, f2, f0\n; multiply a*X(i)\nfld\nf6, 0(x2)\n; load Y(i)\nfadd.d\nf6, f4, f6\n; add a*X(i) + Y(i)\nfsd\n0(x2), f6\n; store Y(i)\naddi\nx1, x1, 8\n; increment X index\naddi\nx2, x2, 8\n; increment Y index\nsltiu\nx3, x1, done ; test if done\nbnez\nx3, foo\n; loop if not done\nFor parts (a) to (c), assume that integer operations issue and complete in 1 clock\ncycle (including loads) and that their results are fully bypassed. You will use the FP\nlatencies (only) shown in Figure C.29, but assume that the FP unit is fully pipe-\nlined. For scoreboards below, assume that an instruction waiting for a result from\nanother function unit can pass through read operands at the same time the result is\nwritten. Also assume that an instruction in WB completing will allow a currently\nactive instruction that is waiting on the same functional unit to issue in the same\nclock cycle in which the first instruction completes WB.\na. [20]<C.5>For this problem, use the RISC V pipeline of Section C.5 with the\npipeline latencies from Figure C.29, but a fully pipelined FP unit, so the initi-\nation interval is 1. Draw a timing diagram, similar to Figure C.32, showing the\ntiming of each instruction's execution. How many clock cycles does each loop\niteration take, counting from when the first instruction enters the WB stage to\nwhen the last instruction enters the WB stage?\nb. [20]<C.8>Perform static instruction reordering to reorder the instructions to\nminimize the stalls for this loop, renaming registers where necessary. Use all the\nsame assumptions as in (a). Draw a timing diagram, similar to Figure C.32,\nshowing the timing of each instruction's execution. How many clock cycles\ndoes each loop iteration take, counting from when the first instruction enters\nthe WB stage to when the last instruction enters the WB stage?\nC-76\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 851,
        "text": "c. [20]<C.8>Using the original code above, consider how the instructions\nwould have executed using scoreboarding, a form of dynamic scheduling.\nDraw a timing diagram, similar to Figure C.32, showing the timing of\nthe instructions through stages IF, IS (issue), RO (read operands), EX (exe-\ncution), and WR (write result). How many clock cycles does each loop iter-\nation take, counting from when the first instruction enters the WB stage to\nwhen the last instruction enters the WB stage?\nC.10\n[25]<C.8>It is critical that the scoreboard be able to distinguish RAW and WAR\nhazards, because a WAR hazard requires stalling the instruction doing the writing\nuntil the instruction reading an operand initiates execution, but a RAW hazard\nrequires delaying the reading instruction until the writing instruction finishes\u2014just\nthe opposite. For example, consider the sequence:\nfmul.d\nf0,f6,f4\nfsub.d\nf8,f0,f2\nfadd.d\nf2,f10,f2\nThe fsub.d depends on the fmul.d (a RAW hazard), thus the fmul.d must be\nallowed to complete before the fsub.d. If the fmul.d were stalled for the\nfsub.d due to the inability to distinguish between RAW and WAR hazards,\nthe processor will deadlock. This sequence contains a WAR hazard between the\nfadd.d and the fsub.d, and the fadd.d cannot be allowed to complete until\nthe fsub.d begins execution. The difficulty lies in distinguishing the RAW haz-\nard between fmul.d and fsub.d, and the WAR hazard between the fsub.d and\nfadd.d. To see just why the three-instruction scenario is important, trace the han-\ndling of each instruction stage by stage through issue, read operands, execute, and\nwrite result. Assume that each scoreboard stage other than execute takes 1 clock\ncycle. Assume that the fmul.d instruction requires 3 clock cycles to execute and\nthat the fsub.d and fadd.d instructions each take 1 cycle to execute. Finally,\nassume that the processor has two multiply function units and two add function\nunits. Present the trace as follows.\n1. Make a table with the column headings Instruction, Issue, Read Operands, Exe-\ncute, Write Result, and Comment. In the first column, list the instructions in\nprogram order (be generous with space between instructions; larger table cells\nwill better hold the results of your analysis). Start the table by writing a 1 in the\nIssue column of the fmul.d instruction row to show that fmul.d completes\nthe issue stage in clock cycle 1. Now, fill in the stage columns of the table\nthrough the cycle at which the scoreboard first stalls an instruction.\n2. For a stalled instruction write the words \u201cwaiting at clock cycle X,\u201d where X is\nthe number of the current clock cycle, in the appropriate table column to show\nthat the scoreboard is resolving an RAW or WAR hazard by stalling that stage.\nIn the Comment column, state what type of hazard and what dependent instruc-\ntion is causing the wait.\nUpdated Exercises by Diana Franklin\n\u25a0\nC-77"
    },
    {
        "page": 852,
        "text": "3. Adding the words \u201ccompletes with clock cycle Y\u201d to a \u201cwaiting\u201d table entry, fill\nin the rest of the table through the time when all instructions are complete. For\nan instruction that stalled, add a description in the Comments column telling\nwhy the wait ended when it did and how deadlock was avoided (Hint: Think\nabout how WAW hazards are prevented and what this implies about active\ninstruction sequences.). Note the completion order of the three instructions\nas compared to their program order.\nC.11\n[10/10/10]<C.5>For this problem, you will create a series of small snippets that\nillustrate the issues that arise when using functional units with different latencies.\nFor each one, draw a timing diagram similar to Figure C.32 that illustrates each\nconcept, and clearly indicate the problem.\na. [10]<C.5>Demonstrate, using code different from that used in Figure C.32,\nthe structural hazard of having the hardware for only one MEM and WB stage.\nb. [10]<C.5>Demonstrate a WAW hazard requiring a stall.\nC-78\n\u25a0\nAppendix C Pipelining: Basic and Intermediate Concepts"
    },
    {
        "page": 853,
        "text": "D.1\nIntroduction\nD-2\nD.2\nAdvanced Topics in Disk Storage\nD-2\nD.3\nDefinition and Examples of Real Faults and Failures\nD-10\nD.4\nI/O Performance, Reliability Measures, and Benchmarks\nD-15\nD.5\nA Little Queuing Theory\nD-23\nD.6\nCrosscutting Issues\nD-34\nD.7\nDesigning and Evaluating an I/O System\u2014The\nInternet Archive Cluster\nD-36\nD.8\nPutting It All Together: NetApp FAS6000 Filer\nD-41\nD.9\nFallacies and Pitfalls\nD-43\nD.10\nConcluding Remarks\nD-47\nD.11\nHistorical Perspective and References\nD-48\nCase Studies with Exercises by Andrea C. Arpaci-Dusseau\nand Remzi H. Arpaci-Dusseau\nD-48"
    },
    {
        "page": 854,
        "text": "D\nStorage Systems\nI think Silicon Valley was misnamed. If you look back at the dollars\nshipped in products in the last decade, there has been more revenue\nfrommagneticdisksthanfromsilicon.Theyoughttorenametheplace\nIron Oxide Valley.\nAl Hoagland\nA pioneer of magnetic disks (1982)\nCombiningbandwidthandstorage\u2026enablesswiftandreliableaccess\nto the ever expanding troves of content on the proliferating disks and\n\u2026 repositories of the Internet \u2026 the capacity of storage arrays of all\nkinds is rocketing ahead of the advance of computer performance.\nGeorge Gilder\n\u201cThe End Is Drawing Nigh,\u201d\nForbes ASAP (April 4, 2000)"
    },
    {
        "page": 855,
        "text": "D.1\nIntroduction\nThe popularity of Internet services such as search engines and auctions has\nenhanced the importance of I/O for computers, since no one would want a desktop\ncomputer that couldn\u2019t access the Internet. This rise in importance of I/O is\nreflected by the names of our times. The 1960s to 1980s were called the Computing\nRevolution; the period since 1990 has been called the Information Age, with\nconcerns focused on advances in information technology versus raw computa-\ntional power. Internet services depend upon massive storage, which is the focus\nof this chapter, and networking, which is the focus of Appendix F.\nThis shift in focus from computation to communication and storage of infor-\nmation emphasizes reliability and scalability as well as cost-performance.\nAlthough it is frustrating when a program crashes, people become hysterical if they\nlose their data; hence, storage systems are typically held to a higher standard of\ndependability than the rest of the computer. Dependability is the bedrock of\nstorage, yet it also has its own rich performance theory\u2014queuing theory\u2014that\nbalances throughput versus response time. The software that determines which\nprocessor features get used is the compiler, but the operating system usurps that\nrole for storage.\nThus, storage has a different, multifaceted culture from processors, yet it is still\nfound within the architecture tent. We start our exploration with advances in mag-\nnetic disks, as they are the dominant storage device today in desktop and server\ncomputers. We assume that readers are already familiar with the basics of storage\ndevices, some of which were covered in Chapter 1.\nD.2\nAdvanced Topics in Disk Storage\nThe disk industry historically has concentrated on improving the capacity of disks.\nImprovement in capacity is customarily expressed as improvement in areal\ndensity, measured in bits per square inch:\nAreal density \u00bc Tracks\nInch on a disk surface Bits\nInchon a track\nThrough about 1988, the rate of improvement of areal density was 29% per\nyear, thus doubling density every 3 years. Between then and about 1996, the\nrate improved to 60% per year, quadrupling density every 3 years and matching\nthe traditional rate of DRAMs. From 1997 to about 2003, the rate increased to\n100%, doubling every year. After the innovations that allowed this renaissances\nhad largely played out, the rate has dropped recently to about 30% per year. In\n2011, the highest density in commercial products is 400 billion bits per square\ninch. Cost per gigabyte has dropped at least as fast as areal density has\nincreased, with smaller diameter drives playing the larger role in this improve-\nment. Costs per gigabyte improved by almost a factor of 1,000,000 between\n1983 and 2011.\nD-2\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 856,
        "text": "Magnetic disks have been challenged many times for supremacy of secondary\nstorage. Figure D.1 shows one reason: the fabled access time gap between disks\nand DRAM. DRAM latency is about 100,000 times less than disk, and that per-\nformance advantage costs 30 to 150 times more per gigabyte for DRAM.\nThe bandwidth gap is more complex. For example, a fast disk in 2011 transfers\nat 200 MB/sec from the disk media with 600 GB of storage and costs about $400.\nA 4 GB DRAM module costing about $200 in 2011 could transfer at 16,000 MB/\nsec (see Chapter 2), giving the DRAM module about 80 times higher bandwidth\nthan the disk. However, the bandwidth per GB is 6000 times higher for DRAM,\nand the bandwidth per dollar is 160 times higher.\nMany have tried to invent a technology cheaper than DRAM but faster than\ndisk to fill that gap, but thus far all have failed. Challengers have never had a prod-\nuct to market at the right time. By the time a new product ships, DRAMs and disks\nhave made advances as predicted earlier, costs have dropped accordingly, and the\nchallenging product is immediately obsolete.\nThe closest challenger is Flash memory. This semiconductor memory is non-\nvolatile like disks, and it has about the same bandwidth as disks, but latency is 100\nto 1000 times faster than disk. In 2011, the price per gigabyte of Flash was 15 to 20\ntimes cheaper than DRAM. Flash is popular in cell phones because it comes in\nmuch smaller capacities and it is more power efficient than disks, despite the cost\nper gigabyte being 15 to 25 times higher than disks. Unlike disks and DRAM,\n0.1\n1\n10\n100\n1000\n10,000\n100,000\n1,000,000\n1\n10\n100\n1000\n10,000\n100,000\n1,000,000\n10,000,000 100,000,000\nCost ($/GB)\nAccess time (ns)\nAccess time gap\n1980\n1980\n1985\n1985\n1990\n1990\n1995\n1995\n2000\n2000\n2005\n2005\nDRAM\nDisk\nFigure D.1 Cost versus access time for DRAM and magnetic disk in 1980, 1985, 1990, 1995, 2000, and 2005. The\ntwo-order-of-magnitude gap in cost and five-order-of-magnitude gap in access times between semiconductor mem-\nory and rotating magnetic disks have inspired a host of competing technologies to try to fill them. So far, such\nattempts have been made obsolete before production by improvements in magnetic disks, DRAMs, or both. Note\nthat between 1990 and 2005 the cost per gigabyte DRAM chips made less improvement, while disk cost made dra-\nmatic improvement.\nD.2\nAdvanced Topics in Disk Storage\n\u25a0\nD-3"
    },
    {
        "page": 857,
        "text": "Flash memory bits wear out\u2014typically limited to 1 million writes\u2014and so they are\nnot popular in desktop and server computers.\nWhile disks will remain viable for the foreseeable future, the conventional\nsector-track-cylinder model did not. The assumptions of the model are that nearby\nblocks are on the same track, blocks in the same cylinder take less time to access\nsince there is no seek time, and some tracks are closer than others.\nFirst, disks started offering higher-level intelligent interfaces, like ATA and\nSCSI, when they included a microprocessor inside a disk. To speed up sequential\ntransfers, these higher-level interfaces organize disks more like tapes than like ran-\ndom access devices. The logical blocks are ordered in serpentine fashion across a\nsingle surface, trying to capture all the sectors that are recorded at the same bit den-\nsity. (Disks vary the recording density since it is hard for the electronics to keep up\nwith the blocks spinning much faster on the outer tracks, and lowering linear den-\nsity simplifies the task.) Hence, sequential blocks may be on different tracks. We\nwill see later in Figure D.22 on page D-45 an illustration of the fallacy of assuming\nthe conventional sector-track model when working with modern disks.\nSecond, shortly after the microprocessors appeared inside disks, the disks\nincluded buffers to hold the data until the computer was ready to accept it, and later\ncaches to avoid read accesses. They were joined by a command queue that allowed\nthe disk to decide in what order to perform the commands to maximize perfor-\nmance while maintaining correct behavior. Figure D.2 shows how a queue depth\nof 50 can double the number of I/Os per second of random I/Os due to better sched-\nuling of accesses. Although it\u2019s unlikely that a system would really have 256 com-\nmands in a queue, it would triple the number of I/Os per second. Given buffers,\ncaches, and out-of-order accesses, an accurate performance model of a real disk\nis much more complicated than sector-track-cylinder.\n0\n300\n200\n100\n400\nI/O per second\n500\n600\n0\n300\n250\n200\n150\nQueue depth\nRandom 512-byte reads per second\n100\n50\nFigure D.2 Throughput versus command queue depth using random 512-\nbyte reads. The disk performs 170 reads per second starting at no command queue\nand doubles performance at 50 and triples at 256 [Anderson 2003].\nD-4\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 858,
        "text": "Finally, the number of platters shrank from 12 in the past to 4 or even 1 today,\nso the cylinder has less importance than before because the percentage of data in a\ncylinder is much less.\nDisk Power\nPower is an increasing concern for disks as well as for processors. A typical ATA\ndisk in 2011 might use 9 watts when idle, 11 watts when reading or writing, and 13\nwatts when seeking. Because it is more efficient to spin smaller mass, smaller-\ndiameter disks can save power. One formula that indicates the importance of rota-\ntion speed and the size of the platters for the power consumed by the disk motor is\nthe following [Gurumurthi et al. 2005]:\nPower \u0003 Diameter4:6 RPM2:8 Number of platters\nThus, smaller platters, slower rotation, and fewer platters all help reduce disk motor\npower, and most of the power is in the motor.\nFigure D.3 shows the specifications of two 3.5-inch disks in 2011. The Serial\nATA (SATA) disks shoot for high capacity and the best cost per gigabyte, so the\n2000 GB drives cost less than $0.05 per gigabyte. They use the widest platters that\nfit the form factor and use four or five of them, but they spin at 5900 RPM and seek\nrelatively slowly to allow a higher areal density and to lower power. The corre-\nsponding Serial Attach SCSI (SAS) drive aims at performance, so it spins at\n15,000 RPM and seeks much faster. It uses a lower areal density to spin at that\nhigh rate. To reduce power, the platter is much narrower than the form factor. This\ncombination reduces capacity of the SAS drive to 600 GB.\nThe cost per gigabyte is about a factor of five better for the SATA drives, and,\nconversely, the cost per I/O per second or MB transferred per second is about a\nfactor of five better for the SAS drives. Despite using smaller platters and many\nfewer of them, the SAS disks use twice the power of the SATA drives, due to\nthe much faster RPM and seeks.\nCapacity (GB)\nPrice\nPlatters\nRPM\nDiameter (inches)\nAverage seek (ms)\nPower (watts)\nI/O/sec\nDisk BW (MB/sec)\nBuffer BW (MB/sec)\nBuffer size (MB)\nMTTF (hrs)\nSATA\n2000\n$85\n4\n5900\n3.7\n16\n12\n47\n45\u201395\n300\n32\n0.6 M\nSAS\n600\n$400\n4\n15,000\n2.6\n3\u20134\n16\n285\n122\u2013204\n750\n16\n1.6 M\nFigure D.3 Serial ATA (SATA) versus Serial Attach SCSI (SAS) drives in 3.5-inch form factor in 2011. The I/Os per\nsecond were calculated using the average seek plus the time for one-half rotation plus the time to transfer one sector\nof 512 KB.\nD.2\nAdvanced Topics in Disk Storage\n\u25a0\nD-5"
    },
    {
        "page": 859,
        "text": "Advanced Topics in Disk Arrays\nAn innovation that improves both dependability and performance of storage\nsystems is disk arrays. One argument for arrays is that potential throughput can\nbe increased by having many disk drives and, hence, many disk arms, rather than\nfewer large drives. Simply spreading data over multiple disks, called striping, auto-\nmatically forces accesses to several disks if the data files are large. (Although\narrays improve throughput, latency is not necessarily improved.) As we saw in\nChapter 1, the drawback is that with more devices, dependability decreases: N\ndevices generally have 1/N the reliability of a single device.\nAlthough a disk array would have more faults than a smaller number of larger\ndisks when each disk has the same reliability, dependability is improved by adding\nredundant disks to the array to tolerate faults. That is, if a single disk fails, the lost\ninformation is reconstructed from redundant information. The only danger is in\nhaving another disk fail during the mean time to repair (MTTR). Since the mean\ntime to failure (MTTF) of disks is tens of years, and the MTTR is measured in\nhours, redundancy can make the measured reliability of many disks much higher\nthan that of a single disk.\nSuch redundant disk arrays have become known by the acronym RAID, which\noriginally stood for redundant array of inexpensive disks, although some prefer\nthe word independent for I in the acronym. The ability to recover from failures plus\nthe higher throughput, measured as either megabytes per second or I/Os per second,\nmakeRAIDattractive.Whencombinedwiththeadvantagesofsmallersizeandlower\npower of small-diameter drives, RAIDs now dominate large-scale storage systems.\nFigure D.4 summarizes the five standard RAID levels, showing how eight\ndisks of user data must be supplemented by redundant or check disks at each RAID\nlevel, and it lists the pros and cons of each level. The standard RAID levels are well\ndocumented, so we will just do a quick review here and discuss advanced levels in\nmore depth.\n\u25a0\nRAID 0\u2014It has no redundancy and is sometimes nicknamed JBOD, for just a\nbunch of disks, although the data may be striped across the disks in the array.\nThis level is generally included to act as a measuring stick for the other RAID\nlevels in terms of cost, performance, and dependability.\n\u25a0\nRAID 1\u2014Also called mirroring or shadowing, there are two copies of every\npiece of data. It is the simplest and oldest disk redundancy scheme, but it also\nhas the highest cost. Some array controllers will optimize read performance by\nallowing the mirrored disks to act independently for reads, but this optimiza-\ntion means it may take longer for the mirrored writes to complete.\n\u25a0\nRAID 2\u2014This organization was inspired by applying memory-style error-\ncorrecting codes (ECCs) to disks. It was included because there was such a disk\narray product at the time of the original RAID paper, but none since then as\nother RAID organizations are more attractive.\n\u25a0\nRAID 3\u2014Since the higher-level disk interfaces understand the health of a disk,\nit\u2019s easy to figure out which disk failed. Designers realized that if one extra disk\nD-6\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 860,
        "text": "contains the parity of the information in the data disks, a single disk allows\nrecovery from a disk failure. The data are organized in stripes, with N data\nblocks and one parity block. When a failure occurs, we just \u201csubtract\u201d the good\ndata from the good blocks, and what remains is the missing data. (This works\nwhether the failed disk is a data disk or the parity disk.) RAID 3 assumes that\nthe data are spread across all disks on reads and writes, which is attractive when\nreading or writing large amounts of data.\n\u25a0\nRAID 4\u2014Many applications are dominated by small accesses. Since sectors\nhave their own error checking, you can safely increase the number of reads\nper second by allowing each disk to perform independent reads. It would seem\nthat writes would still be slow, if you have to read every disk to calculate parity.\nTo increase the number of writes per second, an alternative approach involves\nonly two disks. First, the array reads the old data that are about to be overwrit-\nten, and then calculates what bits would change before it writes the new data. It\nthen reads the old value of the parity on the check disks, updates parity accord-\ning to the list of changes, and then writes the new value of parity to the check\nRAID level\nDisk failures tolerated,\ncheck space overhead\nfor 8 data disks\nPros\nCons\nCompany\nproducts\n0\nNonredundant\nstriped\n0 failures, 0 check disks\nNo space overhead\nNo protection\nWidely\nused\n1\nMirrored\n1 failure, 8 check disks\nNo parity calculation; fast\nrecovery; small writes faster\nthan higher RAIDs; fast reads\nHighest check\nstorage overhead\nEMC, HP\n(Tandem),\nIBM\n2\nMemory-style\nECC\n1 failure, 4 check disks\nDoesn\u2019t rely on failed disk to\nself-diagnose\n\u0004 Log 2 check\nstorage overhead\nNot used\n3\nBit-interleaved\nparity\n1 failure, 1 check disk\nLow check overhead; high\nbandwidth for large reads or\nwrites\nNo support for\nsmall, random\nreads or writes\nStorage\nConcepts\n4\nBlock-\ninterleaved\nparity\n1 failure, 1 check disk\nLow check overhead; more\nbandwidth for small reads\nParity disk is small\nwrite bottleneck\nNetwork\nAppliance\n5\nBlock-\ninterleaved\ndistributed\nparity\n1 failure, 1 check disk\nLow check overhead; more\nbandwidth for small reads and\nwrites\nSmall writes!4\ndisk accesses\nWidely\nused\n6\nRow-diagonal\nparity, EVEN-\nODD\n2 failures, 2 check disks\nProtects against 2 disk failures\nSmall writes!6\ndisk accesses; 2\ncheck overhead\nNetwork\nAppliance\nFigure D.4 RAID levels, their fault tolerance, and their overhead in redundant disks. The paper that introduced the\nterm RAID [Patterson, Gibson, and Katz 1987] used a numerical classification that has become popular. In fact, the non-\nredundant disk array is often called RAID 0, indicating that the data are striped across several disks but without redun-\ndancy. Note that mirroring (RAID 1) in this instance can survive up to eight disk failures provided only one disk of each\nmirrored pair fails; worst case is both disks in a mirrored pair fail. In 2011, there may be no commercial implementations\nof RAID 2; the rest are found in a wide range of products. RAID 0+1, 1+0, 01, 10, and 6 are discussed in the text.\nD.2\nAdvanced Topics in Disk Storage\n\u25a0\nD-7"
    },
    {
        "page": 861,
        "text": "disk. Hence, these so-called \u201csmall writes\u201d are still slower than small reads\u2014\nthey involve four disks accesses\u2014but they are faster than if you had to read all\ndisks on every write. RAID 4 has the same low check disk overhead as RAID 3,\nand it can still do large reads and writes as fast as RAID 3 in addition to small\nreads and writes, but control is more complex.\n\u25a0\nRAID 5\u2014Note that a performance flaw for small writes in RAID 4 is that they\nall must read and write the same check disk, so it is a performance bottleneck.\nRAID 5 simply distributes the parity information across all disks in the array,\nthereby removing the bottleneck. The parity block in each stripe is rotated so\nthat parity is spread evenly across all disks. The disk array controller must now\ncalculate which disk has the parity for when it wants to write a given block, but\nthat can be a simple calculation. RAID 5 has the same low check disk overhead\nas RAID 3 and 4, and it can do the large reads and writes of RAID 3 and the\nsmall reads of RAID 4, but it has higher small write bandwidth than RAID 4.\nNevertheless, RAID 5 requires the most sophisticated controller of the classic\nRAID levels.\nHaving completed our quick review of the classic RAID levels, we can now look at\ntwo levels that have become popular since RAID was introduced.\nRAID 10 versus 01 (or 1+0 versus RAID 0+1)\nOne topic not always described in the RAID literature involves how mirroring in\nRAID 1 interacts with striping. Suppose you had, say, four disks\u2019 worth of data to\nstore and eight physical disks to use. Would you create four pairs of disks\u2014each\norganized as RAID 1\u2014and then stripe data across the four RAID 1 pairs? Alter-\nnatively, would you create two sets of four disks\u2014each organized as RAID 0\u2014and\nthen mirror writes to both RAID 0 sets? The RAID terminology has evolved to call\nthe former RAID 1+0 or RAID 10 (\u201cstriped mirrors\u201d) and the latter RAID 0+1 or\nRAID 01 (\u201cmirrored stripes\u201d).\nRAID 6: Beyond a Single Disk Failure\nThe parity-based schemes of the RAID 1 to 5 protect against a single self-\nidentifying failure; however, if an operator accidentally replaces the wrong disk\nduring a failure, then the disk array will experience two failures, and data will\nbe lost. Another concern is that since disk bandwidth is growing more slowly than\ndisk capacity, the MTTR of a disk in a RAID system is increasing, which in turn\nincreases the chances of a second failure. For example, a 500 GB SATA disk could\ntake about 3 hours to read sequentially assuming no interference. Given that the\ndamaged RAID is likely to continue to serve data, reconstruction could be\nstretched considerably, thereby increasing MTTR. Besides increasing reconstruc-\ntion time, another concern is that reading much more data during reconstruction\nmeans increasing the chance of an uncorrectable media failure, which would result\nin data loss. Other arguments for concern about simultaneous multiple failures are\nD-8\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 862,
        "text": "the increasing number of disks in arrays and the use of ATA disks, which are\nslower and larger than SCSI disks.\nHence, over the years, there has been growing interest in protecting against\nmore than one failure. Network Appliance (NetApp), for example, started by build-\ning RAID 4 file servers. As double failures were becoming a danger to customers,\nthey created a more robust scheme to protect data, called row-diagonal parity or\nRAID-DP [Corbett et al. 2004]. Like the standard RAID schemes, row-diagonal\nparity uses redundant space based on a parity calculation on a per-stripe basis.\nSince it is protecting against a double failure, it adds two check blocks per stripe\nof data. Let\u2019s assume there are p+1 disks total, so p\u00051 disks have data. Figure D.5\nshows the case when p is 5.\nThe row parity disk is just like in RAID 4; it contains the even parity across the\nother four data blocks in its stripe. Each block of the diagonal parity disk contains\nthe even parity of the blocks in the same diagonal. Note that each diagonal does not\ncover one disk; for example, diagonal 0 does not cover disk 1. Hence, we need just\np\u00051 diagonals to protect the p disks, so the disk only has diagonals 0 to 3 in\nFigure D.5.\nLet\u2019s see how row-diagonal parity works by assuming that data disks 1 and 3\nfail in Figure D.5. We can\u2019t perform the standard RAID recovery using the first\nrow using row parity, since it is missing two data blocks from disks 1 and 3. How-\never, we can perform recovery on diagonal 0, since it is only missing the data block\nassociated with disk 3. Thus, row-diagonal parity starts by recovering one of the\nfour blocks on the failed disk in this example using diagonal parity. Since each\ndiagonal misses one disk, and all diagonals miss a different disk, two diagonals\nare only missing one block. They are diagonals 0 and 2 in this example, so we next\nrestore the block from diagonal 2 from failed disk 1. When the data for those blocks\nhave been recovered, then the standard RAID recovery scheme can be used to\n0\n1\n2\n3\n1\n2\n3\n4\n2\n3\n4\n0\n3\n4\n0\n1\n4\n0\n1\n2\n0\n1\n2\n3\nData disk 0\nData disk 1\nData disk 2\nData disk 3\nRow parity\nDiagonal parity\nFigure D.5 Row diagonal parity for p55, which protects four data disks from double\nfailures [Corbett et al. 2004]. This figure shows the diagonal groups for which parity is\ncalculated and stored in the diagonal parity disk. Although this shows all the check data\nin separate disks for row parity and diagonal parity as in RAID 4, there is a rotated version\nof row-diagonal parity that is analogous to RAID 5. Parameter p must be prime and\ngreater than 2; however, you can make p larger than the number of data disks by assum-\ning that the missing disks have all zeros and the scheme still works. This trick makes it\neasy to add disks to an existing system. NetApp picks p to be 257, which allows the sys-\ntem to grow to up to 256 data disks.\nD.2\nAdvanced Topics in Disk Storage\n\u25a0\nD-9"
    },
    {
        "page": 863,
        "text": "recover two more blocks in the standard RAID 4 stripes 0 and 2, which in turn\nallows us to recover more diagonals. This process continues until two failed disks\nare completely restored.\nThe EVEN-ODD scheme developed earlier by researchers at IBM is similar to\nrow diagonal parity, but it has a bit more computation during operation and\nrecovery [Blaum 1995]. Papers that are more recent show how to expand\nEVEN-ODD to protect against three failures [Blaum, Bruck, and Vardy 1996;\nBlaum et al. 2001].\nD.3\nDefinition and Examples of Real Faults and Failures\nAlthough people may be willing to live with a computer that occasionally crashes\nand forces all programs to be restarted, they insist that their information is never\nlost. The prime directive for storage is then to remember information, no matter\nwhat happens.\nChapter 1 covered the basics of dependability, and this section expands that\ninformation to give the standard definitions and examples of failures.\nThe first step is to clarify confusion over terms. The terms fault, error, and fail-\nure are often used interchangeably, but they have different meanings in the depend-\nability literature. For example, is a programming mistake a fault, error, or failure?\nDoes it matter whether we are talking about when it was designed or when the pro-\ngram is run? If the running program doesn\u2019t exercise the mistake, is it still a fault/\nerror/failure? Try another one. Suppose an alpha particle hits a DRAM memory\ncell. Is it a fault/error/failure if it doesn\u2019t change the value? Is it a fault/error/failure\nif the memory doesn\u2019t access the changed bit? Did a fault/error/failure still occur if\nthe memory had error correction and delivered the corrected value to the CPU?\nYou get the drift of the difficulties. Clearly, we need precise definitions to discuss\nsuch events intelligently.\nTo avoid such imprecision, this subsection is based on the terminology used by\nLaprie [1985] and Gray and Siewiorek [1991], endorsed by IFIP Working Group\n10.4 and the IEEE Computer Society Technical Committee on Fault Tolerance.\nWe talk about a system as a single module, but the terminology applies to submo-\ndules recursively. Let\u2019s start with a definition of dependability:\nComputer system dependability is the quality of delivered service such that reli-\nance can justifiably be placed on this service. The service delivered by a system is\nits observed actual behavior as perceived by other system(s) interacting with this\nsystem\u2019s users. Each module also has an ideal specified behavior, where a service\nspecification is an agreed description of the expected behavior. A system failure\noccurs when the actual behavior deviates from the specified behavior. The failure\noccurred because of an error, a defect in that module. The cause of an error is a\nfault.\nWhen a fault occurs, it creates a latent error, which becomes effective when it is\nactivated; when the error actually affects the delivered service, a failure occurs. The\nD-10\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 864,
        "text": "time between the occurrence of an error and the resulting failure is the error\nlatency. Thus, an error is the manifestation in the system of a fault, and a failure\nis the manifestation on the service of an error. [p. 3]\nLet\u2019s go back to our motivating examples above. A programming mistake is a fault.\nThe consequence is an error (or latent error) in the software. Upon activation, the\nerror becomes effective. When this effective error produces erroneous data that\naffect the delivered service, a failure occurs.\nAn alpha particle hitting a DRAM can be considered a fault. If it changes the\nmemory, it creates an error. The error will remain latent until the affected memory\nword is read. If the effective word error affects the delivered service, a failure\noccurs. If ECC corrected the error, a failure would not occur.\nA mistake by a human operator is a fault. The resulting altered data is an error.\nIt is latent until activated, and so on as before.\nTo clarify, the relationship among faults, errors, and failures is as follows:\n\u25a0\nA fault creates one or more latent errors.\n\u25a0\nThe properties of errors are (1) a latent error becomes effective once activated;\n(2) an error may cycle between its latent and effective states; and (3) an effective\nerror often propagates from one component to another, thereby creating new\nerrors. Thus, either an effective error is a formerly latent error in that component\nor it has propagated from another error in that component or from elsewhere.\n\u25a0\nA component failure occurs when the error affects the delivered service.\n\u25a0\nThese properties are recursive and apply to any component in the system.\nGray and Siewiorek classified faults into four categories according to their cause:\n1. Hardware faults\u2014Devices that fail, such as perhaps due to an alpha particle\nhitting a memory cell\n2. Design faults\u2014Faults in software (usually) and hardware design (occasionally)\n3. Operation faults\u2014Mistakes by operations and maintenance personnel\n4. Environmental faults\u2014Fire, flood, earthquake, power failure, and sabotage\nFaults are also classified by their duration into transient, intermittent, and perma-\nnent [Nelson 1990]. Transient faults exist for a limited time and are not recurring.\nIntermittent faults cause a system to oscillate between faulty and fault-free oper-\nation. Permanent faults do not correct themselves with the passing of time.\nNow that we have defined the difference between faults, errors, and failures,\nwe are ready to see some real-world examples. Publications of real error rates\nare rare for two reasons. First, academics rarely have access to significant hardware\nresources to measure. Second, industrial researchers are rarely allowed to publish\nfailure information for fear that it would be used against their companies in the\nmarketplace. A few exceptions follow.\nD.3\nDefinition and Examples of Real Faults and Failures\n\u25a0\nD-11"
    },
    {
        "page": 865,
        "text": "Berkeley\u2019s Tertiary Disk\nThe Tertiary Disk project at the University of California created an art image server\nfor the Fine Arts Museums of San Francisco in 2000. This database consisted of\nhigh-quality images of over 70,000 artworks [Talagala et al., 2000]. The database\nwas stored on a cluster, which consisted of 20 PCs connected by a switched\nEthernet and containing 368 disks. It occupied seven 7-foot-high racks.\nFigure D.6 shows the failure rates of the various components of Tertiary Disk.\nIn advance of building the system, the designers assumed that SCSI data disks\nwould be the least reliable part of the system, as they are both mechanical and plen-\ntiful. Next would be the IDE disks since there were fewer of them, then the power\nsupplies, followed by integrated circuits. They assumed that passive devices such\nas cables would scarcely ever fail.\nFigure D.6 shatters some of those assumptions. Since the designers followed\nthe manufacturer\u2019s advice of making sure the disk enclosures had reduced vibra-\ntion and good cooling, the data disks were very reliable. In contrast, the PC chassis\ncontaining the IDE/ATA disks did not afford the same environmental controls.\n(The IDE/ATA disks did not store data but helped the application and operating\nComponent\nTotal in system\nTotal failed\nPercentage failed\nSCSI controller\n44\n1\n2.3%\nSCSI cable\n39\n1\n2.6%\nSCSI disk\n368\n7\n1.9%\nIDE/ATA disk\n24\n6\n25.0%\nDisk enclosure\u2014backplane\n46\n13\n28.3%\nDisk enclosure\u2014power supply\n92\n3\n3.3%\nEthernet controller\n20\n1\n5.0%\nEthernet switch\n2\n1\n50.0%\nEthernet cable\n42\n1\n2.3%\nCPU/motherboard\n20\n0\n0%\nFigure D.6 Failures of components in Tertiary Disk over 18 months of operation. For\neach type of component, the table shows the total number in the system, the number\nthat failed, and the percentage failure rate. Disk enclosures have two entries in the table\nbecause they had two types of problems: backplane integrity failures and power supply\nfailures. Since each enclosure had two power supplies, a power supply failure did not\naffect availability. This cluster of 20 PCs, contained in seven 7-foot-high, 19-inch-wide\nracks, hosted 368 8.4 GB, 7200 RPM, 3.5-inch IBM disks. The PCs were P6-200 MHz with\n96 MB of DRAM each. They ran FreeBSD 3.0, and the hosts were connected via switched\n100 Mbit/sec Ethernet. All SCSI disks were connected to two PCs via double-ended SCSI\nchains to support RAID 1. The primary application was called the Zoom Project, which in\n1998 was the world\u2019s largest art image database, with 72,000 images. See Talagala et al.\n[2000b].\nD-12\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 866,
        "text": "system to boot the PCs.) Figure D.6 shows that the SCSI backplane, cables, and\nEthernet cables were no more reliable than the data disks themselves!\nAs Tertiary Disk was a large system with many redundant components, it could\nsurvive this wide range of failures. Components were connected and mirrored\nimages were placed so that no single failure could make any image unavailable.\nThis strategy, which initially appeared to be overkill, proved to be vital.\nThis experience also demonstrated the difference between transient faults and\nhard faults. Virtually all the failures in Figure D.6 appeared first as transient faults.\nIt was up to the operator to decide if the behavior was so poor that they needed to be\nreplaced or if they could continue. In fact, the word \u201cfailure\u201d was not used; instead,\nthe group borrowed terms normally used for dealing with problem employees, with\nthe operator deciding whether a problem component should or should not be\n\u201cfired.\u201d\nTandem\nThe next example comes from industry. Gray [1990] collected data on faults for\nTandem Computers, which was one of the pioneering companies in fault-tolerant\ncomputing and used primarily for databases. Figure D.7 graphs the faults that\ncaused system failures between 1985 and 1989 in absolute faults per system\nand in percentage of faults encountered. The data show a clear improvement in\nthe reliability of hardware and maintenance. Disks in 1985 required yearly service\nby Tandem, but they were replaced by disks that required no scheduled mainte-\nnance. Shrinking numbers of chips and connectors per system plus software\u2019s abil-\nity to tolerate hardware faults reduced hardware\u2019s contribution to only 7% of\nfailures by 1989. Moreover, when hardware was at fault, software embedded in\nthe hardware device (firmware) was often the culprit. The data indicate that soft-\nware in 1989 was the major source of reported outages (62%), followed by system\noperations (15%).\nThe problem with any such statistics is that the data only refer to what is\nreported; for example, environmental failures due to power outages were not\nreported to Tandem because they were seen as a local problem. Data on operation\nfaults are very difficult to collect because operators must report personal mistakes,\nwhich may affect the opinion of their managers, which in turn can affect job secu-\nrity and pay raises. Gray suggested that both environmental faults and operator\nfaults are underreported. His study concluded that achieving higher availability\nrequires improvement in software quality and software fault tolerance, simpler\noperations, and tolerance of operational faults.\nOther Studies of the Role of Operators in Dependability\nWhile Tertiary Disk and Tandem are storage-oriented dependability studies, we\nneed to look outside storage to find better measurements on the role of humans\nin failures. Murphy and Gent [1995] tried to improve the accuracy of data on\nD.3\nDefinition and Examples of Real Faults and Failures\n\u25a0\nD-13"
    },
    {
        "page": 867,
        "text": "operator faults by having the system automatically prompt the operator on each\nboot for the reason for that reboot. They classified consecutive crashes to the same\nfault as operator fault and included operator actions that directly resulted in\ncrashes, such as giving parameters bad values, bad configurations, and bad appli-\ncation installation. Although they believed that operator error is under-reported,\nUnknown\nEnvironment (power, network)\nOperations (by customer)\nMaintenance (by Tandem)\nHardware\nSoftware (applications + OS)\n20\n40\n60\nFaults per 1000 systems\nPercentage faults per category\n80\n100\n120\n100%\n80%\n4%\n6%\n9%\n19%\n29%\n34%\n5%\n6%\n15%\n5%\n7%\n62%\n5%\n9%\n12%\n13%\n22%\n39%\n60%\n40%\n20%\n0%\n0\n9\n8\n9\n1\n7\n8\n9\n1\n5\n8\n9\n1\n9\n8\n9\n1\n7\n8\n9\n1\n5\n8\n9\n1\nFigure D.7 Faults in Tandem between 1985 and 1989. Gray [1990] collected these\ndata for fault-tolerant Tandem Computers based on reports of component failures\nby customers.\nD-14\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 868,
        "text": "they did get more accurate information than did Gray, who relied on a form that the\noperator filled out and then sent up the management chain. The hardware/operating\nsystem went from causing 70% of the failures in VAX systems in 1985 to 28% in\n1993, and failures due to operators rose from 15% to 52% in that same period. Mur-\nphy and Gent expected managing systems to be the primary dependability chal-\nlenge in the future.\nThe final set of data comes from the government. The Federal Communications\nCommission (FCC) requires that all telephone companies submit explanations\nwhen they experience an outage that affects at least 30,000 people or lasts\n30 minutes. These detailed disruption reports do not suffer from the self-reporting\nproblem of earlier figures, as investigators determine the cause of the outage rather\nthan operators of the equipment. Kuhn [1997] studied the causes of outages\nbetween 1992 and 1994, and Enriquez [2001] did a follow-up study for the first\nhalf of 2001. Although there was a significant improvement in failures due to over-\nloading of the network over the years, failures due to humans increased, from about\none-third to two-thirds of the customer-outage minutes.\nThese four examples and others suggest that the primary cause of failures in\nlarge systems today is faults by human operators. Hardware faults have declined\ndue to a decreasing number of chips in systems and fewer connectors. Hardware\ndependability has improved through fault tolerance techniques such as memory\nECC and RAID. At least some operating systems are considering reliability impli-\ncations before adding new features, so in 2011 the failures largely occurred\nelsewhere.\nAlthough failures may be initiated due to faults by operators, it is a poor reflec-\ntion on the state of the art of systems that the processes of maintenance and upgrad-\ning are so error prone. Most storage vendors claim today that customers spend\nmuch more on managing storage over its lifetime than they do on purchasing\nthe storage. Thus, the challenge for dependable storage systems of the future is\neither to tolerate faults by operators or to avoid faults by simplifying the tasks\nof system administration. Note that RAID 6 allows the storage system to survive\neven if the operator mistakenly replaces a good disk.\nWe have now covered the bedrock issue of dependability, giving definitions,\ncase studies, and techniques to improve it. The next step in the storage tour is\nperformance.\nD.4\nI/O Performance, Reliability Measures, and Benchmarks\nI/O performance has measures that have no counterparts in design. One of these is\ndiversity: Which I/O devices can connect to the computer system? Another is\ncapacity: How many I/O devices can connect to a computer system?\nIn addition to these unique measures, the traditional measures of performance\n(namely, response time and throughput) also apply to I/O. (I/O throughput is some-\ntimes called I/O bandwidth and response time is sometimes called latency.) The\nnext two figures offer insight into how response time and throughput trade off\nD.4\nI/O Performance, Reliability Measures, and Benchmarks\n\u25a0\nD-15"
    },
    {
        "page": 869,
        "text": "against each other. Figure D.8 shows the simple producer-server model. The pro-\nducer creates tasks to be performed and places them in a buffer; the server takes\ntasks from the first in, first out buffer and performs them.\nResponse time is defined as the time a task takes from the moment it is placed in\nthe buffer until the server finishes the task. Throughput is simply the average num-\nber of tasks completed by the server over a time period. To get the highest possible\nthroughput, the server should never be idle, thus the buffer should never be empty.\nResponse time, on the other hand, counts time spent in the buffer, so an empty\nbuffer shrinks it.\nAnother measure of I/O performance is the interference of I/O with processor\nexecution. Transferring data may interfere with the execution of another process.\nThere is also overhead due to handling I/O interrupts. Our concern here is how\nmuch longer a process will take because of I/O for another process.\nThroughput versus Response Time\nFigure D.9 shows throughput versus response time (or latency) for a typical I/O\nsystem. The knee of the curve is the area where a little more throughput results\nin much longer response time or, conversely, a little shorter response time results\nin much lower throughput.\nHow does the architect balance these conflicting demands? If the computer is\ninteracting with human beings, Figure D.10 suggests an answer. An interaction, or\ntransaction, with a computer is divided into three parts:\n1. Entry time\u2014The time for the user to enter the command.\n2. System response time\u2014The time between when the user enters the command\nand the complete response is displayed.\n3. Think time\u2014The time from the reception of the response until the user begins to\nenter the next command.\nThe sum of these three parts is called the transaction time. Several studies report\nthat user productivity is inversely proportional to transaction time. The results in\nFigure D.10 show that cutting system response time by 0.7 seconds saves 4.9\nseconds (34%) from the conventional transaction and 2.0 seconds (70%) from\nr\ne\nv\nr\ne\nS\nr\ne\nc\nu\nd\no\nr\nP\nQueue\nFigure D.8 The traditional producer-server model of response time and throughput.\nResponse time begins when a task is placed in the buffer and ends when it is completed\nby the server. Throughput is the number of tasks completed by the server in unit time.\nD-16\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 870,
        "text": "300\n0%\nPercentage of maximum throughput (bandwidth)\nResponse time (latency) (ms)\n20%\n40%\n60%\n80%\n100%\n200\n100\n0\nFigure D.9 Throughput versus response time. Latency is normally reported as\nresponse time. Note that the minimum response time achieves only 11% of the\nthroughput, while the response time for 100% throughput takes seven times the min-\nimum response time. Note also that the independent variable in this curve is implicit; to\ntrace the curve, you typically vary load (concurrency). Chen et al. [1990] collected these\ndata for an array of magnetic disks.\n0\nTime (sec)\nHigh-function graphics workload\n(0.3 sec system response time)\n5\n10 \n15\nHigh-function graphics workload\n(1.0 sec system response time)\nConventional interactive workload\n(0.3 sec system response time)\nConventional interactive workload\n(1.0 sec system response time)\nWorkload\n\u201370% total\n(\u201381% think)\n\u201334% total\n(\u201370% think)\nEntry time\nSystem response time\nThink time\nFigure D.10 A user transaction with an interactive computer divided into entry time,\nsystem response time, and user think time for a conventional system and graphics\nsystem. The entry times are the same, independent of system response time. The entry\ntime was 4 seconds for the conventional system and 0.25 seconds for the graphics sys-\ntem. Reduction in response time actually decreases transaction time by more than just\nthe response time reduction. (From Brady [1986].)\nD.4\nI/O Performance, Reliability Measures, and Benchmarks\n\u25a0\nD-17"
    },
    {
        "page": 871,
        "text": "the graphics transaction. This implausible result is explained by human nature:\nPeople need less time to think when given a faster response. Although this study\nis 20 years old, response times are often still much slower than 1 second, even if\nprocessors are 1000 times faster. Examples of long delays include starting an appli-\ncation on a desktop PC due to many disk I/Os, or network delays when clicking on\nWeb links.\nTo reflect the importance of response time to user productivity, I/O bench-\nmarks also address the response time versus throughput trade-off. Figure D.11\nshows the response time bounds for three I/O benchmarks. They report maximum\nthroughput given either that 90% of response times must be less than a limit or that\nthe average response time must be less than a limit.\nLet\u2019s next look at these benchmarks in more detail.\nTransaction-Processing Benchmarks\nTransaction processing (TP, or OLTP for online transaction processing) is chiefly\nconcerned with I/O rate (the number of disk accesses per second), as opposed to\ndata rate (measured as bytes of data per second). TP generally involves changes to\na large body of shared information from many terminals, with the TP system\nguaranteeing proper behavior on a failure. Suppose, for example, that a bank\u2019s\ncomputer fails when a customer tries to withdraw money from an ATM. The\nTP system would guarantee that the account is debited if the customer received\nthe money and that the account is unchanged if the money was not received. Air-\nline reservations systems as well as banks are traditional customers for TP.\nAs mentioned in Chapter 1, two dozen members of the TP community con-\nspired to form a benchmark for the industry and, to avoid the wrath of their legal\ndepartments, published the report anonymously [Anon. et al. 1985]. This report led\nto the Transaction Processing Council, which in turn has led to eight benchmarks\nsince its founding. Figure D.12 summarizes these benchmarks.\nLet\u2019s describe TPC-C to give a flavor of these benchmarks. TPC-C uses a data-\nbase to simulate an order-entry environment of a wholesale supplier, including\nI/O benchmark\nResponse time restriction\nThroughput\nmetric\nTPC-C: Complex\nQuery OLTP\n\u000690% of transaction must meet response time\nlimit; 5 seconds for most types of transactions\nNew order\ntransactions per\nminute\nTPC-W:\nTransactional Web\nbenchmark\n\u000690% of Web interactions must meet\nresponse time limit; 3 seconds for most types\nof Web interactions\nWeb interactions\nper second\nSPECsfs97\nAverage response time \u000740 ms\nNFS operations\nper second\nFigure D.11 Response time restrictions for three I/O benchmarks.\nD-18\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 872,
        "text": "entering and delivering orders, recording payments, checking the status of orders,\nand monitoring the level of stock at the warehouses. It runs five concurrent trans-\nactions of varying complexity, and the database includes nine tables with a scalable\nrange of records and customers. TPC-C is measured in transactions per minute\n(tpmC) and in price of system, including hardware, software, and three years of\nmaintenance support. Figure 1.17 on page 42 in Chapter 1 describes the top sys-\ntems in performance and cost-performance for TPC-C.\nThese TPC benchmarks were the first\u2014and in some cases still the only ones\u2014\nthat have these unusual characteristics:\n\u25a0\nPrice is included with the benchmark results. The cost of hardware, software,\nand maintenance agreements is included in a submission, which enables eval-\nuations based on price-performance as well as high performance.\n\u25a0\nThe dataset generally must scale in size as the throughput increases. The\nbenchmarks are trying to model real systems, in which the demand on the sys-\ntem and the size of the data stored in it increase together. It makes no sense, for\nexample, to have thousands of people per minute access hundreds of bank\naccounts.\n\u25a0\nThe benchmark results are audited. Before results can be submitted, they must\nbe approved by a certified TPC auditor, who enforces the TPC rules that try to\nmake sure that only fair results are submitted. Results can be challenged and\ndisputes resolved by going before the TPC.\n\u25a0\nThroughput is the performance metric, but response times are limited. For\nexample, with TPC-C, 90% of the new order transaction response times must\nbe less than 5 seconds.\nBenchmark\nData size (GB)\nPerformance metric\nDate of first\nresults\nA: debit credit (retired)\n0.1\u201310\nTransactions per second\nJuly 1990\nB: batch debit credit (retired)\n0.1\u201310\nTransactions per second\nJuly 1991\nC: complex query OLTP\n100\u20133000 (minimum\n0.07*TPM)\nNew order transactions per\nminute (TPM)\nSeptember\n1992\nD: decision support (retired)\n100, 300, 1000\nQueries per hour\nDecember\n1995\nH: ad hoc decision support\n100, 300, 1000\nQueries per hour\nOctober 1999\nR: business reporting decision support\n(retired)\n1000\nQueries per hour\nAugust 1999\nW: transactional Web benchmark\n\u000350, 500\nWeb interactions per second\nJuly 2000\nApp: application server and Web\nservices benchmark\n\u00032500\nWeb service interactions per\nsecond (SIPS)\nJune 2005\nFigure D.12 Transaction Processing Council benchmarks. The summary results include both the performance met-\nric and the price-performance of that metric. TPC-A, TPC-B, TPC-D, and TPC-R were retired.\nD.4\nI/O Performance, Reliability Measures, and Benchmarks\n\u25a0\nD-19"
    },
    {
        "page": 873,
        "text": "\u25a0\nAn independent organization maintains the benchmarks. Dues collected by\nTPC pay for an administrative structure including a chief operating office. This\norganization settles disputes, conducts mail ballots on approval of changes to\nbenchmarks, holds board meetings, and so on.\nSPEC System-Level File Server, Mail, and Web Benchmarks\nThe SPEC benchmarking effort is best known for its characterization of processor\nperformance, but it has created benchmarks for file servers, mail servers, and Web\nservers.\nSeven companies agreed on a synthetic benchmark, called SFS, to evaluate\nsystems running the Sun Microsystems network file service (NFS). This bench-\nmark was upgraded to SFS 3.0 (also called SPEC SFS97_R1) to include support\nfor NFS version 3, using TCP in addition to UDP as the transport protocol, and\nmaking the mix of operations more realistic. Measurements on NFS systems led\nto a synthetic mix of reads, writes, and file operations. SFS supplies default param-\neters for comparative performance. For example, half of all writes are done in 8 KB\nblocks and half are done in partial blocks of 1, 2, or 4 KB. For reads, the mix is 85%\nfull blocks and 15% partial blocks.\nLike TPC-C, SFS scales the amount of data stored according to the reported\nthroughput: For every 100 NFS operations per second, the capacity must increase\nby 1 GB. It also limits the average response time, in this case to 40 ms. Figure D.13\n0\n1\n2\n3\n5\n4\n6\nResponse time (ms)\n7\n8\n0\n150,000\n125,000\n34,089\n2 Xeons\nFAS3000\nFAS6000\n4 Xeons\n8 Opterons\n4 Opterons\n47,927\n100,295\n136,048\n100,000\n75,000\nOperations/second\n50,000\n25,000\nFigure D.13 SPEC SFS97_R1 performance for the NetApp FAS3050c NFS servers in\ntwo configurations. Two processors reached 34,089 operations per second and four\nprocessors did 47,927. Reported in May 2005, these systems used the Data ONTAP\n7.0.1R1 operating system, 2.8 GHz Pentium Xeon microprocessors, 2 GB of DRAM per\nprocessor, 1 GB of nonvolatile memory per system, and 168 15 K RPM, 72 GB, Fibre\nChannel disks. These disks were connected using two or four QLogic ISP-2322 FC disk\ncontrollers.\nD-20\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 874,
        "text": "shows average response time versus throughput for two NetApp systems. Unfor-\ntunately, unlike the TPC benchmarks, SFS does not normalize for different price\nconfigurations.\nSPECMail is a benchmark to help evaluate performance of mail servers at an\nInternet service provider. SPECMail2001 is based on the standard Internet proto-\ncols SMTP and POP3, and it measures throughput and user response time while\nscaling the number of users from 10,000 to 1,000,000.\nSPECWeb is a benchmark for evaluating the performance of World Wide Web\nservers, measuring number of simultaneous user sessions. The SPECWeb2005\nworkload simulates accesses to a Web service provider, where the server supports\nhome pages for several organizations. It has three workloads: Banking (HTTPS),\nE-commerce (HTTP and HTTPS), and Support (HTTP).\nExamples of Benchmarks of Dependability\nThe TPC-C benchmark does in fact have a dependability requirement. The bench-\nmarked system must be able to handle a single disk failure, which means in\npractice that all submitters are running some RAID organization in their storage\nsystem.\nEfforts that are more recent have focused on the effectiveness of fault tolerance\nin systems. Brown and Patterson [2000] proposed that availability be measured by\nexamining the variations in system quality-of-service metrics over time as faults\nare injected into the system. For a Web server, the obvious metrics are performance\n(measured as requests satisfied per second) and degree of fault tolerance (measured\nas the number of faults that can be tolerated by the storage subsystem, network\nconnection topology, and so forth).\nThe initial experiment injected a single fault\u2014such as a write error in disk sec-\ntor\u2014and recorded the system\u2019s behavior as reflected in the quality-of-service met-\nrics. The example compared software RAID implementations provided by Linux,\nSolaris, and Windows 2000 Server. SPECWeb99 was used to provide a workload\nand to measure performance. To inject faults, one of the SCSI disks in the software\nRAID volume was replaced with an emulated disk. It was a PC running software\nusing a SCSI controller that appears to other devices on the SCSI bus as a disk. The\ndisk emulator allowed the injection of faults. The faults injected included a variety\nof transient disk faults, such as correctable read errors, and permanent faults, such\nas disk media failures on writes.\nFigure D.14 shows the behavior of each system under different faults. The two\ntop graphs show Linux (on the left) and Solaris (on the right). As RAID systems\ncan lose data if a second disk fails before reconstruction completes, the longer the\nreconstruction (MTTR), the lower the availability. Faster reconstruction implies\ndecreased application performance, however, as reconstruction steals I/O\nresources from running applications. Thus, there is a policy choice between taking\na performance hit during reconstruction or lengthening the window of vulnerability\nand thus lowering the predicted MTTF.\nD.4\nI/O Performance, Reliability Measures, and Benchmarks\n\u25a0\nD-21"
    },
    {
        "page": 875,
        "text": "Although none of the tested systems documented their reconstruction policies\noutside of the source code, even a single fault injection was able to give insight into\nthose policies. The experiments revealed that both Linux and Solaris initiate auto-\nmatic reconstruction of the RAID volume onto a hot spare when an active disk is\ntaken out of service due to a failure. Although Windows supports RAID\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n0\n10\n20\n30\n40\nReconstruction\n50\n60\n70\n80\n90\n100 110\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\nTime (minutes)\nReconstruction\nHits per second\nHits per second\nHits per second\ns\nir\na\nl\no\nS\nx\nu\nn\ni\nL\nWindows\nTime (minutes)\nTime (minutes)\nReconstruction\n200\n190\n180\n170\n160\n150\n220\n225\n215\n210\n205\n200\n195\n190\n80\n90\n100\n110\n120\n130\n140\n150\n160\nFigure D.14 Availability benchmark for software RAID systems on the same computer running Red Hat 6.0 Linux,\nSolaris 7, and Windows 2000 operating systems. Note the difference in philosophy on speed of reconstruction of\nLinux versus Windows and Solaris. The y-axis is behavior in hits per second running SPECWeb99. The arrow indicates\ntime of fault insertion. The lines at the top give the 99% confidence interval of performance before the fault is\ninserted. A 99% confidence interval means that if the variable is outside of this range, the probability is only 1% that\nthis value would appear.\nD-22\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 876,
        "text": "reconstruction, the reconstruction must be initiated manually. Thus, without\nhuman intervention, a Windows system that did not rebuild after a first failure\nremains susceptible to a second failure, which increases the window of vulnerabil-\nity. It does repair quickly once told to do so.\nThe fault injection experiments also provided insight into other availability\npolicies of Linux, Solaris, and Windows 2000 concerning automatic spare utiliza-\ntion, reconstruction rates, transient errors, and so on. Again, no system documented\ntheir policies.\nIn terms of managing transient faults, the fault injection experiments revealed\nthat Linux\u2019s software RAID implementation takes an opposite approach than do\nthe RAID implementations in Solaris and Windows. The Linux implementation\nis paranoid\u2014it would rather shut down a disk in a controlled manner at the first\nerror, rather than wait to see if the error is transient. In contrast, Solaris and Win-\ndows are more forgiving\u2014they ignore most transient faults with the expectation\nthat they will not recur. Thus, these systems are substantially more robust to tran-\nsients than the Linux system. Note that both Windows and Solaris do log the tran-\nsient faults, ensuring that the errors are reported even if not acted upon. When\nfaults were permanent, the systems behaved similarly.\nD.5\nA Little Queuing Theory\nIn processor design, we have simple back-of-the-envelope calculations of perfor-\nmance associated with the CPI formula in Chapter 1, or we can use full-scale sim-\nulation for greater accuracy at greater cost. In I/O systems, we also have a bestcase\nanalysis as a back-of-the-envelope calculation. Full-scale simulation is also much\nmore accurate and much more work to calculate expected performance.\nWith I/O systems, however, we also have a mathematical tool to guide I/O\ndesign that is a little more work and much more accurate than best-case analysis,\nbut much less work than full-scale simulation. Because of the probabilistic nature\nof I/O events and because of sharing of I/O resources, we can give a set of simple\ntheorems that will help calculate response time and throughput of an entire I/O sys-\ntem. This helpful field is called queuing theory. Since there are many books and\ncourses on the subject, this section serves only as a first introduction to the topic.\nHowever, even this small amount can lead to better design of I/O systems.\nLet\u2019s start with a black-box approach to I/O systems, as shown in Figure D.15.\nIn our example, the processor is making I/O requests that arrive at the I/O device,\nand the requests \u201cdepart\u201d when the I/O device fulfills them.\nWe are usually interested in the long term, or steady state, of a system rather\nthan in the initial start-up conditions. Suppose we weren\u2019t. Although there is a\nmathematics that helps (Markov chains), except for a few cases, the only way\nto solve the resulting equations is simulation. Since the purpose of this section\nis to show something a little harder than back-of-the-envelope calculations but less\nthan simulation, we won\u2019t cover such analyses here. (See the references in Appen-\ndix M for more details.)\nD.5\nA Little Queuing Theory\n\u25a0\nD-23"
    },
    {
        "page": 877,
        "text": "Hence, in this section we make the simplifying assumption that we are evalu-\nating systems with multiple independent requests for I/O service that are in equi-\nlibrium: The input rate must be equal to the output rate. We also assume there is a\nsteady supply of tasks independent for how long they wait for service. In many real\nsystems, such as TPC-C, the task consumption rate is determined by other system\ncharacteristics, such as memory capacity.\nThis leads us to Little\u2019s law, which relates the average number of tasks in the\nsystem, the average arrival rate of new tasks, and the average time to perform a\ntask:\nMean number of tasks in system \u00bc Arrival rateMean response time\nLittle\u2019s law applies to any system in equilibrium, as long as nothing inside the\nblack box is creating new tasks or destroying them. Note that the arrival rate\nand the response time must use the same time unit; inconsistency in time units\nis a common cause of errors.\nLet\u2019s try to derive Little\u2019s law. Assume we observe a system for Timeobserve\nminutes. During that observation, we record how long it took each task to\nbe serviced, and then sum those times. The number of tasks completed during\nTimeobserve is Numbertask, and the sum of the times each task spends in the system\nis Timeaccumulated. Note that the tasks can overlap in time, so Timeaccumulated\u0006\nTimeobserved. Then,\nMean number of tasks in system \u00bc Timeaccumulated\nTimeobserve\nMean response time \u00bc Timeaccumulated\nNumbertasks\nArrival rate \u00bc Numbertasks\nTimeobserve\nAlgebra lets us split the first formula:\nTimeaccumulated\nTimeobserve\n\u00bc Timeaccumulated\nNumbertasks\n\u221eNumbertasks\nTimeobserve\nArrivals\nDepartures\nFigure D.15 Treating the I/O system as a black box. This leads to a simple but impor-\ntant observation: If the system is in steady state, then the number of tasks entering the\nsystem must equal the number of tasks leaving the system. This flow-balanced state is\nnecessary but not sufficient for steady state. If the system has been observed or mea-\nsured for a sufficiently long time and mean waiting times stabilize, then we say that the\nsystem has reached steady state.\nD-24\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 878,
        "text": "If we substitute the three definitions above into this formula, and swap the resulting\ntwo terms on the right-hand side, we get Little\u2019s law:\nMean number of tasks in system \u00bc Arrival rateMean response time\nThis simple equation is surprisingly powerful, as we shall see.\nIf we open the black box, we see Figure D.16. The area where the tasks accu-\nmulate, waiting to be serviced, is called the queue, or waiting line. The device per-\nforming the requested service is called the server. Until we get to the last two pages\nof this section, we assume a single server.\nLittle\u2019s law and a series of definitions lead to several useful equations:\n\u25a0\nTimeserver\u2014Average time to service a task; average service rate is 1/Timeserver,\ntraditionally represented by the symbol \u03bc in many queuing texts.\n\u25a0\nTimequeue\u2014Average time per task in the queue.\n\u25a0\nTimesystem\u2014Average time/task in the system, or the response time, which is\nthe sum of Timequeue and Timeserver.\n\u25a0\nArrival\nrate\u2014Average\nnumber\nof\narriving\ntasks/second,\ntraditionally\nrepresented by the symbol \u03bb in many queuing texts.\n\u25a0\nLengthserver\u2014Average number of tasks in service.\n\u25a0\nLengthqueue\u2014Average length of queue.\n\u25a0\nLengthsystem\u2014Average number of tasks in system, which is the sum of\nLengthqueue and Lengthserver.\nOne common misunderstanding can be made clearer by these definitions: whether\nthe question is how long a task must wait in the queue before service starts (Time-\nqueue) or how long a task takes until it is completed (Timesystem). The latter term is\nwhat we mean by response time, and the relationship between the terms is\nTimesystem\u00bcTimequeue+Timeserver.\nThe mean number of tasks in service (Lengthserver) is simply Arrival rate\nTimeserver, which is Little\u2019s law. Server utilization is simply the mean number\nof tasks being serviced divided by the service rate. For a single server, the service\nrate is 1/Timeserver. Hence, server utilization (and, in this case, the mean number of\ntasks per server) is simply:\nServer utilization \u00bc Arrival rateTimeserver\nArrivals\nQueue\nServer\nI/O controller\nand device\nFigure D.16 The single-server model for this section. In this situation, an I/O request\n\u201cdeparts\u201d by being completed by the server.\nD.5\nA Little Queuing Theory\n\u25a0\nD-25"
    },
    {
        "page": 879,
        "text": "Service utilization must be between 0 and 1; otherwise, there would be more tasks\narriving than could be serviced, violating our assumption that the system is in equi-\nlibrium. Note that this formula is just a restatement of Little\u2019s law. Utilization is also\ncalledtrafficintensityandisrepresentedbythesymbol\u03c1inmanyqueuingtheorytexts.\nExample\nSuppose an I/O system with a single disk gets on average 50 I/O requests per\nsecond. Assume the average time for a disk to service an I/O request is 10 ms. What\nis the utilization of the I/O system?\nAnswer\nUsing the equation above, with 10 ms represented as 0.01 seconds, we get: 50\nServer utilization \u00bc Arrival rateTimeserver \u00bc 50\nsec 0:01sec \u00bc 0:50\nTherefore, the I/O system utilization is 0.5.\nHow the queue delivers tasks to the server is called the queue discipline. The sim-\nplest and most common discipline is first in, first out (FIFO). If we assume FIFO,\nwe can relate time waiting in the queue to the mean number of tasks in the queue:\nTimequeue \u00bc Lengthqueue Timeserver + Mean time to complete service of task when\nnew task arrives if server is busy\nThat is, the time in the queue is the number of tasks in the queue times the mean\nservice time plus the time it takes the server to complete whatever task is being\nserviced when a new task arrives. (There is one more restriction about the arrival\nof tasks, which we reveal on page D-28.)\nThe last component of the equation is not as simple as it first appears. A new\ntask can arrive at any instant, so we have no basis to know how long the existing\ntask has been in the server. Although such requests are random events, if we know\nsomething about the distribution of events, we can predict performance.\nPoisson Distribution of Random Variables\nTo estimate the last component of the formula we need to know a little about dis-\ntributions of random variables. A variable is random if it takes one of a specified\nset of values with a specified probability; that is, you cannot know exactly what its\nnext value will be, but you may know the probability of all possible values.\nRequests for service from an I/O system can be modeled by a random variable\nbecause the operating system is normally switching between several processes that\ngenerate independent I/O requests. We also model I/O service times by a random\nvariable given the probabilistic nature of disks in terms of seek and rotational delays.\nOne way to characterize the distribution of values of a random variable with\ndiscrete values is a histogram, which divides the range between the minimum\nand maximum values into subranges called buckets. Histograms then plot the\nnumber in each bucket as columns.\nD-26\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 880,
        "text": "Histograms work well for distributions that are discrete values\u2014for example,\nthe number of I/O requests. For distributions that are not discrete values, such as\ntime waiting for an I/O request, we have two choices. Either we need a curve to plot\nthe values over the full range, so that we can estimate accurately the value, or we\nneed a very fine time unit so that we get a very large number of buckets to estimate\ntime accurately. For example, a histogram can be built of disk service times mea-\nsured in intervals of 10 \u03bcs although disk service times are truly continuous.\nHence, to be able to solve the last part of the previous equation we need to char-\nacterize the distribution of this random variable. The mean time and some measure\nof the variance are sufficient for that characterization.\nFor the first term, we use the weighted arithmetic mean time. Let\u2019s first assume\nthat after measuring the number of occurrences, say, ni, of tasks, you could\ncompute frequency of occurrence of task i:\nfi \u00bc\nni\nX\nn\ni\u00bc1\nni\n \n!\nThen weighted arithmetic mean is\nWeighted arithmetic mean time \u00bc f1 T1 + f2 T2 + \u2026 + fn Tn\nwhere Ti is the time for task i and fi is the frequency of occurrence of task i.\nTo characterize variability about the mean, many people use the standard devi-\nation. Let\u2019s use the variance instead, which is simply the square of the standard\ndeviation, as it will help us with characterizing the probability distribution. Given\nthe weighted arithmetic mean, the variance can be calculated as\nVariance \u00bc f1 T2\n1 + f2 T2\n2 + \u2026 + fn T2\nn\n\n\u0003\n\u0005Weighted arithmetic mean time2\nIt is important to remember the units when computing variance. Let\u2019s assume the\ndistribution is of time. If time is about 100 milliseconds, then squaring it yields\n10,000 square milliseconds. This unit is certainly unusual. It would be more\nconvenient if we had a unitless measure.\nTo avoid this unit problem, we use the squared coefficient of variance,\ntraditionally called C2:\nC2 \u00bc\nVariance\nWeighted arithmetic mean time2\nWe can solve for C, the coefficient of variance, as\nC \u00bc\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\nVariance\np\nWeighted arithmetic mean time \u00bc\nStandard deviation\nWeighted arithmetic mean time\nWeare tryingtocharacterizerandomevents, but tobeabletopredict performance\nwe need a distribution of random events where the mathematics is tractable. The most\npopular such distribution is the exponential distribution, which has a C value of 1.\nNote that we are using a constant to characterize variability about the mean. The\ninvariance of C over time reflects the property that the history of events has no impact\nD.5\nA Little Queuing Theory\n\u25a0\nD-27"
    },
    {
        "page": 881,
        "text": "on the probability of an event occurring now. This forgetful property is called mem-\noryless, and this property is an important assumption used to predict behavior using\nthese models. (Suppose this memoryless property did not exist; then, we would have\nto worry about the exact arrival times of requests relative to each other, which would\nmake the mathematics considerably less tractable!)\nOne of the most widely used exponential distributions is called a Poisson dis-\ntribution, named after the mathematician Sim\u0001eon Poisson. It is used to characterize\nrandom events in a given time interval and has several desirable mathematical\nproperties. The Poisson distribution is described by the following equation (called\nthe probability mass function):\nProbability k\n\u00f0 \u00de \u00bc e\u0005a ak\nk!\nwhere a\u00bcRate of eventsElapsed time. If interarrival times are exponentially dis-\ntributed and we use the arrival rate from above for rate of events, the number of\narrivals in a time interval t is a Poisson process, which has the Poisson distribution\nwith a\u00bcArrival ratet. As mentioned on page D-26, the equation for Timeserver\nhas another restriction on task arrival: It holds only for Poisson processes.\nFinally, we can answer the question about the length of time a new task must\nwait for the server to complete a task, called the average residual service time,\nwhich again assumes Poisson arrivals:\nAverage residual service time \u00bc 1=2Arithemtic mean 1 + C2\n\n\u0003\nAlthough we won\u2019t derive this formula, we can appeal to intuition. When the dis-\ntribution is not random and all possible values are equal to the average, the standard\ndeviation is 0 and so C is 0. The average residual service time is then just half the\naverage service time, as we would expect. If the distribution is random and it is\nPoisson, then C is 1 and the average residual service time equals the weighted arith-\nmetic mean time.\nExample\nUsing the definitions and formulas above, derive the average time waiting in the\nqueue (Timequeue) in terms of the average service time (Timeserver) and server\nutilization.\nAnswer\nAll tasks in the queue (Lengthqueue) ahead of the new task must be completed\nbefore the task can be serviced; each takes on average Timeserver. If a task is at\nthe server, it takes average residual service time to complete. The chance the server\nis busy is server utilization; hence, the expected time for service is Server utiliza-\ntionAverage residual service time. This leads to our initial formula:\nTimequeue \u00bcLengthqueue Timeserver\n+ Server utilizationAverage residual service time\nReplacing the average residual service time by its definition and Lengthqueue by\nArrival rateTimequeue yields\nD-28\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 882,
        "text": "Timequeue \u00bcServer utilization 1=2Timeserver  1 + C2\n\n\u0003\n\u0005\n\u0006\n+ Arrival rateTimequeue\n\n\u0003\nTimeserver\nSince this section is concerned with exponential distributions, C2 is 1. Thus\nTimequeue \u00bc Server utilizationTimeserver + Arrival rateTimequeue\n\n\u0003\nTimeserver\nRearranging the last term, let us replace Arrival rateTimeserver by Server\nutilization:\nTimequeue \u00bc Server utilizationTimeserver + Arrival rateTimeserver\n\u00f0\n\u00deTimequeue\n\u00bc Server utilizationTimeserver + Server utilizationTimequeue\nRearranging terms and simplifying gives us the desired equation:\nTimequeue \u00bc Server utilizationTimeserver + Server utilizationTimequeue\nTimequeue \u0005Server utilizationTimequeue \u00bc Server utilizationTimeserver\nTimequeue  1\u0005Server utilization\n\u00f0\n\u00de \u00bc Server utilizationTimeserver\nTimequeue \u00bc Timeserver \nServer utilization\n1\u0005Server utilization\n\u00f0\n\u00de\nLittle\u2019s law can be applied to the components of the black box as well, since they\nmust also be in equilibrium:\nLengthqueue \u00bc Arrival rateTimequeue\nIf we substitute for Timequeue from above, we get:\nLengthqueue \u00bc Arrival rateTimeserver \nServer utilization\n1\u0005Server utilization\n\u00f0\n\u00de\nSince Arrival rateTimeserver\u00bcServer utilization, we can simplify further:\nLengthqueue \u00bc Server utilization\nServer utilization\n1\u0005Server utilization\n\u00f0\n\u00de \u00bc\nServer utilization2\n1\u0005Server utilization\n\u00f0\n\u00de\nThis relates number of items in queue to service utilization.\nExample\nFor the system in the example on page D-26, which has a server utilization of 0.5,\nwhat is the mean number of I/O requests in the queue?\nAnswer\nUsing the equation above,\nLengthqueue \u00bc\nServer uti1ization2\n1\u0005Server uti1ization\n\u00f0\n\u00de \u00bc\n0:52\n1\u00050:5\n\u00f0\n\u00de \u00bc 0:25\n0:50 \u00bc 0:5\nTherefore, there are 0.5 requests on average in the queue.\nAs mentioned earlier, these equations and this section are based on an area of\napplied mathematics called queuing theory, which offers equations to predict\nD.5\nA Little Queuing Theory\n\u25a0\nD-29"
    },
    {
        "page": 883,
        "text": "behavior of such random variables. Real systems are too complex for queuing\ntheory to provide exact analysis, hence queuing theory works best when only\napproximate answers are needed.\nQueuing theory makes a sharp distinction between past events, which can be\ncharacterized by measurements using simple arithmetic, and future events, which\nare predictions requiring more sophisticated mathematics. In computer systems,\nwe commonly predict the future from the past; one example is least recently used\nblock replacement (see Chapter 2). Hence, the distinction between measurements\nand predicted distributions is often blurred; we use measurements to verify the type\nof distribution and then rely on the distribution thereafter.\nLet\u2019s review the assumptions about the queuing model:\n\u25a0\nThe system is in equilibrium.\n\u25a0\nThe times between two successive requests arriving, called the interarrival times,\nare exponentially distributed, which characterizes the arrival rate mentioned\nearlier.\n\u25a0\nThe number of sources of requests is unlimited. (This is called an infinite\npopulation model in queuing theory; finite population models are used when\narrival rates vary with the number of jobs already in the system.)\n\u25a0\nThe server can start on the next job immediately after finishing the prior one.\n\u25a0\nThere is no limit to the length of the queue, and it follows the first in, first out\norder discipline, so all tasks in line must be completed.\n\u25a0\nThere is one server.\nSuch a queue is called M/M/1:\nM5exponentially random request arrival (C2\u00bc1), with M standing for A. A.\nMarkov, the mathematician who defined and analyzed the memoryless\nprocesses mentioned earlier\nM5exponentially random service time (C2\u00bc1), with M again for Markov\n1\u00bcsingle server\nThe M/M/1 model is a simple and widely used model.\nThe assumption of exponential distribution is commonly used in queuing exam-\nples for three reasons\u2014one good, one fair, and one bad. The good reason is that a\nsuperpositionofmanyarbitrarydistributionsactsasanexponentialdistribution.Many\ntimes in computer systems, a particular behavior is the result of many components\ninteracting, so an exponential distribution of interarrival times is the right model.\nThe fair reason is that when variability is unclear, an exponential distribution with\nintermediate variability (C\u00bc1) is a safer guess than low variability (C\u00030) or high\nvariability (large C). The bad reason is that the math is simpler if you assume expo-\nnential distributions.\nD-30\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 884,
        "text": "Let\u2019s put queuing theory to work in a few examples.\nExample\nSuppose a processor sends 40 disk I/Os per second, these requests are exponen-\ntially distributed, and the average service time of an older disk is 20 ms. Answer\nthe following questions:\n1. On average, how utilized is the disk?\n2. What is the average time spent in the queue?\n3. What is the average response time for a disk request, including the queuing\ntime and disk service time?\nAnswer\nLet\u2019s restate these facts:\nAverage number of arriving tasks/second is 40.\nAverage disk time to service a task is 20 ms (0.02 sec).\nThe server utilization is then\nServer utilization \u00bc Arrival rateTimeserver \u00bc 400:02 \u00bc 0:8\nSince the service times are exponentially distributed, we can use the simplified for-\nmula for the average time spent waiting in line:\nTimequeue \u00bc Timeserver \nServer utilization\n1\u0005Server utilization\n\u00f0\n\u00de\n\u00bc 20 ms\n0:8\n1\u00050:8 \u00bc 200:8\n0:2 \u00bc 204 \u00bc 80 ms\nThe average response time is\nTime system \u00bc Timequeue + Timeserver \u00bc 80 + 20 ms \u00bc 100 ms\nThus, on average we spend 80% of our time waiting in the queue!\nExample\nSuppose we get a new, faster disk. Recalculate the answers to the questions above,\nassuming the disk service time is 10 ms.\nAnswer\nThe disk utilization is then\nServer utilization \u00bc Arrival rateTimeserver \u00bc 400:01 \u00bc 0:4\nThe formula for the average time spent waiting in line:\nTimequeue \u00bc Timeserver \nServer utilization\n1\u0005Server utilization\n\u00f0\n\u00de\n\u00bc 10 ms\n0:4\n1\u00050:4 \u00bc 100:4\n0:6 \u00bc 102\n3 \u00bc 6:7 ms\nThe average response time is 10+6.7 ms or 16.7 ms, 6.0 times faster than the old\nresponse time even though the new service time is only 2.0 times faster.\nD.5\nA Little Queuing Theory\n\u25a0\nD-31"
    },
    {
        "page": 885,
        "text": "Thus far, we have been assuming a single server, such as a single disk. Many real\nsystems have multiple disks and hence could use multiple servers, as in\nFigure D.17. Such a system is called an M/M/m model in queuing theory.\nLet\u2019s give the same formulas for the M/M/m queue, using Nservers to represent\nthe number of servers. The first two formulas are easy:\nUtilization\n\u00bc Arrival rateTimeserver\nNservers\nLengthqueue \u00bc Arrival rateTimequeue\nThe time waiting in the queue is\nTimequeue \u00bc Timeserver \nPtasks\u0006Nservers\nNservers  1\u0005Utilization\n\u00f0\n\u00de\nThis formula is related to the one for M/M/1, except we replace utilization of\na single server with the probability that a task will be queued as opposed to being\nimmediately serviced, and divide the time in queue by the number of servers.\nAlas, calculating the probability of jobs being in the queue is much more compli-\ncated when there are Nservers. First, the probability that there are no tasks in the\nsystem is\nProb0 tasks \u00bc 1 + Nservers Utilization\n\u00f0\n\u00deNservers\nNservers! 1\u0005Utilization\n\u00f0\n\u00de +\nX\nNservers\u00051\nn\u00bc1\nNservers Utilization\n\u00f0\n\u00den\nn!\n\"\n#\u00051\nThen the probability there are as many or more tasks than we have servers is\nProbtasks\u0006Nservers \u00bc Nservers UtilizationNservers\nNservers! 1\u0005Utilization\n\u00f0\n\u00deProb0 tasks\nArrivals\nQueue\nServer\nI/O controller\nand device\nServer\nI/O controller\nand device\nServer\nI/O controller\nand device\nFigure D.17 The M/M/m multiple-server model.\nD-32\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 886,
        "text": "Note that if Nservers is 1, Probtask\u0006Nservers simplifies back to Utilization, and we get the\nsame formula as for M/M/1. Let\u2019s try an example.\nExample\nSuppose instead of a new, faster disk, we add a second slow disk and duplicate the\ndata so that reads can be serviced by either disk. Let\u2019s assume that the requests are\nall reads. Recalculate the answers to the earlier questions, this time using an M/M/\nm queue.\nAnswer\nThe average utilization of the two disks is then\nServer utilization \u00bc Arrival rateTimeserver\nNservers\n\u00bc 400:02\n2\n\u00bc 0:4\nWe first calculate the probability of no tasks in the queue:\nProb0 tasks \u00bc 1 +\n2Utilization\n\u00f0\n\u00de2\n2! 1\u0005Utilization\n\u00f0\n\u00de +\nX\n1\nn\u00bc1\n2Utilization\n\u00f0\n\u00den\nn!\n\"\n#\u00051\n\u00bc 1 +\n20:4\n\u00f0\n\u00de2\n2 1\u00050:4\n\u00f0\n\u00de + 20:4\n\u00f0\n\u00de\n\"\n#\u00051\n\u00bc 1 + 0:640\n1:2 + 0:800\n\u0007\n\b\u00051\n\u00bc 1 + 0:533 + 0:800\n\u00bd\n\b\u00051 \u00bc 2:333\u00051\nWe use this result to calculate the probability of tasks in the queue:\nProbtasks\u0006Nservers \u00bc\n2Utilization2\n2! 1\u0005Utilization\n\u00f0\n\u00deProb0 tasks\n\u00bc\n20:4\n\u00f0\n\u00de2\n2 1\u00050:4\n\u00f0\n\u00de2:333\u00051 \u00bc 0:640\n1:2 2:333\u00051\n\u00bc 0:533=2:333 \u00bc 0:229\nFinally, the time waiting in the queue:\nTimequeue \u00bc Timeserver \nProbtasks\u0006Nservers\nNservers  1\u0005Utilization\n\u00f0\n\u00de\n\u00bc 0:020\n0:229\n2 1\u00050:4\n\u00f0\n\u00de \u00bc 0:0200:229\n1:2\n\u00bc 0:0200:190 \u00bc 0:0038\nThe average response time is 20+3.8 ms or 23.8 ms. For this workload, two disks\ncut the queue waiting time by a factor of 21 over a single slow disk and a factor of\n1.75 versus a single fast disk. The mean service time of a system with a single fast\ndisk, however, is still 1.4 times faster than one with two disks since the disk service\ntime is 2.0 times faster.\nD.5\nA Little Queuing Theory\n\u25a0\nD-33"
    },
    {
        "page": 887,
        "text": "It would be wonderful if we could generalize the M/M/m model to multiple queues\nand multiple servers, as this step is much more realistic. Alas, these models are very\nhard to solve and to use, and so we won\u2019t cover them here.\nD.6\nCrosscutting Issues\nPoint-to-Point Links and Switches Replacing Buses\nPoint-to-point links and switches are increasing in popularity as Moore\u2019s law con-\ntinues to reduce the cost of components. Combined with the higher I/O bandwidth\ndemands from faster processors, faster disks, and faster local area networks, the\ndecreasing cost advantage of buses means the days of buses in desktop and server\ncomputers are numbered. This trend started in high-performance computers in the\nlast edition of the book, and by 2011 has spread itself throughout storage.\nFigure D.18 shows the old bus-based standards and their replacements.\nThe number of bits and bandwidth for the new generation is per direction, so\nthey double for both directions. Since these new designs use many fewer wires, a\ncommon way to increase bandwidth is to offer versions with several times the num-\nber of wires and bandwidth.\nBlock Servers versus Filers\nThus far, we have largely ignored the role of the operating system in storage. In a\nmanner analogous to the way compilers use an instruction set, operating systems\ndetermine what I/O techniques implemented by the hardware will actually be used.\nThe operating system typically provides the file abstraction on top of blocks stored\non the disk. The terms logical units, logical volumes, and physical volumes are\nrelated terms used in Microsoft and UNIX systems to refer to subset collections\nof disk blocks.\nStandard\nWidth\n(bits)\nLength\n(meters)\nClock rate\nMB/sec\nMax I/O\ndevices\n(Parallel) ATA\n8\n0.5\n133 MHz\n133\n2\nSerial ATA\n2\n2\n3 GHz\n300\n?\nSCSI\n16\n12\n80 MHz\n320\n15\nSerial Attach SCSI\n1\n10\n(DDR)\n375\n16,256\nPCI\n32/64\n0.5\n33/66 MHz\n533\n?\nPCI Express\n2\n0.5\n3 GHz\n250\n?\nFigure D.18 Parallel I/O buses and their point-to-point replacements. Note the\nbandwidth and wires are per direction, so bandwidth doubles when sending both\ndirections.\nD-34\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 888,
        "text": "A logical unit is the element of storage exported from a disk array, usually con-\nstructed from a subset of the array\u2019s disks. A logical unit appears to the server as a\nsingle virtual \u201cdisk.\u201d In a RAID disk array, the logical unit is configured as a par-\nticular RAID layout, such as RAID 5. A physical volume is the device file used by\nthe file system to access a logical unit. A logical volume provides a level of vir-\ntualization that enables the file system to split the physical volume across multiple\npieces or to stripe data across multiple physical volumes. A logical unit is an\nabstraction of a disk array that presents a virtual disk to the operating system, while\nphysical and logical volumes are abstractions used by the operating system to\ndivide these virtual disks into smaller, independent file systems.\nHaving covered some of the terms for collections of blocks, we must now ask:\nWhere should the file illusion be maintained: in the server or at the other end of the\nstorage area network?\nThe traditional answer is the server. It accesses storage as disk blocks and\nmaintains the metadata. Most file systems use a file cache, so the server must main-\ntain consistency of file accesses. The disks may be direct attached\u2014found inside a\nserver connected to an I/O bus\u2014or attached over a storage area network, but the\nserver transmits data blocks to the storage subsystem.\nThe alternative answer is that the disk subsystem itself maintains the file\nabstraction, and the server uses a file system protocol to communicate with\nstorage. Example protocols are Network File System (NFS) for UNIX systems\nand Common Internet File System (CIFS) for Windows systems. Such devices\nare called network attached storage (NAS) devices since it makes no sense for\nstorage to be directly attached to the server. The name is something of a misnomer\nbecause a storage area network like FC-AL can also be used to connect to\nblock servers. The term filer is often used for NAS devices that only provide file\nservice and file storage. Network Appliance was one of the first companies to make\nfilers.\nThe driving force behind placing storage on the network is to make it easier for\nmany computers to share information and for operators to maintain the shared\nsystem.\nAsynchronous I/O and Operating Systems\nDisks typically spend much more time in mechanical delays than in transferring\ndata. Thus, a natural path to higher I/O performance is parallelism, trying to get\nmany disks to simultaneously access data for a program.\nThe straightforward approach to I/O is to request data and then start using it.\nThe operating system then switches to another process until the desired data arrive,\nand then the operating system switches back to the requesting process. Such a style\nis called synchronous I/O\u2014the process waits until the data have been read\nfrom disk.\nThe alternative model is for the process to continue after making a request, and\nit is not blocked until it tries to read the requested data. Such asynchronous I/O\nD.6\nCrosscutting Issues\n\u25a0\nD-35"
    },
    {
        "page": 889,
        "text": "allows the process to continue making requests so that many I/O requests can be\noperating simultaneously. Asynchronous I/O shares the same philosophy as caches\nin out-of-order CPUs, which achieve greater bandwidth by having multiple out-\nstanding events.\nD.7\nDesigning and Evaluating an I/O System\u2014\nThe Internet Archive Cluster\nThe art of I/O system design is to find a design that meets goals for cost, depend-\nability, and variety of devices while avoiding bottlenecks in I/O performance\nand dependability. Avoiding bottlenecks means that components must be bal-\nanced between main memory and the I/O device, because performance and\ndependability\u2014and hence effective cost-performance or cost-dependability\u2014\ncan only be as good as the weakest link in the I/O chain. The architect must also\nplan for expansion so that customers can tailor the I/O to their applications. This\nexpansibility, both in numbers and types of I/O devices, has its costs in longer I/O\nbuses and networks, larger power supplies to support I/O devices, and larger\ncabinets.\nIn designing an I/O system, we analyze performance, cost, capacity, and avail-\nability using varying I/O connection schemes and different numbers of I/O devices\nof each type. Here is one series of steps to follow in designing an I/O system. The\nanswers for each step may be dictated by market requirements or simply by cost,\nperformance, and availability goals.\n1. List the different types of I/O devices to be connected to the machine, or list the\nstandard buses and networks that the machine will support.\n2. List the physical requirements for each I/O device. Requirements include size,\npower, connectors, bus slots, expansion cabinets, and so on.\n3. List the cost of each I/O device, including the portion of cost of any controller\nneeded for this device.\n4. List the reliability of each I/O device.\n5. Record the processor resource demands of each I/O device. This list should\ninclude:\n\u25a0\nClock cycles for instructions used to initiate an I/O, to support operation of\nan I/O device (such as handling interrupts), and to complete I/O\n\u25a0\nProcessor clock stalls due to waiting for I/O to finish using the memory, bus,\nor cache\n\u25a0\nProcessor clock cycles to recover from an I/O activity, such as a cache flush\n6. List the memory and I/O bus resource demands of each I/O device. Even when\nthe processor is not using memory, the bandwidth of main memory and the I/O\nconnection is limited.\nD-36\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 890,
        "text": "7. The final step is assessing the performance and availability of the different ways\nto organize these I/O devices. When you can afford it, try to avoid single points\nof failure. Performance can only be properly evaluated with simulation,\nalthough it may be estimated using queuing theory. Reliability can be calculated\nassuming I/O devices fail independently and that the times to failure are expo-\nnentially distributed. Availability can be computed from reliability by estimat-\ning MTTF for the devices, taking into account the time from failure to repair.\nGiven your cost, performance, and availability goals, you then select the best\norganization.\nCost-performance goals affect the selection of the I/O scheme and physical\ndesign. Performance can be measured either as megabytes per second or I/Os\nper second, depending on the needs of the application. For high performance,\nthe only limits should be speed of I/O devices, number of I/O devices, and speed\nof memory and processor. For low cost, most of the cost should be the I/O devices\nthemselves. Availability goals depend in part on the cost of unavailability to an\norganization.\nRather than create a paper design, let\u2019s evaluate a real system.\nThe Internet Archive Cluster\nTo make these ideas clearer, we\u2019ll estimate the cost, performance, and availability\nof a large storage-oriented cluster at the Internet Archive. The Internet Archive\nbegan in 1996 with the goal of making a historical record of the Internet as it chan-\nged over time. You can use the Wayback Machine interface to the Internet Archive\nto perform time travel to see what the Web site at a URL looked like sometime in\nthe past. It contains over a petabyte (1015 bytes) and is growing by 20 terabytes\n(1012 bytes) of new data per month, so expansible storage is a requirement. In addi-\ntion to storing the historical record, the same hardware is used to crawl the Web\nevery few months to get snapshots of the Internet.\nClusters of computers connected by local area networks have become a very\neconomical computation engine that works well for some applications. Clusters\nalso play an important role in Internet services such the Google search engine,\nwhere the focus is more on storage than it is on computation, as is the case here.\nAlthough it has used a variety of hardware over the years, the Internet Archive\nis moving to a new cluster to become more efficient in power and in floor space.\nThe basic building block is a 1U storage node called the PetaBox GB2000 from\nCapricorn Technologies. In 2006, it used four 500 GB Parallel ATA (PATA) disk\ndrives, 512 MB of DDR266 DRAM, one 10/100/1000 Ethernet interface, and a\n1 GHz C3 processor from VIA, which executes the 80x86 instruction set. This\nnode dissipates about 80 watts in typical configurations.\nFigure D.19 shows the cluster in a standard VME rack. Forty of the GB2000s\nfit in a standard VME rack, which gives the rack 80 TB of raw capacity. The 40\nnodes are connected together with a 48-port 10/100 or 10/100/1000 switch, and it\nD.7\nDesigning and Evaluating an I/O System\u2014The Internet Archive Cluster\n\u25a0\nD-37"
    },
    {
        "page": 891,
        "text": "dissipates about 3 KW. The limit is usually 10 KW per rack in computer facilities,\nso it is well within the guidelines.\nA petabyte needs 12 of these racks, connected by a higher-level switch that\nconnects the Gbit links coming from the switches in each of the racks.\nEstimating Performance, Dependability, and Cost of the\nInternet Archive Cluster\nTo illustrate how to evaluate an I/O system, we\u2019ll make some guesses about the\ncost, performance, and reliability of the components of this cluster. We make\nthe following assumptions about cost and performance:\n\u25a0\nThe VIA processor, 512 MB of DDR266 DRAM, ATA disk controller, power\nsupply, fans, and enclosure cost $500.\n\u25a0\nEach of the four 7200 RPM Parallel ATA drives holds 500 GB, has an average\ntime seek of 8.5 ms, transfers at 50 MB/sec from the disk, and costs $375. The\nPATA link speed is 133 MB/sec.\nFigure D.19 The TB-80 VME rack from Capricorn Systems used by the Internet\nArchive. All cables, switches, and displays are accessible from the front side, and the\nback side is used only for airflow. This allows two racks to be placed back-to-back, which\nreduces the floor space demands in machine rooms.\nD-38\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 892,
        "text": "\u25a0\nThe 48-port 10/100/1000 Ethernet switch and all cables for a rack cost $3000.\n\u25a0\nThe performance of the VIA processor is 1000 MIPS.\n\u25a0\nThe ATA controller adds 0.1 ms of overhead to perform a disk I/O.\n\u25a0\nThe operating system uses 50,000 CPU instructions for a disk I/O.\n\u25a0\nThe network protocol stacks use 100,000 CPU instructions to transmit a data\nblock between the cluster and the external world.\n\u25a0\nThe average I/O size is 16 KB for accesses to the historical record via the Way-\nback interface, and 50 KB when collecting a new snapshot.\nExample\nEvaluate the cost per I/O per second (IOPS) of the 80 TB rack. Assume that every\ndisk I/O requires an average seek and average rotational delay. Assume that the\nworkload is evenly divided among all disks and that all devices can be used at\n100% of capacity; that is, the system is limited only by the weakest link, and it\ncan operate that link at 100% utilization. Calculate for both average I/O sizes.\nAnswer\nI/O performance is limited by the weakest link in the chain, so we evaluate the max-\nimum performance of each link in the I/O chain for each organization to determine\nthe maximum performance of that organization.\nLet\u2019s start by calculating the maximum number of IOPS for the CPU, main\nmemory, and I/O bus of one GB2000. The CPU I/O performance is determined\nby the speed of the CPU and the number of instructions to perform a disk I/O\nand to send it over the network:\nMaximum IOPS for CPU \u00bc\n1000 MIPS\n50,000 instructions per I=O + 100,000 instructions per message\n\u00bc 6667 IOPS\nThe maximum performance of the memory system is determined by the memory\nbandwidth and the size of the I/O transfers:\nMaximum IOPS for main memory \u00bc\n2668\n16 KB per I=O \u0003 133,000 IOPS\nMaximum IOPS for main memory \u00bc\n2668\n50 KB per I=O \u0003 42,500 IOPS\nThe Parallel ATA link performance is limited by the bandwidth and the size of the\nI/O:\nMaximum IOPS for the I=O bus \u00bc\n133 MB=sec\n16 KB per I=O \u0003 8300 IOPS\nMaximum IOPS for the I=O bus \u00bc\n133 MB=sec\n50 KB per I=O \u0003 2700 IOPS\nSince the box has two buses, the I/O bus limits the maximum performance to no\nmore than 18,600 IOPS for 16 KB blocks and 5400 IOPS for 50 KB blocks.\nD.7\nDesigning and Evaluating an I/O System\u2014The Internet Archive Cluster\n\u25a0\nD-39"
    },
    {
        "page": 893,
        "text": "Now it\u2019s time to look at the performance of the next link in the I/O chain, the\nATA controllers. The time to transfer a block over the PATA channel is\nParallel ATA transfer time \u00bc\n16 KB\n133 MB=sec \u0003 0:1 ms\nParallel ATA transfer time \u00bc\n50 KB\n133 MB=sec \u0003 0:4 ms\nAdding the 0.1 ms ATA controller overhead means 0.2 ms to 0.5 ms per I/O, mak-\ning the maximum rate per controller\nMaximum IOPS per ATA controller \u00bc\n1\n0:2 ms \u00bc 5000 IOPS\nMaximum IOPS per ATA controller \u00bc\n1\n0:5 ms \u00bc 2000 IOPS\nThe next link in the chain is the disks themselves. The time for an average\ndisk I/O is\nI=O time \u00bc 8:5 ms +\n0:5\n7200 RPM +\n16 KB\n50 MB=sec \u00bc 8:5 + 4:2 + 0:3 \u00bc 13:0 ms\nI=O time \u00bc 8:5 ms +\n0:5\n7200 RPM +\n50 KB\n50 MB=sec \u00bc 8:5 + 4:2 + 1:0 \u00bc 13:7 ms\nTherefore, disk performance is\nMaximum IOPS using average seeks\n\u00f0\n\u00de per disk \u00bc\n1\n13:0 ms \u0003 77 IOPS\nMaximum IOPS using average seeks\n\u00f0\n\u00de per disk \u00bc\n1\n13:7 ms \u0003 73 IOPS\nor 292 to 308 IOPS for the four disks.\nThe final link in the chain is the network that connects the computers to the out-\nside world. The link speed determines the limit:\nMaximum IOPS per 1000 Mbit Ethernet link \u00bc 1000 Mbit\n16 K8 \u00bc 7812 IOPS\nMaximum IOPS per 1000 Mbit Ethernet link \u00bc 1000 Mbit\n50 K8 \u00bc 2500 IOPS\nClearly, the performance bottleneck of the GB2000 is the disks. The IOPS for the\nwhole rack is 40308 or 12,320 IOPS to 40292 or 11,680 IOPS. The network\nswitch would be the bottleneck if it couldn\u2019t support 12,32016 K8 or 1.6\nGbits/sec for 16 KB blocks and 11,68050 K8 or 4.7 Gbits/sec for 50 KB\nblocks. We assume that the extra 8 Gbit ports of the 48-port switch connects\nthe rack to the rest of the world, so it could support the full IOPS of the collective\n160 disks in the rack.\nUsing these assumptions, the cost is 40($500+4$375)+$3000+$1500 or\n$84,500 for an 80 TB rack. The disks themselves are almost 60% of the cost. The\ncost per terabyte is almost $1000, which is about a factor of 10 to 15 better than\nstorage cluster from the prior edition in 2001. The cost per IOPS is about $7.\nD-40\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 894,
        "text": "Calculating MTTF of the TB-80 Cluster\nInternet services such as Google rely on many copies of the data at the application\nlevel to provide dependability, often at different geographic sites to protect against\nenvironmental faults as well as hardware faults. Hence, the Internet Archive has\ntwo copies of the data in each site and has sites in San Francisco, Amsterdam,\nand Alexandria, Egypt. Each site maintains a duplicate copy of the high-value con-\ntent\u2014music, books, film, and video\u2014and a single copy of the historical Web\ncrawls. To keep costs low, there is no redundancy in the 80 TB rack.\nExample\nLet\u2019s look at the resulting mean time to fail of the rack. Rather than use the man-\nufacturer\u2019s quoted MTTF of 600,000 hours, we\u2019ll use data from a recent survey of\ndisk drives [Gray and van Ingen 2005]. As mentioned in Chapter 1, about 3% to\n7% of ATA drives fail per year, for an MTTF of about 125,000 to 300,000 hours.\nMake the following assumptions, again assuming exponential lifetimes:\n\u25a0\nCPU/memory/enclosure MTTF is 1,000,000 hours.\n\u25a0\nPATA Disk MTTF is 125,000 hours.\n\u25a0\nPATA controller MTTF is 500,000 hours.\n\u25a0\nEthernet Switch MTTF is 500,000 hours.\n\u25a0\nPower supply MTTF is 200,000 hours.\n\u25a0\nFan MTTF is 200,000 hours.\n\u25a0\nPATA cable MTTF is 1,000,000 hours.\nAnswer\nCollecting these together, we compute these failure rates:\nFailure rate \u00bc\n40\n1,000,000 +\n160\n125,000 +\n40\n500,000 +\n1\n500,000 +\n40\n200,000 +\n40\n200,000 +\n80\n1,000,000\n\u00bc 40 + 1280 + 80 + 2 + 200 + 200 + 80\n1,000,000 hours\n\u00bc\n1882\n1,000,000 hours\nThe MTTF for the system is just the inverse of the failure rate:\nMTTF \u00bc\n1\nFailure rate \u00bc 1,000,000 hours\n1882\n\u00bc 531 hours\nThat is, given these assumptions about the MTTF of components, something in a\nrack fails on average every 3 weeks. About 70% of the failures would be the disks,\nand about 20% would be fans or power supplies.\nD.8\nPutting It All Together: NetApp FAS6000 Filer\nNetwork Appliance entered the storage market in 1992 with a goal of providing an\neasy-to-operate file server running NSF using their own log-structured file system\nand a RAID 4 disk array. The company later added support for the Windows CIFS\nD.8\nPutting It All Together: NetApp FAS6000 Filer\n\u25a0\nD-41"
    },
    {
        "page": 895,
        "text": "file system and a RAID 6 scheme called row-diagonal parity or RAID-DP (see\npage D-8). To support applications that want access to raw data blocks without\nthe overhead of a file system, such as database systems, NetApp filers can serve\ndata blocks over a standard Fibre Channel interface. NetApp also supports iSCSI,\nwhich allows SCSI commands to run over a TCP/IP network, thereby allowing the\nuse of standard networking gear to connect servers to storage, such as Ethernet, and\nhence at a greater distance.\nThe latest hardware product is the FAS6000. It is a multiprocessor based on\nthe AMD Opteron microprocessor connected using its HyperTransport links.\nThe microprocessors run the NetApp software stack, including NSF, CIFS,\nRAID-DP, SCSI, and so on. The FAS6000 comes as either a dual processor\n(FAS6030) or a quad processor (FAS6070). As mentioned in Chapter 5, DRAM\nis distributed to each microprocessor in the Opteron. The FAS6000 connects 8 GB\nof DDR2700 to each Opteron, yielding 16 GB for the FAS6030 and 32 GB for the\nFAS6070. As mentioned in Chapter 4, the DRAM bus is 128 bits wide, plus extra\nbits for SEC/DED memory. Both models dedicate four HyperTransport links\nto I/O.\nAs a filer, the FAS6000 needs a lot of I/O to connect to the disks and to connect\nto the servers. The integrated I/O consists of:\n\u25a0\n8 Fibre Channel (FC) controllers and ports\n\u25a0\n6 Gigabit Ethernet links\n\u25a0\n6 slots for x8 (2 GB/sec) PCI Express cards\n\u25a0\n3 slots for PCI-X 133 MHz, 64-bit cards\n\u25a0\nStandard I/O options such as IDE, USB, and 32-bit PCI\nThe 8 Fibre Channel controllers can each be attached to 6 shelves containing 14\n3.5-inch FC disks. Thus, the maximum number of drives for the integrated I/O is\n8614 or 672 disks. Additional FC controllers can be added to the option slots\nto connect up to 1008 drives, to reduce the number of drives per FC network so as\nto reduce contention, and so on. At 500 GB per FC drive, if we assume the RAID\nRDP group is 14 data disks and 2 check disks, the available data capacity is 294 TB\nfor 672 disks and 441 TB for 1008 disks.\nIt can also connect to Serial ATA disks via a Fibre Channel to SATA bridge\ncontroller, which, as its name suggests, allows FC and SATA to communicate.\nThe six 1-gigabit Ethernet links connect to servers to make the FAS6000 look\nlike a file server if running NTFS or CIFS or like a block server if running iSCSI.\nFor greater dependability, FAS6000 filers can be paired so that if one fails, the\nother can take over. Clustered failover requires that both filers have access to all\ndisks in the pair of filers using the FC interconnect. This interconnect also allows\neach filer to have a copy of the log data in the NVRAM of the other filer and to keep\nthe clocks of the pair synchronized. The health of the filers is constantly monitored,\nand failover happens automatically. The healthy filer maintains its own network\nidentity and its own primary functions, but it also assumes the network identity\nD-42\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 896,
        "text": "of the failed filer and handles all its data requests via a virtual filer until an admin-\nistrator restores the data service to the original state.\nD.9\nFallacies and Pitfalls\nFallacy\nComponents fail fast\nA good deal of the fault-tolerant literature is based on the simplifying assumption\nthat a component operates perfectly until a latent error becomes effective, and then\na failure occurs that stops the component.\nThe Tertiary Disk project had the opposite experience. Many components\nstarted acting strangely long before they failed, and it was generally up to the sys-\ntem operator to determine whether to declare a component as failed. The compo-\nnent would generally be willing to continue to act in violation of the service\nagreement until an operator \u201cterminated\u201d that component.\nFigure D.20 shows the history of four drives that were terminated, and the num-\nber of hours they started acting strangely before they were replaced.\nFallacy\nComputers systems achieve 99.999% availability (\u201cfive nines\u201d), as advertised\nMarketing departments of companies making servers started bragging about the\navailability of their computer hardware; in terms of Figure D.21, they claim avail-\nability of 99.999%, nicknamed five nines. Even the marketing departments of oper-\nating system companies tried to give this impression.\nFive minutes of unavailability per year is certainly impressive, but given the\nfailure data collected in surveys, it\u2019s hard to believe. For example, Hewlett-Packard\nclaims that the HP-9000 server hardware and HP-UX operating system can deliver\nMessages in system log for failed disk\nNumber of log\nmessages\nDuration\n(hours)\nHardware Failure (Peripheral device write fault [for]\nField Replaceable Unit)\n1763\n186\nNot Ready (Diagnostic failure: ASCQ\u00bcComponent ID\n[of] Field Replaceable Unit)\n1460\n90\nRecovered Error (Failure Prediction Threshold Exceeded\n[for] Field Replaceable Unit)\n1313\n5\nRecovered Error (Failure Prediction Threshold Exceeded\n[for] Field Replaceable Unit)\n431\n17\nFigure D.20 Record in system log for 4 of the 368 disks in Tertiary Disk that were\nreplaced over 18 months. See Talagala and Patterson [1999]. These messages, match-\ning the SCSI specification, were placed into the system log by device drivers. Messages\nstarted occurring as much as a week before one drive was replaced by the operator. The\nthird and fourth messages indicate that the drive\u2019s failure prediction mechanism\ndetected and predicted imminent failure, yet it was still hours before the drives were\nreplaced by the operator.\nD.9\nFallacies and Pitfalls\n\u25a0\nD-43"
    },
    {
        "page": 897,
        "text": "a 99.999% availability guarantee \u201cin certain pre-defined, pre-tested customer envi-\nronments\u201d (see Hewlett-Packard [1998]). This guarantee does not include failures\ndue to operator faults, application faults, or environmental faults, which are likely\nthe dominant fault categories today. Nor does it include scheduled downtime. It is\nalso unclear what the financial penalty is to a company if a system does not match\nits guarantee.\nMicrosoft also promulgated a five nines marketing campaign. In January 2001,\nwww.microsoft.com was unavailable for 22 hours. For its Web site to achieve\n99.999% availability, it will require a clean slate for 250 years.\nIn contrast to marketing suggestions, well-managed servers typically achieve\n99% to 99.9% availability.\nPitfall\nWhere a function is implemented affects its reliability\nIn theory, it is fine to move the RAID function into software. In practice, it is very\ndifficult to make it work reliably.\nThe software culture is generally based on eventual correctness via a series of\nreleases and patches. It is also difficult to isolate from other layers of software. For\nexample, proper software behavior is often based on having the proper version and\npatch release of the operating system. Thus, many customers have lost data due to\nsoftware bugs or incompatibilities in environment in software RAID systems.\nObviously, hardware systems are not immune to bugs, but the hardware culture\ntends to place a greater emphasis on testing correctness in the initial release. In\naddition, the hardware is more likely to be independent of the version of the oper-\nating system.\nFallacy\nOperating systems are the best place to schedule disk accesses\nHigher-level interfaces such as ATA and SCSI offer logical block addresses to the\nhost operating system. Given this high-level abstraction, the best an OS can do is to\ntry to sort the logical block addresses into increasing order. Since only the disk\nknows the mapping of the logical addresses onto the physical geometry of sectors,\ntracks, and surfaces, it can reduce the rotational and seek latencies.\nUnavailability\n(minutes per year)\nAvailability\n(percent)\nAvailability class\n(\u201cnumber of nines\u201d)\n50,000\n90%\n1\n5000\n99%\n2\n500\n99.9%\n3\n50\n99.99%\n4\n5\n99.999%\n5\n0.5\n99.9999%\n6\n0.05\n99.99999%\n7\nFigure D.21 Minutes unavailable per year to achieve availability class. (From Gray\nand Siewiorek [1991].) Note that five nines mean unavailable five minutes per year.\nD-44\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 898,
        "text": "For example, suppose the workload is four reads [Anderson 2003]:\nOperation\nStarting LBA\nLength\nRead\n724\n8\nRead\n100\n16\nRead\n9987\n1\nRead\n26\n128\nThe host might reorder the four reads into logical block order:\nRead\n26\n128\nRead\n100\n16\nRead\n724\n8\nRead\n9987\n1\nDepending on the relative location of the data on the disk, reordering could make it\nworse, as Figure D.22 shows. The disk-scheduled reads complete in three-quarters\nof a disk revolution, but the OS-scheduled reads take three revolutions.\nFallacy\nThe time of an average seek of a disk in a computer system is the time for a seek of\none-third the number of cylinders\nThis fallacy comes from confusing the way manufacturers market disks with the\nexpected performance, and from the false assumption that seek times are linear in dis-\ntance. The one-third-distance rule of thumb comes from calculating the distance of a\nseek from one random location to another random location, not including the current\ntrack and assuming there is a large number of tracks. In the past, manufacturers listed\nthe seek of this distance to offer a consistent basis for comparison. (Today, they\n724\n100\n26\n9987\nHost-ordered queue\nDrive-ordered queue\nFigure D.22 Example showing OS versus disk schedule accesses, labeled host-\nordered versus drive-ordered. The former takes 3 revolutions to complete the 4 reads,\nwhile the latter completes them in just 3/4 of a revolution. (From Anderson [2003].)\nD.9\nFallacies and Pitfalls\n\u25a0\nD-45"
    },
    {
        "page": 899,
        "text": "calculate the \u201caverage\u201d by timing all seeks and dividing by the number.) Assuming\n(incorrectly) thatseektimeislinear indistance, and usingthe manufacturer\u2019s reported\nminimum and \u201caverage\u201d seek times, a common technique to predict seek time is\nTimeseek \u00bc Timeminimum +\nDistance\nDistanceaverage\n Timeaverage \u0005Timeminimum\n\n\u0003\nThe fallacy concerning seek time is twofold. First, seek time is not linear with\ndistance; the arm must accelerate to overcome inertia, reach its maximum traveling\nspeed, decelerate as it reaches the requested position, and then wait to allow the\narm to stop vibrating (settle time). Moreover, sometimes the arm must pause to\ncontrol vibrations. For disks with more than 200 cylinders, Chen and Lee\n[1995] modeled the seek distance as:\nSeek time Distance\n\u00f0\n\u00de \u00bc a\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\nDistance\u00051\np\n+ b Distance\u00051\n\u00f0\n\u00de + c\nwhere a, b, and c are selected for a particular disk so that this formula will match the\nquoted\ntimes\nfor\nDistance\u00bc1, Distance\u00bcmax,\nand Distance\u00bc1/3\nmax.\nFigure D.23 plots this equation versus the fallacy equation. Unlike the first equa-\ntion, the square root of the distance reflects acceleration and deceleration.\nThe second problem is that the average in the product specification would only\nbe true if there were no locality to disk activity. Fortunately, there is both temporal\nand spatial locality (see page B-2 in Appendix B). For example, Figure D.24 shows\nsample measurements of seek distances for two workloads: a UNIX time-sharing\nworkload and a business-processing workload. Notice the high percentage of disk\n30\n25\n20\n15\n10\n5\nAccess time (ms)\n0\nSeek distance\n0\n250\n500\n750\n1000\n1250\n1500\nNaive seek formula\nNew seek formula\n1750\n2000\n2250\n2500\na =\n3 \u00d7\nNumber of cylinders\n\u2013 10 \u00d7 Timemin+ 15 \u00d7 Timeavg\u2013 5 \u00d7 Timemax\nb =\n3 \u00d7 Number of cylinders\n7 \u00d7 Timemin\u2013 15 \u00d7 Timeavg+ 8 \u00d7 Timemax\nc = Timemin\nFigure D.23 Seek time versus seek distance for sophisticated model versus\nnaive model. Chen and Lee [1995] found that the equations shown above for param-\neters a, b, and c worked well for several disks.\nD-46\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 900,
        "text": "accesses to the same cylinder, labeled distance 0 in the graphs, in both workloads.\nThus, this fallacy couldn\u2019t be more misleading.\nD.10\nConcluding Remarks\nStorage is one of those technologies that we tend to take for granted. And yet, if\nwe look at the true status of things today, storage is king. One can even argue that\nservers, which have become commodities, are now becoming peripheral to\nstorage devices. Driving that point home are some estimates from IBM, which\nexpects storage sales to surpass server sales in the next two years.\nMichael Vizard\nEditor-in-chief, Infoworld (August 11, 2001)\nAs their value is becoming increasingly evident, storage systems have become the\ntarget of innovation and investment.\nThe challenges for storage systems today are dependability and maintainabil-\nity. Not only do users want to be sure their data are never lost (reliability),\n0%\n10%\nPercentage of seeks (UNIX time-sharing workload)\n23%\n8%\n4%\n20%\n40%\n30%\n50%\n60%\n70%\n24%\n3%\n3%\n1%\n3%\n3%\n3%\n3%\n3%\n2%\n2%\n0%\n10%\nPercentage of seeks (business workload)\nSeek\ndistance\nSeek\ndistance\n11%\n20%\n40%\n30%\n50%\n60%\n70%\n61%\n3%\n0%\n3%\n0%\n0%\n1%\n1%\n1%\n1%\n1%\n3%\n0%\n195\n180\n165\n150\n135\n120\n105\n90\n75\n60\n45\n30\n15\n0\n208\n192\n176\n160\n144\n128\n112\n96\n80\n64\n48\n32\n16\n0\nFigure D.24 Sample measurements of seek distances for two systems. The measurements on the left were taken\non a UNIX time-sharing system. The measurements on the right were taken from a business-processing application in\nwhich the disk seek activity was scheduled to improve throughput. Seek distance of 0 means the access was made to\nthe same cylinder. The rest of the numbers show the collective percentage for distances between numbers on the y-\naxis. For example, 11% for the bar labeled 16 in the business graph means that the percentage of seeks between 1\nand 16 cylinders was 11%. The UNIX measurements stopped at 200 of the 1000 cylinders, but this captured 85% of the\naccesses. The business measurements tracked all 816 cylinders of the disks. The only seek distances with 1% or\ngreater of the seeks that are not in the graph are 224 with 4%, and 304, 336, 512, and 624, each having 1%. This\ntotal is 94%, with the difference being small but nonzero distances in other categories. Measurements courtesy\nof Dave Anderson of Seagate.\nD.10\nConcluding Remarks\n\u25a0\nD-47"
    },
    {
        "page": 901,
        "text": "applications today increasingly demand that the data are always available to access\n(availability). Despite improvements in hardware and software reliability and fault\ntolerance, the awkwardness of maintaining such systems is a problem both for cost\nand for availability. A widely mentioned statistic is that customers spend $6 to\n$8 operating a storage system for every $1 of purchase price. When dependability\nis attacked by having many redundant copies at a higher level of the system\u2014such\nas for search\u2014then very large systems can be sensitive to the price-performance of\nthe storage components.\nToday, challenges in storage dependability and maintainability dominate the\nchallenges of I/O.\nD.11\nHistorical Perspective and References\nSection M.9 (available online) covers the development of storage devices and tech-\nniques, including who invented disks, the story behind RAID, and the history of\noperating systems and databases. References for further reading are included.\nCase Studies with Exercises by Andrea C. Arpaci-Dusseau\nand Remzi H. Arpaci-Dusseau\nCase Study 1: Deconstructing a Disk\nConcepts illustrated by this case study\n\u25a0\nPerformance Characteristics\n\u25a0\nMicrobenchmarks\nThe internals of a storage system tend to be hidden behind a simple interface, that\nof a linear array of blocks. There are many advantages to having a common inter-\nface for all storage systems: An operating system can use any storage system with-\nout modification, and yet the storage system is free to innovate behind this\ninterface. For example, a single disk can map its internal<sector, track, surfa-\nce>geometry to the linear array in whatever way achieves the best performance;\nsimilarly, a multidisk RAID system can map the blocks on any number of disks to\nthis same linear array. However, this fixed interface has a number of disadvantages,\nas well; in particular, the operating system is not able to perform some perfor-\nmance, reliability, and security optimizations without knowing the precise layout\nof its blocks inside the underlying storage system.\nIn this case study, we will explore how software can be used to uncover the\ninternal structure of a storage system hidden behind a block-based interface.\nThe basic idea is to fingerprint the storage system: by running a well-defined work-\nload on top of the storage system and measuring the amount of time required for\ndifferent requests, one is able to infer a surprising amount of detail about the under-\nlying system.\nD-48\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 902,
        "text": "The Skippy algorithm, from work by Nisha Talagala and colleagues at the Uni-\nversity of California\u2013Berkeley, uncovers the parameters of a single disk. The key is\nto factor out disk rotational effects by making consecutive seeks to individual sec-\ntors with addresses that differ by a linearly increasing amount (increasing by 1, 2, 3,\nand so forth). Thus, the basic algorithm skips through the disk, increasing the dis-\ntance of the seek by one sector before every write, and outputs the distance and\ntime for each write. The raw device interface is used to avoid file system optimi-\nzations. The SECTOR SIZE is set equal to the minimum amount of data that can be\nread at once from the disk (e.g., 512 bytes). (Skippy is described in more detail in\nTalagala and Patterson [1999].)\nfd = open(\"raw disk device\");\nfor (i = 0; i < measurements; i++) {\nbegin_time = gettime();\nlseek(fd, i*SECTOR_SIZE, SEEK_CUR);\nwrite(fd, buffer, SECTOR_SIZE);\ninterval_time = gettime() -begin_time;\nprintf(\"Stride: %d Time: %d\\n\", i, interval_time);\n}\nclose(fd);\nBy graphing the time required for each write as a function of the seek distance,\none can infer the minimal transfer time (with no seek or rotational latency), head\nswitch time, cylinder switch time, rotational latency, and the number of heads in\nthe disk. A typical graph will have four distinct lines, each with the same slope, but\nwith different offsets. The highest and lowest lines correspond to requests that\nincur different amounts of rotational delay, but no cylinder or head switch costs;\nthe difference between these two lines reveals the rotational latency of the disk. The\nsecond lowest line corresponds to requests that incur a head switch (in addition to\nincreasing amounts of rotational delay). Finally, the third line corresponds to\nrequests that incur a cylinder switch (in addition to rotational delay).\nD.1\n[10/10/10/10/10]<D.2>The results of running Skippy are shown for a mock disk\n(Disk Alpha) in Figure D.25.\na. [10]<D.2>What is the minimal transfer time?\nb. [10]<D.2>What is the rotational latency?\nc. [10]<D.2>What is the head switch time?\nd. [10]<D.2>What is the cylinder switch time?\ne. [10]<D.2>What is the number of disk heads?\nD.2\n[25]<D.2>Draw an approximation of the graph that would result from running\nSkippy on Disk Beta, a disk with the following parameters:\n\u25a0\nMinimal transfer time, 2.0 ms\n\u25a0\nRotational latency, 6.0 ms\nCase Studies with Exercises by Andrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau\n\u25a0\nD-49"
    },
    {
        "page": 903,
        "text": "\u25a0\nHead switch time, 1.0 ms\n\u25a0\nCylinder switch time, 1.5 ms\n\u25a0\nNumber of disk heads, 4\n\u25a0\nSectors per track, 100\nD.3\n[10/10/10/10/10/10/10]<D.2>Implement and run the Skippy algorithm on a disk\ndrive of your choosing.\na. [10]<D.2>Graph the results of running Skippy. Report the manufacturer and\nmodel of your disk.\nb. [10]<D.2>What is the minimal transfer time?\nc. [10]<D.2>What is the rotational latency?\nd. [10]<D.2>What is the head switch time?\ne. [10]<D.2>What is the cylinder switch time?\nf. [10]<D.2>What is the number of disk heads?\ng. [10]<D.2>Do the results of running Skippy on a real disk differ in any qual-\nitative way from that of the mock disk?\nTime (ms)\n14\n12\n10\n0\n6\n4\n2\n8\n0\nDistance (sectors)\n300\n250\n200\n150\n100\n50\nFigure D.25 Results from running Skippy on Disk Alpha.\nD-50\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 904,
        "text": "Case Study 2: Deconstructing a Disk Array\nConcepts illustrated by this case study\n\u25a0\nPerformance Characteristics\n\u25a0\nMicrobenchmarks\nThe Shear algorithm, from work by Timothy Denehy and colleagues at the Uni-\nversity of Wisconsin [Denehy et al. 2004], uncovers the parameters of a RAID sys-\ntem. The basic idea is to generate a workload of requests to the RAID array and\ntime those requests; by observing which sets of requests take longer, one can infer\nwhich blocks are allocated to the same disk.\nWe define RAID properties as follows. Data are allocated to disks in the RAID\nat the block level, where a block is the minimal unit of data that the file system reads\nor writes from the storage system; thus, block size is known by the file system and\nthe fingerprinting software. A chunk is a set of blocks that is allocated contiguously\nwithin a disk. A stripe is a set of chunks across each of D data disks. Finally, a\npattern is the minimum sequence of data blocks such that block offset i within\nthe pattern is always located on disk j.\nD.4\n[20/20]<D.2>One can uncover the pattern size with the following code. The\ncode accesses the raw device to avoid file system optimizations. The key to all\nof the Shear algorithms is to use random requests to avoid triggering any of the\nprefetch or caching mechanisms within the RAID or within individual disks.\nThe basic idea of this code sequence is to access N random blocks at a fixed interval\np within the RAID array and to measure the completion time of each interval.\nfor (p = BLOCKSIZE; p <= testsize; p += BLOCKSIZE) {\nfor (i = 0; i < N; i++) {\nrequest[i] = random()*p;\n}\nbegin_time = gettime();\nissues all request[N] to raw device in parallel;\nwait for all request[N] to complete;\ninterval_time = gettime() - begin_time;\nprintf(\"PatternSize: %d Time: %d\\n\", p,\ninterval_time);\n}\nIf you run this code on a RAID array and plot the measured time for the N\nrequests as a function of p, then you will see that the time is highest when all N\nrequests fall on the same disk; thus, the value of p with the highest time corre-\nsponds to the pattern size of the RAID.\na. [20]<D.2>Figure D.26 shows the results of running the pattern size algorithm\non an unknown RAID system.\nCase Studies with Exercises by Andrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau\n\u25a0\nD-51"
    },
    {
        "page": 905,
        "text": "\u25a0\nWhat is the pattern size of this storage system?\n\u25a0\nWhat do the measured times of 0.4, 0.8, and 1.6 seconds correspond to in\nthis storage system?\n\u25a0\nIf this is a RAID 0 array, then how many disks are present?\n\u25a0\nIf this is a RAID 0 array, then what is the chunk size?\nb. [20]<D.2>Draw the graph that would result from running this Shear code on\na storage system with the following characteristics:\n\u25a0\nNumber of requests, N\u00bc1000\n\u25a0\nTime for a random read on disk, 5 ms\n\u25a0\nRAID level, RAID 0\n\u25a0\nNumber of disks, 4\n\u25a0\nChunk size, 8 KB\nD.5\n[20/20]<D.2>One can uncover the chunk size with the following code. The basic\nidea is to perform reads from N patterns chosen at random but always at controlled\noffsets, c and c\u00051, within the pattern.\nfor (c = 0; c < patternsize; c += BLOCKSIZE) {\nfor (i = 0; i < N; i++) {\nrequestA[i] = random()*patternsize + c;\nrequestB[i] = random()*patternsize +\n(c-1)%patternsize;\n}\nbegin_time = gettime();\nissue all requestA[N] and requestB[N] to raw device\nin parallel;\nwait for requestA[N] and requestB[N] to complete;\ninterval_time = gettime() - begin_time;\nprintf(\"ChunkSize: %d Time: %d\\n\", c,\ninterval_time);\n}\nTime (s)\n1.5\n0\n1.0\n0.5\n0.0\nPattern size assumed (KB)\n256\n160\n192\n224\n128\n96\n64\n32\nFigure D.26 Results from running the pattern size algorithm of Shear on a mock storage system.\nD-52\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 906,
        "text": "If you run this code and plot the measured time as a function of c, then you will\nsee that the measured time is lowest when the requestA and requestB reads fall on\ntwo different disks. Thus, the values of c with low times correspond to the chunk\nboundaries between disks of the RAID.\na. [20]<D.2>Figure D.27 shows the results of running the chunk size algorithm\non an unknown RAID system.\n\u25a0\nWhat is the chunk size of this storage system?\n\u25a0\nWhat do the measured times of 0.75 and 1.5 seconds correspond to in this\nstorage system?\nb. [20]<D.2>Draw the graph that would result from running this Shear code on\na storage system with the following characteristics:\n\u25a0\nNumber of requests, N\u00bc1000\n\u25a0\nTime for a random read on disk, 5 ms\n\u25a0\nRAID level, RAID 0\n\u25a0\nNumber of disks, 8\n\u25a0\nChunk size, 12 KB\nD.6\n[10/10/10/10]<D.2>Finally, one can determine the layout of chunks to disks with\nthe following code. The basic idea is to select N random patterns and to exhaustively\nread together all pairwise combinations of the chunks within the pattern.\nfor (a = 0; a < numchunks; a += chunksize) {\nfor (b = a; b < numchunks; b += chunksize) {\nfor (i = 0; i < N; i++) {\nrequestA[i] = random()*patternsize + a;\nrequestB[i] = random()*patternsize + b;\n}\nbegin_time = gettime();\nissue all requestA[N] and requestB[N] to raw device\nin parallel;\nTime (s)\n1.5\n0\n1.0\n0.5\n0.0\nBoundary offset assumed (KB)\n64\n48\n32\n16\nFigure D.27 Results from running the chunk size algorithm of Shear on a mock stor-\nage system.\nCase Studies with Exercises by Andrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau\n\u25a0\nD-53"
    },
    {
        "page": 907,
        "text": "wait for all requestA[N] and requestB[N] to\ncomplete;\ninterval_time = gettime() - begin_time;\nprintf(\"A: %d B: %d Time: %d\\n\", a, b,\ninterval_time);\n}\n}\nAfter running this code, you can report the measured time as a function of a and b.\nThe simplest way to graph this is to create a two-dimensional table with a and b as\nthe parameters and the time scaled to a shaded value; we use darker shadings for\nfaster times and lighter shadings for slower times. Thus, a light shading indicates\nthat the two offsets of a and b within the pattern fall on the same disk.\nFigure D.28 shows the results of running the layout algorithm on a storage system\nthat is known to have a pattern size of 384 KB and a chunk size of 32 KB.\na. [20]<D.2>How many chunks are in a pattern?\nb. [20]<D.2>Which chunks of each pattern appear to be allocated on the\nsame disks?\nc. [20]<D.2>How many disks appear to be in this storage system?\nd. [20]<D.2>Draw the likely layout of blocks across the disks.\nD.7\n[20]<D.2>Draw the graph that would result from running the layout algorithm\non the storage system shown in Figure D.29. This storage system has four disks and\na chunk size of four 4 KB blocks (16 KB) and is using a RAID 5 Left-Asymmetric\nlayout.\nChunk\n10\n0\n6\n4\n2\n8\n0\nChunk\n10\n8\n6\n4\n2\nFigure D.28 Results from running the layout algorithm of Shear on a mock storage\nsystem.\nD-54\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 908,
        "text": "Case Study 3: RAID Reconstruction\nConcepts illustrated by this case study\n\u25a0\nRAID Systems\n\u25a0\nRAID Reconstruction\n\u25a0\nMean Time to Failure (MTTF)\n\u25a0\nMean Time until Data Loss (MTDL)\n\u25a0\nPerformability\n\u25a0\nDouble Failures\nA RAID system ensures that data are not lost when a disk fails. Thus, one of the key\nresponsibilities of a RAID is to reconstruct the data that were on a disk when it\nfailed; this process is called reconstruction and is what you will explore in this case\nstudy. You will consider both a RAID system that can tolerate one disk failure and\na RAID-DP, which can tolerate two disk failures.\nReconstruction is commonly performed in two different ways. In offline recon-\nstruction, the RAID devotes all of its resources to performing reconstruction and\ndoes not service any requests from the workload. In online reconstruction, the\nRAID continues to service workload requests while performing the reconstruction;\nthe reconstruction process is often limited to use some fraction of the total band-\nwidth of the RAID system.\nHow reconstruction is performed impacts both the reliability and the perform-\nability of the system. In a RAID 5, data are lost if a second disk fails before the data\nfrom the first disk can be recovered; therefore, the longer the reconstruction time\n(MTTR), the lower the reliability or the mean time until data loss (MTDL). Per-\nformability is a metric meant to combine both the performance of a system and its\n00 01 02 03 04 05 06 07 08 09 10 11\nP\nP\nP\nP\n12 13 14 15 16 17 18 19\nP\nP\nP\nP\n20 21 22 23\n24 25 26 27\nP\nP\nP\nP\n28 29 30 31 32 33 34 35\nP\nP\nP\nP\n36 37 38 39 40 41 42 43 44 45 46 47\n48 49 50 51 52 53 54 55 56 57 58 59\nP\nP\nP\nP\n60 61 62 63 64 65 66 67\nP\nP\nP\nP\n68 69 70 71\n72 73 74 75\nP\nP\nP\nP\n76 77 78 79 80 81 82 83\nP\nP\nP\nP\n84 85 86 87 88 89 90 91 92 93 94 95\nParity: RAID 5 Left-Asymmetric, stripe = 16, pattern = 48\nFigure D.29 A storage system with four disks, a chunk size of four 4 KB blocks, and\nusing a RAID 5 Left-Asymmetric layout. Two repetitions of the pattern are shown.\nCase Studies with Exercises by Andrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau\n\u25a0\nD-55"
    },
    {
        "page": 909,
        "text": "availability; it is defined as the performance of the system in a given state multi-\nplied by the probability of that state. For a RAID array, possible states include nor-\nmal operation with no disk failures, reconstruction with one disk failure, and\nshutdown due to multiple disk failures.\nFor these exercises, assume that you have built a RAID system with six disks,\nplus a sufficient number of hot spares. Assume that each disk is the 37 GB SCSI\ndisk shown in Figure D.3 and that each disk can sequentially read data at a peak of\n142 MB/sec and sequentially write data at a peak of 85 MB/sec. Assume that the\ndisks are connected to an Ultra320 SCSI bus that can transfer a total of 320 MB/\nsec. You can assume that each disk failure is independent and ignore other potential\nfailures in the system. For the reconstruction process, you can assume that the over-\nhead for any XOR computation or memory copying is negligible. During online\nreconstruction, assume that the reconstruction process is limited to use a total band-\nwidth of 10 MB/sec from the RAID system.\nD.8\n[10]<D.2>Assume that you have a RAID 4 system with six disks. Draw a simple\ndiagram showing the layout of blocks across disks for this RAID system.\nD.9\n[10]<D.2, D.4>When a single disk fails, the RAID 4 system will perform recon-\nstruction. What is the expected time until a reconstruction is needed?\nD.10\n[10/10/10]<D.2, D.4>Assume that reconstruction of the RAID 4 array begins at\ntime t.\na. [10]<D.2, D.4>What read and write operations are required to perform the\nreconstruction?\nb. [10]<D.2, D.4>For offline reconstruction, when will the reconstruction pro-\ncess be complete?\nc. [10]<D.2, D.4>For online reconstruction, when will the reconstruction pro-\ncess be complete?\nD.11\n[10/10/10/10]<D.2, D.4>In this exercise, we will investigate the mean time until\ndata loss (MTDL). In RAID 4, data are lost only if a second disk fails before the\nfirst failed disk is repaired.\na. [10]<D.2, D.4>What is the likelihood of having a second failure during off-\nline reconstruction?\nb. [10]<D.2, D.4>Given this likelihood of a second failure during reconstruc-\ntion, what is the MTDL for offline reconstruction?\nc. [10]<D.2, D.4>What is the likelihood of having a second failure during\nonline reconstruction?\nd. [10]<D.2, D.4>Given this likelihood of a second failure during reconstruc-\ntion, what is the MTDL for online reconstruction?\nD.12\n[10]<D.2, D.4>What is performability for the RAID 4 array for offline recon-\nstruction? Calculate the performability using IOPS, assuming a random readonly\nworkload that is evenly distributed across the disks of the RAID 4 array.\nD-56\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 910,
        "text": "D.13\n[10]<D.2, D.4>What is the performability for the RAID 4 array for online recon-\nstruction? During online repair, you can assume that the IOPS drop to 70% of their\npeak rate. Does offline or online reconstruction lead to better performability?\nD.14\n[10]<D.2, D.4>RAID 6 is used to tolerate up to two simultaneous disk failures.\nAssume that you have a RAID 6 system based on row-diagonal parity, or RAID-\nDP; your six-disk RAID-DP system is based on RAID 4, with p\u00bc5, as shown in\nFigure D.5. If data disk 0 and data disk 3 fail, how can those disks be recon-\nstructed? Show the sequence of steps that are required to compute the missing\nblocks in the first four stripes.\nCase Study 4: Performance Prediction for RAIDs\nConcepts illustrated by this case study\n\u25a0\nRAID Levels\n\u25a0\nQueuing Theory\n\u25a0\nImpact of Workloads\n\u25a0\nImpact of Disk Layout\nIn this case study, you will explore how simple queuing theory can be used to pre-\ndict the performance of the I/O system. You will investigate how both storage sys-\ntem configuration and the workload influence service time, disk utilization, and\naverage response time.\nThe configuration of the storage system has a large impact on performance. Dif-\nferent RAID levels can be modeled using queuing theory in different ways. For\nexample, a RAID 0 array containing N disks can be modeled as N separate systems\nof M/M/1 queues, assuming that requests are appropriately distributed across the N\ndisks. The behavior of a RAID 1 array depends upon the workload: A read operation\ncan be sent to either mirror, whereas a write operation must be sent to both disks.\nTherefore, for a read-only workload, a two-disk RAID 1 array can be modeled as\nan M/M/2 queue, whereas for a write-only workload, it can be modeled as an M/\nM/1 queue. The behavior of a RAID 4 array containing N disks also depends upon\nthe workload: A read will be sent to a particular data disk, whereas writes must all\nupdate the parity disk, which becomes the bottleneck of the system. Therefore, for a\nread-only workload, RAID 4 can be modeled as N\u00051 separate systems, whereas for\na write-only workload, it can be modeled as one M/M/1 queue.\nThe layout of blocks within the storage system can have a significant impact on\nperformance. Consider a single disk with a 40 GB capacity. If the workload ran-\ndomly accesses 40 GB of data, then the layout of those blocks to the disk does not\nhave much of an impact on performance. However, if the workload randomly\naccesses only half of the disk\u2019s capacity (i.e., 20 GB of data on that disk), then\nlayout does matter: To reduce seek time, the 20 GB of data can be compacted\nwithin 20 GB of consecutive tracks instead of allocated uniformly distributed over\nthe entire 40 GB capacity.\nCase Studies with Exercises by Andrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau\n\u25a0\nD-57"
    },
    {
        "page": 911,
        "text": "For this problem, we will use a rather simplistic model to estimate the service\ntime of a disk. In this basic model, the average positioning and transfer time for a\nsmall random request is a linear function of the seek distance. For the 40 GB disk in\nthis problem, assume that the service time is 5 ms * space utilization. Thus, if the\nentire 40 GB disk is used, then the average positioning and transfer time for a ran-\ndom request is 5 ms; if only the first 20 GB of the disk is used, then the average\npositioning and transfer time is 2.5 ms.\nThroughout this case study, you can assume that the processor sends 167 small\nrandom disk requests per second and that these requests are exponentially\ndistributed. You can assume that the size of the requests is equal to the block size\nof 8 KB. Each disk in the system has a capacity of 40 GB. Regardless of the storage\nsystem configuration, the workload accesses a total of 40 GB of data; you should\nallocate the 40 GB of data across the disks in the system in the most efficient\nmanner.\nD.15\n[10/10/10/10/10]<D.5>Begin by assuming that the storage system consists of a\nsingle 40 GB disk.\na. [10]<D.5>Given this workload and storage system, what is the average\nservice time?\nb. [10]<D.5>On average, what is the utilization of the disk?\nc. [10]<D.5>On average, how much time does each request spend waiting for\nthe disk?\nd. [10]<D.5>What is the mean number of requests in the queue?\ne. [10]<D.5>Finally, what is the average response time for the disk requests?\nD.16\n[10/10/10/10/10/10]<D.2, D.5>Imagine that the storage system is now config-\nured to contain two 40 GB disks in a RAID 0 array; that is, the data are striped in\nblocks of 8 KB equally across the two disks with no redundancy.\na. [10]<D.2, D.5>How will the 40 GB of data be allocated across the disks?\nGiven a random request workload over a total of 40 GB, what is the expected\nservice time of each request?\nb. [10]<D.2, D.5>How can queuing theory be used to model this storage\nsystem?\nc. [10]<D.2, D.5>What is the average utilization of each disk?\nd. [10]<D.2, D.5>On average, how much time does each request spend waiting\nfor the disk?\ne. [10]<D.2, D.5>What is the mean number of requests in each queue?\nf. [10]<D.2, D.5>Finally, what is the average response time for the disk\nrequests?\nD.17\n[20/20/20/20/20]<D.2, D.5>Instead imagine that the storage system is config-\nured to contain two 40 GB disks in a RAID 1 array; that is, the data are mirrored\nD-58\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 912,
        "text": "across the two disks. Use queuing theory to model this system for a read-only\nworkload.\na. [20]<D.2, D.5>How will the 40 GB of data be allocated across the disks?\nGiven a random request workload over a total of 40 GB, what is the expected\nservice time of each request?\nb. [20]<D.2, D.5>How can queuing theory be used to model this storage\nsystem?\nc. [20]<D.2, D.5>What is the average utilization of each disk?\nd. [20]<D.2, D.5>On average, how much time does each request spend waiting\nfor the disk?\ne. [20]<D.2, D.5>Finally, what is the average response time for the disk\nrequests?\nD.18\n[10/10]<D.2, D.5>Imagine that instead of a read-only workload, you now have a\nwrite-only workload on a RAID 1 array.\na. [10]<D.2, D.5>Describe how you can use queuing theory to model this sys-\ntem and workload.\nb. [10]<D.2, D.5>Given this system and workload, what are the average utili-\nzation, average waiting time, and average response time?\nCase Study 5: I/O Subsystem Design\nConcepts illustrated by this case study\n\u25a0\nRAID Systems\n\u25a0\nMean Time to Failure (MTTF)\n\u25a0\nPerformance and Reliability Trade-Offs\nIn this case study, you will design an I/O subsystem, given a monetary budget.\nYour system will have a minimum required capacity and you will optimize for per-\nformance, reliability, or both. You are free to use as many disks and controllers as\nfit within your budget.\nHere are your building blocks:\n\u25a0\nA 10,000 MIPS CPU costing $1000. Its MTTF is 1,000,000 hours.\n\u25a0\nA 1000 MB/sec I/O bus with room for 20 Ultra320 SCSI buses and controllers.\n\u25a0\nUltra320 SCSI buses that can transfer 320 MB/sec and support up to 15 disks\nper bus (these are also called SCSI strings). The SCSI cable MTTF is\n1,000,000 hours.\n\u25a0\nAn Ultra320 SCSI controller that is capable of 50,000 IOPS, costs $250, and\nhas an MTTF of 500,000 hours.\nCase Studies with Exercises by Andrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau\n\u25a0\nD-59"
    },
    {
        "page": 913,
        "text": "\u25a0\nA $2000 enclosure supplying power and cooling to up to eight disks. The\nenclosure MTTF is 1,000,000 hours, the fan MTTF is 200,000 hours, and\nthe power supply MTTF is 200,000 hours.\n\u25a0\nThe SCSI disks described in Figure D.3.\n\u25a0\nReplacing any failed component requires 24 hours.\nYou may make the following assumptions about your workload:\n\u25a0\nThe operating system requires 70,000 CPU instructions for each disk I/O.\n\u25a0\nThe workload consists of many concurrent, random I/Os, with an average size\nof 16 KB.\nAll of your constructed systems must have the following properties:\n\u25a0\nYou have a monetary budget of $28,000.\n\u25a0\nYou must provide at least 1 TB of capacity.\nD.19\n[10]<D.2>You will begin by designing an I/O subsystem that is optimized only\nfor capacity and performance (and not reliability), specifically IOPS. Discuss the\nRAID level and block size that will deliver the best performance.\nD.20\n[20/20/20/20]<D.2, D.4, D.7>What configuration of SCSI disks, controllers,\nand enclosures results in the best performance given your monetary and capacity\nconstraints?\na. [20]<D.2, D.4, D.7>How many IOPS do you expect to deliver with your\nsystem?\nb. [20]<D.2, D.4, D.7>How much does your system cost?\nc. [20]<D.2, D.4, D.7>What is the capacity of your system?\nd. [20]<D.2, D.4, D.7>What is the MTTF of your system?\nD.21\n[10]<D.2, D.4, D.7>You will now redesign your system to optimize for reliabil-\nity, by creating a RAID 10 or RAID 01 array. Your storage system should be robust\nnot only to disk failures but also to controller, cable, power supply, and fan failures\nas well; specifically, a single component failure should not prohibit accessing both\nreplicas of a pair. Draw a diagram illustrating how blocks are allocated across disks\nin the RAID 10 and RAID 01 configurations. Is RAID 10 or RAID 01 more appro-\npriate in this environment?\nD.22\n[20/20/20/20/20]<D.2, D.4, D.7>Optimizing your RAID 10 or RAID 01 array\nonly for reliability (but staying within your capacity and monetary constraints),\nwhat is your RAID configuration?\na. [20]<D.2, D.4, D.7>What is the overall MTTF of the components in your\nsystem?\nD-60\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 914,
        "text": "b. [20]<D.2, D.4, D.7>What is the MTDL of your system?\nc. [20]<D.2, D.4, D.7>What is the usable capacity of this system?\nd. [20]<D.2, D.4, D.7>How much does your system cost?\ne. [20]<D.2, D.4, D.7>Assuming a write-only workload, how many IOPS can\nyou expect to deliver?\nD.23\n[10]<D.2, D.4, D.7>Assume that you now have access to a disk that has twice\nthe capacity, for the same price. If you continue to design only for reliability, how\nwould you change the configuration of your storage system? Why?\nCase Study 6: Dirty Rotten Bits\nConcepts illustrated by this case study\n\u25a0\nPartial Disk Failure\n\u25a0\nFailure Analysis\n\u25a0\nPerformance Analysis\n\u25a0\nParity Protection\n\u25a0\nChecksumming\nYou are put in charge of avoiding the problem of \u201cbit rot\u201d\u2014bits or blocks in a file\ngoing bad over time. This problem is particularly important in archival scenarios,\nwhere data are written once and perhaps accessed many years later; without taking\nextra measures to protect the data, the bits or blocks of a file may slowly change or\nbecome unavailable due to media errors or other I/O faults.\nDealing with bit rot requires two specific components: detection and recovery.\nTo detect bit rot efficiently, one can use checksums over each block of the file in\nquestion; a checksum is just a function of some kind that takes a (potentially long)\nstring of data as input and outputs a fixed-size string (the checksum) of the data as\noutput. The property you will exploit is that if the data changes then the computed\nchecksum is very likely to change as well.\nOnce detected, recovering from bit rot requires some form of redundancy.\nExamples include mirroring (keeping multiple copies of each block) and parity\n(some extra redundant information, usually more space efficient than mirroring).\nIn this case study, you will analyze how effective these techniques are given\nvarious scenarios. You will also write code to implement data integrity protection\nover a set of files.\nD.24\n[20/20/20]<D.2>Assume that you will use simple parity protection in Exercises\nD.24 through D.27. Specifically, assume that you will be computing one parity\nblock for each file in the file system. Further, assume that you will also use a\n20-byte MD5 checksum per 4 KB block of each file.\nCase Studies with Exercises by Andrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau\n\u25a0\nD-61"
    },
    {
        "page": 915,
        "text": "We first tackle the problem of space overhead. According to studies by Douceur and\nBolosky [1999], these file size distributions are what is found in modern PCs:\n\u00071 KB\n2 KB\n4 KB\n8 KB\n16 KB\n32 KB\n64 KB\n128 KB\n256 KB\n512 KB\n\u00061 MB\n26.6%\n11.0%\n11.2%\n10.9%\n9.5%\n8.5%\n7.1%\n5.1%\n3.7%\n2.4%\n4.0%\nThe study also finds that file systems are usually about half full. Assume that you\nhave a 37 GB disk volume that is roughly half full and follows that same distribu-\ntion, and answer the following questions:\na. [20]<D.2>How much extrainformation (bothinbytes and asapercent of the vol-\nume) must you keep on disk to be able to detect a single error with checksums?\nb. [20]<D.2>How much extra information (both in bytes and as a percent of the\nvolume) would you need to be able to both detect a single error with checksums\nas well as correct it?\nc. [20]<D.2>Given this file distribution, is the block size you are using to com-\npute checksums too big, too little, or just right?\nD.25\n[10/10]<D.2, D.3>One big problem that arises in data protection is error detec-\ntion. One approach is to perform error detection lazily\u2014that is, wait until a file is\naccessed, and at that point, check it and make sure the correct data are there. The\nproblem with this approach is that files that are not accessed frequently may slowly\nrot away and when finally accessed have too many errors to be corrected. Hence, an\neager approach is to perform what is sometimes called disk scrubbing\u2014\nperiodically go through all data and find errors proactively.\na. [10]<D.2, D.3>Assume that bit flips occur independently, at a rate of 1 flip\nper GB of data per month. Assuming the same 20 GB volume that is half full,\nand assuming that you are using the SCSI disk as specified in Figure D.3 (4 ms\nseek, roughly 100 MB/sec transfer), how often should you scan through files to\ncheck and repair their integrity?\nb. [10]<D.2, D.3>At what bit flip rate does it become impossible to maintain\ndata integrity? Again assume the 20 GB volume and the SCSI disk.\nD.26\n[10/10/10/10]<D.2, D.4>Another potential cost of added data protection is\nfound in performance overhead. We now study the performance overhead of this\ndata protection approach.\na. [10]<D.2, D.4>Assume we write a 40 MB file to the SCSI disk sequentially,\nand then write out the extra information to implement our data protection\nscheme to disk once. How much write traffic (both in total volume of bytes\nand as a percentage of total traffic) does our scheme generate?\nb. [10]<D.2, D.4>Assume we now are updating the file randomly, similar to a\ndatabase table. That is, assume we perform a series of 4 KB random writes to\nthe file, and each time we perform a single write, we must update the on-disk\nprotection information. Assuming that we perform 10,000 random writes, how\nD-62\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 916,
        "text": "much I/O traffic (both in total volume of bytes and as a percentage of total traf-\nfic) does our scheme generate?\nc. [10]<D.2, D.4>Now assume that the data protection information is always\nkept in a separate portion of the disk, away from the file it is guarding (that\nis, assume for each file A, there is another file Achecksums that holds all the\ncheck-sums for A). Hence, one potential overhead we must incur arises upon\nreads\u2014that is, upon each read, we will use the checksum to detect data\ncorruption.\nAssume you read 10,000 blocks of 4 KB each sequentially from disk. Assuming\na 4 ms average seek cost and a 100 MB/sec transfer rate (like the SCSI disk in\nFigure D.3), how long will it take to read the file (and corresponding check-\nsums) from disk? What is the time penalty due to adding checksums?\nd. [10]<D.2, D.4>Again assuming that the data protection information is kept\nseparate as in part (c), now assume you have to read 10,000 random blocks of\n4 KB each from a very large file (much bigger than 10,000 blocks, that is). For\neach read, you must again use the checksum to ensure data integrity. How long\nwill it take to read the 10,000 blocks from disk, again assuming the same disk\ncharacteristics? What is the time penalty due to adding checksums?\nD.27\n[40]<D.2, D.3, D.4>Finally, we put theory into practice by developing a user-\nlevel tool to guard against file corruption. Assume you are to write a simple set of\ntools to detect and repair data integrity. The first tool is used for checksums and\nparity. It should be called build and used like this:\nbuild <filename>\nThe build program should then store the needed checksum and redundancy\ninformation for the file filename in a file in the same directory called .file\nname.cp (so it is easy to find later).\nA second program is then used to check and potentially repair damaged files.\nIt should be called repair and used like this:\nrepair <filename>\nThe repair program should consult the .cp file for the filename in question and\nverify that all the stored checksums match the computed checksums for the data. If\nthe checksums don\u2019t match for a single block, repair should use the redundant\ninformation to reconstruct the correct data and fix the file. However, if two or more\nblocks are bad, repair should simply report that the file has been corrupted\nbeyond repair. To test your system, we will provide a tool to corrupt files called\ncorrupt. It works as follows:\ncorrupt <filename> <blocknumber>\nAll corrupt does is fill the specified block number of the file with random noise.\nFor checksums you will be using MD5. MD5 takes an input string and gives you a\nCase Studies with Exercises by Andrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau\n\u25a0\nD-63"
    },
    {
        "page": 917,
        "text": "128-bit \u201cfingerprint\u201d or checksum as an output. A great and simple implementation\nof MD5 is available here:\nhttp://sourceforge.net/project/showfiles.php?group_\nid=42360\nParity is computed with the XOR operator. In C code, you can compute the\nparity of two blocks, each of size BLOCKSIZE, as follows:\nunsigned char block1[BLOCKSIZE];\nunsigned char block2[BLOCKSIZE];\nunsigned char parity[BLOCKSIZE];\n// first, clear parity block\nfor (int i = 0; i < BLOCKSIZE; i++)\nparity[i] = 0;\n// then compute parity; carat symbol does XOR in C\nfor (int i = 0; i < BLOCKSIZE; i++) {\nparity[i] = block1[i] ^block2[i];\n}\nCase Study 7: Sorting Things Out\nConcepts illustrated by this case study\n\u25a0\nBenchmarking\n\u25a0\nPerformance Analysis\n\u25a0\nCost/Performance Analysis\n\u25a0\nAmortization of Overhead\n\u25a0\nBalanced Systems\nThe database field has a long history of using benchmarks to compare systems. In\nthis question, you will explore one of the benchmarks introduced by Anon. et al.\n[1985] (see Chapter 1): external, or disk-to-disk, sorting.\nSorting is an exciting benchmark for a number of reasons. First, sorting exercises\na computer system across all its components, including disk, memory, and proces-\nsors. Second, sorting at the highest possible performance requires a great deal of\nexpertise about how the CPU caches, operating systems, and I/O subsystems work.\nThird, it is simple enough to be implemented by a student (see below!).\nDepending on how much data you have, sorting can be done in one or multiple\npasses. Simply put, if you have enough memory to hold the entire dataset in mem-\nory, you can read the entire dataset into memory, sort it, and then write it out; this is\ncalled a \u201cone-pass\u201d sort.\nIf you do not have enough memory, you must sort the data in multiple passes.\nThere are many different approaches possible. One simple approach is to sort each\nD-64\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 918,
        "text": "chunk of the input file and write it to disk; this leaves (input file size)/(memory\nsize) sorted files on disk. Then, you have to merge each sorted temporary file into\na final sorted output. This is called a \u201ctwo-pass\u201d sort. More passes are needed in the\nunlikely case that you cannot merge all the streams in the second pass.\nIn this case study, you will analyze various aspects of sorting, determining its\neffectiveness and cost-effectiveness in different scenarios. You will also write your\nown version of an external sort, measuring its performance on real hardware.\nD.28\n[20/20/20]<D.4>We will start by configuring a system to complete a sort in the\nleast possible time, with no limits on how much we can spend. To get peak band-\nwidth from the sort, we have to make sure all the paths through the system have\nsufficient bandwidth.\nAssume for simplicity that the time to perform the in-memory sort of keys is lin-\nearly proportional to the CPU rate and memory bandwidth of the given machine\n(e.g., sorting 1 MB of records on a machine with 1 MB/sec of memory bandwidth\nand a 1 MIPS processor will take 1 second). Assume further that you have carefully\nwritten the I/O phases of the sort so as to achieve sequential bandwidth. And, of\ncourse, realize that if you don\u2019t have enough memory to hold all of the data at once\nthat sort will take two passes.\nOne problem you may encounter in performing I/O is that systems often\nperform extra memory copies; for example, when the read() system call is\ninvoked, data may first be read from disk into a system buffer and then subsequently\ncopied into the specified user buffer. Hence, memory bandwidth during I/O can be\nan issue.\nFinally, for simplicity, assume that there is no overlap of reading, sorting, or writ-\ning. That is, when you are reading data from disk, that is all you are doing; when\nsorting, you are just using the CPU and memory bandwidth; when writing, you are\njust writing data to disk.\nYour job in this task is to configure a system to extract peak performance when\nsorting 1 GB of data (i.e., roughly 10 million 100-byte records). Use the following\ntable to make choices about which machine, memory, I/O interconnect, and disks\nto buy.\nCPU\nI/O interconnect\nSlow\n1 GIPS\n$200\nSlow\n80 MB/sec\n$50\nStandard\n2 GIPS\n$1000\nStandard\n160 MB/sec\n$100\nFast\n4 GIPS\n$2000\nFast\n320 MB/sec\n$400\nMemory\nDisks\nSlow\n512 MB/sec\n$100/GB\nSlow\n30 MB/sec\n$70\nStandard\n1 GB/sec\n$200/GB\nStandard\n60 MB/sec\n$120\nFast\n2 GB/sec\n$500/GB\nFast\n110 MB/sec\n$300\nCase Studies with Exercises by Andrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau\n\u25a0\nD-65"
    },
    {
        "page": 919,
        "text": "Note: Assume that you are buying a single-processor system and that you can have\nup to two I/O interconnects. However, the amount of memory and number of disks\nare up to you (assume there is no limit on disks per I/O interconnect).\na. [20]<D.4>What is the total cost of your machine? (Break this down by part,\nincluding the cost of the CPU, amount of memory, number of disks, and I/O\nbus.)\nb. [20]<D.4>How much time does it take to complete the sort of 1 GB worth of\nrecords? (Break this down into time spent doing reads from disk, writes to disk,\nand time spent sorting.)\nc. [20]<D.4>What is the bottleneck in your system?\nD.29\n[25/25/25]<D.4>We will now examine cost-performance issues in sorting. After\nall, it is easy to buy a high-performing machine; it is much harder to buy a\ncosteffective one.\nOne place where this issue arises is with the PennySort competition (research.\nmicrosoft.com/barc/SortBenchmark/). PennySort asks that you sort as many\nrecords as you can for a single penny. To compute this, you should assume that\na system you buy will last for 3 years (94,608,000 seconds), and divide this by\nthe total cost in pennies of the machine. The result is your time budget per penny.\nOur task here will be a little simpler. Assume you have a fixed budget of $2000 (or\nless). What is the fastest sorting machine you can build? Use the same hardware\ntable as in Exercise D.28 to configure the winning machine.\n(Hint: You might want to write a little computer program to generate all the pos-\nsible configurations.)\na. [25]<D.4>What is the total cost of your machine? (Break this down by part,\nincluding the cost of the CPU, amount of memory, number of disks, and I/O\nbus.)\nb. [25]<D.4>How does the reading, writing, and sorting time break down with\nthis configuration?\nc. [25]<D.4>What is the bottleneck in your system?\nD.30\n[20/20/20]<D.4, D.6>Getting good disk performance often requires amortiza-\ntion of overhead. The idea is simple: If you must incur an overhead of some kind,\ndo as much useful work as possible after paying the cost and hence reduce its\nimpact. This idea is quite general and can be applied to many areas of computer\nsystems; with disks, it arises with the seek and rotational costs (overheads) that\nyou must incur before transferring data. You can amortize an expensive seek\nand rotation by transferring a large amount of data.\nIn this exercise, we focus on how to amortize seek and rotational costs during the\nsecond pass of a two-pass sort. Assume that when the second pass begins, there are\nN sorted runs on the disk, each of a size that fits within main memory. Our task here\nis to read in a chunk from each sorted run and merge the results into a final sorted\nD-66\n\u25a0\nAppendix D Storage Systems"
    },
    {
        "page": 920,
        "text": "output. Note that a read from one run will incur a seek and rotation, as it is very\nlikely that the last read was from a different run.\na. [20]<D.4, D.6>Assume that you have a disk that can transfer at 100 MB/sec,\nwith an average seek cost of 7 ms, and a rotational rate of 10,000 RPM. Assume\nfurther that every time you read from a run, you read 1 MB of data and that there\nare 100 runs each of size 1 GB. Also assume that writes (to the final sorted out-\nput) take place in large 1 GB chunks. How long will the merge phase take,\nassuming I/O is the dominant (i.e., only) cost?\nb. [20]<D.4, D.6>Now assume that you change the read size from 1 MB to\n10 MB. How is the total time to perform the second pass of the sort affected?\nc. [20]<D.4, D.6>In both cases, assume that what we wish to maximize is disk\nefficiency. We compute disk efficiency as the ratio of the time spent transferring\ndata over the total time spent accessing the disk. What is the disk efficiency in\neach of the scenarios mentioned above?\nD.31\n[40]<D.2, D.4, D.6>In this exercise, you will write your own external sort. To\ngenerate the data set, we provide a tool generate that works as follows:\ngenerate <filename> <size (in MB)>\nBy running generate, you create a file named filename of size size MB.\nThe file consists of 100 byte keys, with 10-byte records (the part that must be\nsorted).\nWe also provide a tool called check that checks whether a given input file is\nsorted or not. It is run as follows:\ncheck <filename>\nThe basic one-pass sort does the following: reads in the data, sorts the data, and\nthen writes the data out. However, numerous optimizations are available to you:\noverlapping reading and sorting, separating keys from the rest of the record for\nbetter cache behavior and hence faster sorting, overlapping sorting and writing,\nand so forth.\nOne important rule is that data must always start on disk (and not in the file system\ncache). The easiest way to ensure this is to unmount and remount the file system.\nOne goal: Beat the Datamation sort record. Currently, the record for sorting 1 mil-\nlion 100-byte records is 0.44 seconds, which was obtained on a cluster of 32\nmachines. If you are careful, you might be able to beat this on a single PC config-\nured with a few disks.\nCase Studies with Exercises by Andrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau\n\u25a0\nD-67"
    },
    {
        "page": 921,
        "text": "E.1\nIntroduction\nE-2\nE.2\nSignal Processing and Embedded Applications: The Digital\nSignal Processor\nE-5\nE.3\nEmbedded Benchmarks\nE-12\nE.4\nEmbedded Multiprocessors\nE-14\nE.5\nCase Study: The Emotion Engine of the Sony PlayStation 2\nE-15\nE.6\nCase Study: Sanyo VPC-SX500 Digital Camera\nE-19\nE.7\nCase Study: Inside a Cell Phone\nE-20\nE.8\nConcluding Remarks\nE-25"
    },
    {
        "page": 922,
        "text": "E\nEmbedded Systems\nBy Thomas M. Conte\nNorth Carolina State University\nWhere a calculator on the ENIAC is equipped with 18,000 vacuum\ntubes and weighs 30 tons, computers in the future may have only\n1,000 vacuum tubes and perhaps weigh 1 1/2 tons.\nPopular Mechanics\nMarch 1949"
    },
    {
        "page": 923,
        "text": "E.1\nIntroduction\nEmbedded computer systems\u2014computers lodged in other devices where the pres-\nence of the computers is not immediately obvious\u2014are the fastest-growing portion\nof the computer market. These devices range from everyday machines (most\nmicrowaves, most washing machines, printers, network switches, and automobiles\ncontain simple to very advanced embedded microprocessors) to handheld digital\ndevices (such as PDAs, cell phones, and music players) to video game consoles\nand digital set-top boxes. Although in some applications (such as PDAs) the com-\nputers are programmable, in many embedded applications the only programming\noccurs in connection with the initial loading of the application code or a later soft-\nware upgrade of that application. Thus, the application is carefully tuned for the\nprocessor and system. This process sometimes includes limited use of assembly\nlanguage in key loops, although time-to-market pressures and good software engi-\nneering practice restrict such assembly language coding to a fraction of the\napplication.\nCompared to desktop and server systems, embedded systems have a much\nwider range of processing power and cost\u2014from systems containing low-end 8-\nbit and 16-bit processors that may cost less than a dollar, to those containing full\n32-bit microprocessors capable of operating in the 500 MIPS range that cost\napproximately 10 dollars, to those containing high-end embedded processors that\ncost hundreds of dollars and can execute several billions of instructions per second.\nAlthough the range of computing power in the embedded systems market is very\nlarge, price is a key factor in the design of computers for this space. Performance\nrequirements do exist, of course, but the primary goal is often meeting the perfor-\nmance need at a minimum price, rather than achieving higher performance at a\nhigher price.\nEmbedded systems often process information in very different ways from\ngeneral-purpose processors. Typically these applications include deadline-driven\nconstraints\u2014so-called real-time constraints. In these applications, a particular\ncomputation must be completed by a certain time or the system fails (there are other\nconstraints considered real time, discussed in the next subsection).\nEmbedded systems applications typically involve processing information as\nsignals. The lay term \u201csignal\u201d often connotes radio transmission, and that is true\nfor some embedded systems (e.g., cell phones). But a signal may be an image, a\nmotion picture composed of a series of images, a control sensor measurement, and\nso on. Signal processing requires specific computation that many embedded pro-\ncessors are optimized for. We discuss this in depth below. A wide range of bench-\nmark requirements exist, from the ability to run small, limited code segments to the\nability to perform well on applications involving tens to hundreds of thousands of\nlines of code.\nTwo other key characteristics exist in many embedded applications: the need to\nminimize memory and the need to minimize power. In many embedded applica-\ntions, the memory can be a substantial portion of the system cost, and it is important\nto optimize memory size in such cases. Sometimes the application is expected to fit\nE-2\n\u25a0\nAppendix E Embedded Systems"
    },
    {
        "page": 924,
        "text": "entirely in the memory on the processor chip; other times the application needs to\nfit in its entirety in a small, off-chip memory. In either case, the importance of\nmemory size translates to an emphasis on code size, since data size is dictated\nby the application. Some architectures have special instruction set capabilities\nto reduce code size. Larger memories also mean more power, and optimizing\npower is often critical in embedded applications. Although the emphasis on low\npower is frequently driven by the use of batteries, the need to use less expensive\npackaging (plastic versus ceramic) and the absence of a fan for cooling also limit\ntotal power consumption. We examine the issue of power in more detail later in this\nappendix.\nAnother important trend in embedded systems is the use of processor cores\ntogether with application-specific circuitry\u2014so-called \u201ccore plus ASIC\u201d or \u201csys-\ntem on a chip\u201d (SOC), which may also be viewed as special-purpose multiproces-\nsors (see Section E.4). Often an application\u2019s functional and performance\nrequirements are met by combining a custom hardware solution together with soft-\nware running on a standardized embedded processor core, which is designed to\ninterface to such special-purpose hardware. In practice, embedded problems are\nusually solved by one of three approaches:\n1. The designer uses a combined hardware/software solution that includes some\ncustom hardware and an embedded processor core that is integrated with the\ncustom hardware, often on the same chip.\n2. The designer uses custom software running on an off-the-shelf embedded\nprocessor.\n3. The designer uses a digital signal processor and custom software for the proces-\nsor. Digital signal processors are processors specially tailored for signal-\nprocessing applications. We discuss some of the important differences between\ndigital signal processors and general-purpose embedded processors below.\nFigure E.1 summarizes these three classes of computing environments and their\nimportant characteristics.\nReal-Time Processing\nOften, the performance requirement in an embedded application is a real-time\nrequirement. A real-time performance requirement is one where a segment of\nthe application has an absolute maximum execution time that is allowed. For exam-\nple, in a digital set-top box the time to process each video frame is limited, since the\nprocessor must accept and process the frame before the next frame arrives (typi-\ncally called hard real-time systems). In some applications, a more sophisticated\nrequirement exists: The average time for a particular task is constrained as well\nas is the number of instances when some maximum time is exceeded. Such\napproaches (typically called soft real-time) arise when it is possible to occasionally\nmiss the time constraint on an event, as long as not too many are missed. Real-time\nE.1\nIntroduction\n\u25a0\nE-3"
    },
    {
        "page": 925,
        "text": "performance tends to be highly application dependent. It is usually measured using\nkernels either from the application or from a standardized benchmark (see\nSection E.3).\nThe construction of a hard real-time system involves three key variables. The\nfirst is the rate at which a particular task must occur. Coupled to this are the hard-\nware and software required to achieve that real-time rate. Often, structures that are\nvery advantageous on the desktop are the enemy of hard real-time analysis. For\nexample, branch speculation, cache memories, and so on introduce uncertainty\ninto code. A particular sequence of code may execute either very efficiently or very\ninefficiently, depending on whether the hardware branch predictors and caches \u201cdo\ntheir jobs.\u201d Engineers must analyze code assuming the worst-case execution time\n(WCET). In the case of traditional microprocessor hardware, if one assumes that\nall branches are mispredicted and all caches miss, the WCET is overly pessimistic.\nThus, the system designer may end up overdesigning a system to achieve a given\nWCET, when a much less expensive system would have sufficed.\nIn order to address the challenges of hard real-time systems, and yet still exploit\nsuch well-known architectural properties as branch behavior and access locality, it\nis possible to change how a processor is designed. Consider branch prediction:\nAlthough dynamic branch prediction is known to perform far more accurately than\nstatic \u201chint bits\u201d added to branch instructions, the behavior of static hints is much\nmore predictable. Furthermore, although caches perform better than software-\nmanaged on-chip memories, the latter produces predictable memory latencies.\nIn some embedded processors, caches can be converted into software-managed\non-chip memories via line locking. In this approach, a cache line can be locked\nin the cache so that it cannot be replaced until the line is unlocked\nFeature\nDesktop\nServer\nEmbedded\nPrice of system\n$1000\u2013$10,000\n$10,000\u2013$10,000,000\n$10\u2013$100,000 (including\nnetwork routers at the high end)\nPrice of microprocessor\nmodule\n$100\u2013$1000\n$200\u2013$2000\n(per processor)\n$0.20\u2013$200 (per processor)\nMicroprocessors sold per\nyear (estimates for 2000)\n150,000,000\n4,000,000\n300,000,000 (32-bit and 64-bit\nprocessors only)\nCritical system design issues\nPrice-performance,\ngraphics performance\nThroughput, availability,\nscalability\nPrice, power consumption,\napplication-specific performance\nFigure E.1 A summary of the three computing classes and their system characteristics. Note the wide range in\nsystem price for servers and embedded systems. For servers, this range arises from the need for very large-scale mul-\ntiprocessor systems for high-end transaction processing and Web server applications. For embedded systems, one\nsignificant high-end application is a network router, which could include multiple processors as well as lots of mem-\nory and other electronics. The total number of embedded processors sold in 2000 is estimated to exceed 1 billion, if\nyou include 8-bit and 16-bit microprocessors. In fact, the largest-selling microprocessor of all time is an 8-bit micro-\ncontroller sold by Intel! It is difficult to separate the low end of the server market from the desktop market, since low-\nend servers\u2014especially those costing less than $5000\u2014are essentially no different from desktop PCs. Hence, up to a\nfew million of the PC units may be effectively servers.\nE-4\n\u25a0\nAppendix E Embedded Systems"
    },
    {
        "page": 926,
        "text": "E.2\nSignal Processing and Embedded Applications:\nThe Digital Signal Processor\nA digital signal processor (DSP) is a special-purpose processor optimized for\nexecuting digital signal processing algorithms. Most of these algorithms, from\ntime-domain filtering (e.g., infinite impulse response and finite impulse response\nfiltering), to convolution, to transforms (e.g., fast Fourier transform, discrete cosine\ntransform), to even forward error correction (FEC) encodings, all have as their\nkernel the same operation: a multiply-accumulate operation. For example, the\ndiscrete Fourier transform has the form:\nX k\n\u00f0 \u00de \u00bc\nX\nN1\nn\u00bc0\nx n\n\u00f0 \u00deWkn\nN\nwhere Wkn\nN \u00bc ej2\u03c0kn\nN \u00bc cos\n2\u03c0kn\nN\n\n\u0003\n+ jsin\n2\u03c0kn\nN\n\n\u0003\nThe discrete cosine transform is often a replacement for this because it does not\nrequire complex number operations. Either transform has as its core the sum of\na product. To accelerate this, DSPs typically feature special-purpose hardware\nto perform multiply-accumulate (MAC). A MAC instruction of \u201cMAC A,B,C\u201d\nhas the semantics of \u201cA \u00bc A + B * C.\u201d In some situations, the performance of this\noperation is so critical that a DSP is selected for an application based solely upon its\nMAC operation throughput.\nDSPs often employ fixed-point arithmetic. If you think of integers as having a\nbinary point to the right of the least-significant bit, fixed point has a binary point\njust to the right of the sign bit. Hence, fixed-point data are fractions between 1\nand +1.\nExample\nHere are three simple 16-bit patterns:\n0100 0000 0000 0000\n0000 1000 0000 0000\n0100 1000 0000 1000\nWhat values do they represent if they are two\u2019s complement integers? Fixedpoint\nnumbers?\nAnswer\nNumber representation tells us that the ith digit to the left of the binary point\nrepresents 2i1 and the ith digit to the right of the binary point represents 2i. First\nassume these three patterns are integers. Then the binary point is to the far right,\nso they represent 214, 211, and (214+ 211+ 23), or 16,384, 2048, and 18,440.\nFixed point places the binary point just to the right of the sign bit, so as fixed\npoint these patterns represent 21, 24, and (21 + 24 + 212). The fractions are\n1/2, 1/16, and (2048 + 256 + 1)/4096 or 2305/4096, which represents about\n0.50000, 0.06250, and 0.56274. Alternatively, for an n-bit two\u2019s complement,\nE.2\nSignal Processing and Embedded Applications: The Digital Signal Processor\n\u25a0\nE-5"
    },
    {
        "page": 927,
        "text": "fixed-point number we could just divide the integer presentation by 2n1 to derive\nthe same results:\n16,384=32,768 \u00bc 1=2, 2048=32,768 \u00bc 1=16, and 18,440=32,768 \u00bc 2305=4096:\nFixed point can be thought of as a low-cost floating point. It doesn\u2019t include an\nexponent in every word and doesn\u2019t have hardware that automatically aligns and\nnormalizes operands. Instead, fixed point relies on the DSP programmer to keep\nthe exponent in a separate variable and ensure that each result is shifted left or right\nto keep the answer aligned to that variable. Since this exponent variable is often\nshared by a set of fixed-point variables, this style of arithmetic is also called\nblocked floating point, since a block of variables has a common exponent.\nTo support such manual calculations, DSPs usually have some registers that are\nwider to guard against round-off error, just as floating-point units internally have\nextra guard bits. Figure E.2 surveys four generations of DSPs, listing data sizes and\nwidth of the accumulating registers. Note that DSP architects are not bound by the\npowers of 2 for word sizes. Figure E.3 shows the size of data operands for the TI\nTMS320C55 DSP.\nIn addition to MAC operations, DSPs often also have operations to accelerate\nportions of communications algorithms. An important class of these algorithms\nrevolve around encoding and decoding forward error correction codes\u2014codes\nin which extra information is added to the digital bit stream to guard against errors\nin transmission. A code of rate m/n has m information bits for (m + n) check bits.\nSo, for example, a 1/2 rate code would have 1 information bit per every 2 bits. Such\ncodes are often called trellis codes because one popular graphical flow diagram of\nGeneration\nYear\nExample DSP\nData width\nAccumulator width\n1\n1982\nTI TMS32010\n16 bits\n32 bits\n2\n1987\nMotorola DSP56001\n24 bits\n56 bits\n3\n1995\nMotorola DSP56301\n24 bits\n56 bits\n4\n1998\nTI TMS320C6201\n16 bits\n40 bits\nFigure E.2 Four generations of DSPs, their data width, and the width of the registers\nthat reduces round-off error.\nData size\nMemory operand in operation\nMemory operand in data transfer\n16 bits\n89.3%\n89.0%\n32 bits\n10.7%\n11.0%\nFigure E.3 Size of data operands for the TMS320C55 DSP. About 90% of operands are\n16 bits. This DSP has two 40-bit accumulators. There are no floating-point operations, as\nis typical of many DSPs, so these data are all fixed-point integers.\nE-6\n\u25a0\nAppendix E Embedded Systems"
    },
    {
        "page": 928,
        "text": "their encoding resembles a garden trellis. A common algorithm for decoding trellis\ncodes is due to Viterbi. This algorithm requires a sequence of compares and selects\nin order to recover a transmitted bit\u2019s true value. Thus DSPs often have compare-\nselect operations to support Viterbi decode for FEC codes.\nTo explain DSPs better, we will take a detailed look at two DSPs, both pro-\nduced by Texas Instruments. The TMS320C55 series is a DSP family targeted\ntoward battery-powered embedded applications. In stark contrast to this, the\nTMS VelociTI 320C6x series is a line of powerful, eight-issue VLIW processors\ntargeted toward a broader range of applications that may be less power sensitive.\nThe TI 320C55\nAt one end of the DSP spectrum is the TI 320C55 architecture. The C55 is opti-\nmized for low-power, embedded applications. Its overall architecture is shown in\nFigure E.4. At the heart of it, the C55 is a seven-staged pipelined CPU. The stages\nare outlined below:\n\u25a0\nFetch stage reads program data from memory into the instruction buffer queue.\n\u25a0\nDecode stage decodes instructions and dispatches tasks to the other primary\nfunctional units.\n\u25a0\nAddress stage computes addresses for data accesses and branch addresses for\nprogram discontinuities.\n\u25a0\nAccess 1/Access 2 stages send data read addresses to memory.\n\u25a0\nRead stage transfers operand data on the B bus, C bus, and D bus.\n\u25a0\nExecute stage executes operation in the A unit and D unit and performs writes\non the E bus and F bus.\nInstruction\nbuffer\nunit\n(IU)\nProgram\nflow\nunit\n(PU)\nAddress\ndata flow\nunit\n(AU)\nData\ncomputation\nunit\n(DU)\nData read buses BB, CB, DB (3 x 16)\nData read address buses BAB, CAB, DAB (3 x 24)\nCPU\nData write address buses EAB, FAB (2 x 24)\nData write buses EB, FB (2 x 16)\nProgram address bus PAB (24)\nProgram read bus PB (32)\nFigure E.4 Architecture of the TMS320C55 DSP. The C55 is a seven-stage pipelined pro-\ncessor with some unique instruction execution facilities. (Courtesy Texas Instruments.)\nE.2\nSignal Processing and Embedded Applications: The Digital Signal Processor\n\u25a0\nE-7"
    },
    {
        "page": 929,
        "text": "The C55 pipeline performs pipeline hazard detection and will stall on write after\nread (WAR) and read after write (RAW) hazards.\nThe C55 does have a 24 KB instruction cache, but it is configurable to support\nvarious workloads. It may be configured to be two-way set associative, direct-\nmapped, or as a \u201cramset.\u201d This latter mode is a way to support hard realtime appli-\ncations. In this mode, blocks in the cache cannot be replaced.\nThe C55 also has advanced power management. It allows dynamic power man-\nagement through software-programmable \u201cidle domains.\u201d Blocks of circuitry on\nthe device are organized into these idle domains. Each domain can operate nor-\nmally or can be placed in a low-power idle state. A programmer-accessible Idle\nControl Register (ICR) determines which domains will be placed in the idle state\nwhen the execution of the next IDLE instruction occurs. The six domains are CPU,\ndirect memory access (DMA), peripherals, clock generator, instruction cache, and\nexternal memory interface. When each domain is in the idle state, the functions of\nthat particular domain are not available. However, in the peripheral domain, each\nperipheral has an Idle Enable bit that controls whether or not the peripheral will\nrespond to the changes in the idle state. Thus, peripherals can be individually con-\nfigured to idle or remain active when the peripheral domain is idled.\nSince the C55 is a DSP, the central feature is its MAC units. The C55 has two\nMAC units, each comprised of a 17-bit by 17-bit multiplier coupled to a 40-bit\ndedicated adder. Each MAC unit performs its work in a single cycle; thus, the\nC55 can execute two MACs per cycle in full pipelined operation. This kind of\ncapability is critical for efficiently performing signal processing applications.\nThe C55 also has a compare, select, and store unit (CSSU) for the add/compare\nsection of the Viterbi decoder.\nThe TI 320C6x\nIn stark contrast to the C55 DSP family is the high-end Texas Instruments VelociTI\n320C6x family of processors. The C6x processors are closer to traditional very\nlong instruction word (VLIW) processors because they seek to exploit the high\nlevels of instruction-level parallelism (ILP) in many signal processing algorithms.\nTexas Instruments is not alone in selecting VLIW for exploiting ILP in the embed-\nded space. Other VLIW DSP vendors include Ceva, StarCore, Philips/TriMedia,\nand STMicroelectronics. Why do these vendors favor VLIW over superscalar? For\nthe embedded space, code compatibility is less of a problem, and so new applica-\ntions can be either hand tuned or recompiled for the newest generation of proces-\nsor. The other reason superscalar excels on the desktop is because the compiler\ncannot predict memory latencies at compile time. In embedded, however, memory\nlatencies are often much more predictable. In fact, hard real-time constraints force\nmemory latencies to be statically predictable. Of course, a superscalar would also\nperform well in this environment with these constraints, but the extra hardware to\ndynamically schedule instructions is both wasteful in terms of precious chip area\nand in terms of power consumption. Thus VLIW is a natural choice for high-\nperformance embedded.\nE-8\n\u25a0\nAppendix E Embedded Systems"
    },
    {
        "page": 930,
        "text": "The C6x family employs different pipeline depths depending on the family\nmember. For the C64x, for example, the pipeline has 11 stages. The first four stages\nof the pipeline perform instruction fetch, followed by two stages for instruction\ndecode, and finally four stages for instruction execution. The overall architecture\nof the C64x is shown below in Figure E.5.\nThe C6x family\u2019s execution stage is divided into two parts, the left or \u201c1\u201d side\nand the right or \u201c2\u201d side. The L1 and L2 units perform logical and arithmetic oper-\nations. D units in contrast perform a subset of logical and arithmetic operations but\nalso perform memory accesses (loads and stores). The two M units perform multi-\nplication and related operations (e.g., shifts). Finally the S units perform compari-\nsons, branches, and some SIMD operations (see the next subsection for a detailed\nexplanation of SIMD operations). Each side has its own 32-entry, 32-bit register file\n(the A file for the 1 side, the B file for the 2 side). A side may access the other side\u2019s\nregisters, but with a 1- cycle penalty. Thus, an instruction executing on side 1 may\naccess B5, for example, but it will take 1- cycle extra to execute because of this.\nVLIWs are traditionally very bad when it comes to code size, which runs con-\ntrary to the needs of embedded systems. However, the C6x family\u2019s approach\n\u201ccompresses\u201d instructions, allowing the VLIW code to achieve the same density\nas equivalent RISC (reduced instruction set computer) code. To do so, instruction\nfetch is carried out on an \u201cinstruction packet,\u201d shown in Figure E.6. Each instruc-\ntion has a p bit that specifies whether this instruction is a member of the current\nProgram cache/program memory\n32-bit address\n256-bit data\nData cache/data memory\n32-bit address\n8-, 16-, 32-, 64-bit data\nProgram fetch\nInstruction dispatch\nInstruction decode\nControl\nregisters\nC6000 CPU\nControl\nlogic\nTest\nEmulation\nInterrupts\nEDMA,\nEMIF\nAdditional\nperipherals:\ntimers,\nserial ports,\netc.\nRegister file A\nData path A\n.L1 .S1 .M1 .D1\nPower\ndown\nRegister file B\nData path B\n.D2 .M2 .S2 .L2\nFigure E.5 Architecture of the TMS320C64x family of DSPs. The C6x is an eight-issue\ntraditional VLIW processor. (Courtesy Texas Instruments.)\nE.2\nSignal Processing and Embedded Applications: The Digital Signal Processor\n\u25a0\nE-9"
    },
    {
        "page": 931,
        "text": "VLIW word or the next VLIW word (see the figure for a detailed explanation).\nThus, there are now no NOPs that are needed for VLIW encoding.\nSoftware pipelining is an important technique for achieving high performance\nin a VLIW. But software pipelining relies on each iteration of the loop having an\nidentical schedule to all other iterations. Because conditional branch instructions\ndisrupt this pattern, the C6x family provides a means to conditionally execute\ninstructions using predication. In predication, the instruction performs its work.\nBut when it is done executing, an additional register, for example A1, is checked.\nIf A1 is zero, the instruction does not write its results. If A1 is nonzero, the instruc-\ntion proceeds normally. This allows simple if-then and if-then-else structures to be\ncollapsed into straight-line code for software pipelining.\nMedia Extensions\nThere is a middle ground between DSPs and microcontrollers: media extensions.\nThese extensions add DSP-like capabilities to microcontroller architectures at rela-\ntively low cost. Because media processing is judged by human perception, the data\nfor multimedia operations are often much narrower than the 64-bit data word of mod-\nern desktop and server processors. For example, floating-point operations for\ngraphics are normally in single precision, not double precision, and often at a pre-\ncision less than is required by IEEE 754. Rather than waste the 64-bit arithmetic-\nlogical units (ALUs) when operating on 32-bit, 16-bit, or even 8-bit integers, mul-\ntimedia instructions can operate on several narrower data items at the same time.\nThus, a partitioned add operation on 16-bit data with a 64-bit ALU would perform\nfour 16-bit adds in a single clock cycle. The extra hardware cost is simply to prevent\ncarries between the four 16-bit partitions of the ALU. For example, such instructions\nmight be used for graphical operations on pixels. These operations are commonly\ncalled single-instruction multiple-data (SIMD) or vector instructions.\nMost graphics multimedia applications use 32-bit floating-point operations.\nSome computers double peak performance of single-precision, floating-point oper-\nations; they allow a single instruction to launch two 32-bit operations on operands\nfound side by side in a double-precision register. The two partitions must be insu-\nlated to prevent operations on one half from affecting the other. Such floating-point\noperations are called paired single operations. For example, such an operation\n31\n0 31\n0 31\n0 31\n0 31\n0 31\n0 31\n0 31\n0\nInstruction\nA\np\np\np\np\np\np\np\np\nInstruction\nB\nInstruction\nC\nInstruction\nD\nInstruction\nE\nInstruction\nF\nInstruction\nG\nInstruction\nH\nFigure E.6 Instruction packet of the TMS320C6x family of DSPs. The p bits determine\nwhether an instruction begins a new VLIW word or not. If the p bit of instruction i is 1,\nthen instruction i + 1 is to be executed in parallel with (in the same cycle as) instruction i.\nIf the p bit of instruction i is 0, then instruction i + 1 is executed in the cycle after instruc-\ntion i. (Courtesy Texas Instruments.)\nE-10\n\u25a0\nAppendix E Embedded Systems"
    },
    {
        "page": 932,
        "text": "might be used for graphical transformations of vertices. This doubling in perfor-\nmance is typically accomplished by doubling the number of floating-point units,\nmaking it more expensive than just suppressing carries in integer adders.\nFigure E.7 summarizes the SIMD multimedia instructions found in several\nrecent computers.\nDSPs also provide operations found in the first three rows of Figure E.7, but\nthey change the semantics a bit. First, because they are often used in real-time\napplications, there is not an option of causing an exception on arithmetic overflow\n(otherwise it could miss an event); thus, the result will be used no matter what the\ninputs. To support such an unyielding environment, DSP architectures use saturat-\ning arithmetic: If the result is too large to be represented, it is set to the largest rep-\nresentable number, depending on the sign of the result. In contrast, two\u2019s\ncomplement arithmetic can add a small positive number to a large positive.\nInstruction category\nAlpha MAX\nHP PA-RISC\nMAX2\nIntel Pentium\nMMX\nPowerPC\nAltiVec\nSPARC VIS\nAdd/subtract\n4H\n8B, 4H, 2W\n16B, 8H, 4W\n4H, 2W\nSaturating add/subtract\n4H\n8B, 4H\n16B, 8H, 4W\nMultiply\n4H\n16B, 8H\nCompare\n8B (>\u00bc)\n8B, 4H, 2W\n(\u00bc, >)\n16B, 8H, 4W\n(\u00bc, >, > \u00bc, <, <\u00bc)\n4H, 2W\n(\u00bc, not \u00bc, >, <\u00bc)\nShift right/left\n4H\n4H, 2W\n16B, 8H, 4W\nShift right arithmetic\n4H\n16B, 8H, 4W\nMultiply and add\n8H\nShift and add\n(saturating)\n4H\nAND/OR/XOR\n8B, 4H, 2W\n8B, 4H, 2W\n8B, 4H, 2W\n16B, 8H, 4W\n8B, 4H, 2W\nAbsolute difference\n8B\n16B, 8H, 4W\n8B\nMaximum/minimum\n8B, 4W\n16B, 8H, 4W\nPack (2n bits ! n bits)\n2W ! 2B,\n4H ! 4B\n2*4H ! 8B\n4H ! 4B,\n2W ! 2H\n4W ! 4B,\n8H ! 8B\n2W ! 2H,\n2W ! 2B,\n4H ! 4B\nUnpack/merge\n2B ! 2W,\n4B ! 4H\n2B ! 2W,\n4B ! 4H\n4B ! 4W,\n8B ! 8H\n4B ! 4H,\n2*4B ! 8B\nPermute/shuffle\n4H\n16B, 8H, 4W\nFigure E.7 Summary of multimedia support for desktop processors. Note the diversity of support, with little in\ncommon across the five architectures. All are fixed-width operations, performing multiple narrow operations on\neither a 64-bit or 128-bit ALU. B stands for byte (8 bits), H for half word (16 bits), and W for word (32 bits). Thus,\n8B means an operation on 8 bytes in a single instruction. Note that AltiVec assumes a 128-bit ALU, and the rest\nassume 64 bits. Pack and unpack use the notation 2*2W to mean 2 operands each with 2 words. This table is a sim-\nplification of the full multimedia architectures, leaving out many details. For example, HP MAX2 includes an instruc-\ntion to calculate averages, and SPARC VIS includes instructions to set registers to constants. Also, this table does not\ninclude the memory alignment operation of AltiVec, MAX, and VIS.\nE.2\nSignal Processing and Embedded Applications: The Digital Signal Processor\n\u25a0\nE-11"
    },
    {
        "page": 933,
        "text": "E.3\nEmbedded Benchmarks\nIt used to be the case just a couple of years ago that in the embedded market, many\nmanufacturers quoted Dhrystone performance, a benchmark that was criticized and\ngiven up by desktop systems more than 20 years ago! As mentioned earlier, the\nenormous variety in embedded applications, as well as differences in performance\nrequirements (hard real time, soft real time, and overall cost-performance), make\nthe use of a single set of benchmarks unrealistic. In practice, many designers of\nembedded systems devise benchmarks that reflect their application, either as ker-\nnels or as stand-alone versions of the entire application.\nFor those embedded applications that can be characterized well by kernel per-\nformance, the best standardized set of benchmarks appears to be a new benchmark\nset: the EDN Embedded Microprocessor Benchmark Consortium (or EEMBC,\npronounced \u201cembassy\u201d). The EEMBC benchmarks fall into six classes (called\n\u201csubcommittees\u201d in the parlance of EEMBC): automotive/industrial, consumer,\ntelecommunications, digital entertainment, networking (currently in its second ver-\nsion), and office automation (also the second version of this subcommittee).\nFigure E.8 shows the six different application classes, which include 50\nbenchmarks.\nAlthough many embedded applications are sensitive to the performance of\nsmall kernels, remember that often the overall performance of the entire application\n(which may be thousands of lines) is also critical. Thus, for many embedded sys-\ntems, the EMBCC benchmarks can only be used to partially assess performance.\nBenchmark type\n(\u201csubcommittee\u201d)\nNumber of\nkernels\nExample benchmarks\nAutomotive/industrial\n16\n6 microbenchmarks (arithmetic operations, pointer chasing, memory\nperformance, matrix arithmetic, table lookup, bit manipulation), 5 automobile\ncontrol benchmarks, and 5 filter or FFT benchmarks\nConsumer\n5\n5 multimedia benchmarks (JPEG compress/decompress, filtering, and RGB\nconversions)\nTelecommunications\n5\nFiltering and DSP benchmarks (autocorrelation, FFT, decoder, encoder)\nDigital entertainment\n12\nMP3 decode, MPEG-2 and MPEG-4 encode and decode (each of which is\napplied to five different datasets), MPEG Encode Floating Point, 4 benchmark\ntests for common cryptographic standards and algorithms (AES, DES, RSA,\nand Huffman decoding for data decompression), and enhanced JPEG and\ncolor-space conversion tests\nNetworking version 2\n6\nIP Packet Check (borrowed from the RFC1812 standard), IP Reassembly, IP\nNetwork Address Translator (NAT), Route Lookup, OSPF, Quality of Service\n(QOS), and TCP\nOffice automation\nversion 2\n6\nGhostscript, text parsing, image rotation, dithering, B\u0001ezier\nFigure E.8 The EEMBC benchmark suite, consisting of 50 kernels in six different classes. See www.eembc.org for\nmore information on the benchmarks and for scores.\nE-12\n\u25a0\nAppendix E Embedded Systems"
    },
    {
        "page": 934,
        "text": "Power Consumption and Efficiency as the Metric\nCost and power are often at least as important as performance in the embedded\nmarket. In addition to the cost of the processor module (which includes any\nrequired interface chips), memory is often the next most costly part of an embedded\nsystem. Unlike a desktop or server system, most embedded systems do not have\nsecondary storage; instead, the entire application must reside in either FLASH or\nDRAM. Because many embedded systems, such as PDAs and cell phones, are con-\nstrained by both cost and physical size, the amount of memory needed for the appli-\ncation is critical. Likewise, power is often a determining factor in choosing a\nprocessor, especially for battery-powered systems.\nEEMBC EnergyBench provides data on the amount of energy a processor con-\nsumes while running EEMBC\u2019s performance benchmarks. An EEMBC-certified\nEnergymark score is an optional metric that a device manufacturer may choose\nto supply in conjunction with certified scores for device performance as a way\nof indicating a processor\u2019s efficient use of power and energy. EEMBC has stan-\ndardized on the use of National Instruments\u2019 LabVIEW graphical development\nenvironment and data acquisition hardware to implement EnergyBench.\nFigure E.9 shows the relative performance per watt of typical operating power.\nCompare this figure to Figure E.10, which plots raw performance, and notice how\ndifferent the results are. The NEC VR 4122 has a clear advantage in performance\nper watt, but is the second-lowest performing processor! From the viewpoint of\npower consumption, the NEC VR 4122, which was designed for battery-based sys-\ntems, is the big winner. The IBM PowerPC displays efficient use of power to\nachieve its high performance, although at 6 W typical, it is probably not suitable\nfor most battery-based devices.\nRelative performance per watt\n0\n3.5\n4.0\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\nAutomotive\nOffice\nTelecomm\nAMD ElanSC520\nAMD K6-2E+\nIBM PowerPC 750CX\nNEC VR 5432\nNEC VR 4122\nFigure E.9 Relative performance per watt for the five embedded processors. The\npower is measured as typical operating power for the processor and does not include\nany interface chips.\nE.3\nEmbedded Benchmarks\n\u25a0\nE-13"
    },
    {
        "page": 935,
        "text": "E.4\nEmbedded Multiprocessors\nMultiprocessors are now common in server environments, and several desktop\nmultiprocessors are available from vendors, such as Sun, Compaq, and Apple.\nIn the embedded space, a number of special-purpose designs have used customized\nmultiprocessors, including the Sony PlayStation 2 (see Section E.5).\nMany special-purpose embedded designs consist of a general-purpose pro-\ngrammable processor or DSP with special-purpose, finite-state machines that\nare used for stream-oriented I/O. In applications ranging from computer graphics\nand media processing to telecommunications, this style of special-purpose multi-\nprocessor is becoming common. Although the interprocessor interactions in such\ndesigns are highly regimented and relatively simple\u2014consisting primarily of a\nsimple communication channel\u2014because much of the design is committed to sil-\nicon, ensuring that the communication protocols among the input/output proces-\nsors and the general-purpose processor are correct is a major challenge in such\ndesigns.\nMore recently, we have seen the first appearance, in the embedded space, of\nembedded multiprocessors built from several general-purpose processors. These\nmultiprocessors have been focused primarily on the high-end telecommunications\nand networking market, where scalability is critical. An example of such a design is\nthe MXP processor designed by empowerTel Networks for use in voice-over-IP\nsystems. The MXP processor consists of four main components:\n\u25a0\nAn interface to serial voice streams, including support for handling jitter\n\u25a0\nSupport for fast packet routing and channel lookup\n\u25a0\nA complete Ethernet interface, including the MAC layer\n\u25a0\nFour MIPS32 R4000-class processors, each with its own cache (a total of\n48 KB or 12 KB per processor)\nPerformance relative to AMD\nElan SC520\n14.0\n12.0\n10.0\n8.0\n6.0\n4.0\n2.0\n0\nAutomotive\nOffice\nTelecomm\nAMD Elan SC520\nAMD K6-2E+\nIBM PowerPC 750CX\nNEC VR 5432\nNEC VR 4122\nFigure E.10 Raw performance for the five embedded processors. The performance is\npresented as relative to the performance of the AMD ElanSC520.\nE-14\n\u25a0\nAppendix E Embedded Systems"
    },
    {
        "page": 936,
        "text": "The MIPS processors are used to run the code responsible for maintaining the\nvoice-over-IP channels, including the assurance of quality of service, echo cancel-\nlation, simple compression, and packet encoding. Since the goal is to run as many\nindependent voice streams as possible, a multiprocessor is an ideal solution.\nBecause of the small size of the MIPS cores, the entire chip takes only 13.5 M\ntransistors. Future generations of the chip are expected to handle more voice chan-\nnels, as well as do more sophisticated echo cancellation, voice activity detection,\nand more sophisticated compression.\nMultiprocessing is becoming widespread in the embedded computing arena for\ntwo primary reasons. First, the issues of binary software compatibility, which pla-\ngue desktop and server systems, are less relevant in the embedded space. Often\nsoftware in an embedded application is written from scratch for an application\nor significantly modified (note that this is also the reason VLIW is favored over\nsuperscalar in embedded instruction-level parallelism). Second, the applications\noften have natural parallelism, especially at the high end of the embedded space.\nExamples of this natural parallelism abound in applications such as a settop box, a\nnetwork switch, a cell phone (see Section E.7) or a game system (see Section E.5).\nThe lower barriers to use of thread-level parallelism together with the greater sen-\nsitivity to die cost (and hence efficient use of silicon) are leading to widespread\nadoption of multiprocessing in the embedded space, as the application needs grow\nto demand more performance.\nE.5\nCase Study: The Emotion Engine of the Sony\nPlayStation 2\nDesktop computers and servers rely on the memory hierarchy to reduce average\naccess time to relatively static data, but there are embedded applications where data\nare often a continuous stream. In such applications there is still spatial locality, but\ntemporal locality is much more limited.\nTo give another look at memory performance beyond the desktop, this section\nexamines the microprocessor at the heart of the Sony PlayStation 2. As we will see,\nthe steady stream of graphics and audio demanded by electronic games leads to a\ndifferent approach to memory design. The style is high bandwidth via many ded-\nicated independent memories.\nFigure E.11 shows a block diagram of the Sony PlayStation 2 (PS2). Not sur-\nprisingly for a game machine, there are interfaces for video, sound, and a DVD\nplayer. Surprisingly, there are two standard computer I/O buses, USB and IEEE\n1394, a PCMCIA slot as found in portable PCs, and a modem. These additions\nshow that Sony had greater plans for the PS2 beyond traditional games. Although\nit appears that the I/O processor (IOP) simply handles the I/O devices and the game\nconsole, it includes a 34 MHz MIPS processor that also acts as the emulation com-\nputer to run games for earlier Sony PlayStations. It also connects to a standard PC\naudio card to provide the sound for the games.\nE.5\nCase Study: The Emotion Engine of the Sony PlayStation 2\n\u25a0\nE-15"
    },
    {
        "page": 937,
        "text": "Thus, one challenge for the memory system of this embedded application is to act\nas source or destination for the extensive number of I/O devices. The PS2 designers\nmet this challenge with two PC800 (400 MHz) DRDRAM chips using two channels,\noffering 32 MB of storage and a peak memory bandwidth of 3.2 GB/sec.\nWhat\u2019s left in the figure are basically two big chips: the Graphics Synthesizer\nand the Emotion Engine.\n300 MHz\nSuperscalar\nCPU Core\nw/128-bit SIMD\nEmotion Engine\nI/O processor\nGraphics Synthesizer\n16 parallel pixel\nprocessors\n(150 MHz)\nVideo memory\n(4 MB multiported\nembedded DRAM)\nMemory\ncontrol\n10-\nchannel\nDMA\nIPU\n(MPEG\ndecoder)\nI/O\nI/F\nVector\nUnit 0\n(VPU0)\n128-bit/150-MHz bus\nVector\nUnit 1\n(VPU1)\n64-bit\n16-bit\n16-bit\n150 MHz\n400 MHz\nGraphics I/F\n32-bit\n1024-bit\nNTSC, PAL, DTV, VESA\n1024-bit\n512\n37.5 MHz\nMain memory\n32 MB DRDRAM\n34 MHz\nMIPS CPU\n(PlayStation\ncompatible)\n48-channel\nsound chip\nLocal bus\nUSB\nDVD-ROM\nPCMCIA\nModem\nI/O\ncircuits\nIEEE-1394\nFigure E.11 Block diagram of the Sony PlayStation 2. The 10 DMA channels orchestrate the transfers between all\nthe small memories on the chip, which when completed all head toward the Graphics Interface so as to be rendered\nby the Graphics Synthesizer. The Graphics Synthesizer uses DRAM on chip to provide an entire frame buffer plus\ngraphics processors to perform the rendering desired based on the display commands given from the Emotion\nEngine. The embedded DRAM allows 1024-bit transfers between the pixel processors and the display buffer. The\nSuperscalar CPU is a 64-bit MIPS III with two-instruction issue, and comes with a two-way, set associative, 16 KB\ninstruction cache; a two-way, set associative, 8 KB data cache; and 16 KB of scratchpad memory. It has been extended\nwith 128-bit SIMD instructions for multimedia applications (see Section E.2). Vector Unit 0 is primarily a DSP-like\ncoprocessor for the CPU (see Section E.2), which can operate on 128-bit registers in SIMD manner between 8 bits\nand 32 bits per word. It has 4 KB of instruction memory and 4 KB of data memory. Vector Unit 1 has similar functions\nto VPU0, but it normally operates independently of the CPU and contains 16 KB of instruction memory and 16 KB of\ndata memory. All three units can communicate over the 128-bit system bus, but there is also a 128-bit dedicated path\nbetween the CPU and VPU0 and a 128-bit dedicated path between VPU1 and the Graphics Interface. Although VPU0\nand VPU1 have identical microarchitectures, the differences in memory size and units to which they have direct con-\nnections affect the roles that they take in a game. At 0.25-micron line widths, the Emotion Engine chip uses 13.5M\ntransistors and is 225 mm2, and the Graphics Synthesizer is 279 mm2. To put this in perspective, the Alpha 21264\nmicroprocessor in 0.25-micron technology is about 160 mm2 and uses 15M transistors. (This figure is based on\nFigure 1 in \u201cSony\u2019s Emotionally Charged Chip,\u201d Microprocessor Report 13:5.)\nE-16\n\u25a0\nAppendix E Embedded Systems"
    },
    {
        "page": 938,
        "text": "The Graphics Synthesizer takes rendering commands from the Emotion Engine\nin what are commonly called display lists. These are lists of 32-bit commands that\ntell the renderer what shape to use and where to place them, plus what colors and\ntextures to fill them.\nThis chip also has the highest bandwidth portion of the memory system. By\nusing embedded DRAM on the Graphics Synthesizer, the chip contains the full\nvideo buffer and has a 2048-bit-wide interface so that pixel filling is not a bottle-\nneck. This embedded DRAM greatly reduces the bandwidth demands on the\nDRDRAM. It illustrates a common technique found in embedded applications:\nseparate memories dedicated to individual functions to inexpensively achieve\ngreater memory bandwidth for the entire system.\nThe remaining large chip is the Emotion Engine, and its job is to accept inputs\nfrom the IOP and create the display lists of a video game to enable 3D video trans-\nformations in real time. A major insight shaped the design of the Emotion Engine:\nGenerally, in a racing car game there are foreground objects that are constantly\nchanging and background objects that change less in reaction to the events,\nalthough the background can be most of the screen. This observation led to a split\nof responsibilities.\nThe CPU works with VPU0 as a tightly coupled coprocessor, in that every\nVPU0 instruction is a standard MIPS coprocessor instruction, and the addresses\nare generated by the MIPS CPU. VPU0 is called a vector processor, but it is similar\nto 128-bit SIMD extensions for multimedia found in several desktop processors\n(see Section E.2).\nVPU1, in contrast, fetches its own instructions and data and acts in parallel with\nCPU/VPU0, acting more like a traditional vector unit. With this split, the more\nflexible CPU/VPU0 handles the foreground action and the VPU1 handles the back-\nground. Both deposit their resulting display lists into the Graphics Interface to send\nthe lists to the Graphics Synthesizer.\nThus, the programmers of the Emotion Engine have three processor sets to\nchoose from to implement their programs: the traditional 64-bit MIPS architecture\nincluding a floating-point unit, the MIPS architecture extended with multimedia\ninstructions (VPU0), and an independent vector processor (VPU1). To accelerate\nMPEG decoding, there is another coprocessor (Image Processing Unit) that can act\nindependent of the other two.\nWith this split of function, the question then is how to connect the units\ntogether, how to make the data flow between units, and how to provide the\nmemory bandwidth needed by all these units. As mentioned earlier, the Emo-\ntion Engine designers chose many dedicated memories. The CPU has a 16 KB\nscratch pad memory (SPRAM) in addition to a 16 KB instruction cache and an\n8 KB data cache. VPU0 has a 4 KB instruction memory and a 4 KB data\nmemory, and VPU1 has a 16 KB instruction memory and a 16 KB data mem-\nory. Note that these are four memories, not caches of a larger memory else-\nwhere. In each memory the latency is just 1 clock cycle. VPU1 has more\nmemory than VPU0 because it creates the bulk of the display lists and because\nit largely acts independently.\nE.5\nCase Study: The Emotion Engine of the Sony PlayStation 2\n\u25a0\nE-17"
    },
    {
        "page": 939,
        "text": "The programmer organizes all memories as two double buffers, one pair for the\nincoming DMA data and one pair for the outgoing DMA data. The programmer\nthen uses the various processors to transform the data from the input buffer to\nthe output buffer. To keep the data flowing among the units, the programmer next\nsets up the 10 DMA channels, taking care to meet the real-time deadline for real-\nistic animation of 15 frames per second.\nFigure E.12 shows that this organization supports two main operating modes:\nserial, where CPU/VPU0 acts as a preprocessor on what to give VPU1 for it to\ncreate for the Graphics Interface using the scratchpad memory as the buffer,\nand parallel, where both the CPU/VPU0 and VPU1 create display lists. The display\nlists and the Graphics Synthesizer have multiple context identifiers to distinguish\nthe parallel display lists to produce a coherent final image.\nAll units in the Emotion Engine are linked by a common 150 MHz, 128-bit-\nwide bus. To offer greater bandwidth, there are also two dedicated buses: a\n128-bit path between the CPU and VPU0 and a 128-bit path between VPU1\nand the Graphics Interface. The programmer also chooses which bus to use when\nsetting up the DMA channels.\nLooking at the big picture, if a server-oriented designer had been given the\nproblem, we might see a single common bus with many local caches and cache-\ncoherent mechanisms to keep data consistent. In contrast, the PlayStation 2 fol-\nlowed the tradition of embedded designers and has at least nine distinct\nmemory modules. To keep the data flowing in real time from memory to the dis-\nplay, the PS2 uses dedicated memories, dedicated buses, and DMA channels.\nCoherency is the responsibility of the programmer, and, given the continuous flow\nfrom main memory to the graphics interface and the real-time requirements,\nprogrammer-controlled coherency works well for this application.\nVPU0\nSPRAM\nParallel connection\nVPU1\nRendering\nengine\nMain\nmemory\nCPU\nVPU0\nSPRAM\nSerial connection\nRendering\nengine\nCPU\nVPU1\nFigure E.12 Two modes of using Emotion Engine organization. The first mode\ndivides the work between the two units and then allows the Graphics Interface to prop-\nerly merge the display lists. The second mode uses CPU/VPU0 as a filter of what to send\nto VPU1, which then does all the display lists. It is up to the programmer to choose\nbetween serial and parallel data flow. SPRAM is the scratchpad memory.\nE-18\n\u25a0\nAppendix E Embedded Systems"
    },
    {
        "page": 940,
        "text": "E.6\nCase Study: Sanyo VPC-SX500 Digital Camera\nAnother very familiar embedded system is a digital camera. Here we consider the\nSanyo VPC-SX500. When powered on, the microprocessor of the camera first\nruns diagnostics on all components and writes any error messages to the liquid\ncrystal display (LCD) on the back of the camera. This camera uses a 1.8-inch\nlow-temperature polysilicon thin-film transistor (TFT) color LCD. When a pho-\ntographer takes a picture, he first holds the shutter halfway so that the micropro-\ncessor can take a light reading. The microprocessor then keeps the shutter open to\nget the necessary light, which is captured by a charge-coupled device (CCD) as\nred, green, and blue pixels. The CCD is a 1/2-inch, 1360 \u0003 1024-pixel,\nprogressive-scan chip. The pixels are scanned out row by row; passed through\nroutines for white balance, color, and aliasing correction; and then stored in a\n4 MB frame buffer. The next step is to compress the image into a standard format,\nsuch as JPEG, and store it in the removable Flash memory. The photographer\npicks the compression, in this camera called either fine or normal, with a com-\npression ratio of 10 to 20 times. A 512 MB Flash memory can store at least 1200\nfine-quality compressed images or approximately 2000 normal-quality com-\npressed images. The microprocessor then updates the LCD display to show that\nthere is room for one less picture.\nAlthough the previous paragraph covers the basics of a digital camera, there\nare many more features that are included: showing the recorded images on the\ncolor LCD display, sleep mode to save battery life, monitoring battery energy,\nbuffering to allow recording a rapid sequence of uncompressed images, and,\nin this camera, video recording using MPEG format and audio recording using\nWAV format.\nThe electronic brain of this camera is an embedded computer with several\nspecial functions embedded on the chip [Okada et al. 1999]. Figure E.13 shows\nthe block diagram of a chip similar to the one in the camera. As mentioned in\nSection E.1, such chips have been called systems on a chip (SOCs) because\nthey essentially integrate into a single chip all the parts that were found on\na small printed circuit board of the past. A SOC generally reduces size and\nlowers power compared to less integrated solutions. Sanyo claims their SOC\nenables the camera to operate on half the number of batteries and to offer a\nsmaller form factor than competitors\u2019 cameras. For higher performance, it\nhas two buses. The 16-bit bus is for the many slower I/O devices: SmartMedia\ninterface, program and data memory, and DMA. The 32-bit bus is for the\nSDRAM, the signal processor (which is connected to the CCD), the Motion\nJPEG encoder, and the NTSC/PAL encoder (which is connected to the\nLCD). Unlike desktop microprocessors, note the large variety of I/O buses that\nthis chip must integrate. The 32-bit RISC MPU is a proprietary design and runs\nat 28.8 MHz, the same clock rate as the buses. This 700 mW chip contains\n1.8M transistors in a 10.5 \u0003 10.5 mm die implemented using a 0.35-micron\nprocess.\nE.6\nCase Study: Sanyo VPC-SX500 Digital Camera\n\u25a0\nE-19"
    },
    {
        "page": 941,
        "text": "E.7\nCase Study: Inside a Cell Phone\nAlthough gaming consoles and digital cameras are familiar embedded systems,\ntoday the most familiar embedded system is the cell phone. In 1999, there were\n76 million cellular subscribers in the United States, a 25% growth rate from the\nyear before. That growth rate is almost 35% per year worldwide, as developing\ncountries find it much cheaper to install cellular towers than copper-wire-based\ninfrastructure. Thus, in many countries, the number of cell phones in use exceeds\nthe number of wired phones in use.\nNot surprisingly, the cellular handset market is growing at 35% per year, with\nabout 280 million cellular phone handsets sold worldwide in 1999. To put that in\nperspective, in the same year sales of personal computers were 120 million. These\nnumbers mean that tremendous engineering resources are available to improve cell\nphones, and cell phones are probably leaders in engineering innovation per cubic\ninch [Grice and Kanellos 2000].\nBefore unveiling the anatomy of a cell phone, let\u2019s try a short introduction to\nwireless technology.\nAudio\nD/A, A/D\nDRAM\ncontroller\nDMA\ncontroller\nPCMCIA\ncontroller\nUART\nx 2\nSIO\nPIO\nPWM\nIrDA\nCCD\nSDRAM\nDRAM\nSignal\nprocessor\nMJPEG\nNTSC/PAL\nencoder\n2-channel\nvideo D/A\nLCD/TV\nMIC\nSpeaker\nSmart\nMedia\nFlash\n(program)\nSSFDC\ncontroller\nRISC\nSDRAM\ncontroller\nRS-232\n16 bits\n16 bits\n32 bits\n10 bits\n16 bits\nSignal bus\nCPU bus\nBus bridge\nIrDA\nport\nPCMCIA\ncard\nOthers\nFigure E.13 The system on a chip (SOC) found in Sanyo digital cameras. This block diagram, found in Okada et al.\n[1999], is for the predecessor of the SOC in the camera described in the text. The successor SOC, called Super Advanced\nIC, uses three buses instead of two, operates at 60 MHz, consumes 800 mW, and fits 3.1M transistors in a 10.2 \u0003\n10.2 mm die using a 0.35-micron process. Note that this embedded system has twice as many transistors as the\nstate-of-the-art, high-performance microprocessor in 1990! The SOC in the figure is limited to processing 1024 \u0003\n768 pixels, but its successor supports 1360 \u0003 1024 pixels.\nE-20\n\u25a0\nAppendix E Embedded Systems"
    },
    {
        "page": 942,
        "text": "Background on Wireless Networks\nNetworks can be created out of thin air as well as out of copper and glass, creating\nwireless networks. Much of this section is based on a report from the National\nResearch Council [1997].\nA radio wave is an electromagnetic wave propagated by an antenna. Radio\nwaves are modulated, which means that the sound signal is superimposed on\nthe stronger radio wave that carries the sound signal, and hence is called the carrier\nsignal. Radio waves have a particular wavelength or frequency: They are measured\neither as the length of the complete wave or as the number of waves per second.\nLong waves have low frequencies, and short waves have high frequencies. FM\nradio stations transmit on the band of 88 MHz to 108 MHz using frequency mod-\nulations (FM) to record the sound signal.\nBy tuning in to different frequencies, a radio receiver can pick up a specific\nsignal. In addition to AM and FM radio, other frequencies are reserved for citizens\nband radio, television, pagers, air traffic control radar, Global Positioning System,\nand so on. In the United States, the Federal Communications Commission decides\nwho gets to use which frequencies and for what purpose.\nThe bit error rate (BER) of a wireless link is determined by the received signal\npower, noise due to interference caused by the receiver hardware, interference from\nother sources, and characteristics of the channel. Noise is typically proportional to\nthe radio frequency bandwidth, and a key measure is the signal-to-noise ratio\n(SNR) required to achieve a given BER. Figure E.14 lists more challenges for wire-\nless communication.\nTypically, wireless communication is selected because the communicating\ndevices are mobile or because wiring is inconvenient, which means the wireless\nnetwork must rearrange itself dynamically. Such rearrangement makes routing\nChallenge\nDescription\nImpact\nPath loss\nReceived power divided by transmitted power;\nthe radio must overcome signal-to-noise ratio\n(SNR) of noise from interference. Path loss is\nexponential in distance and depends on\ninterference if it is above 100 meters.\n1 W transmit power, 1 GHz transmit frequency,\n1 Mbit/sec data rate at 107 BER, distance\nbetween radios can be 728 meters in free space\nvs. 4 meters in a dense jungle.\nShadow fading\nReceived signal blocked by objects, buildings\noutdoors, or walls indoors; increase power to\nimprove received SNR. It depends on the number\nof objects and their dielectric properties.\nIf transmitter is moving, need to change transmit\npower to ensure received SNR in region.\nMultipath fading\nInterference between multiple versions of signal\nthat arrive at different times, determined by time\nbetween fastest signal and slowest signal relative\nto signal bandwidth.\n900 MHz transmit frequency signal power\nchanges every 30 cm.\nInterference\nFrequency reuse, adjacent channel, narrow band\ninterference.\nRequires filters, spread spectrum.\nFigure E.14 Challenges for wireless communication.\nE.7\nCase Study: Inside a Cell Phone\n\u25a0\nE-21"
    },
    {
        "page": 943,
        "text": "more challenging. A second challenge is that wireless signals are not protected and\nhence are subject to mutual interference, especially as devices move. Power is\nanother challenge for wireless communication, both because the devices tend to\nbe battery powered and because antennas radiate power to communicate and little\nof it reaches the receiver. As a result, raw bit error rates are typically a thousand to a\nmillion times higher than copper wire.\nThere are two primary architectures for wireless networks: base station archi-\ntectures and peer-to-peer architectures. Base stations are connected by landlines\nfor longer-distance communication, and the mobile units communicate only with\na single local base station. Peer-to-peer architectures allow mobile units to commu-\nnicate with each other, and messages hop from one unit to the next until delivered\nto the desired unit. Although peer-to-peer is more reconfigurable, base stations\ntend to be more reliable since there is only one hop between the device and the\nstation. Cellular telephony, the most popular example of wireless networks, relies\non radio with base stations.\nCellular systems exploit exponential path loss to reuse the same frequency at\nspatially separated locations, thereby greatly increasing the number of customers\nserved. Cellular systems will divide a city into nonoverlapping hexagonal cells that\nuse different frequencies if nearby, reusing a frequency only when cells are far\nenough apart so that mutual interference is acceptable.\nAt the intersection of three hexagonal cells is a base station with transmitters\nand antennas that is connected to a switching office that coordinates handoffs when\na mobile device leaves one cell and goes into another, as well as accepts and places\ncalls over landlines. Depending on topography, population, and so on, the radius of\na typical cell is 2 to 10 miles.\nThe Cell Phone\nFigure E.15 shows the components of a radio, which is the heart of a cell phone.\nRadio signals are first received by the antenna, amplified, passed through a mixer,\nthen filtered, demodulated, and finally decoded. The antenna acts as the interface\nbetween the medium through which radio waves travel and the electronics of the\ntransmitter or receiver. Antennas can be designed to work best in particular direc-\ntions, giving both transmission and reception directional properties. Modulation\nencodes information in the amplitude, phase, or frequency of the signal to increase\nits robustness under impaired conditions. Radio transmitters go through the same\nsteps, just in the opposite order.\nOriginally, all components were analog, but over time most were replaced by\ndigital components, requiring the radio signal to be converted from analog to dig-\nital. The desire for flexibility in the number of radio bands led to software routines\nreplacing some of these functions in programmable chips, such as digital signal\nprocessors. Because such processors are typically found in mobile devices, empha-\nsis is placed on performance per joule to extend battery life, performance per\nsquare millimeter of silicon to reduce size and cost, and bytes per task to reduce\nmemory size.\nE-22\n\u25a0\nAppendix E Embedded Systems"
    },
    {
        "page": 944,
        "text": "Figure E.16 shows the generic block diagram of the electronics of a cell phone\nhandset, with the DSP performing the signal processing and the microcontroller\nhandling the rest of the tasks. Cell phone handsets are basically mobile computers\nacting as a radio. They include standard I/O devices\u2014keyboard and LCD dis-\nplay\u2014plus a microphone, speaker, and antenna for wireless networking. Battery\nefficiency affects sales, both for standby power when waiting for a call and for\nminutes of speaking.\nWhen a cell phone is turned on, the first task is to find a cell. It scans the full\nbandwidth to find the strongest signal, which it keeps doing every seven seconds\nor if the signal strength drops, since it is designed to work from moving vehicles.\nIt then picks an unused radio channel. The local switching office registers the cell\nphone and records its phone number and electronic serial number, and assigns it\na voice channel for the phone conversation. To be sure the cell phone got the right\nchannel, the base station sends a special tone on it, which the cell phone sends back to\nacknowledge it. The cell phone times out after 5 seconds if it doesn\u2019t hear the super-\nvisory tone, and it starts the process all over again. The original base station makes a\nhandoff request to the incoming base station as the signal strength drops offs.\nRF amp\nFilter\nAntenna\nDemodulator\nDecoder\nMixer\nFigure E.15 A radio receiver consists of an antenna, radio frequency amplifier, mixer,\nfilters, demodulator, and decoder. A mixer accepts two signal inputs and forms an out-\nput signal at the sum and difference frequencies. Filters select a narrower band of fre-\nquencies to pass on to the next stage. Modulation encodes information to make it more\nrobust. Decoding turns signals into information. Depending on the application, all elec-\ntrical components can be either analog or digital. For example, a car radio is all analog\ncomponents, but a PC modem is all digital except for the amplifier. Today analog silicon\nchips are used for the RF amplifier and first mixer in cellular phones.\nSpeaker\nMicrophone\nDSP\nMicro-\ncontroller\nAntenna\nRF receiver (Rx)\nRF transmitter (Tx)\nDisplay\nKeyboard\nFigure E.16 Block diagram of a cell phone. The DSP performs the signal processing\nsteps of Figure E.15, and the microcontroller controls the user interface, battery manage-\nment, and call setup. (Based on Figure 1.3 of Groe and Larson [2000].)\nE.7\nCase Study: Inside a Cell Phone\n\u25a0\nE-23"
    },
    {
        "page": 945,
        "text": "To achieve a two-way conversation over radio, frequency bands are set aside\nfor each direction, forming a frequency pair or channel. The original cellular base\nstations transmitted at 869.04 to 893.97 MHz (called the forward path), and cell\nphones transmitted at 824.04 to 848.97 MHz (called the reverse path), with the\nfrequency gap to keep them from interfering with each other. Cells might have\nhad between 4 and 80 channels. Channels were divided into setup channels for call\nsetup and voice channels to handle the data or voice traffic.\nThe communication is done digitally, just like a modem, at 9600 bits/sec. Since\nwireless is a lossy medium, especially from a moving vehicle, the handset sends\neach message five times. To preserve battery life, the original cell phones typically\ntransmit at two signal strengths\u20140.6 W and 3.0 W\u2014depending on the distance to\nthe cell. This relatively low power not only allows smaller batteries and thus smal-\nler cell phones, but it also aids frequency reuse, which is the key to cellular\ntelephony.\nFigure E.17 shows a circuit board from a Nokia digital phone, with the com-\nponents identified. Note that the board contains two processors. A Z-80 microcon-\ntroller is responsible for controlling the functions of the board, I/O with the\nkeyboard and display, and coordinating with the base station. The DSP handles\nall signal compression and decompression. In addition there are dedicated chips\nfor analog-to-digital and digital-to-analog conversion, amplifiers, power manage-\nment, and RF interfaces.\nIn 2001, a cell phone had about 10 integrated circuits, including parts made in\nexotic technologies like gallium arsinide and silicon germanium as well as standard\nCMOS. The economics and desire for flexibility have shrunk this to just a few\nchips. However, these SOCs still contain a separate microcontroller and DSP, with\ncode implementing many of the functions just described.\nRF and\npower\nAudio D/A\nand A/D\nBattery\nMemory\nMicroprocessor\nand control logic\nFigure E.17 Circuit board from a Nokia cell phone. (Courtesy HowStuffWorks, Inc.)\nE-24\n\u25a0\nAppendix E Embedded Systems"
    },
    {
        "page": 946,
        "text": "Cell Phone Standards and Evolution\nImproved communication speeds for cell phones were developed with multiple\nstandards. Code division multiple access (CDMA), as one popular example, uses\na wider radio frequency band for a path than the original cell phones, called\nadvanced mobile phone service (AMPS), a mostly analog system. The wider fre-\nquency makes it more difficult to block and is called spread spectrum. Other stan-\ndards are time division multiple access (TDMA) and global system for mobile\ncommunication (GSM). These second-generation standards\u2014CDMA, GSM,\nand TDMA\u2014are mostly digital.\nThe big difference for CDMA is that all callers share the same channel, which\noperates at a much higher rate, and it then distinguishes the different calls by\nencoding each one uniquely. Each CDMA phone call starts at 9600 bits/sec; it\nis then encoded and transmitted as equal-sized messages at 1.25 Mbits/sec. Rather\nthan send each signal five times as in AMPS, each bit is stretched so that it takes 11\ntimes the minimum frequency, thereby accommodating interference and yet suc-\ncessful transmission. The base station receives the messages, and it separates them\ninto the separate 9600 bit/sec streams for each call.\nTo enhance privacy, CDMA uses pseudorandom sequences from a set of 64\npredefined codes. To synchronize the handset and base station so as to pick a com-\nmon pseudorandom seed, CDMA relies on a clock from the Global Positioning\nSystem, which continuously transmits an accurate time signal. By carefully select-\ning the codes, the shared traffic sounds like random noise to the listener. Hence, as\nmore users share a channel there is more noise, and the signal-to-noise ratio grad-\nually degrades. Thus, the capacity of the CDMA system is a matter of taste,\ndepending upon the sensitivity of the listener to background noise.\nIn addition, CDMA uses speech compression and varies the rate of data trans-\nferred depending upon how much activity is going on in the call. Both these tech-\nniques preserve bandwidth, which allows for more calls per cell. CDMA must\nregulate power carefully so that signals near the cell tower do not overwhelm those\nfrom far away, with the goal of all signals reaching the tower at about the same\nlevel. The side benefit is that CDMA handsets emit less power, which both helps\nbattery life and increases capacity when users are close to the tower.\nThus, compared to AMPS, CDMA improves the capacity of a system by up to\nan order of magnitude, has better call quality, has better battery life, and enhances\nusers\u2019 privacy. After considerable commercial turmoil, there is a new third-\ngeneration standard called International Mobile Telephony 2000 (IMT-2000),\nbased primarily on two competing versions of CDMA and one TDMA. This stan-\ndard may lead to cell phones that work anywhere in the world.\nE.8\nConcluding Remarks\nEmbedded systems are a very broad category of computing devices. This appendix\nhas shown just some aspects of this. For example, the TI 320C55 DSP is a rela-\ntively \u201cRISC-like\u201d processor designed for embedded applications, with very\nE.8\nConcluding Remarks\n\u25a0\nE-25"
    },
    {
        "page": 947,
        "text": "fine-tuned capabilities. On the other end of the spectrum, the TI 320C64x is a very\nhigh-performance, eight-issue VLIW processor for very demanding tasks. Some\nprocessors must operate on battery power alone; others have the luxury of being\nplugged into line current. Unifying all of these is a need to perform some level of\nsignal processing for embedded applications. Media extensions attempt to merge\nDSPs with some more general-purpose processing abilities to make these proces-\nsors usable for signal processing applications. We examined several case studies,\nincluding the Sony PlayStation 2, digital cameras, and cell phones. The PS2 per-\nforms detailed three-dimensional graphics, whereas a cell phone encodes and\ndecodes signals according to elaborate communication standards. But both have\nsystem architectures that are very different from general-purpose desktop or server\nplatforms. In general, architectural decisions that seem practical for general-\npurpose applications, such as multiple levels of caching or out-of-order superscalar\nexecution, are much less desirable in embedded applications. This is due to chip\narea, cost, power, and real-time constraints. The programming model that these\nsystems present places more demands on both the programmer and the compiler\nfor extracting parallelism.\nE-26\n\u25a0\nAppendix E Embedded Systems"
    },
    {
        "page": 948,
        "text": "F.1\nIntroduction\nF-2\nF.2\nInterconnecting Two Devices\nF-6\nF.3\nConnecting More Than Two Devices\nF-20\nF.4\nNetwork Topology\nF-30\nF.5\nNetwork Routing, Arbitration, and Switching\nF-44\nF.6\nSwitch Microarchitecture\nF-56\nF.7\nPractical Issues for Commercial Interconnection Networks\nF-66\nF.8\nExamples of Interconnection Networks\nF-73\nF.9\nInternetworking\nF-85\nF.10\nCrosscutting Issues for Interconnection Networks\nF-89\nF.11\nFallacies and Pitfalls\nF-92\nF.12\nConcluding Remarks\nF-100\nF.13\nHistorical Perspective and References\nF-101\nReferences\nF-109\nExercises\nF-111"
    },
    {
        "page": 949,
        "text": "F\nInterconnection Networks\nRevised by Timothy M. Pinkston, University of Southern California;\nJos\u0001e Duato, Universitat Polit\u00e8cnica de Val\u00e8ncia, and Simula\n\u201cThe Medium is the Message\u201d because it is the medium that shapes\nand controls the search and form of human associations and actions.\nMarshall McLuhan\nUnderstanding Media (1964)\nThe marvels\u2014of film, radio, and television\u2014are marvels of one-\nway communication, which is not communication at all.\nMilton Mayer\nOn the Remote Possibility of\nCommunication (1967)\nThe interconnection network is the heart of parallel architecture.\nChuan-Lin Wu and Tse-Yun Feng\nInterconnection Networks for Parallel\nand Distributed Processing (1984)\nIndeed, as system complexity and integration continues to\nincrease, many designers are finding it more efficient to route\npackets, not wires.\nBill Dally\nPrinciples and Practices of\nInterconnection Networks (2004)"
    },
    {
        "page": 950,
        "text": "F.1\nIntroduction\nPrevious chapters and appendices cover the components of a single computer but\ngive little consideration to the interconnection of those components and how mul-\ntiple computer systems are interconnected. These aspects of computer architecture\nhave gained significant importance in recent years. In this appendix we see how to\nconnect individual devices together into a community of communicating devices,\nwhere the term device is generically used to signify anything from a component or\nset of components within a computer to a single computer to a system of com-\nputers. Figure F.1 shows the various elements comprising this community: end\nnodes consisting of devices and their associated hardware and software interfaces,\nlinks from end nodes to the interconnection network, and the interconnection net-\nwork. Interconnection networks are also called networks, communication subnets,\nor communication subsystems. The interconnection of multiple networks is called\ninternetworking. This relies on communication standards to convert information\nfrom one kind of network to another, such as with the Internet.\nThere are several reasons why computer architects should devote attention to\ninterconnection networks. In addition to providing external connectivity, networks\nare commonly used to interconnect the components within a single computer at\nmany levels, including the processor microarchitecture. Networks have long been\nused in mainframes, but today such designs can be found in personal computers as\nwell, given the high demand on communication bandwidth needed to enable\nincreased computing power and storage capacity. Switched networks are replacing\nbuses as the normal means of communication between computers, between I/O\ndevices, between boards, between chips, and even between modules inside chips.\nComputer architects must understand interconnect problems and solutions in order\nto more effectively design and evaluate computer systems.\nInterconnection networks cover a wide range of application domains, very\nmuch like memory hierarchy covers a wide range of speeds and sizes. Networks\nimplemented within processor chips and systems tend to share characteristics\nmuch in common with processors and memory, relying more on high-speed hard-\nware solutions and less on a flexible software stack. Networks implemented across\nsystems tend to share much in common with storage and I/O, relying more on the\noperating system and software protocols than high-speed hardware\u2014though we\nare seeing a convergence these days. Across the domains, performance includes\nlatency and effective bandwidth, and queuing theory is a valuable analytical tool\nin evaluating performance, along with simulation techniques.\nThis topic is vast\u2014portions of Figure F.1 are the subject of entire books and\ncollege courses. The goal of this appendix is to provide for the computer architect\nan overview of network problems and solutions. This appendix gives introductory\nexplanations of key concepts and ideas, presents architectural implications of inter-\nconnection network technology and techniques, and provides useful references to\nmore detailed descriptions. It also gives a common framework for evaluating all\ntypes of interconnection networks, using a single set of terms to describe the basic\nF-2\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 951,
        "text": "alternatives. As we will see, many types of networks have common preferred alter-\nnatives, but for others the best solutions are quite different. These differences\nbecome very apparent when crossing between the networking domains.\nInterconnection Network Domains\nInterconnection networks are designed for use at different levels within and across\ncomputer systems to meet the operational demands of various application areas\u2014\nhigh-performance computing, storage I/O, cluster/workgroup/enterprise systems,\ninternetworking, and so on. Depending on the number of devices to be connected\nand their proximity, we can group interconnection networks into four major net-\nworking domains:\n\u25a0\nOn-chip networks (OCNs)\u2014Also referred to as network-on-chip (NoC), this\ntype of network is used for interconnecting microarchitecture functional units,\nregister files, caches, compute tiles, and processor and IP cores within chips or\nmultichip modules. Current and near future OCNs support the connection of a\nfew tens to a few hundred of such devices with a maximum interconnection\ndistance on the order of centimeters. Most OCNs used in high-performance\nchips are custom designed to mitigate chip-crossing wire delay problems\ncaused by increased technology scaling and transistor integration, though some\nproprietary designs are gaining wider use (e.g., IBM\u2019s CoreConnect, ARM\u2019s\nAMBA, and Sonic\u2019s Smart Interconnect). Examples of current OCNs are those\nfound in the Intel Teraflops processor chip [Hoskote07], connecting 80 simple\ncores; the Intel Single-Chip Cloud Computer (SCCC) [Howard10], connecting\n48 IA-32 architecture cores; and Tilera\u2019s TILE-Gx line of processors [TILE-\nGX], connecting 100 processing cores in 4Q 2011 using TSMC\u2019s 40 nanome-\nter process and 200 cores planned for 2013 (code named \u201cStratton\u201d) using\nDevice\nLink\nSW interface\nEnd node\nHW interface\nDevice\nLink\nSW interface\nEnd node\nHW interface\nDevice\nLink\nSW interface\nEnd node\nHW interface\nDevice\nLink\nSW interface\nEnd node\nHW interface\nInterconnection network\nFigure F.1 A conceptual illustration of an interconnected community of devices.\nF.1\nIntroduction\n\u25a0\nF-3"
    },
    {
        "page": 952,
        "text": "TSMC\u2019s 28 nanometer process. The networks peak at 256 GBps for both Intel\nprototypes and up to 200 Tbps for the TILE-Gx100 processor. More detailed\ninformation for OCNs is provided in Flich [2010].\n\u25a0\nSystem/storage area networks (SANs)\u2014This type of network is used for inter-\nprocessor and processor-memory interconnections within multiprocessor and\nmulticomputer systems, and also for the connection of storage and I/O compo-\nnents within server and data center environments. Typically, several hundreds\nof such devices can be connected, although some supercomputer SANs support\nthe interconnection of many thousands of devices, like the IBM Blue Gene/L\nsupercomputer. The maximum interconnection distance covers a relatively\nsmall area\u2014on the order of a few tens of meters usually\u2014but some SANs have\ndistances spanning a few hundred meters. For example, InfiniBand, a popular\nSAN standard introduced in late 2000, supports system and storage I/O inter-\nconnects at up to 120 Gbps over a distance of 300 m.\n\u25a0\nLocal area networks (LANs)\u2014This type of network is used for intercon-\nnecting autonomous computer systems distributed across a machine room\nor throughout a building or campus environment. Interconnecting PCs in\na cluster is a prime example. Originally, LANs connected only up to a hun-\ndred devices, but with bridging LANs can now connect up to a few thou-\nsand devices. The maximum interconnect distance covers an area of a few\nkilometers usually, but some have distance spans of a few tens of kilome-\nters. For instance, the most popular and enduring LAN, Ethernet, has a 10\nGbps standard version that supports maximum performance over a distance\nof 40 km.\n\u25a0\nWide area networks (WANs)\u2014Also called long-haul networks, WANs con-\nnect computer systems distributed across the globe, which requires internet-\nworking support. WANs connect many millions of computers over distance\nscales of many thousands of kilometers. Asynchronous Transfer Mode\n(ATM) is an example of a WAN.\nFigure F.2 roughly shows the relationship of these networking domains in\nterms of the number of devices interconnected and their distance scales. Overlap\nexists for some of these networks in one or both dimensions, which leads to\nproduct competition. Some network solutions have become commercial stan-\ndards while others remain proprietary. Although the preferred solutions may sig-\nnificantly differ from one interconnection network domain to another depending\non the design requirements, the problems and concepts used to address network\nproblems remain remarkably similar across the domains. No matter the target\ndomain, networks should be designed so as not to be the bottleneck to system\nperformance and cost efficiency. Hence, the ultimate goal of computer architects\nis to design interconnection networks of the lowest possible cost that are capable\nof transferring the maximum amount of available information in the shortest\npossible time.\nF-4\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 953,
        "text": "Approach and Organization of This Appendix\nInterconnection networks can be well understood by taking a top-down approach\nto unveiling the concepts and complexities involved in designing them. We do this\nby viewing the network initially as an opaque \u201cblack box\u201d that simply and ideally\nperforms certain necessary functions. Then we systematically open various layers\nof the black box, allowing more complex concepts and nonideal network behavior\nto be revealed. We begin this discussion by first considering the interconnection of\njust two devices in Section F.2, where the black box network can be viewed as a\nsimple dedicated link network\u2014that is, wires or collections of wires running bidi-\nrectionally between the devices. We then consider the interconnection of more than\ntwo devices in Section F.3, where the black box network can be viewed as a shared\nlink network or as a switched point-to-point network connecting the devices. We\ncontinue to peel away various other layers of the black box by considering in more\ndetail the network topology (Section F.4); routing, arbitration, and switching\n(Section F.5); and switch microarchitecture (Section F.6). Practical issues for com-\nmercial networks are considered in Section F.7, followed by examples illustrating\nthe trade-offs for each type of network in Section F.8. Internetworking is briefly\ndiscussed in Section F.9, and additional crosscutting issues for interconnection net-\nworks are presented in Section F.10. Section F.11 gives some common fallacies\n1\n10\n100\n1000\nNumber of devices interconnected\nSAN\nOCN\nLAN\nWAN\n10,000\n>100,000\nDistance (meters)\n5 \u00d7 106\n5 \u00d7 103\n5 \u00d7 100\n5 \u00d7 10\u20133\nFigure F.2 Relationship of the four interconnection network domains in terms of\nnumber of devices connected and their distance scales: on-chip network (OCN), sys-\ntem/storage area network (SAN), local area network (LAN), and wide area\nnetwork (WAN). Note that there are overlapping ranges where some of these networks\ncompete. Some supercomputer systems use proprietary custom networks to intercon-\nnect several thousands of computers, while other systems, such as multicomputer clus-\nters, use standard commercial networks.\nF.1\nIntroduction\n\u25a0\nF-5"
    },
    {
        "page": 954,
        "text": "and pitfalls related to interconnection networks, and Section F.12 presents some\nconcluding remarks. Finally, we provide a brief historical perspective and some\nsuggested reading in Section F.13.\nF.2\nInterconnecting Two Devices\nThis section introduces the basic concepts required to understand how communi-\ncation between just two networked devices takes place. This includes concepts that\ndeal with situations in which the receiver may not be ready to process incoming\ndata from the sender and situations in which transport errors may occur. To ease\nunderstanding, the black box network at this point can be conceptualized as an\nideal network that behaves as simple dedicated links between the two devices.\nFigure F.3 illustrates this, where unidirectional wires run from device A to device\nB and vice versa, and each end node contains a buffer to hold the data. Regardless\nof the network complexity, whether dedicated link or not, a connection exists from\neach end node device to the network to inject and receive information to/from the\nnetwork. We first describe the basic functions that must be performed at the end\nnodes to commence and complete communication, and then we discuss network\nmedia and the basic functions that must be performed by the network to carry\nout communication. Later, a simple performance model is given, along with sev-\neral examples to highlight implications of key network parameters.\nNetwork Interface Functions: Composing and Processing\nMessages\nSuppose we want two networked devices to read a word from each other\u2019s mem-\nory. The unit of information sent or received is called a message. To acquire the\ndesired data, the two devices must first compose and send a certain type of message\nin the form of a request containing the address of the data within the other device.\nThe address (i.e., memory or operand location) allows the receiver to identify\nwhere to find the information being requested. After processing the request, each\ndevice then composes and sends another type of message, a reply, containing the\ndata. The address and data information is typically referred to as the message\npayload.\nB\n \ne\nnih\nc\na\nM\nA\n \ne\nnih\nc\na\nM\nFigure F.3 A simple dedicated link network bidirectionally interconnecting two\ndevices.\nF-6\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 955,
        "text": "In addition to payload, every message contains some control bits needed by the\nnetwork to deliver the message and process it at the receiver. The most typical are\nbits to distinguish between different types of messages (e.g., request, reply, request\nacknowledge, reply acknowledge) and bits that allow the network to transport the\ninformation properly to the destination. These additional control bits are encoded\nin the header and/or trailer portions of the message, depending on their location\nrelative to the message payload. As an example, Figure F.4 shows the format of a\nmessage for the simple dedicated link network shown in Figure F.3. This example\nshows a single-word payload, but messages in some interconnection networks can\ninclude several thousands of words.\nBefore message transport over the network occurs, messages have to be com-\nposed. Likewise, upon receipt from the network, they must be processed. These\nand other functions described below are the role of the network interface (also\nreferred to as the channel adapter) residing at the end nodes. Together with some\ndirect memory access (DMA) engine and link drivers to transmit/receive messages\nto/from the network, some dedicated memory or register(s) may be used to buffer\noutgoing and incoming messages. Depending on the network domain and design\nspecifications for the network, the network interface hardware may consist of noth-\ning more than the communicating device itself (i.e., for OCNs and some SANs) or\na separate card that integrates several embedded processors and DMA engines with\nthousands of megabytes of RAM (i.e., for many SANs and most LANs\nand WANs).\nIn addition to hardware, network interfaces can include software or firmware to\nperform the needed operations. Even the simple example shown in Figure F.3 may\ninvoke messaging software to translate requests and replies into messages with the\nappropriate headers. This way, user applications need not worry about composing\nand processing messages as these tasks can be performed automatically at a lower\nlevel. An application program usually cooperates with the operating or runtime\nDestination port\nMessage ID\nData\nSequence number \nType\n00 = Request\n01 = Reply\n10 = Request acknowledge\n11 = Reply acknowledge\nChecksum\nHeader\nPayload\nTrailer\nFigure F.4 An example packet format with header, payload, and checksum in the\ntrailer.\nF.2\nInterconnecting Two Devices\n\u25a0\nF-7"
    },
    {
        "page": 956,
        "text": "system to send and receive messages. As the network is likely to be shared by many\nprocesses running on each device, the operating system cannot allow messages\nintended for one process to be received by another. Thus, the messaging software\nmust include protection mechanisms that distinguish between processes. This dis-\ntinction could be made by expanding the header with a port number that is known\nby both the sender and intended receiver processes.\nIn addition to composing and processing messages, additional functions need\nto be performed by the end nodes to establish communication among the commu-\nnicating devices. Although hardware support can reduce the amount of work, some\ncan be done by software. For example, most networks specify a maximum amount\nof information that can be transferred (i.e., maximum transfer unit) so that network\nbuffers can be dimensioned appropriately. Messages longer than the maximum\ntransfer unit are divided into smaller units, called packets (or datagrams), that\nare transported over the network. Packets are reassembled into messages at the des-\ntination end node before delivery to the application. Packets belonging to the same\nmessage can be distinguished from others by including a message ID field in the\npacket header. If packets arrive out of order at the destination, they are reordered\nwhen reassembled into a message. Another field in the packet header containing a\nsequence number is usually used for this purpose.\nThe sequence of steps the end node follows to commence and complete com-\nmunication over the network is called a communication protocol. It generally has\nsymmetric but reversed steps between sending and receiving information. Commu-\nnication protocols are implemented by a combination of software and hardware to\naccelerate execution. For instance, many network interface cards implement hard-\nware timers as well as hardware support to split messages into packets and reas-\nsemble them, compute the cyclic redundancy check (CRC) checksum, handle\nvirtual memory addresses, and so on.\nSome network interfaces include extra hardware to offload protocol processing\nfrom the host computer, such as TCP offload engines for LANs and WANs. But,\nfor interconnection networks such as SANs that have low latency requirements,\nthis may not be enough even when lighter-weight communication protocols are\nused such as message passing interface (MPI). Communication performance\ncan be further improved by bypassing the operating system (OS). OS bypassing\ncan be implemented by directly allocating message buffers in the network interface\nmemory so that applications directly write into and read from those buffers. This\navoids extra memory-to-memory copies. The corresponding protocols are referred\nto as zero-copy protocols or user-level communication protocols. Protection can\nstill be maintained by calling the OS to allocate those buffers at initialization\nand preventing unauthorized memory accesses in hardware.\nIn general, some or all of the following are the steps needed to send a message\nat end node devices over a network:\n1. The application executes a system call, which copies data to be sent into an\noperating system or network interface buffer, divides the message into packets\n(if needed), and composes the header and trailer for packets.\nF-8\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 957,
        "text": "2. The checksum is calculated and included in the header or trailer of packets.\n3. The timer is started, and the network interface hardware sends the packets.\nMessage reception is in the reverse order:\n3. The network interface hardware receives the packets and puts them into its\nbuffer or the operating system buffer.\n2. The checksum is calculated for each packet. If the checksum matches the\nsender\u2019s checksum, the receiver sends an acknowledgment back to the packet\nsender. If not, it deletes the packet, assuming that the sender will resend the\npacket when the associated timer expires.\n1. Once all packets pass the test, the system reassembles the message, copies the\ndata to the user\u2019s address space, and signals the corresponding application.\nThe sender must still react to packet acknowledgments:\n\u25a0\nWhen the sender gets an acknowledgment, it releases the copy of the corre-\nsponding packet from the buffer.\n\u25a0\nIf the sender reaches the time-out instead of receiving an acknowledgment, it\nresends the packet and restarts the timer.\nJust as a protocol is implemented at network end nodes to support communi-\ncation, protocols are also used across the network structure at the physical, data\nlink, and network layers responsible primarily for packet transport, flow control,\nerror handling, and other functions described next.\nBasic Network Structure and Functions: Media and Form Factor,\nPacket Transport, Flow Control, and Error Handling\nOnce a packet is ready for transmission at its source, it is injected into the network\nusing some dedicated hardware at the network interface. The hardware includes\nsome transceiver circuits to drive the physical network media\u2014either electrical\nor optical. The type of media and form factor depends largely on the interconnect\ndistances over which certain signaling rates (e.g., transmission speed) should be\nsustainable. For centimeter or less distances on a chip or multichip module, typi-\ncally the middle to upper copper metal layers can be used for interconnects at multi-\nGbps signaling rates per line. A dozen or more layers of copper traces or tracks\nimprinted on circuit boards, midplanes, and backplanes can be used for Gbps\ndifferential-pair signaling rates at distances of about a meter or so. Category 5E\nunshielded twisted-pair copper wiring allows 0.25 Gbps transmission speed over\ndistances of 100 meters. Coaxial copper cables can deliver 10 Mbps over kilometer\ndistances. In these conductor lines, distance can usually be traded off for higher\ntransmission speed, up to a certain point. Optical media enable faster transmission\nF.2\nInterconnecting Two Devices\n\u25a0\nF-9"
    },
    {
        "page": 958,
        "text": "speeds at distances of kilometers. Multimode fiber supports 100 Mbps transmis-\nsion rates over a few kilometers, and more expensive single-mode fiber supports\nGbps transmission speeds over distances of several kilometers. Wavelength divi-\nsion multiplexing allows several times more bandwidth to be achieved in fiber (i.e.,\nby a factor of the number of wavelengths used).\nThe hardware used to drive network links may also include some encoders to\nencode the signal in a format other than binary that is suitable for the given trans-\nport distance. Encoding techniques can use multiple voltage levels, redundancy,\ndata and control rotation (e.g., 4b5b encoding), and/or a guaranteed minimum\nnumber of signal transitions per unit time to allow for clock recovery at the\nreceiver. The signal is decoded at the receiver end, and the packet is stored in\nthe corresponding buffer. All of these operations are performed at the network\nphysical layer, the details of which are beyond the scope of this appendix. Fortu-\nnately, we do not need to worry about them. From the perspective of the data link\nand higher layers, the physical layer can be viewed as a long linear pipeline without\nstaging in which signals propagate as waves through the network transmission\nmedium. All of the above functions are generally referred to as packet transport.\nBesides packet transport, the network hardware and software are jointly\nresponsible at the data link and network protocol layers for ensuring reliable\ndelivery of packets. These responsibilities include: (1) preventing the sender\nfrom sending packets at a faster rate than they can be processed by the receiver,\nand (2) ensuring that the packet is neither garbled nor lost in transit. The first\nresponsibility is met by either discarding packets at the receiver when its buffer\nis full and later notifying the sender to retransmit them, or by notifying the sender\nto stop sending packets when the buffer becomes full and to resume later once it has\nroom for more packets. The latter strategy is generally known as flow control.\nThere are several interesting techniques commonly used to implement flow\ncontrol beyond simple handshaking between the sender and receiver. The more\npopular techniques are Xon/Xoff (also referred to as Stop & Go) and credit-based\nflow control. Xon/Xoff consists of the receiver notifying the sender either to stop or\nto resume sending packets once high and low buffer occupancy levels are reached,\nrespectively, with some hysteresis to reduce the number of notifications. Notifica-\ntions are sent as \u201cstop\u201d and \u201cgo\u201d signals using additional control wires or encoded\nin control packets. Credit-based flow control typically uses a credit counter at the\nsender that initially contains a number of credits equal to the number of buffers at\nthe receiver. Every time a packet is transmitted, the sender decrements the credit\ncounter. When the receiver consumes a packet from its buffer, it returns a credit to\nthe sender in the form of a control packet that notifies the sender to increment its\ncounter upon receipt of the credit. These techniques essentially control the flow of\npackets into the network by throttling packet injection at the sender when the\nreceiver reaches a low watermark or when the sender runs out of credits.\nXon/Xoff usually generates much less control traffic than credit-based flow\ncontrol because notifications are only sent when the high or low buffer occupancy\nlevels are crossed. On the other hand, credit-based flow control requires less than\nhalf the buffer size required by Xon/Xoff. Buffers for Xon/Xoff must be large\nF-10\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 959,
        "text": "enough to prevent overflow before the \u201cstop\u201d control signal reaches the sender.\nOverflow cannot happen when using credit-based flow control because the sender\nwill run out of credits, thus stopping transmission. For both schemes, full link\nbandwidth utilization is possible only if buffers are large enough for the distance\nover which communication takes place.\nLet\u2019s compare the buffering requirements of the two flow control techniques in\na simple example covering the various interconnection network domains.\nExample\nSuppose we have a dedicated-link network with a raw data bandwidth of 8 Gbps\nfor each link in each direction interconnecting two devices. Packets of 100 bytes\n(including the header) are continuously transmitted from one device to the other to\nfully utilize network bandwidth. What is the minimum amount of credits and buffer\nspace required by credit-based flow control assuming interconnect distances of\n1 cm, 1 m, 100 m, and 10 km if only link propagation delay is taken into account?\nHow does the minimum buffer space compare against Xon/Xoff?\nAnswer\nAt the start, the receiver buffer is initially empty and the sender contains a number\nof credits equal to buffer capacity. The sender will consume a credit every time a\npacket is transmitted. For the sender to continue transmitting packets at network\nspeed, the first returned credit must reach the sender before the sender runs out\nof credits. After receiving the first credit, the sender will keep receiving credits\nat the same rate it transmits packets. As we are considering only propagation delay\nover the link and no other sources of delay or overhead, null processing time at the\nsender and receiver are assumed. The time required for the first credit to reach the\nsender since it started transmission of the first packet is equal to the round-trip\npropagation delay for the packet transmitted to the receiver and the return credit\ntransmitted back to the sender. This time must be less than or equal to the packet\ntransmission time multiplied by the initial credit count:\nPacket propagation delay + Credit propagation delay \u0001 Packet size\nBandwidth \u0003Credit count\nThe speed of light is about 300,000 km/sec. Assume we can achieve 66% of that in\na conductor. Thus, the minimum number of credits for each distance is given by\nDistance\n2=3\u0003300,000 km=sec\n\u0001\n\u0003\n\u00032 \u0001 100 bytes\n8 Gbits=sec \u0003Credit count\nAs each credit represents one packet-sized buffer entry, the minimum amount of\ncredits (and, likewise, buffer space) needed by each device is one for the 1 cm and\n1 m distances, 10 for the 100 m distance, and 1000 packets for the 10 km distance.\nFor Xon/Xoff, this minimum buffer size corresponds to the buffer fragment from\nthe high occupancy level to the top of the buffer and from the low occupancy level\nto the bottom of the buffer. With the added hysteresis between both occupancy\nlevels to reduce notifications, the minimum buffer space for Xon/Xoff turns out\nto be more than twice that for credit-based flow control.\nF.2\nInterconnecting Two Devices\n\u25a0\nF-11"
    },
    {
        "page": 960,
        "text": "Networks that implement flow control do not need to drop packets and are\nsometimes referred to as lossless networks; networks that drop packets are some-\ntimes referred to as lossy networks. This single difference in the way packets are\nhandled by the network drastically constrains the kinds of solutions that can be\nimplemented to address other related network problems, including packet routing,\ncongestion, deadlock, and reliability, as we will see later in this appendix. This\ndifference also affects performance significantly as dropped packets need to be\nretransmitted, thus consuming more link bandwidth and suffering extra delay.\nThese behavioral and performance differences ultimately restrict the interconnec-\ntion network domains for which certain solutions are applicable. For instance, most\nnetworks delivering packets over relatively short distances (e.g., OCNs and SANs)\ntend to implement flow control; on the other hand, networks delivering packets\nover relatively long distances (e.g., LANs and WANs) tend to be designed to drop\npackets. For the shorter distances, the delay in propagating flow control informa-\ntion back to the sender can be negligible, but not so for longer distance scales. The\nkinds of applications that are usually run also influence the choice of lossless ver-\nsus lossy networks. For instance, dropping packets sent by an Internet client like a\nWeb browser affects only the delay observed by the corresponding user. However,\ndropping a packet sent by a process from a parallel application may lead to a sig-\nnificant increase in the overall execution time of the application if that packet\u2019s\ndelay is on the critical path.\nThe second responsibility of ensuring that packets are neither garbled nor lost\nin transit can be met by implementing some mechanisms to detect and recover from\ntransport errors. Adding a checksum or some other error detection field to the\npacket format, as shown in Figure F.4, allows the receiver to detect errors. This\nredundant information is calculated when the packet is sent and checked upon\nreceipt. The receiver then sends an acknowledgment in the form of a control packet\nif the packet passes the test. Note that this acknowledgment control packet may\nsimultaneously contain flow control information (e.g., a credit or stop signal), thus\nreducing control packet overhead. As described earlier, the most common way to\nrecover from errors is to have a timer record the time each packet is sent and to\npresume the packet is lost or erroneously transported if the timer expires before\nan acknowledgment arrives. The packet is then resent.\nThe communication protocol across the network and network end nodes must\nhandle many more issues other than packet transport, flow control, and reliability.\nFor example, if two devices are from different manufacturers, they might order\nbytes differently within a word (Big Endian versus Little Endian byte ordering).\nThe protocol must reverse the order of bytes in each word as part of the delivery\nsystem. It must also guard against the possibility of duplicate packets if a delayed\npacket were to become unstuck. Depending on the system requirements, the pro-\ntocol may have to implement pipelining among operations to improve perfor-\nmance. Finally, the protocol may need to handle network congestion to prevent\nperformance degradation when more than two devices are connected, as described\nlater in Section F.7.\nF-12\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 961,
        "text": "Characterizing Performance: Latency and Effective Bandwidth\nNow that we have covered the basic steps for sending and receiving messages\nbetween two devices, we can discuss performance. We start by discussing the\nlatency when transporting a single packet. Then we discuss the effective bandwidth\n(also known as throughput) that can be achieved when the transmission of multiple\npackets is pipelined over the network at the packet level.\nFigure F.5 shows the basic components of latency for a single packet. Note that\nsome latency components will be broken down further in later sections as the inter-\nnals of the \u201cblack box\u201d network are revealed. The timing parameters in Figure F.5\napply to many interconnection network domains: inside a chip, between chips on a\nboard, between boards in a chassis, between chassis within a computer, between\ncomputers in a cluster, between clusters, and so on. The values may change, but the\ncomponents of latency remain the same.\nThe following terms are often used loosely, leading to confusion, so we define\nthem here more precisely:\n\u25a0\nBandwidth\u2014Strictly speaking, the bandwidth of a transmission medium refers\nto the range of frequencies for which the attenuation per unit length introduced\nby that medium is below a certain threshold. It must be distinguished from the\ntransmission speed, which is the amount of information transmitted over a\nmedium per unit time. For example, modems successfully increased transmis-\nsion speed in the late 1990s for a fixed bandwidth (i.e., the 3 KHz bandwidth\nprovided by voice channels over telephone lines) by encoding more voltage\nlevels and, hence, more bits per signal cycle. However, to be consistent with\nSender\noverhead\nSender\nReceiver\nTransmission\ntime\n(bytes/bandwidth)\nTime of\nflight\nTransmission\ntime\n(bytes/bandwidth)\nReceiver\noverhead\nTransport latency\nTotal latency\nTime\nFigure F.5 Components of packet latency. Depending on whether it is an OCN, SAN,\nLAN, or WAN, the relative amounts of sending and receiving overhead, time of flight,\nand transmission time are usually quite different from those illustrated here.\nF.2\nInterconnecting Two Devices\n\u25a0\nF-13"
    },
    {
        "page": 962,
        "text": "its more widely understood meaning, we use the term band-width to refer to the\nmaximum rate at which information can be transferred, where information\nincludes packet header, payload, and trailer. The units are traditionally bits\nper second, although bytes per second is sometimes used. The term bandwidth\nis also used to mean the measured speed of the medium (i.e., network links).\nAggregate bandwidth refers to the total data bandwidth supplied by the net-\nwork, and effective bandwidth or throughput is the fraction of aggregate band-\nwidth delivered by the network to an application.\n\u25a0\nTime of flight\u2014This is the time for the first bit of the packet to arrive at the\nreceiver, including the propagation delay over the links and delays due to other\nhardware in the network such as link repeaters and network switches. The unit\nof measure for time of flight can be in milliseconds for WANs, microseconds\nfor LANs, nanoseconds for SANs, and picoseconds for OCNs.\n\u25a0\nTransmission time\u2014This is the time for the packet to pass through the network,\nnot including time of flight. One way to measure it is the difference in time\nbetween when the first bit of the packet arrives at the receiver and when the\nlast bit of that packet arrives at the receiver. By definition, transmission time\nis equal to the size of the packet divided by the data bandwidth of network\nlinks. This measure assumes there are no other packets contending for that\nbandwidth (i.e., a zero-load or no-load network).\n\u25a0\nTransport latency\u2014This is the sum of time of flight and transmission time.\nTransport latency is the time that the packet spends in the interconnection net-\nwork. Stated alternatively, it is the time between when the first bit of the packet\nis injected into the network and when the last bit of that packet arrives at the\nreceiver. It does not include the overhead of preparing the packet at the sender\nor processing it when it arrives at the receiver.\n\u25a0\nSending overhead\u2014This is the time for the end node to prepare the packet (as\nopposed to the message) for injection into the network, including both hard-\nware and software components. Note that the end node is busy for the entire\ntime, hence the use of the term overhead. Once the end node is free, any sub-\nsequent delays are considered part of the transport latency. We assume that\noverhead consists of a constant term plus a variable term that depends on\npacket size. The constant term includes memory allocation, packet header\npreparation, setting up DMA devices, and so on. The variable term is mostly\ndue to copies from buffer to buffer and is usually negligible for very short\npackets.\n\u25a0\nReceiving overhead\u2014This is the time for the end node to process an incoming\npacket, including both hardware and software components. We also assume\nhere that overhead consists of a constant term plus a variable term that depends\non packet size. In general, the receiving overhead is larger than the sending\noverhead. For example, the receiver may pay the cost of an interrupt or may\nhave to reorder and reassemble packets into messages.\nF-14\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 963,
        "text": "The total latency of a packet can be expressed algebraically by the following:\nLatency \u00bc Sending overhead + Time of flight + Packet size\nBandwidth + Receiving overhead\nLet\u2019s see how the various components of transport latency and the sending and\nreceiving overheads change in importance as we go across the interconnection\nnetwork domains: from OCNs to SANs to LANs to WANs.\nExample\nAssume that we have a dedicated link network with a data bandwidth of 8 Gbps\nfor each link in each direction interconnecting two devices within an OCN, SAN,\nLAN, or WAN, and we wish to transmit packets of 100 bytes (including the\nheader) between the devices. The end nodes have a per-packet sending overhead\nof x+0.05 ns/byte and receiving overhead of 4/3(x)+0.05 ns/byte, where x is 0 \u03bcs\nfor the OCN, 0.3 \u03bcs for the SAN, 3 \u03bcs for the LAN, and 30 \u03bcs for the WAN, which\nare typical for these network types. Calculate the total latency to send packets from\none device to the other for interconnection distances of 0.5 cm, 5 m, 5000 m, and\n5000 km assuming that time of flight consists only of link propagation delay\n(i.e., no switching or other sources of delay).\nAnswer\nUsing the above expression and the calculation for propagation delay through a\nconductor given in the previous example, we can plug in the parameters for each\nof the networks to find their total packet latency. For the OCN:\nLatency\u00bcSending overhead + Time of flight + Packet size\nBandwidth + Receiving overhead\n\u00bc5 ns +\n0:5 cm\n2=3\u0003300,000 km=sec + 100 bytes\n8 Gbits=sec + 5 ns\nConverting all terms into nanoseconds (ns) leads to the following for\nthe OCN:\nTotal latency OCN\n\u00f0\n\u00de\u00bc5 ns +\n0:5 cm\n2=3\u0003300,000 km=sec + 100\u00038\n8\nns + 5 ns\n\u00bc5 ns + 0:025 ns + 100 ns + 5 ns\n\u00bc110:025 ns\nSubstituting in the appropriate values for the SAN gives the following latency:\nTotal latency SAN\n\u00f0\n\u00de\u00bc0:305 \u03bcs +\n5 m\n2=3\u0003300,000 km=sec + 100 bytes\n8 Gbits=sec + 0:405 \u03bcs\n\u00bc0:305 \u03bcs + 0:025 \u03bcs + 0:1 \u03bcs + 0:405 \u03bcs\n\u00bc0:835 \u03bcs\nF.2\nInterconnecting Two Devices\n\u25a0\nF-15"
    },
    {
        "page": 964,
        "text": "Substituting in the appropriate values for the LAN gives the following latency:\nTotal latency LAN\n\u00f0\n\u00de\u00bc3:005 \u03bcs +\n5 km\n2=3\u0003300,000 km=sec + 100 bytes\n8 Gbits=sec + 4:005 \u03bcs\n\u00bc3:005 \u03bcs + 25 \u03bcs + 0:1 \u03bcs + 4:005 \u03bcs\n\u00bc32:11 \u03bcs\nSubstituting in the appropriate values for the WAN gives the following latency:\nTotal latency WAN\n\u00f0\n\u00de\u00bc30:005 \u03bcs +\n5000 km\n2=3\u0003300,000 km=sec + 100 bytes\n8 Gbits=sec + 40:005 \u03bcs\n\u00bc30:005 \u03bcs + 25000 \u03bcs + 0:1 \u03bcs + 40:005 \u03bcs\n\u00bc25:07 ms\nThe increased fraction of the latency required by time of flight for the longer\ndistances along with the greater likelihood of errors over the longer distances are\namongthereasonswhyWANsandLANsusemoresophisticatedandtime-consuming\ncommunication protocols, which increase sending and receiving overheads. The need\nfor standardization is another reason. Complexity also increases due to the require-\nments imposed on the protocol by the typical applications that run over the various\ninterconnectionnetworkdomainsaswegofromtenstohundredstothousandstomany\nthousands of devices. We will consider this in later sections when we discuss connect-\ning morethan twodevices. Theabove exampleshowsthat thepropagationdelay com-\nponent of time of flight for WANs and some LANs is so long that other latency\ncomponents\u2014including the sending and receiving overheads\u2014can practically be\nignored. This is not so for SANs and OCNs where thepropagation delaypales in com-\nparisontotheoverheadsandtransmissiondelay.Rememberthattime-of-flightlatency\ndue to switches and other hardware in the network besides sheer propagation delay\nthrough the links is neglected in the above example. For noncongested networks,\nswitch latency generally is small compared to the overheads and propagation delay\nthrough the links in WANs and LANs, but this is not necessarily so for multiprocessor\nSANs and multicore OCNs, as we will see in later sections.\nSo far, we have considered the transport of a single packet and computed the\nassociated end-to-end total packet latency. In order to compute the effective band-\nwidth for two networked devices, we have to consider a continuous stream of\npackets transported between them. We must keep in mind that, in addition to min-\nimizing packet latency, the goal of any network optimized for a given cost and\npower consumption target is to transfer the maximum amount of available infor-\nmation in the shortest possible time, as measured by the effective bandwidth deliv-\nered by the network. For applications that do not require a response before sending\nthe next packet, the sender can overlap the sending overhead of later packets with\nthe transport latency and receiver overhead of prior packets. This essentially pipe-\nlines the transmission of packets over the network, also known as link pipelining.\nFortunately, as discussed in prior chapters of this book, there are many application\nF-16\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 965,
        "text": "areas where communication from either several applications or several threads\nfrom the same application can run concurrently (e.g., a Web server concurrently\nserving thousands of client requests or streaming media), thus allowing a device\nto send a stream of packets without having to wait for an acknowledgment or a\nreply. Also, as long messages are usually divided into packets of maximum size\nbefore transport, a number of packets are injected into the network in succession\nfor such cases. If such overlap were not possible, packets would have to wait for\nprior packets to be acknowledged before being transmitted and thus suffer signif-\nicant performance degradation.\nPackets transported in a pipelined fashion can be acknowledged quite straight-\nforwardly simply by keeping a copy at the source of all unacknowledged packets\nthat have been sent and keeping track of the correspondence between returned\nacknowledgments and packets stored in the buffer. Packets will be removed from\nthe buffer when the corresponding acknowledgment is received by the sender. This\ncan be done by including the message ID and packet sequence number associated\nwith the packet in the packet\u2019s acknowledgment. Furthermore, a separate timer\nmust be associated with each buffered packet, allowing the packet to be resent\nif the associated time-out expires.\nPipelining packet transport over the network has many similarities with pipe-\nlining computation within a processor. However, among some differences are that\nit does not require any staging latches. Information is simply propagated through\nnetwork links as a sequence of signal waves. Thus, the network can be considered\nas a logical pipeline consisting of as many stages as are required so that the time of\nflight does not affect the effective bandwidth that can be achieved. Transmission of\na packet can start immediately after the transmission of the previous one, thus over-\nlapping the sending overhead of a packet with the transport and receiver latency of\nprevious packets. If the sending overhead is smaller than the transmission time,\npackets follow each other back-to-back, and the effective bandwidth approaches\nthe raw link bandwidth when continuously transmitting packets. On the other hand,\nif the sending overhead is greater than the transmission time, the effective band-\nwidth at the injection point will remain well below the raw link bandwidth. The\nresulting link injection bandwidth, BWLinkInjection, for each link injecting a contin-\nuous stream of packets into a network is calculated with the following expression:\nBWLinkInjection \u00bc\nPacket size\nmax Sending overhead,Transmission time\n\u00f0\n\u00de\nWe must also consider what happens if the receiver is unable to consume packets\nat the same rate they arrive. This occurs if the receiving overhead is greater than the\nsending overhead and the receiver cannot process incoming packets fast enough.\nIn this case, the link reception bandwidth, BWLinkReception, for each reception link\nof the network is less than the link injection bandwidth and is obtained with this\nexpression:\nBWLinkReception \u00bc\nPacket size\nmax Receiving overhead,Transmission time\n\u00f0\n\u00de\nF.2\nInterconnecting Two Devices\n\u25a0\nF-17"
    },
    {
        "page": 966,
        "text": "When communication takes place between two devices interconnected by ded-\nicated links, all the packets sent by one device will be received by the other. If the\nreceiver cannot process packets fast enough, the receiver buffer will become full,\nand flowcontrolwill throttle transmissionatthesender.Asthis situationisproduced\nby causes external to the network, we will not consider it further here. Moreover, if\nthe receiving overhead is greater than the sending overhead, the receiver buffer will\nfill upand flow control will, likewise,throttletransmissionat thesender.Inthis case,\nthe effect of flow control is, on average, the same as if we replace sending overhead\nwith receiving overhead. Assuming an ideal network that behaves like two dedi-\ncated links running in opposite directions at the full link bandwidth between the\ntwo devices\u2014which is consistent with our black box view of the network to this\npoint\u2014the resulting effective bandwidth is the smaller of twice the injection band-\nwidth (to account for the two injection links, one for each device) or twice the recep-\ntion bandwidth. This results in the following expression for effective bandwidth:\nEffective bandwidth \u00bc min 2\u0003BWLinkInjection,2\u0003BWLinkReception\n\u0004\n\u0005\n\u00bc\n2\u0003Packet size\nmax Overhead,Transmission time\n\u00f0\n\u00de\nwhere Overhead\u00bcmax(Sending overhead, Receiving overhead). Taking into\naccount the expression for the transmission time, it is obvious that the effective\nbandwidth delivered by the network is identical to the aggregate network band-\nwidth when the transmission time is greater than the overhead. Therefore, full\nnetwork utilization is achieved regardless of the value for the time of flight\nand, thus, regardless of the distance traveled by packets, assuming ideal network\nbehavior (i.e., enough credits and buffers are provided for credit-based and Xon/\nXoff flow control). This analysis assumes that the sender and receiver network\ninterfaces can process only one packet at a time. If multiple packets can be pro-\ncessed in parallel (e.g., as is done in IBM\u2019s Federation network interfaces),\nthe overheads for those packets can be overlapped, which increases effective band-\nwidth by that overlap factor up to the amount bounded by the transmission time.\nLet\u2019s use the equation on page F-17 to explore the impact of packet size, trans-\nmission time, and overhead on BWLink Injection, BWLinkReception, and effective band-\nwidth for the various network domains: OCNs, SANs, LANs, and WANs.\nExample\nAs in the previous example, assume we have a dedicated link network with a data\nbandwidth of 8 Gbps for each link in each direction interconnecting the two\ndevices within an OCN, SAN, LAN, or WAN. Plot effective bandwidth versus\npacket size for each type of network for packets ranging in size from 4 bytes\n(i.e., a single 32-bit word) to 1500 bytes (i.e., the maximum transfer unit for Ether-\nnet), assuming that end nodes have the same per-packet sending and receiving\noverheads as before: x+0.05 ns/byte and 4/3(x)+0.05 ns/byte, respectively, where\nx is 0 \u03bcs for the OCN, 0.3 \u03bcs for the SAN, 3 \u03bcs for the LAN, and 30 \u03bcs for the\nWAN. What limits the effective bandwidth, and for what packet sizes is the effec-\ntive bandwidth within 10% of the aggregate network bandwidth?\nF-18\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 967,
        "text": "Answer\nFigure F.6 plots effective bandwidth versus packet size for the four network\ndomains using the simple equation and parameters given above. For all packet\nsizes in the OCN, transmission time is greater than overhead (sending or receiv-\ning), allowing full utilization of the aggregate bandwidth, which is 16 Gbps\u2014that\nis, injection link (alternatively, reception link) bandwidth times two to account for\nboth devices. For the SAN, overhead\u2014specifically, receiving overhead\u2014is larger\nthan transmission time for packets less than about 800 bytes; consequently, packets\nof 655 bytes and larger are needed to utilize 90% or more of the aggregate band-\nwidth. For LANs and WANs, most of the link bandwidth is not utilized since over-\nhead in this example is many times larger than transmission time for all\npacket sizes.\nThis example highlights the importance of reducing the sending and receiving\noverheads relative to packet transmission time in order to maximize the effective\nbandwidth delivered by the network.\nThe analysis above suggests that it is possible to provide some upper bound for\nthe effective bandwidth by analyzing the path followed by packets and determining\nwhere the bottleneck occurs. We can extend this idea beyond the network\ninterfaces by defining a model that considers the entire network from end to\nEffective bandwidth (Gbits/sec)\n100\n10\n1\n4\n0.01\n0.1\n0.001\nPacket size (bytes)\n1400\n1200\n1000\n800\n600\n400\n200\nOCN\nSAN\nLAN\nWAN\nFigure F.6 Effective bandwidth versus packet size plotted in semi-log form for the\nfour network domains. Overhead can be amortized by increasing the packet size, but\nfor too large of an overhead (e.g., for WANs and some LANs) scaling the packet size is of\nlittle help. Other considerations come into play that limit the maximum packet size.\nF.2\nInterconnecting Two Devices\n\u25a0\nF-19"
    },
    {
        "page": 968,
        "text": "end as a pipe and identifying the narrowest section of that pipe. There are three\nareas of interest in that pipe: the aggregate of all network injection links and the\ncorresponding network injection bandwidth (BWNetworkInjection), the aggregate of\nall network reception links and the corresponding network reception bandwidth\n(BWNetworkReception), and the aggregate of all network links and the corresponding\nnetwork bandwidth (BWNetwork). Expressions for these will be given in\nlater sections as various layers of the black box view of the network are\npeeled away.\nTo this point, we have assumed that for just two interconnected devices the\nblack box network behaves ideally and the network bandwidth is equal to\nthe aggregate raw network bandwidth. In reality, it can be much less than the aggre-\ngate bandwidth as we will see in the following sections. In general, the effective\nbandwidth delivered end-to-end by the network to an application is upper bounded\nby the minimum across all three potential bottleneck areas:\nEffective bandwidth \u00bc min BWNetworkInjection, BWNetwork, BWNetworkReception\n\u0004\n\u0005\nWe will expand upon this expression further in the following sections as we reveal\nmore about interconnection networks and consider the more general case of inter-\nconnecting more than two devices.\nIn some sections of this appendix, we show how the concepts introduced in\nthe section take shape in example high-end commercial products. Figure F.7\nlists several commercial computers that, at one point in time in their existence,\nwere among the highest-performing systems in the world within their class.\nAlthough these systems are capable of interconnecting more than two devices,\nthey implement the basic functions needed for interconnecting only two\ndevices. In addition to being applicable to the SANs used in those systems,\nthe issues discussed in this section also apply to other interconnect domains:\nfrom OCNs to WANs.\nF.3\nConnecting More than Two Devices\nTo this point, we have considered the connection of only two devices communi-\ncating over a network viewed as a black box, but what makes interconnection net-\nworks interesting is the ability to connect hundreds or even many thousands of\ndevices together. Consequently, what makes them interesting also makes them\nmore challenging to build. In order to connect more than two devices, a suitable\nstructure and more functionality must be supported by the network. This section\ncontinues with our black box approach by introducing, at a conceptual level, addi-\ntional network structure and functions that must be supported when interconnect-\ning more than two devices. More details on these individual subjects are given in\nSections F.4 through F.7. Where applicable, we relate the additional structure and\nfunctions to network media, flow control, and other basics presented in the previ-\nous section. In this section, we also classify networks into two broad categories\nF-20\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 969,
        "text": "based on their connection structure\u2014shared-media versus switched-media net-\nworks\u2014and we compare them. Finally, expanded expressions for characterizing\nnetwork performance are given, followed by an example.\nAdditional Network Structure and Functions: Topology,\nRouting, Arbitration, and Switching\nNetworks interconnecting more than two devices require mechanisms to physi-\ncally connect the packet source to its destination in order to transport the packet\nand deliver it to the correct destination. These mechanisms can be implemented\nin different ways and significantly vary across interconnection network domains.\nHowever, the types of network structure and functions performed by those mech-\nanisms are very much the same, regardless of the domain.\nWhen multiple devices are interconnected by a network, the connections\nbetween them oftentimes cannot be permanently established with dedicated links.\nCompany\nSystem\n[network] name\nIntro year\nMax. number of\ncompute nodes\n[ # CPUs]\nSystem footprint for\nmax. configuration\nPacket [header]\nmax size (bytes)\nInjection [reception]\nnode BW in MB/sec\nMinimum send/\nreceive overhead\nMaximum copper\nlink length; flow\ncontrol; error\nIntel\nASCI Red\nParagon\n2001\n4510\n[\u00032]\n2500 ft2\n1984\n[4]\n400\n[400]\nFew \u03bcs\nHandshaking;\nCRC+parity\nIBM\nASCI White\nSP Power3\n[Colony]\n2001\n512\n[\u000316]\n10,000 ft2\n1024\n[6]\n500\n[500]\n\u00043 \u03bcs\n25 m; credit-\nbased; CRC\nIntel\nThunder\nItanium2\nTiger4\n[QsNetII]\n2004\n1024\n[\u00034]\n120 m2\n2048\n[14]\n928\n[928]\n0.240 \u03bcs\n13 m; credit-\nbased; CRC for\nlink, dest.\nCray\nXT3 [SeaStar]\n2004\n30,508\n[\u00031]\n263.8 m2\n80\n[16]\n3200\n[3200]\nFew \u03bcs\n7 m; credit-\nbased; CRC\nCray\nX1E\n2004\n1024\n[\u00031]\n27 m2\n32\n[16]\n1600\n[1600]\n0 (direct LD ST\naccesses)\n5 m; credit-\nbased; CRC\nIBM\nASC Purple\npSeries 575\n[Federation]\n2005\n>1280\n[\u00038]\n6720 ft2\n2048\n[7]\n2000\n[2000]\n\u00041 \u03bcs with up\nto 4 packets\nprocessed in k\n25 m; credit-\nbased; CRC\nIBM\nBlue Gene/L\neServer Sol.\n[Torus Net.]\n2005\n65,536\n[\u00032]\n2500 ft2\n(.9\u0003.9\u00031.9 m3/\n1 K node rack)\n256\n[8]\n612.5\n[1050]\n\u00043 \u03bcs\n(2300 cycles)\n8.6 m; credit-\nbased; CRC\n(header/pkt)\nFigure F.7 Basic characteristics of interconnection networks in commercial high-performance computer systems.\nF.3\nConnecting More than Two Devices\n\u25a0\nF-21"
    },
    {
        "page": 970,
        "text": "This could either be too restrictive as all the packets from a given source would go\nto the same one destination (and not to others) or prohibitively expensive as a ded-\nicated link would be needed from every source to every destination (we will eval-\nuate this further in the next section). Therefore, networks usually share paths\namong different pairs of devices, but how those paths are shared is determined\nby the network connection structure, commonly referred to as the network topol-\nogy. Topology addresses the important issue of \u201cWhat paths are possible for\npackets?\u201d so packets reach their intended destinations.\nEvery network that interconnects more than two devices also requires some\nmechanism to deliver each packet to the correct destination. The associated func-\ntion is referred to as routing, which can be defined as the set of operations that need\nto be performed to compute a valid path from the packet source to its destinations.\nRouting addresses the important issue of \u201cWhich of the possible paths are allow-\nable (valid) for packets?\u201d so packets reach their intended destinations. Depending\non the network, this function may be executed at the packet source to compute the\nentire path, at some intermediate devices to compute fragments of the path on\nthe fly, or even at every possible destination device to verify whether that device\nis the intended destination for the packet. Usually, the packet header shown in\nFigure F.4 is extended to include the necessary routing information.\nIn general, as networks usually contain shared paths or parts thereof among dif-\nferent pairs of devices, packets may request some shared resources. When several\npackets request the same resources at the same time, an arbitration function is\nrequired to resolve the conflict. Arbitration, along with flow control, addresses\nthe important issue of \u201cWhen are paths available for packets?\u201d Every time arbitra-\ntion is performed, there is a winner and possibly several losers. The losers are not\ngranted access to the requested resources and are typically buffered. As indicated in\nthe previous section, flow control may be implemented to prevent buffer overflow.\nThe winner proceeds toward its destination once the granted resources are switched\nin, providing a path for the packet to advance. This function is referred to as switch-\ning. Switching addresses the important issue of \u201cHow are paths allocated to\npackets?\u201d To achieve better utilization of existing communication resources, most\nnetworks do not establish an entire end-to-end path at once. Instead, as explained in\nSection F.5, paths are usually established one fragment at a time.\nThese three network functions\u2014routing, arbitration, and switching\u2014must be\nimplemented in every network connecting more than two devices, no matter what\nform the network topology takes. This is in addition to the basic functions men-\ntioned in the previous section. However, the complexity of these functions and\nthe order in which they are performed depends on the category of network topol-\nogy, as discussed below. In general, routing, arbitration, and switching are required\nto establish a valid path from source to destination from among the possible paths\nprovided by the network topology. Once the path has been established, the packet\ntransport functions previously described are used to reliably transmit packets and\nreceive them at the corresponding destination. Flow control, if implemented, pre-\nvents buffer overflow by throttling the sender. It can be implemented at the end-to-\nend level, the link level within the network, or both.\nF-22\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 971,
        "text": "Shared-Media Networks\nThe simplest way to connect multiple devices is to have them share the network\nmedia, as shown for the bus in Figure F.8 (a). This has been the traditional way\nof interconnecting devices. The shared media can operate in half-duplex mode,\nwhere data can be carried in either direction over the media but simultaneous trans-\nmission and reception of data by the same device is not allowed, or in full-duplex,\nwhere the data can be carried in both directions and simultaneously transmitted and\nreceived by the same device. Until very recently, I/O devices in most systems typ-\nically shared a single I/O bus, and early system-on-chip (SoC) designs made use of\na shared bus to interconnect on-chip components. The most popular LAN, Ether-\nnet, was originally implemented as a half-duplex bus shared by up to a hundred\ncomputers, although now switched-media versions also exist.\nGiven that network media are shared, there must be a mechanism to coordinate\nand arbitrate the use of the shared media so that only one packet is sent at a time. If\nthe physical distance between network devices is small, it may be possible to have\na central arbiter to grant permission to send packets. In this case, the network nodes\nmay use dedicated control lines to interface with the arbiter. Centralized arbitration\nis impractical, however, for networks with a large number of nodes spread over\nlarge distances, so distributed forms of arbitration are also used. This is the case\nfor the original Ethernet shared-media LAN.\nA first step toward distributed arbitration of shared media is \u201clooking before\nyou leap.\u201d A node first checks the network to avoid trying to send a packet while\nanother packet is already in the network. Listening before transmission to avoid\ncollisions is called carrier sensing. If the interconnection is idle, the node tries\nto send. Looking first is not a guarantee of success, of course, as some other node\nmay also decide to send at the same instant. When two nodes send at the same time,\nNode\nNode\nShared-media network\nSwitched-media network\n(B)\nSwitch fabric\n(A)\nNode\nNode\nNode\nNode\nNode\nFigure F.8 (a) A shared-media network versus (b) a switched-media network. Ether-\nnet was originally a shared media network, but switched Ethernet is now available. All\nnodes on the shared-media networks must dynamically share the raw bandwidth of one\nlink, but switched-media networks can support multiple links, providing higher raw\naggregate bandwidth.\nF.3\nConnecting More than Two Devices\n\u25a0\nF-23"
    },
    {
        "page": 972,
        "text": "a collision occurs. Let\u2019s assume that the network interface can detect any resulting\ncollisions by listening to hear if the data become garbled by other data appearing\non the line. Listening to detect collisions is called collision detection. This is the\nsecond step of distributed arbitration.\nThe problem is not solved yet. If, after detecting a collision, every node on the\nnetwork waited exactly the same amount of time, listened to be sure there was no\ntraffic, and then tried to send again, we could still have synchronized nodes that\nwould repeatedly bump heads. To avoid repeated head-on collisions, each node\nwhose packet gets garbled waits (or backs off) a random amount of time before\nresending. Randomization breaks the synchronization. Subsequent collisions\nresult in exponentially increasing time between attempts to retransmit, so as not\nto tax the network.\nAlthough this approach controls congestion on the shared media, it is not guar-\nanteed to be fair\u2014some subsequent node may transmit while those that collided\nare waiting. If the network does not have high demand from many nodes, this\nsimple approach works well. Under high utilization, however, performance\ndegrades since the media are shared and fairness is not ensured. Another distrib-\nuted approach to arbitration of shared media that can support fairness is to pass a\ntoken between nodes. The function of the token is to grant the acquiring node the\nright to use the network. If the token circulates in a cyclic fashion between the\nnodes, a certain amount of fairness is ensured in the arbitration process.\nOnce arbitration has been performed and a device has been granted access to\nthe shared media, the function of switching is straightforward. The granted device\nsimply needs to connect itself to the shared media, thus establishing a path to every\npossible destination. Also, routing is very simple to implement. Given that the\nmedia are shared and attached to all the devices, every device will see every packet.\nTherefore, each device just needs to check whether or not a given packet is\nintended for that device. A beneficial side effect of this strategy is that a device\ncan send a packet to all the devices attached to the shared media through a single\ntransmission. This style of communication is called broadcasting, in contrast to\nunicasting, in which each packet is intended for only one device. The shared media\nmake it easy to broadcast a packet to every device or, alternatively, to a subset of\ndevices, called multicasting.\nSwitched-Media Networks\nThe alternative to sharing the entire network media at once across all attached\nnodes is to switch between disjoint portions of it shared by the nodes. Those por-\ntions consist of passive point-to-point links between active switch components that\ndynamically establish communication between sets of source-destination pairs.\nThese passive and active components make up what is referred to as the network\nswitch fabric or network fabric, to which end nodes are connected. This approach\nis shown conceptually in Figure F.8(b). The switch fabric is described in greater\ndetail in Sections F.4 through F.7, where various black box layers for switched-\nmedia networks are further revealed. Nevertheless, the high-level view shown\nF-24\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 973,
        "text": "in Figure F.8(b) illustrates the potential bandwidth improvement of switched-\nmedia networks over shared-media networks: aggregate bandwidth can be many\ntimes higher than that of shared-media networks, allowing the possibility of greater\neffective bandwidth to be achieved. At best, only one node at a time can transmit\npackets over the shared media, whereas it is possible for all attached nodes to do so\nover the switched-media network.\nLike their shared-media counterparts, switched-media networks must imple-\nment the three additional functions previously mentioned: routing, arbitration,\nand switching. Every time a packet enters the network, it is routed in order to select\na path toward its destination provided by the topology. The path requested by the\npacket must be granted by some centralized or distributed arbiter, which resolves\nconflicts among concurrent requests for resources along the same path. Once the\nrequested resources are granted, the network \u201cswitches in\u201d the required connec-\ntions to establish the path and allows the packet to be forwarded toward its desti-\nnation. If the requested resources are not granted, the packet is usually buffered, as\nmentioned previously. Routing, arbitration, and switching functions are usually\nperformed within switched networks in this order, whereas in shared-media net-\nworks routing typically is the last function performed.\nComparison of Shared- and Switched-Media Networks\nIn general, the advantage of shared-media networks is their low cost, but, conse-\nquently, their aggregate network bandwidth does not scale at all with the number of\ninterconnected devices. Also, a global arbitration scheme is required to resolve\nconflicting demands, possibly introducing another type of bottleneck and again\nlimiting scalability. Moreover, every device attached to the shared media increases\nthe parasitic capacitance of the electrical conductors, thus increasing the time of\nflight propagation delay accordingly and, possibly, clock cycle time. In addition,\nit is more difficult to pipeline packet transmission over the network as the shared\nmedia are continuously granted to different requesting devices.\nThe main advantage of switched-media networks is that the amount of network\nresources implemented scales with the number of connected devices, increasing\nthe aggregate network bandwidth. These networks allow multiple pairs of nodes\nto communicate simultaneously, allowing much higher effective network band-\nwidth than that provided by shared-media networks. Also, switched-media net-\nworks allow the system to scale to very large numbers of nodes, which is not\nfeasible when using shared media. Consequently, this scaling advantage can, at\nthe same time, be a disadvantage if network resources grow superlinearly. Net-\nworks of superlinear cost that provide an effective network bandwidth that grows\nonly sublinearly with the number of interconnected devices are inefficient designs\nfor many applications and interconnection network domains.\nCharacterizing Performance: Latency and Effective Bandwidth\nThe routing, switching, and arbitration functionality described above introduces\nsome additional components of packet transport latency that must be taken into\nF.3\nConnecting More than Two Devices\n\u25a0\nF-25"
    },
    {
        "page": 974,
        "text": "account in the expression for total packet latency. Assuming there is no contention\nfor network resources\u2014as would be the case in an unloaded network\u2014total packet\nlatency is given by the following:\nLatency \u00bc Sending overhead + TTotalProp + TR + TA + TS\n\u0004\n\u0005\n+ Packet size\nBandwidth + Receiving overhead\nHere TR, TA, and TS are the total routing time, arbitration time, and switching time\nexperienced by the packet, respectively, and are either measured quantities or cal-\nculated quantities derived from more detailed analyses. These components are\nadded to the total propagation delay through the network links, TTotalProp, to give\nthe overall time of flight of the packet.\nThe expression above gives only a lower bound for the total packet latency as it\ndoes not account for additional delays due to contention for resources that may\noccur. When the network is heavily loaded, several packets may request the same\nnetwork resources concurrently, thus causing contention that degrades perfor-\nmance. Packets that lose arbitration have to be buffered, which increases packet\nlatency by some contention delay amount of waiting time. This additional delay\nis not included in the above expression. When the network or part of it approaches\nsaturation, contention delay may be several orders of magnitude greater than the\ntotal packet latency suffered by a packet under zero load or even under slightly\nloaded network conditions. Unfortunately, it is not easy to compute analytically\nthe total packet latency when the network is more than moderately loaded. Mea-\nsurement of these quantities using cycle-accurate simulation of a detailed network\nmodel is a better and more precise way of estimating packet latency under such\ncircumstances. Nevertheless, the expression given above is useful in calculating\nbest-case lower bounds for packet latency.\nFor similar reasons, effective bandwidth is not easy to compute exactly, but we\ncan estimate best-case upper bounds for it by appropriately extending the model\npresented at the end of the previous section. What we need to do is to find the nar-\nrowest section of the end-to-end network pipe by finding the network injection\nbandwidth (BWNetworkInjection), the network reception bandwidth (BWNetworkRecep-\ntion), and the network bandwidth (BWNetwork) across the entire network intercon-\nnecting the devices.\nThe BWNetworkInjection can be calculated simply by multiplying the expression\nfor link injection bandwidth, BWLinkInjection, by the total number of network injec-\ntion links. The BWNetworkReception is calculated similarly using BWLinkReception, but\nit must also be scaled by a factor that reflects application traffic and other charac-\nteristics. For more than two interconnected devices, it is no longer valid to assume a\none-to-one relationship among sources and destinations when analyzing the effect\nof flow control on link reception bandwidth. It could happen, for example, that\nseveral packets from different injection links arrive concurrently at the same recep-\ntion link for applications that have many-to-one traffic characteristics, which\ncauses contention at the reception links. This effect can be taken into account\nby an average reception factor parameter, \u03c3, which is either a measured quantity\nor a calculated quantity derived from detailed analysis. It is defined as the average\nF-26\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 975,
        "text": "fraction or percentage of packets arriving at reception links that can be accepted.\nOnly those packets can be immediately delivered, thus reducing network reception\nbandwidth by that factor. This reduction occurs as a result of application behavior\nregardless of internal network characteristics. Finally, BWNetwork takes into\naccount the internal characteristics of the network, including contention. We\nwill progressively derive expressions in the following sections that will enable\nus to calculate this as more details are revealed about the internals of our black\nbox interconnection network.\nOverall, the effective bandwidth delivered by the network end-to-end to an\napplication is determined by the minimum across the three sections, as described\nby the following:\nEffective bandwidth\u00bc min BWNetworkInjection,BWNetwork,\u03c3\u0003BWNetworkReception\n\u0004\n\u0005\n\u00bc min N\u0003BWLinkInjection,BWNetwork,\u03c3\u0003N\u0003BWLinkReception\n\u0004\n\u0005\nLet\u2019s use the above expressions to compare the latency and effective bandwidth\nof shared-media networks against switched-media networks for the four intercon-\nnection network domains: OCNs, SANs, LANs, and WANs.\nExample\nPlot the total packet latency and effective bandwidth as the number of intercon-\nnected nodes, N, scales from 4 to 1024 for shared-media and switched-media\nOCNs, SANs, LANs, and WANs. Assume that all network links, including the\ninjection and reception links at the nodes, each have a data bandwidth of 8 Gbps,\nand unicast packets of 100 bytes are transmitted. Shared-media networks share one\nlink, and switched-media networks have at least as many network links as there are\nnodes. For both, ignore latency and bandwidth effects due to contention within\nthe network. End nodes have per-packet sending and receiving overheads of\nx+0.05 ns/byte and 4/3(x)+0.05 ns/byte, respectively, where x is 0 \u03bcs for the\nOCN, 0.3 \u03bcs for the SAN, 3 \u03bcs for the LAN, and 30 \u03bcs for the WAN, and inter-\nconnection distances are 0.5 cm, 5 m, 5000 m, and 5000 km, respectively. Also\nassume that the total routing, arbitration, and switching times are constants or func-\ntions of the number of interconnected nodes: TR\u00bc2.5 ns, TA\u00bc2.5(N) ns, and\nTS\u00bc2.5 ns for shared-media networks and TR\u00bcTA\u00bcTS\u00bc2.5(log2 N) ns for\nswitched-media networks. Finally, taking into account application traffic charac-\nteristics for the network structure, the average reception factor, \u03c3, is assumed to be\nN\u00051 for shared media and polylogarithmic (log2 N)\u00051/4 for switched media.\nAnswer\nAll components of total packet latency are the same as in the example given in the\nprevious section except for time of flight, which now has additional routing, arbi-\ntration, and switching delays. For shared-media networks, the additional delays\ntotal 5+2.5(N) ns; for switched-media networks, they total 7.5(log2 N) ns. Latency\nis plotted only for OCNs and SANs in Figure F.9 as these networks give the more\ninteresting results. For OCNs, TR, TA, and TS combine to dominate time of flight\nF.3\nConnecting More than Two Devices\n\u25a0\nF-27"
    },
    {
        "page": 976,
        "text": "and are much greater than each of the other latency components for a moderate to\nlarge number of nodes. This is particularly so for the shared-media network. The\nlatency increases much more dramatically with the number of nodes for shared\nmedia as compared to switched media given the difference in arbitration delay\nbetween the two. For SANs, TR, TA, and TS dominate time of flight for most net-\nwork sizes but are greater than each of the other latency components in shared-\nmedia networks only for large-sized networks; they are less than the other latency\ncomponents for switched-media networks but are not negligible. For LANs and\nWANs, time of flight is dominated by propagation delay, which dominates other\nlatency components as calculated in the previous section; thus, TR, TA, and TS are\nnegligible for both shared and switched media.\nFigure F.10 plots effective bandwidth versus number of interconnected nodes\nfor the four network domains. The effective bandwidth for all shared-media net-\nworks is constant through network scaling as only one unicast packet can be\nreceived at a time over all the network reception links, and that is further limited\nby the receiving overhead of each network for all but the OCN. The effective band-\nwidth for all switched-media networks increases with the number of intercon-\nnected nodes, but it is scaled down by the average reception factor. The\nreceiving overhead further limits effective bandwidth for all but the OCN.\nLatency (ns)\n10,000\n1000\n4\n100\nNumber of nodes (N)\n512\n1024\n256\n128\n64\n32\n16\n8\nSAN\u2014 shared\nOCN\u2014 shared\nSAN\u2014 switched\nOCN\u2014 switched\nFigure F.9 Latency versus number of interconnected nodes plotted in semi-log form\nfor OCNs and SANs. Routing,arbitration,andswitchinghavemoreofanimpactonlatency\nfor networks inthese two domains, particularly for networks with a largenumber ofnodes,\ngiven the low sending and receiving overheads and low propagation delay.\nF-28\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 977,
        "text": "Given the obvious advantages, why weren\u2019t switched networks always used?\nEarlier computers were much slower and could share the network media with little\nimpact on performance. In addition, the switches for earlier LANs and WANs took\nup several large boards and were about as large as an entire computer. As a con-\nsequence of Moore\u2019s law, the size of switches has reduced considerably, and sys-\ntems have a much greater need for high-performance communication. Switched\nnetworks allow communication to harvest the same rapid advancements from sil-\nicon as processors and main memory. Whereas switches from telecommunication\ncompanies were once the size of mainframe computers, today we see single-chip\nswitches and even entire switched networks within a chip. Thus, technology and\napplication trends favor switched networks today. Just as single-chip processors\nled to processors replacing logic circuits in a surprising number of places,\nsingle-chip switches and switched on-chip networks are increasingly replacing\nshared-media networks (i.e., buses) in several application domains. As an example,\nPCI-Express (PCIe)\u2014a switched network\u2014was introduced in 2005 to replace the\ntraditional PCI-X bus on personal computer motherboards.\nThe previous example also highlights the importance of optimizing the routing,\narbitration, and switching functions in OCNs and SANs. For these network\ndomains in particular, the interconnect distances and overheads typically are small\nEffective bandwidth (Gbits/sec)\n10,000\n1000\n100\n10\n1\n1\n0.1\n0.01\nNumber of nodes (N)\n1200\n1000\n800\n600\n400\n200\nOCN\u2014 switched\nSAN\u2014 switched\nLAN\u2014 switched\nWAN\u2014 switched\nOCN\u2014 shared\nSAN\u2014 shared\nLAN\u2014 shared\nWAN\u2014 shared\nFigure F.10 Effective bandwidth versus number of interconnected nodes plotted in semi-log form for the four\nnetwork domains. The disparity in effective bandwidth between shared- and switched-media networks for all inter-\nconnect domains widens significantly as the number of nodes in the network increases. Only the switched on-chip\nnetwork is able to achieve an effective bandwidth equal to the aggregate bandwidth for the parameters given in this\nexample.\nF.3\nConnecting More than Two Devices\n\u25a0\nF-29"
    },
    {
        "page": 978,
        "text": "enough to make latency and effective bandwidth much more sensitive to how well\nthese functions are implemented, particularly for larger-sized networks. This leads\nmostly to implementations based mainly on the faster hardware solutions for these\ndomains. In LANs and WANs, implementations based on the slower but more flex-\nible software solutions suffice given that performance is largely determined by\nother factors. The design of the topology for switched-media networks also plays\na major role in determining how close to the lower bound on latency and the upper\nbound on effective bandwidth the network can achieve for OCN and SAN\ndomains.\nThe next three sections touch on these important issues in switched networks,\nwith the next section focused on topology.\nF.4\nNetwork Topology\nWhen the number of devices is small enough, a single switch is sufficient to inter-\nconnect them within a switched-media network. However, the number of switch\nports is limited by existing very-large-scale integration (VLSI) technology, cost\nconsiderations, power consumption, and so on. When the number of required net-\nwork ports exceeds the number of ports supported by a single switch, a fabric of\ninterconnected switches is needed. To embody the necessary property of full\naccess (i.e., connectedness), the network switch fabric must provide a path from\nevery end node device to every other device. All the connections to the network\nfabric and between switches within the fabric use point-to-point links as opposed\nto shared links\u2014that is, links with only one switch or end node device on either\nend. The interconnection structure across all the components\u2014including switches,\nlinks, and end node devices\u2014is referred to as the network topology.\nThe number of network topologies described in the literature would be difficult\nto count, but the number that have been used commercially is no more than about a\ndozen or so. During the 1970s and early 1980s, researchers struggled to propose\nnew topologies that could reduce the number of switches through which packets\nmust traverse, referred to as the hop count. In the 1990s, thanks to the introduction\nof pipelined transmission and switching techniques, the hop count became less crit-\nical. Nevertheless, today, topology is still important, particularly for OCNs and\nSANs, as subtle relationships exist between topology and other network design\nparameters that impact performance, especially when the number of end nodes\nis very large (e.g., 64 K in the Blue Gene/L supercomputer) or when the latency\nis critical (e.g., in multicore processor chips). Topology also greatly impacts the\nimplementation cost of the network.\nTopologies for parallel supercomputer SANs have been the most visible and\nimaginative, usually converging on regularly structured ones to simplify routing,\npackaging, and scalability. Those for LANs and WANs tend to be more haphazard\nor ad hoc, having more to do with the challenges of long distance or connecting\nacross different communication subnets. Switch-based topologies for OCNs are\nonly recently emerging but are quickly gaining in popularity. This section\nF-30\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 979,
        "text": "describes the more popular topologies used in commercial products. Their advan-\ntages, disadvantages, and constraints are also briefly discussed.\nCentralized Switched Networks\nAs mentioned above, a single switch suffices to interconnect a set of devices when\nthe number of switch ports is equal to or larger than the number of devices. This\nsimple network is usually referred to as a crossbar or crossbar switch. Within the\ncrossbar, crosspoint switch complexity increases quadratically with the number of\nports, as illustrated in Figure F.11(a). Thus, a cheaper solution is desirable when\nthe number of devices to be interconnected scales beyond the point supportable by\nimplementation technology.\nA common way of addressing the crossbar scaling problem consists of splitting\nthe large crossbar switch into several stages of smaller switches interconnected in\nsuch a way that a single pass through the switch fabric allows any destination to be\nreached from any source. Topologies arranged in this way are usually referred to as\nmultistage interconnection networks or multistage switch fabrics, and these net-\nworks typically have complexity that increases in proportion to N log N. Multistage\ninterconnection networks (MINs) were initially proposed for telephone exchanges\nin the 1950s and have since been used to build the communication backbone for\nparallel supercomputers, symmetric multiprocessors, multicomputer clusters, and\nIP router switch fabrics.\n(B)\n(A)\n0\n0\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\n7\n0\n1\n2\n3\n4\n5\n6\n7\n0\n1\n2\n3\n4\n5\n6\n7\n7\nFigure F.11 Popular centralized switched networks: (a) the crossbar network requires N2 crosspoint switches,\nshown as black dots; (b) the Omega, a MIN, requires N/2 log2 N switches, shown as vertical rectangles. End node\ndevices are shown as numbered squares (total of eight). Links are unidirectional\u2014data enter at the left and exit out\nthe top or right.\nF.4\nNetwork Topology\n\u25a0\nF-31"
    },
    {
        "page": 980,
        "text": "The interconnection pattern or patterns between MIN stages are permutations\nthat can be represented mathematically by a set of functions, one for each stage.\nFigure F.11(b) shows a well-known MIN topology, the Omega, which uses the\nperfect-shuffle permutation as its interconnection pattern for each stage, followed\nby exchange switches, giving rise to a perfect-shuffle exchange for each stage. In\nthis example, eight input-output ports are interconnected with three stages of 2\u00032\nswitches. It is easy to see that a single pass through the three stages allows any input\nport to reach any output port. In general, when using k\u0003k switches, a MIN with N\ninput-output ports requires at least logk N stages, each of which contains N/k\nswitches, for a total of N/k (logk N) switches.\nDespite their internal structure, MINs can be seen as centralized switch fabrics\nthat have end node devices connected at the network periphery, hence the name\ncentralized switched network. From another perspective, MINs can be viewed\nas interconnecting nodes through a set of switches that may not have any nodes\ndirectly connected to them, which gives rise to another popular name for central-\nized switched networks\u2014indirect networks.\nExample\nCompute the cost of interconnecting 4096 nodes using a single crossbar switch\nrelative to doing so using a MIN built from 2\u00032, 4\u00034, and 16\u000316 switches. Con-\nsider separately the relative cost of the unidirectional links and the relative cost of\nthe switches. Switch cost is assumed to grow quadratically with the number of\ninput (alternatively, output) ports, k, for k\u0003k switches.\nAnswer\nThe switch cost of the network when using a single crossbar is proportional to\n40962. The unidirectional link cost is 8192, which accounts for the set of links from\nthe end nodes to the crossbar and also from the crossbar back to the end nodes.\nWhen using a MIN with k\u0003k switches, the cost of each switch is proportional\nto k2 but there are 4096/k (logk 4096) total switches. Likewise, there are (logk\n4096) stages of N unidirectional links per stage from the switches plus N links\nto the MIN from the end nodes. Therefore, the relative costs of the crossbar with\nrespect to each MIN is given by the following:\nRelative cost 2\u00032\n\u00f0\n\u00deswitches \u00bc 40962= 22 \u00034096=2\u0003 log2 4096\n\u0004\n\u0005\n\u00bc 170\nRelative cost 4\u00034\n\u00f0\n\u00deswitches \u00bc 40962= 42 \u00034096=4\u0003 log4 4096\n\u0004\n\u0005\n\u00bc 170\nRelative cost 16\u000316\n\u00f0\n\u00deswitches \u00bc 40962= 162 \u00034096=16\u0003 log16 4096\n\u0004\n\u0005\n\u00bc 85\nRelative cost 2\u00032\n\u00f0\n\u00delinks \u00bc 8192= 4096\u0003 log2 4096 + 1\n\u00f0\n\u00de\n\u00f0\n\u00de \u00bc 2=13 \u00bc 0:1538\nRelative cost 4\u00034\n\u00f0\n\u00delinks \u00bc 8192= 4096\u0003 log4 4096 + 1\n\u00f0\n\u00de\n\u00f0\n\u00de \u00bc 2=7 \u00bc 0:2857\nRelative cost 16\u000316\n\u00f0\n\u00delinks \u00bc 8192= 4096\u0003 log16 4096 + 1\n\u00f0\n\u00de\n\u00f0\n\u00de \u00bc 2=4 \u00bc 0:5\nIn all cases, the single crossbar has much higher switch cost than the MINs. The\nmost dramatic reduction in cost comes from the MIN composed from the smallest\nsized but largest number of switches, but it is interesting to see that the MINs with\n2\u00032 and 4\u00034 switches yield the same relative switch cost. The relative link cost\nF-32\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 981,
        "text": "of the crossbar is lower than the MINs, but by less than an order of magnitude in\nall cases. We must keep in mind that end node links are different from switch links\nin their length and packaging requirements, so they usually have different associ-\nated costs. Despite the lower link cost, the crossbar has higher overall relative cost.\nThe reduction in switch cost of MINs comes at the price of performance: con-\ntention is more likely to occur on network links, thus degrading performance. Con-\ntention in the form of packets blocking in the network arises due to paths from\ndifferent sources to different destinations simultaneously sharing one or more\nlinks. The amount of contention in the network depends on communication traffic\nbehavior. In the Omega network shown in Figure F.11(b), for example, a packet\nfrom port 0 to port 1 blocks in the first stage of switches while waiting for a packet\nfrom port 4 to port 0. In the crossbar, no such blocking occurs as links are not\nshared among paths to unique destinations. The crossbar, therefore, is nonblock-\ning. Of course, if two nodes try to send packets to the same destination, there will\nbe blocking at the reception link even for crossbar networks. This is accounted for\nby the average reception factor parameter (\u03c3) when analyzing performance, as dis-\ncussed at the end of the previous section.\nTo reduce blocking in MINs, extra switches must be added or larger ones need\nto be used to provide alternative paths from every source to every destination. The\nfirst commonly used solution is to add a minimum of logk N\u00051 extra switch stages\nto the MIN in such a way that they mirror the original topology. The resulting net-\nwork is rearrangeably nonblocking as it allows nonconflicting paths among new\nsource-destination pairs to be established, but it also doubles the hop count and\ncould require the paths of some existing communicating pairs to be rearranged\nunder some centralized control. The second solution takes a different approach.\nInstead of using more switch stages, larger switches\u2014which can be implemented\nby multiple stages if desired\u2014are used in the middle of two other switch stages in\nsuch a way that enough alternative paths through the middle-stage switches allow\nfor nonconflicting paths to be established between the first and last stages. The\nbest-known example of this is the Clos network, which is nonblocking. The multi-\npath property of the three-stage Clos topology can be recursively applied to the\nmiddle-stage switches to reduce the size of all the switches down to 2\u00032, assum-\ning that switches of this size are used in the first and last stages to begin with. What\nresults is a Bene\u015d topology consisting of 2(log2 N)\u00051 stages, which is rearrange-\nably nonblocking. Figure F.12(a) illustrates both topologies, where all switches not\nin the first and last stages comprise the middle-stage switches (recursively) of the\nClos network.\nThe MINs described so far have unidirectional network links, but bidirectional\nforms are easily derived from symmetric networks such as the Clos and Bene\u015d sim-\nply by folding them. The overlapping unidirectional links run in different direc-\ntions, thus forming bidirectional links, and the overlapping switches merge into\na single switch with twice the ports (i.e., 4\u00034 switch). Figure F.12(b) shows\nthe resulting folded Bene\u015d topology but in this case with the end nodes connected\nF.4\nNetwork Topology\n\u25a0\nF-33"
    },
    {
        "page": 982,
        "text": "to the innermost switch stage of the original Bene\u015d. Ports remain free at the other\nside of the network but can be used for later expansion of the network to larger\nsizes. These kind of networks are referred to as bidirectional multistage intercon-\nnection networks. Among many useful properties of these networks are their mod-\nularity and their ability to exploit communication locality, which saves packets\nfrom having to hop across all network stages. Their regularity also reduces routing\ncomplexity and their multipath property enables traffic to be routed more evenly\nacross network resources and to tolerate faults.\nAnother way of deriving bidirectional MINs with nonblocking (rearrangeable)\nproperties is to form a balanced tree, where end node devices occupy leaves of the\ntree and switches occupy vertices within the tree. Enough links in each tree level\nmust be provided such that the total link bandwidth remains constant across all\nlevels.Also, except fortheroot,switchports foreach vertex typicallygrow aski\u0003ki,\nwhere i is the tree level. This can be accomplished by using ki\u00051 total switches at\neach vertex, where each switch has k input and k output ports, or k bidirectional ports\n(i.e., k\u0003k input-output ports). Networks having such topologies are called fat tree\nnetworks. As only half of the k bidirectional ports are used in each direction, 2 N/k\nswitches are needed in each stage, totaling 2 N/k (logk/2 N) switches in the fat tree.\nThe number of switches in the root stage can be halved as no forward links are\nneeded, reducing switch count by N/k. Figure F.12(b) shows a fat tree for 4\u00034\nswitches. As can be seen, this is identical to the folded Bene\u015d.\nThe fat tree is the topology of choice across a wide range of network sizes\nfor most commercial systems that use multistage interconnection networks. Most\nSANs used in multicomputer clusters, and many used in the most powerful\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n(A)\n(B)\n13\n14\n15\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nFigure F.12 Two Bene\u015d networks. (a) A 16-port Clos topology, where the middle-stage switches shown in the darker\nshading are implemented with another Clos network whose middle-stage switches shown in the lighter shading are\nimplemented with yet another Clos network, and so on, until a Bene\u015d network is produced that uses only 2\u00032\nswitches everywhere. (b) A folded Bene\u015d network (bidirectional) in which 4\u00034 switches are used; end nodes attach\nto the innermost set of the Bene\u015d network (unidirectional) switches. This topology is equivalent to a fat tree, where\ntree vertices are shown in shades.\nF-34\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 983,
        "text": "supercomputers, are based on fat trees. Commercial communication subsystems\noffered by Myrinet, Mellanox, and Quadrics are also built from fat trees.\nDistributed Switched Networks\nSwitched-media networks provide a very flexible framework to design communi-\ncation subsystems external to the devices that need to communicate, as presented\nabove. However, there are cases where it is convenient to more tightly integrate the\nend node devices with the network resources used to enable them to communicate.\nInstead of centralizing the switch fabric in an external subsystem, an alternative\napproach is to distribute the network switches among the end nodes, which then\nbecome network nodes or simply nodes, yielding a distributed switched network.\nAs a consequence, each network switch has one or more end node devices directly\nconnected to it, thus forming a network node. These nodes are directly connected to\nother nodes without indirectly going through some external switch, giving rise to\nanother popular name for these networks\u2014direct networks.\nThe topology for distributed switched networks takes on a form much differ-\nent from centralized switched networks in that end nodes are connected across\nthe area of the switch fabric, not just at one or two of the peripheral edges of\nthe fabric. This causes the number of switches in the system to be equal to the\ntotal number of nodes. A quite obvious way of interconnecting nodes consists\nof connecting a dedicated link between each node and every other node in the\nnetwork. This fully connected topology provides the best connectivity (full con-\nnectivity in fact), but it is more costly than a crossbar network, as the following\nexample shows.\nExample\nCompute the cost of interconnecting N nodes using a fully connected topology rel-\native to doing so using a crossbar topology. Consider separately the relative cost of\nthe unidirectional links and the relative cost of the switches. Switch cost is assumed\nto grow quadratically with the number of unidirectional ports for k\u0003k switches but\nto grow only linearly with 1\u0003k switches.\nAnswer\nThe crossbar topology requires an N\u0003N switch, so the switch cost is proportional\nto N2. The link cost is 2N, which accounts for the unidirectional links from the end\nnodes to the centralized crossbar, and vice versa. In the fully connected topology,\ntwo sets of 1\u0003(N\u00051) switches (possibly merged into one set) are used in each of\nthe N nodes to connect nodes directly to and from all other nodes. Thus, the total\nswitch cost for all N nodes is proportional to 2N(N\u00051). Regarding link cost, each\nof the N nodes requires two unidirectional links in opposite directions between its\nend node device and its local switch. In addition, each of the N nodes has N\u00051\nunidirectional links from its local switch to other switches distributed across\nall the other end nodes. Thus, the total number of unidirectional links is\n2N+N(N\u00051), which is equal to N(N+1) for all N nodes. The relative costs of\nthe fully connected topology with respect to the crossbar is, therefore, the\nfollowing:\nF.4\nNetwork Topology\n\u25a0\nF-35"
    },
    {
        "page": 984,
        "text": "Relative costswitches \u00bc 2N N \u00051\n\u00f0\n\u00de=N2 \u00bc 2 N \u00051\n\u00f0\n\u00de=N \u00bc 2 1\u00051=N\n\u00f0\n\u00de\nRelative costlinks \u00bc N N + 1\n\u00f0\n\u00de=2N \u00bc N + 1\n\u00f0\n\u00de=2\nAs the number of interconnected devices increases, the switch cost of the fully\nconnected topology is nearly double the crossbar, with both being very high\n(i.e., quadratic growth). Moreover, the fully connected topology always has higher\nrelative link cost, which grows linearly with the number of nodes. Again, keep in\nmind that end node links are different from switch links in their length and pack-\naging, particularly for direct networks, so they usually have different associated\ncosts. Despite its higher cost, the fully connected topology provides no extra per-\nformance benefits over the crossbar as both are nonblocking. Thus, crossbar net-\nworks are usually used in practice instead of fully connected networks.\nA lower-cost alternative to fully connecting all nodes in the network is to\ndirectly connect nodes in sequence along a ring topology, as shown in\nFigure F.13. For bidirectional rings, each of the N nodes now uses only 3\u00033\nswitches and just two bidirectional network links (shared by neighboring nodes),\nfor a total of N switches and N bidirectional network links. This linear cost excludes\nthe N injection-reception bidirectional links required within nodes.\nUnlike shared-media networks, rings can allow many simultaneous transfers:\nthe first node can send to the second while the second sends to the third, and so on.\nHowever, as dedicated links do not exist between logically nonadjacent node pairs,\npackets must hop across intermediate nodes before arriving at their destination,\nincreasing their transport latency. For bidirectional rings, packets can be trans-\nported in either direction, with the shortest path to the destination usually being\nthe one selected. In this case, packets must travel N/4 network switch hops, on\naverage, with total switch hop count being one more to account for the local switch\nat the packet source node. Along the way, packets may block on network resources\ndue to other packets contending for the same resources simultaneously.\nFully connected and ring-connected networks delimit the two extremes of dis-\ntributed switched topologies, but there are many points of interest in between for a\ngiven set of cost-performance requirements. Generally speaking, the ideal\nswitched-media topology has cost approaching that of a ring but performance\nFigure F.13 A ring network topology, folded to reduce the length of the longest link.\nShaded circles represent switches, and black squares represent end node devices. The\ngray rectangle signifies a network node consisting of a switch, a device, and its\nconnecting link.\nF-36\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 985,
        "text": "approaching that of a fully connected topology. Figure F.14 illustrates three pop-\nular direct network topologies commonly used in systems spanning the cost-\nperformance spectrum. All of them consist of sets of nodes arranged along multiple\ndimensions with a regular interconnection pattern among nodes that can be\nexpressed mathematically. In the mesh or grid topology, all the nodes in each\ndimension form a linear array. In the torus topology, all the nodes in each dimen-\nsion form a ring. Both of these topologies provide direct communication to neigh-\nboring nodes with the aim of reducing the number of hops suffered by packets in\nthe network with respect to the ring. This is achieved by providing greater connec-\ntivity through additional dimensions, typically no more than three in commercial\nsystems. The hypercube or n-cube topology is a particular case of the mesh in\nwhich only two nodes are interconnected along each dimension, leading to a num-\nber of dimensions, n, that must be large enough to interconnect all N nodes in the\nsystem (i.e., n\u00bclog2 N). The hypercube provides better connectivity than meshes\n(A) 2D grid or mesh of 16 nodes \n(B) 2D torus of 16 nodes\n(C) Hypercube of 16 nodes (16 = 24 so n = 4)\nFigure F.14 Direct network topologies that have appeared in commercial systems,\nmostly supercomputers.\nThe shaded circles represent switches, and the black squares represent end node\ndevices. Switches have many bidirectional network links, but at least one link goes\nto the end node device. These basic topologies can be supplemented with extra links\nto improve performance and reliability. For example, connecting the switches on the\nperiphery of the 2D mesh, shown in (a), using the unused ports on each switch forms\na 2D torus, shown in (b). The hypercube topology, shown in (c) is an n-dimensional inter-\nconnect for 2n nodes, requiring n+1 ports per switch: one for the n nearest neighbor\nnodes and one for the end node device.\nF.4\nNetwork Topology\n\u25a0\nF-37"
    },
    {
        "page": 986,
        "text": "and tori at the expense of higher link and switch costs, in terms of the number of\nlinks and number of ports per node.\nExample\nCompute the cost of interconnecting N devices using a torus topology relative to\ndoing so using a fat tree topology. Consider separately the relative cost of the bidi-\nrectional links and the relative cost of the switches\u2014which is assumed to grow\nquadratically with the number of bidirectional ports. Provide an approximate\nexpression for the case of switches being similar in size.\nAnswer\nUsing k\u0003k switches, the fat tree requires 2 N/k (logk/2 N) switches, assuming the\nlast stage (the root) has the same number of switches as each of the other stages.\nGiven that the number of bidirectional ports in each switch is k (i.e., there are k\ninput ports and k output ports for a k\u0003k switch) and that the switch cost grows\nquadratically with this, total network switch cost is proportional to 2kN logk/2\nN. The link cost is N logk/2 N as each of the logk/2 N stages requires N bidirectional\nlinks, including those between the devices and the fat tree. The torus requires as\nmany switches as nodes, each of them having 2n+1 bidirectional ports, including\nthe port to attach the communicating device, where n is the number of dimensions.\nHence, total switch cost for the torus is (2n+1)2N. Each of the torus nodes requires\n2n+1 bidirectional links for the n different dimensions and the connection for its\nend node device, but as the dimensional links are shared by two nodes, the total\nnumber of links is (2n/2+1)N\u00bc(n+1)N bidirectional links for all N nodes. Thus,\nthe relative costs of the torus topology with respect to the fat tree are\nRelative costswitches \u00bc 2n + 1\n\u00f0\n\u00de2N=2kN logk=2 N \u00bc 2n + 1\n\u00f0\n\u00de2=2klogk=2 N\nRelative costlinks \u00bc n + 1\n\u00f0\n\u00deN=N logk=2 N \u00bc n + 1\n\u00f0\n\u00de=logk=2 N\nWhen switch sizes are similar, 2n+1\ufb03k. In this case, the relative cost is\nRelative costswitches \u00bc 2n + 1\n\u00f0\n\u00de2=2klogk=2 N \u00bc 2n + 1\n\u00f0\n\u00de=2logk=2 N \u00bc k=2logk=2 N\nWhen the number of switch ports (also called switch degree) is small, tori have\nlower cost, particularly when the number of dimensions is low. This is an espe-\ncially useful property when N is large. On the other hand, when larger switches\nand/or a high number of tori dimensions are used, fat trees are less costly and pref-\nerable. For example, when interconnecting 256 nodes, a fat tree is four times more\nexpensive in terms of switch and link costs when 4\u00034 switches are used. This\nhigher cost is compensated for by lower network contention, on average. The\nfat tree is comparable in cost to the torus when 8\u00038 switches are used (e.g., for\ninterconnecting 256 nodes). For larger switch sizes beyond this, the torus costs\nmore than the fat tree as each node includes a switch. This cost can be amortized\nby connecting multiple end node devices per switch, called bristling.\nThe topologies depicted in Figure F.14 all have in common the interesting\ncharacteristic of having their network links arranged in several orthogonal\ndimensions in a regular way. In fact, these topologies all happen to be particular\nF-38\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 987,
        "text": "instances of a larger class of direct network topologies known as k-ary n-cubes,\nwhere k signifies the number of nodes interconnected in each of the n dimen-\nsions. The symmetry and regularity of these topologies simplify network imple-\nmentation (i.e, packaging) and packet routing as the movement of a\npacket along a given network dimension does not modify the number of remain-\ning hops in any other dimension toward its destination. As we will see in the\nnext section, this topological property can be readily exploited by simple rout-\ning algorithms.\nLike their indirect counterpart, direct networks can introduce blocking among\npackets that concurrently request the same path, or part of it. The only exception is\nfully connected networks. The same way that the number of stages and switch hops\nin indirect networks can be reduced by using larger switches, the hop count in\ndirect networks can likewise be reduced by increasing the number of topological\ndimensions via increased switch degree.\nIt may seem to be a good idea always to maximize the number of dimen-\nsions for a system of a certain size and switch cost. However, this is not nec-\nessarily the case. Most electronic systems are built within our three-dimensional\n(3D) world using planar (2D) packaging technology such as integrated circuit\nchips, printed circuit boards, and backplanes. Direct networks with up to three\ndimensions can be implemented using relatively short links within this 3D\nspace, independent of system size. Links in higher-dimensioned networks\nwould require increasingly longer wires or fiber. This increase in link length\nwith system size is also indicative of MINs, including fat trees, which require\neither long links within all the stages or increasingly longer links as more stages\nare added. As we saw in the first example given in Section F.2, flow-controlled\nbuffers increase in size proportionally to link length, thus requiring greater sil-\nicon area. This is among the reasons why the supercomputer with the largest\nnumber of compute nodes existing in 2005, the IBM Blue Gene/L, implemented\na 3D torus network for interprocessor communication. A fat tree would have\nrequired much longer links, rendering a 64K node system less feasible. This\nhighlights the importance of correctly selecting the proper network topology\nthat meets system requirements.\nBesides link length, other constraints derived from implementing the topology\nmay also limit the degree to which a topology can scale. These are available pin-out\nand achievable bisection bandwidth. Pin count is a local restriction on the band-\nwidth of a chip, printed circuit board, and backplane (or chassis) connector. In\na direct network that integrates processor cores and switches on a single chip or\nmultichip module, pin bandwidth is used both for interfacing with main memory\nand for implementing node links. In this case, limited pin count could reduce the\nnumber of switch ports or bit lines per link. In an indirect network, switches are\nimplemented separately from processor cores, allowing most of the pins to be ded-\nicated to communication bandwidth. However, as switches are grouped onto\nboards, the aggregate of all input-output links of the switch fabric on a board\nfor a given topology must not exceed the board connector pin-outs.\nThe bisection bandwidth is a more global restriction that gives the interconnect\ndensity and bandwidth that can be achieved by a given implementation\nF.4\nNetwork Topology\n\u25a0\nF-39"
    },
    {
        "page": 988,
        "text": "(packaging) technology. Interconnect density and clock frequency are related to\neach other: When wires are packed closer together, crosstalk and parasitic capac-\nitance increase, which usually impose a lower clock frequency. For example, the\navailability and spacing of metal layers limit wire density and frequency of on-chip\nnetworks, and copper track density limits wire density and frequency on a printed\ncircuit board. To be implementable, the topology of a network must not exceed the\navailable bisection bandwidth of the implementation technology. Most networks\nimplemented to date are constrained more so by pin-out limitations rather than\nbisection bandwidth, particularly with the recent move to blade-based systems.\nNevertheless, bisection bandwidth largely affects performance.\nFor a given topology, bisection bandwidth, BWBisection, is calculated by\ndividing the network into two roughly equal parts\u2014each with half the\nnodes\u2014and summing the bandwidth of the links crossing the imaginary divid-\ning line. For nonsymmetric topologies, bisection bandwidth is the smallest of all\npairs of equal-sized divisions of the network. For a fully connected network, the\nbisection bandwidth is proportional to N2/2 unidirectional links (or N2/4 bidi-\nrectional links), where N is the number of nodes. For a bus, bisection bandwidth\nis the bandwidth of just the one shared half-duplex link. For other topologies,\nvalues lie in between these two extremes. Network injection and reception\nbisection bandwidth is commonly used as a reference value, which is N/2 for\na network with N injection and reception links, respectively. Any network\ntopology that provides this bisection bandwidth is said to have full bisection\nbandwidth.\nFigure F.15 summarizes the number of switches and links required, the corre-\nsponding switch size, the maximum and average switch hop distances between\nnodes, and the bisection bandwidth in terms of links for several topologies\ndiscussed in this section for interconnecting 64 nodes.\nEvaluation category\nBus\nRing\n2D mesh\n2D torus\nHypercube\nFat tree\nFully connected\nPerformance\nBWBisection in # links\n1\n2\n8\n16\n32\n32\n1024\nMax (ave.) hop count\n1 (1)\n32 (16)\n14 (7)\n8 (4)\n6 (3)\n11 (9)\n1 (1)\nCost\nI/O ports per switch\nNA\n3\n5\n5\n7\n4\n64\nNumber of switches\nNA\n64\n64\n64\n64\n192\n64\nNumber of net. links\n1\n64\n112\n128\n192\n320\n2016\nTotal number of links\n1\n128\n176\n192\n256\n384\n2080\nFigure F.15 Performance and cost of several network topologies for 64 nodes. The bus is the standard reference at\nunit network link cost and bisection bandwidth. Values are given in terms of bidirectional links and ports. Hop count\nincludes a switch and its output link, but not the injection link at end nodes. Except for the bus, values are given for\nthe number of network links and total number of links, including injection/reception links between end node devices\nand the network.\nF-40\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 989,
        "text": "Effects of Topology on Network Performance\nSwitched network topologies require packets to take one or more hops to reach\ntheir destination, where each hop represents the transport of a packet through a\nswitch and one of its corresponding links. Interestingly, each switch and its corre-\nsponding links can be modeled as a black box network connecting more than two\ndevices, as was described in the previous section, where the term \u201cdevices\u201d here\nrefers to end nodes or other switches. The only differences are that the sending and\nreceiving overheads are null through the switches, and the routing, switching, and\narbitration delays are not cumulative but, instead, are delays associated with each\nswitch.\nAs a consequence of the above, if the average packet has to traverse d hops to\nits destination, then TR+TA+TS\u00bc(Tr+Ta+Ts)\u0003d, where Tr, Ta, and Ts are the\nrouting, arbitration, and switching delays, respectively, of a switch. With the\nassumption that pipelining over the network is staged on each hop at the packet\nlevel (this assumption will be challenged in the next section), the transmission\ndelay is also increased by a factor of the number of hops. Finally, with the simpli-\nfying assumption that all injection links to the first switch or stage of switches and\nall links (including reception links) from the switches have approximately the same\nlength and delay, the total propagation delay through the network TTotalProp is the\npropagation delay through a single link, TLinkProp, multiplied by d+1, which is the\nhop count plus one to account for the injection link. Thus, the best-case lower-\nbound expression for average packet latency in the network (i.e., the latency in\nthe absence of contention) is given by the following expression:\nLatency \u00bc Sending overhead + TLinkProp \u0003 d + 1\n\u00f0\n\u00de + Tr + Ta + Ts\n\u00f0\n\u00de\u0003d + Packet size\nBandwidth \u0003 d + 1\n\u00f0\n\u00de + Receiving overhead\nAgain, the expression on page F-40 assumes that switches are able to pipeline\npacket transmission at the packet level.\nFollowing the method presented previously, we can estimate the best-case\nupper bound for effective bandwidth by finding the narrowest section of the\nend-to-end network pipe. Focusing on the internal network portion of that pipe,\nnetwork bandwidth is determined by the blocking properties of the topology.\nNon-blocking behavior can be achieved only by providing many alternative paths\nbetween every source-destination pair, leading to an aggregate network bandwidth\nthat is many times higher than the aggregate network injection or reception band-\nwidth. This is quite costly. As this solution usually is prohibitively expensive, most\nnetworks have different degrees of blocking, which reduces the utilization of the\naggregate bandwidth provided by the topology. This, too, is costly but not in terms\nof performance.\nThe amount of blocking in a network depends on its topology and the traffic\ndistribution. Assuming the bisection bandwidth, BWBisection, of a topology is\nimplementable (as typically is the case), it can be used as a constant measure of\nthe maximum degree of blocking in a network. In the ideal case, the network\nalways achieves full bisection bandwidth irrespective of the traffic behavior, thus\nF.4\nNetwork Topology\n\u25a0\nF-41"
    },
    {
        "page": 990,
        "text": "transferring the bottlenecking point to the injection or reception links. However, as\npackets destined to locations in the other half of the network necessarily must cross\nthe bisection links, those links pose as potential bottleneck links\u2014potentially\nreducing the network bandwidth to below full bisection bandwidth. Fortunately,\nnot all of the traffic must cross the network bisection, allowing more of the aggre-\ngate network bandwidth provided by the topology to be utilized. Also, network\ntopologies with a higher number of bisection links tend to have less blocking as\nmore alternative paths are possible to reach destinations and, hence, a higher per-\ncentage of the aggregate network bandwidth can be utilized. If only a fraction of the\ntraffic must cross the network bisection, as captured by a bisection traffic fraction\nparameter \u03b3 (0<\u03b3\u00011), the network pipe at the bisection is, effectively, widened by\nthe reciprocal of that fraction, assuming a traffic distribution that loads the bisec-\ntion links at least as heavily, on average, as other network links. This defines the\nupper limit on achievable network bandwidth, BWNetwork:\nBWNetwork \u00bc BWBisection\n\u03b3\nAccordingly, the expression for effective bandwidth becomes the following when\nnetwork topology is taken into consideration:\nEffective bandwidth \u00bc min\nN \u0003BWLinkInjection, BWBisection\n\u03b3\n,\u03c3\u0003N \u0003BWLinkReception\n\u0001\n\u0003\nIt is important to note that \u03b3 depends heavily on the traffic patterns generated\nby applications. It is a measured quantity or calculated from detailed traffic\nanalysis.\nExample\nA common communication pattern in scientific programs is to have nearest neigh-\nbor elements of a two-dimensional array to communicate in a given direction. This\npattern is sometimes called NEWS communication, standing for north, east, west,\nand south\u2014the directions on a compass. Map an 8\u00038 array of elements one-to-\none onto 64 end node devices interconnected in the following topologies: bus, ring,\n2D mesh, 2D torus, hypercube, fully connected, and fat tree. How long does it take\nin the best case for each node to send one message to its northern neighbor and one\nto its eastern neighbor, assuming packets are allowed to use any minimal path pro-\nvided by the topology? What is the corresponding effective bandwidth? Ignore ele-\nments that have no northern or eastern neighbors. To simplify the analysis, assume\nthat all networks experience unit packet transport time for each network hop\u2014that\nis, TLinkProp, Tr, Ta, Ts, and packet transmission time for each hop sum to one. Also\nassume the delay through injection links is included in this unit time, and sending/\nreceiving overhead is null.\nAnswer\nThis communication pattern requires us to send 2\u0003(64\u00058) or 112 total packets\u2014\nthat is, 56 packets in each of the two communication phases: northward and east-\nward. The number of hops suffered by packets depends on the topology. Commu-\nnication between sources and destinations are one-to-one, so \u03c3 is 100%.\nF-42\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 991,
        "text": "The injection and reception bandwidth cap the effective bandwidth to a maximum\nof 64 BW units (even though the communication pattern requires only 56 BW\nunits). However, this maximum may get scaled down by the achievable network\nbandwidth, which is determined by the bisection bandwidth and the fraction of\ntraffic crossing it, \u03b3, both of which are topology dependent. Here are the various\ncases:\n\u25a0\nBus\u2014The mapping of the 8\u00038 array elements to nodes makes no difference\nfor the bus as all nodes are equally distant at one hop away. However,\nthe 112 transfers are done sequentially, taking a total of 112 time units.\nThe bisection bandwidth is 1, and \u03b3 is 100%. Thus, effective bandwidth\nis only 1 BW unit.\n\u25a0\nRing\u2014Assume the first row of the array is mapped to nodes 0 to 7, the second\nrow to nodes 8 to 15, and so on. It takes just one time unit for all nodes simul-\ntaneously to send to their eastern neighbor (i.e., a transfer from node i to node\ni+1). With this mapping, the northern neighbor for each node is exactly eight\nhops away so it takes eight time units, which also is done in parallel for all\nnodes. Total communication time is, therefore, 9 time units. The bisection\nbandwidth is 2 bidirectional links (assuming a bidirectional ring), which is\nless than the full bisection bandwidth of 32 bidirectional links. For eastward\ncommunication, because only 2 of the eastward 56 packets must cross the\nbisection in the worst case, the bisection links do not pose as bottlenecks.\nFor northward communication, 8 of the 56 packets must cross the two bisec-\ntion links, yielding a \u03b3 of 10/112\u00bc8.93%. Thus, the network bandwidth is\n2/.0893\u00bc22.4 BW units. This limits the effective bandwidth at 22.4 BW\nunits as well, which is less than half the bandwidth required by the commu-\nnication pattern.\n\u25a0\n2D mesh\u2014There are eight rows and eight columns in our grid of 64 nodes,\nwhich is a perfect match to the NEWS communication. It takes a total of just\n2 time units for all nodes to send simultaneously to their northern neighbors\nfollowed by simultaneous communication to their eastern neighbors. The\nbisection bandwidth is 8 bidirectional links, which is less than full bisection\nbandwidth. However, the perfect matching of this nearest neighbor communi-\ncation pattern on this topology allows the maximum effective bandwidth to be\nachieved regardless. For eastward communication, 8 of the 56 packets must\ncross the bisection in the worst case, which does not exceed the bisection band-\nwidth. None of the northward communications crosses the same network bisec-\ntion, yielding a \u03b3 of 8/112\u00bc7.14% and a network bandwidth of 8/0.0714\u00bc112\nBW units. The effective bandwidth is, therefore, limited by the communication\npattern at 56 BW units as opposed to the mesh network.\n\u25a0\n2D torus\u2014Wrap-around links of the torus are not used for this communication\npattern, so the torus has the same mapping and performance as the mesh.\nF.4\nNetwork Topology\n\u25a0\nF-43"
    },
    {
        "page": 992,
        "text": "\u25a0\nHypercube\u2014Assume elements in each row are mapped to the same location\nwithin the eight 3-cubes comprising the hypercube such that consecutive row\nelements are mapped to nodes only one hop away. Northern neighbors can be\nsimilarlymappedtonodesonlyonehopawayinanorthogonaldimension.Thus,\nthe communication pattern takes just 2 time units. The hypercube provides full\nbisection bandwidth of 32 links, but at most only 8 of the 112 packets must cross\nthe bisection. Thus, effective bandwidth is limited only by the communication\npattern to be 56 BW units, not by the hypercube network.\n\u25a0\nFully connected\u2014Here, nodes are equally distant at one hop away, regardless\nof the mapping. Parallel transfer of packets in both the northern and eastern\ndirections would take only 1 time unit if the injection and reception links\ncould source and sink two packets at a time. As this is not the case, 2 time units\nare required. Effective bandwidth is limited by the communication pattern\nat 56 BW units, so the 1024 network bisection links largely go underutilized.\n\u25a0\nFat tree\u2014Assume the same mapping of elements to nodes as is done for the\nring and the use of switches with eight bidirectional ports. This allows simul-\ntaneous communication to eastern neighbors that takes at most three hops and,\ntherefore, 3 time units through the three bidirectional stages interconnecting the\neight nodes in each of the eight groups of nodes. The northern neighbor for\neach node resides in the adjacent group of eight nodes, which requires five\nhops, or 5 time units. Thus, the total time required on the fat tree is 8 time units.\nThe fat tree provides full bisection bandwidth, so in the worst case of half the\ntraffic needing to cross the bisection, an effective bandwidth of 56 BW units (as\nlimited by the communication pattern and not by the fattree network) is\nachieved when packets are continually injected.\nThe above example should not lead one to the wrong conclusion that meshes\nare just as good as tori, hypercubes, fat trees, and other networks with higher bisec-\ntion bandwidth. A number of simplifications that benefit low-bisection networks\nwere assumed to ease the analysis. In practice, packets typically are larger than the\nlink width and occupy links for many more than just one network cycle. Also,\nmany communication patterns do not map so cleanly to the 2D mesh network\ntopology; instead, usually they are more global and irregular in nature. These\nand other factors combine to increase the chances of packets blocking in low-\nbisection networks, increasing latency and reducing effective bandwidth.\nTo put this discussion on topologies into further perspective, Figure F.16\nlistsvariousattributesoftopologiesusedincommercialhigh-performancecomputers.\nF.5\nNetwork Routing, Arbitration, and Switching\nRouting, arbitration, and switching are performed at every switch along a packet\u2019s\npath in a switched media network, no matter what the network topology. Numerous\ninteresting techniques for accomplishing these network functions have been\nF-44\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 993,
        "text": "proposed in the literature. In this section, we focus on describing a representative\nset of approaches used in commercial systems for the more commonly used net-\nwork topologies. Their impact on performance is also highlighted.\nRouting\nThe routing algorithm defines which network path, or paths, are allowed for each\npacket. Ideally, the routing algorithm supplies shortest paths to all packets such that\nCompany\nSystem\n[network]\nname\nMax.\nnumber\nof nodes\n[\u00d7 #\nCPUs]\nBasic network\ntopology\nInjection\n[reception]\nnode BW in\nMB/sec\n# of data\nbits per\nlink per\ndirection\nRaw\nnetwork\nlink BW\nper\ndirection\nin MB/sec\nRaw network\nbisection BW\n(bidirectional)\nin GB/sec\nIntel\nASCI Red\nParagon\n4816\n[\u00032]\n2D mesh\n64\u000364\n400 [400]\n16 bits\n400\n51.2\nIBM\nASCI White\nSP Power3\n[Colony]\n512\n[\u000316]\nBidirectional\nMIN with 8-\nport\nbidirectional\nswitches\n(typically a fat\ntree or Omega)\n500 [500]\n8 bits (+1\nbit of\ncontrol)\n500\n256\nIntel\nThunder\nItanium2\nTiger4\n[QsNetII]\n1024\n[\u00034]\nFat tree with 8-\nport\nbidirectional\nswitches\n928 [928]\n8 bits (+2\nof control\nfor 4b/5b\nencoding)\n1333\n1365\nCray\nXT3\n[SeaStar]\n30,508\n[\u00031]\n3D torus\n40\u000332\u000324\n3200 [3200]\n12 bits\n3800\n5836.8\nCray\nX1E\n1024\n[\u00031]\n4-way bristled\n2D torus\n(\u000423\u000311)\nwith express\nlinks\n1600 [1600]\n16 bits\n1600\n51.2\nIBM\nASC Purple\npSeries 575\n[Federation]\n>1280\n[\u00038]\nBidirectional\nMIN with 8-\nport\nbidirectional\nswitches\n(typically a fat\ntree or Omega)\n2000 [2000]\n8 bits (+2\nbits of\ncontrol for\nnovel 5b/\n6b\nencoding\nscheme)\n2000\n2560\nIBM\nBlue Gene/\nL eServer\nSol. [Torus\nNet.]\n65,536\n[\u00032]\n3D torus\n32\u000332\u000364\n612.5\n[1050]\n1 bit (bit\nserial)\n175\n358.4\nFigure F.16 Topological characteristics of interconnection networks used in commercial high-performance\nmachines.\nF.5\nNetwork Routing, Arbitration, and Switching\n\u25a0\nF-45"
    },
    {
        "page": 994,
        "text": "traffic load is evenly distributed across network links to minimize contention.\nHowever, some paths provided by the network topology may not be allowed in\norder to guarantee that all packets can be delivered, no matter what the traffic\nbehavior. Paths that have an unbounded number of allowed nonminimal hops from\npacket sources, for instance, may result in packets never reaching their destina-\ntions. This situation is referred to as livelock. Likewise, paths that cause a set of\npackets to block in the network forever waiting only for network resources (i.e.,\nlinks or associated buffers) held by other packets in the set also prevent packets\nfrom reaching their destinations. This situation is referred to as deadlock. As dead-\nlock arises due to the finiteness of network resources, the probability of its occur-\nrence increases with increased network traffic and decreased availability of\nnetwork resources. For the network to function properly, the routing algorithm must\nguardagainstthis anomaly,whichcanoccur invariousforms\u2014forexample,routing\ndeadlock, request-reply (protocol) deadlock, and fault-induced (reconfiguration)\ndeadlock, etc. At the same time, for the network to provide the highest possible per-\nformance, the routing algorithm must be efficient\u2014allowing as many routing\noptions to packets as there are paths provided by the topology, in the best case.\nThe simplest way of guarding against livelock is to restrict routing such that\nonly minimal paths from sources to destinations are allowed or, less restrictively,\nonly a limited number of nonminimal hops. The strictest form has the added benefit\nof consuming the minimal amount of network bandwidth, but it prevents packets\nfrom being able to use alternative nonminimal paths in case of contention or faults\nalong the shortest (minimal) paths.\nDeadlock is more difficult to guard against. Two common strategies are used in\npractice: avoidance and recovery. In deadlock avoidance, the routing algorithm\nrestricts the paths allowed by packets to only those that keep the global network\nstate deadlock-free. A common way of doing this consists of establishing an order-\ning between a set of resources\u2014the minimal set necessary to support network full\naccess\u2014and granting those resources to packets in some total or partial order such\nthat cyclic dependency cannot form on those resources. This allows an escape path\nalways to be supplied to packets no matter where they are in the network to avoid\nentering a deadlock state. In deadlock recovery, resources are granted to packets\nwithout regard for avoiding deadlock. Instead, as deadlock is possible, some mech-\nanism is used to detect the likely existence of deadlock. If detected, one or more\npackets are removed from resources in the deadlock set\u2014possibly by regressively\ndropping the packets or by progressively redirecting the packets onto special dead-\nlock recovery resources. The freed network resources are then granted to other\npackets needing them to resolve the deadlock.\nLet us consider routing algorithms designed for distributed switched networks.\nFigure F.17(a) illustrates one of many possible deadlocked configurations for\npackets within a region of a 2D mesh network. The routing algorithm can avoid\nall such deadlocks (and livelocks) by allowing only the use of minimal paths that\ncross the network dimensions in some total order. That is, links of a given dimen-\nsion are not supplied to a packet by the routing algorithm until no other links are\nneeded by the packet in all of the preceding dimensions for it to reach its\nF-46\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 995,
        "text": "destination. This is illustrated in Figure F.17(b), where dimensions are crossed in\nXY dimension order. All the packets must follow the same order when traversing\ndimensions, exiting a dimension only when links are no longer required in that\ndimension. This well-known algorithm is referred to as dimension-order routing\n(DOR) or e-cube routing in hypercubes. It is used in many commercial systems\nbuilt from distributed switched networks and on-chip networks. As this routing\nalgorithm always supplies the same path for a given source-destination pair, it\nis a deterministic routing algorithm.\nCrossing dimensions in order on some minimal set of resources required to\nsupport network full access avoids deadlock in meshes and hypercubes. However,\nfor distributed switched topologies that have wrap-around links (e.g., rings and\ntori), a total ordering on a minimal set of resources within each dimension is also\nneeded if resources are to be used to full capacity. Alternatively, some empty\nresources or bubbles along the dimensions would be required to remain below full\ncapacity and avoid deadlock. To allow full access, either the physical links must be\nduplicated or the logical buffers associated with each link must be duplicated,\nresulting in physical channels or virtual channels, respectively, on which the\nordering is done. Ordering is not necessary on all network resources to avoid dead-\nlock\u2014it is needed only on some minimal set required to support network full\naccess (i.e., some escape resource set). Routing algorithms based on this technique\n(called Duato\u2019s protocol) can be defined that allow alternative paths provided by\nthe topology to be used for a given source-destination pair in addition to the escape\nresource set. One of those allowed paths must be selected, preferably the most\n(A)\n(B)\ns1\ns2\nd3\nd4\nd5\nd2\nd1\ns4\ns5\ns3\ns1\ns2\nd3\nd4\nd5\nd2\nd1\ns4\ns5\ns3\nFigure F.17 A mesh network with packets routing from sources, si, to destinations, di. (a) Deadlock forms from\npackets destined to d1 through d4 blocking on others in the same set that fully occupy their requested buffer\nresources one hop away from their destinations. This deadlock cycle causes other packets needing those resources\nalso to block, like packets from s5 destined to d5 that have reached node s3. (b) Deadlock is avoided using dimension-\norder routing. In this case, packets exhaust their routes in the X dimension before turning into the Y dimension in\norder to complete their routing.\nF.5\nNetwork Routing, Arbitration, and Switching\n\u25a0\nF-47"
    },
    {
        "page": 996,
        "text": "efficient one. Adapting the path in response to prevailing network traffic condi-\ntions enables the aggregate network bandwidth to be better utilized and contention\nto be reduced. Such routing capability is referred to as adaptive routing and is used\nin many commercial systems.\nExample\nHow many of the possible dimensional turns are eliminated by dimension-order\nrouting on an n-dimensional mesh network? What is the fewest number of turns\nthat actually need to be eliminated while still maintaining connectedness and dead-\nlock freedom? Explain using a 2D mesh network.\nAnswer\nThe dimension-order routing algorithm eliminates exactly half of the possible\ndimensional turns as it is easily proven that all turns from any lower-ordered\ndimension into any higher-ordered dimension are allowed, but the converse is\nnot true. For example, of the eight possible turns in the 2D mesh shown in\nFigure F.17, the four turns from X+ to Y+, X+ to Y\u0005, X\u0005 to Y+, and X\u0005 to Y\u0005\nare allowed, where the signs (+ or \u0005) refer to the direction of travel within a dimen-\nsion. The four turns from Y+ to X+, Y+ to X\u0005, Y\u0005 to X+, and Y\u0005 to X\u0005 are dis-\nallowed turns. The elimination of these turns prevents cycles of any kind from\nforming\u2014and, thus, avoids deadlock\u2014while keeping the network connected.\nHowever, it does so at the expense of not allowing any routing adaptivity.\nThe Turn Model routing algorithm proves that the minimum number of elim-\ninated turns to prevent cycles and maintain connectedness is a quarter of the pos-\nsible turns, but the right set of turns must be chosen. Only some particular set of\neliminated turns allow both requirements to be satisfied. With the elimination of\nthe wrong set of a quarter of the turns, it is possible for combinations of allowed\nturns to emulate the eliminated ones (and, thus, form cycles and deadlock) or for\nthe network not to be connected. For the 2D mesh, for example, it is possible to\neliminate only the two turns ending in the westward direction (i.e., Y+ to X\u0005\nand Y\u0005 to X\u0005) by requiring packets to start their routes in the westward direction\n(if needed) to maintain connectedness. Alternatives to this west-first routing for 2D\nmeshes are negative-first routing and north-last routing. For these, the extra quarter\nof turns beyond that supplied by DOR allows for partial adaptivity in routing, mak-\ning these adaptive routing algorithms.\nRouting algorithms for centralized switched networks can similarly be\ndefined to avoid deadlocks by restricting the use of resources in some total\nor partial order. For fat trees, resources can be totally ordered along paths start-\ning from the input leaf stage upward to the root and then back down to the out-\nput leaf stage. The routing algorithm can allow packets to use resources in\nincreasing partial order, first traversing up the tree until they reach some least\ncommon ancestor (LCA) of the source and destination, and then back down the\ntree until they reach their destinations. As there are many least common ances-\ntors for a given destination, multiple alternative paths are allowed while going\nup the tree, making the routing algorithm adaptive. However, only a single\nF-48\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 997,
        "text": "deterministic path to the destination is provided by the fat tree topology from a\nleast common ancestor. This self-routing property is common to many MINs\nand can be readily exploited: The switch output port at each stage is given sim-\nply by shifts of the destination node address.\nMore generally, a tree graph can be mapped onto any topology\u2014whether\ndirect or indirect\u2014and links between nodes at the same tree level can be allowed\nby assigning directions to them, where \u201cup\u201d designates paths moving toward the\ntree root and \u201cdown\u201d designates paths moving away from the root node. This\nallows for generic up*/down* routing to be defined on any topology such\nthat packets follow paths (possibly adaptively) consisting of zero or more up links\nfollowed by zero or more down links to their destination. Up/down ordering pre-\nvents cycles from forming, avoiding deadlock. This routing technique was used in\nAutonet\u2014a self-configuring switched LAN\u2014and in early Myrinet SANs.\nRouting algorithms are implemented in practice by a combination of the rout-\ning information placed in the packet header by the source node and the routing\ncontrol mechanism incorporated in the switches. For source routing, the entire\nrouting path is precomputed by the source\u2014possibly by table lookup\u2014and placed\nin the packet header. This usually consists of the output port or ports supplied for\neach switch along the predetermined path from the source to the destination, which\ncan be stripped off by the routing control mechanism at each switch. An additional\nbit field can be included in the header to signify whether adaptive routing is\nallowed (i.e., that any one of the supplied output ports can be used). For distributed\nrouting, the routing information usually consists of the destination address. This is\nused by the routing control mechanism in each switch along the path to determine\nthe next output port, either by computing it using a finite-state machine or by look-\ning it up in a local routing table (i.e., forwarding table). Compared to distributed\nrouting, source routing simplifies the routing control mechanism within the net-\nwork switches, but it requires more routing bits in the header of each packet, thus\nincreasing the header overhead.\nArbitration\nThe arbitration algorithm determines when requested network paths are available\nfor packets. Ideally, arbiters maximize the matching of free network resources and\npackets requesting those resources. At the switch level, arbiters maximize the\nmatching of free output ports and packets located in switch input ports requesting\nthose output ports. When all requests cannot be granted simultaneously, switch\narbiters resolve conflicts by granting output ports to packets in a fair way such that\nstarvation of requested resources by packets is prevented. This could happen to\npackets in shorter queues if a serve-longest-queue (SLQ) scheme is used. For\npackets having the same priority level, simple round-robin (RR) or age-based\nschemes are sufficiently fair and straightforward to implement.\nArbitration can be distributed to avoid centralized bottlenecks. A straightfor-\nward technique consists of two phases: a request phase and a grant phase. Let\nus assume that each switch input port has an associated queue to hold incoming\nF.5\nNetwork Routing, Arbitration, and Switching\n\u25a0\nF-49"
    },
    {
        "page": 998,
        "text": "packets and that each switch output port has an associated local arbiter implement-\ning a round-robin strategy. Figure F.18(a) shows a possible set of requests for a\nfour-port switch. In the request phase, packets at the head of each input port queue\nsend a single request to the arbiters corresponding to the output ports requested by\nthem. Then, each output port arbiter independently arbitrates among the requests it\nreceives, selecting only one. In the grant phase, one of the requests to each arbiter\nis granted the requested output port. When two packets from different input ports\nrequest the same output port, only one receives a grant, as shown in the figure. As a\nconsequence, some output port bandwidth remains unused even though all input\nqueues have packets to transmit.\nThe simple two-phase technique can be improved by allowing several simul-\ntaneous requests to be made by each input port, possibly coming from different\nvirtual channels or from multiple adaptive routing options. These requests are sent\nto different output port arbiters. By submitting more than one request per input\nport, the probability of matching increases. Now, arbitration requires three phases:\nrequest, grant, and acknowledgment. Figure F.18(b) shows the case in which up to\ntwo requests can be made by packets at each input port. In the request phase,\nrequests are submitted to output port arbiters, and these arbiters select one of\nthe received requests, as is done for the two-phase arbiter. Likewise, in the grant\nphase, the selected requests are granted to the corresponding requesters. Taking\ninto account that an input port can submit more than one request, it may receive\nmore than one grant. Thus, it selects among possibly multiple grants using some\narbitration strategy such as round-robin. The selected grants are confirmed to the\ncorresponding output port arbiters in the acknowledgment phase.\nAs can be seen in Figure F.18(b), it could happen that an input port that submits\nseveral requests does not receive any grants, while some of the requested ports\nremain free. Because of this, a second arbitration iteration can improve the prob-\nability of matching. In this iteration, only the requests corresponding to non-\nmatched input and output ports are submitted. Iterative arbiters with multiple\n(B)\n(A)\nt\nn\ne\nm\ng\nd\nel\nw\no\nn\nk\nc\nA\nt\nn\na\nr\nG\nts\ne\nu\nq\ne\nR\nRequest\nGrant\nFigure F.18 Two arbitration techniques. (a) Two-phased arbitration in which two of\nthe four input ports are granted requested output ports. (b) Three-phased arbitration\nin which three of the four input ports are successful in gaining the requested output\nports, resulting in higher switch utilization.\nF-50\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 999,
        "text": "requests per input port are able to increase the utilization of switch output ports and,\nthus, the network link bandwidth. However, this comes at the expense of additional\narbiter complexity and increased arbitration delay, which could increase the router\nclock cycle time if it is on the critical path.\nSwitching\nThe switching technique defines how connections are established in the network.Ide-\nally, connections between networkresourcesareestablished or\u201cswitchedin\u201donlyfor\nas long as they are actually needed and exactly at the point that they are ready and\nneeded to beused, considering both time and space.This allows efficient use of avail-\nablenetworkbandwidthbycompetingtrafficflowsandminimallatency.Connections\nateachhopalongthetopologicalpathallowedbytheroutingalgorithmandgrantedby\nthe arbitration algorithm can be established in three basic ways: prior to packet arrival\nusing circuit switching, upon receipt of the entire packet using store-and-forward\npacketswitching,oruponreceipt ofonlyportionsofthe packetwithunitsizenosmal-\nler than that of the packet header using cut-through packet switching.\nCircuit switching establishes a circuit a priori such that network bandwidth is\nallocated for packet transmissions along an entire source-destination path. It is\npossible to pipeline packet transmission across the circuit using staging at each\nhop along the path, a technique known as pipelined circuit switching. As routing,\narbitration, and switching are performed only once for one or more packets, routing\nbitsarenotneededintheheaderofpackets,thusreducinglatencyandoverhead.This\ncan be very efficient when information is continuously transmitted between devices\nfor the same circuit setup. However, as network bandwidth is removed from the\nshared poolandpreallocated regardless ofwhether sourcesareinneedofconsuming\nit or not, circuit switching can be very inefficient and highly wasteful of bandwidth.\nPacket switching enables network bandwidth to be shared and used more\nefficiently when packets are transmitted intermittently, which is the more common\ncase. Packet switching comes in two main varieties\u2014store-and-forward and\ncutthrough switching, both of which allow network link bandwidth to be multi-\nplexed on packet-sized or smaller units of information. This better enables band-\nwidth sharing by packets originating from different sources. The finer granularity\nof sharing, however, increases the overhead needed to perform switching: Routing,\narbitration, and switching must be performed for every packet, and routing and\nflow control bits are required for every packet if flow control is used.\nStore-and-forward packet switching establishes connections such that a packet\nis forwarded to the next hop in sequence along its source-destination path only after\nthe entire packet is first stored (staged) at the receiving switch. As packets are\ncompletely stored at every switch before being transmitted, links are completely\ndecoupled, allowing full link bandwidth utilization even if links have very different\nbandwidths. This property is very important in WANs, but the price to pay is\npacket latency; the total routing, arbitration, and switching delay is multiplicative\nwith the number of hops, as we have seen in Section F.4 when analyzing perfor-\nmance under this assumption.\nF.5\nNetwork Routing, Arbitration, and Switching\n\u25a0\nF-51"
    },
    {
        "page": 1000,
        "text": "Cut-through packet switching establishes connections such that a packet can \u201ccut\nthrough\u201d switches in a pipelined manner once the header portion of the packet\n(or equivalent amount of payload trailing the header) is staged at receiving switches.\nThatis,therestofthepacketneednotarrivebeforeswitchinginthegrantedresources.\nThisallowsrouting,arbitration,andswitchingdelaytobeadditivewiththenumberof\nhops rather than multiplicative to reduce total packet latency. Cut-through comes\nin two varieties, the main differences being the size of the unit of information on\nwhich flow control is applied and, consequently, the buffer requirements at switches.\nVirtual cut-through switching implements flow control at the packet level, whereas\nwormhole switching implements it on flow units, or flits, which are smaller than\nthe maximum packet size but usually at least as large as the packet header. Since\nwormhole switches need to be capable of storing only a small portion of a packet,\npackets that block in the network may span several switches. This can cause other\npackets to block on the links they occupy, leading to premature network saturation\nand reduced effective bandwidth unless some centralized buffer is used within the\nswitch to store them\u2014a technique called buffered wormhole switching. As chips\ncan implement relatively large buffers in current technology, virtual cut-through is\nthe more commonly used switching technique. However, wormhole switching\nmay still be preferred in OCNs designed to minimize silicon resources.\nPremature network saturation caused by wormhole switching can be mitigated\nby allowing several packets to share the physical bandwidth of a link simulta-\nneously via time-multiplexed switching at the flit level. This requires physical links\nto have a set of virtual channels (i.e., the logical buffers mentioned previously) at\neach end, into which packets are switched. Before, we saw how virtual channels\ncan be used to decouple physical link bandwidth from buffered packets in such a\nway as to avoid deadlock. Now, virtual channels are multiplexed in such a way that\nbandwidth is switched in and used by flits of a packet to advance even though the\npacket may share some links in common with a blocked packet ahead. This, again,\nallows network bandwidth to be used more efficiently, which, in turn, reduces the\naverage packet latency.\nImpact on Network Performance\nRouting, arbitration, and switching can impact the packet latency of a loaded\nnetwork by reducing the contention delay experienced by packets. For an unloaded\nnetwork that has no contention, the algorithms used to perform routing and\narbitration have no impact on latency other than to determine the amount of delay\nincurred in implementing those functions at switches\u2014typically, the pin-to-pin\nlatency of a switch chip is several tens of nanoseconds. The only change to the\nbest-case packet latency expression given in the previous section comes from\nthe switching technique. Store-and-forward packet switching was assumed before\nin which transmission delay for the entire packet is incurred on all d hops plus at the\nsource node. For cut-through packet switching, transmission delay is pipelined\nacross the network links comprising the packet\u2019s path at the granularity of the\npacket header instead of the entire packet. Thus, this delay component is reduced,\nas shown in the following lower-bound expression for packet latency:\nF-52\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1001,
        "text": "Latency \u00bc Sending overhead + TLinkProp \u0003 d + 1\n\u00f0\n\u00de + Tr + \u03c4a + TS\n\u00f0\n\u00de\u0003d + Packet + d \u0003Header\n\u00f0\n\u00de\n\u00f0\n\u00de\nBandwidth\n+ Receiving overhead\nThe effective bandwidth is impacted by how efficiently routing, arbitration, and\nswitching allow network bandwidth to be used. The routing algorithm can distrib-\nute traffic more evenly across a loaded network to increase the utilization of the\naggregate bandwidth provided by the topology\u2014particularly, by the bisection\nlinks. The arbitration algorithm can maximize the number of switch output ports\nthat accept packets, which also increases the utilization of network bandwidth. The\nswitching technique can increase the degree of resource sharing by packets, which\nfurther increases bandwidth utilization. These combine to affect network band-\nwidth, BWNetwork, by an efficiency factor, \u03c1, where 0<\u03c1\u00011:\nBWNetwork \u00bc \u03c1\u0003BWBisection\n\u03b3\nThe efficiency factor, \u03c1, is difficult to calculate or to quantify by means other than\nsimulation. Nevertheless, with this parameter we can estimate the best-case upper-\nbound effective bandwidth by using the following expression that takes into\naccount the effects of routing, arbitration, and switching:\nEffective bandwidth \u00bc min\nN \u0003BWLinkInjection, \u03c1\u0003BWBisection\n\u03b3\n,\u03c3\u0003N \u0003BWLinkReception\n\u0001\n\u0003\nWe note that \u03c1 also depends on how well the network handles the traffic generated\nby applications. For instance, \u03c1 could be higher for circuit switching than for\ncut-through switching if large streams of packets are continually transmitted\nbetween a source-destination pair, whereas the converse could be true if packets\nare transmitted intermittently.\nExample\nCompare the performance of deterministic routing versus adaptive routing for a 3D\ntorus network interconnecting 4096 nodes. Do so by plotting latency versus\napplied load and throughput versus applied load. Also compare the efficiency\nof the best and worst of these networks. Assume that virtual cut-through switching,\nthree-phase arbitration, and virtual channels are implemented. Consider separately\nthe cases for two and four virtual channels, respectively. Assume that one of the\nvirtual channels uses bubble flow control in dimension order so as to avoid dead-\nlock; the other virtual channels are used either in dimension order (for deterministic\nrouting) or minimally along shortest paths (for adaptive routing), as is done in the\nIBM Blue Gene/L torus network.\nAnswer\nIt is very difficult to compute analytically the performance of routing algorithms\ngiven that their behavior depends on several network design parameters with com-\nplex interdependences among them. As a consequence, designers typically resort\nto cycle-accurate simulators to evaluate performance. One way to evaluate the\neffect of a certain design decision is to run sets of simulations over a range of net-\nwork loads, each time modifying one of the design parameters of interest while\nF.5\nNetwork Routing, Arbitration, and Switching\n\u25a0\nF-53"
    },
    {
        "page": 1002,
        "text": "keeping the remaining ones fixed. The use of synthetic traffic loads is quite fre-\nquent in these evaluations as it allows the network to stabilize at a certain working\npoint and for behavior to be analyzed in detail. This is the method we use here\n(alternatively, trace-driven or execution-driven simulation can be used).\nFigure F.19 shows the typical interconnection network performance plots. On\nthe left, average packet latency (expressed in network cycles) is plotted as a func-\ntion of applied load (traffic generation rate) for the two routing algorithms with two\nand four virtual channels each; on the right, throughput (traffic delivery rate) is\nsimilarly plotted. Applied load is normalized by dividing it by the number of nodes\nin the network (i.e., bytes per cycle per node). Simulations are run under the\nassumption of uniformly distributed traffic consisting of 256-byte packets, where\nflits are byte sized. Routing, arbitration, and switching delays are assumed to sum\nto 1 network cycle per hop while the time-of-flight delay over each link is assumed\nto be 10 cycles. Link bandwidth is 1 byte per cycle, thus providing results that are\nindependent of network clock frequency.\nAs can be seen, the plots within each graph have similar characteristic shapes,\nbut they have different values. For the latency graph, all start at the no-load latency\nAverage packet latency (cycles)\n10,000\n8000\n6000\n4000\n0.01\n2000\n0\nApplied load (bytes/cycle/node)\n(A)\n0.41\n0.33\n0.25\n0.17\n0.09\nThroughput (bytes/cycle/node)\n0.4\n0.3\n0.2\n0.1\n0.01\n0\nApplied load (bytes/cycle/node)\n(B)\n0.97\n0.49\n0.61\n0.73\n0.85\n0.25\n0.37\n0.13\nDeterministic DOR, 2 VC\nDeterministic DOR, 4 VC\nAdaptive routing, 2 VC\nAdaptive routing, 4 VC\nAdaptive routing, 4 VC\nDeterministic DOR, 4 VC\nAdaptive routing, 2 VC\nDeterministic DOR, 2 VC\nFigure F.19 Deterministic routing is compared against adaptive routing, both with either two or four virtual\nchannels, assuming uniformly distributed traffic on a 4 K node 3D torus network with virtual cut-through switch-\ning and bubble flow control to avoid deadlock. (a) Average latency is plotted versus applied load, and (b) through-\nput is plotted versus applied load (the upper grayish plots show peak throughput, and the lower black plots show\nsustained throughput). Simulation data were collected by P. Gilabert and J. Flich at the Universidad Polit\u00e8cnica de\nVal\u00e8ncia, Spain (2006).\nF-54\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1003,
        "text": "as predicted by the latency expression given above, then slightly increase with traf-\nfic load as contention for network resources increases. At higher applied loads,\nlatency increases exponentially, and the network approaches its saturation point\nas it is unable to absorb the applied load, \u00bc causing packets to queue up at their\nsource nodes awaiting injection. In these simulations, the queues keep growing\nover time, making latency tend toward infinity. However, in practice, queues reach\ntheir capacity and trigger the application to stall further packet generation, or the\napplication throttles itself waiting for acknowledgments/responses to outstanding\npackets. Nevertheless, latency grows at a slower rate for adaptive routing as alter-\nnative paths are provided to packets along congested resources.\nFor this same reason, adaptive routing allows the network to reach a higher peak\nthroughput for the same number of virtual channels as compared to deterministic\nrouting. At nonsaturation loads, throughput increases fairly linearly with applied\nload. When the network reaches its saturation point, however, it is unable to deliver\ntraffic at the same rate at which traffic is generated. The saturation point, therefore,\nindicates the maximum achievable or \u201cpeak\u201d throughput, which would be no more\nthan that predicted by the effective bandwidth expression given above. Beyond\nsaturation, throughput tends to drop as a consequence of massive head-of-line\nblocking across the network (as will be explained further in Section F.6), very\nmuch like cars tend to advance more slowly at rush hour. This is an important\nregion of the throughput graph as it shows how significant of a performance drop\nthe routing algorithm can cause if congestion management techniques (discussed\nbriefly in Section F.7) are not used effectively. In this case, adaptive routing has\nmore of a performance drop after saturation than deterministic routing, as mea-\nsured by the postsaturation sustained throughput.\nFor both routing algorithms, more virtual channels (i.e., four) give packets a\ngreater ability to pass over blocked packets ahead, allowing for a higher peak\nthroughput as compared to fewer virtual channels (i.e., two). For adaptive routing\nwith four virtual channels, the peak throughput of 0.43 bytes/cycle/node is near the\nmaximum of 0.5 bytes/cycle/node that can be obtained with 100% efficiency (i.e.,\n\u03c1\u00bc100%), assuming there is enough injection and reception bandwidth to make\nthe network bisection the bottlenecking point. In that case, the network bandwidth\nis simply 100% times the network bisection bandwidth (BWBisection) divided by the\nfraction of traffic crossing the bisection (\u03b3), as given by the expression above. Tak-\ning into account that the bisection splits the torus into two equally sized halves, \u03b3 is\nequal to 0.5 for uniform traffic as only half the injected traffic is destined to a node\nat the other side of the bisection. The BWBisection for a 4096-node 3D torus network\nis 16\u000316\u00034 unidirectional links times the link bandwidth (i.e., 1 byte/cycle). If\nwe normalize the bisection bandwidth by dividing it by the number of nodes (as we\ndid with network bandwidth), the BWBisection is 0.25 bytes/cycle/node. Dividing\nthis by \u03b3 gives the ideal maximally obtainable network bandwidth of 0.5 bytes/\ncycle/node.\nWe can find the efficiency factor, \u03c1, of the simulated network simply by divid-\ning the measured peak throughput by the ideal throughput. The efficiency factor for\nF.5\nNetwork Routing, Arbitration, and Switching\n\u25a0\nF-55"
    },
    {
        "page": 1004,
        "text": "the network with fully adaptive routing and four virtual channels is 0.43/(0.25/\n0.5)\u00bc86%, whereas for the network with deterministic routing and two virtual\nchannels it is 0.37/(0.25/0.5)\u00bc74%. Besides the 12% difference in efficiency\nbetween the two, another 14% gain in efficiency might be obtained with even bet-\nter routing, arbitration, switching, and virtual channel designs.\nTo put this discussion on routing, arbitration, and switching in perspective,\nFigure F.20 lists the techniques used in SANs designed for commercial high-\nperformance computers. In addition to being applied to the SANs as shown in\nthe figure, the issues discussed in this section also apply to other interconnect\ndomains: from OCNs to WANs.\nF.6\nSwitch Microarchitecture\nNetwork switches implement the routing, arbitration, and switching functions of\nswitched-media networks. Switches also implement buffer management mecha-\nnisms and, in the case of lossless networks, the associated flow control. For some\nnetworks, switches also implement part of the network management functions that\nexplore, configure, and reconfigure the network topology in response to boot-up\nand failures. Here, we reveal the internal structure of network switches by describ-\ning a basic switch microarchitecture and various alternatives suitable for different\nrouting, arbitration, and switching techniques presented previously.\nBasic Switch Microarchitecture\nThe internal data path of a switch provides connectivity among the input and output\nports. Although a shared bus or a multiported central memory could be used, these\nsolutions are insufficient or too expensive, respectively, when the required aggre-\ngate switch bandwidth is high. Most high-performance switches implement an\ninternal crossbar to provide nonblocking connectivity within the switch, thus\nallowing concurrent connections between multiple input-output port pairs. Buffer-\ning of blocked packets can be done using first in, first out (FIFO) or circular\nqueues, which can be implemented as dynamically allocatable multi-queues\n(DAMQs) in static RAM to provide high capacity and flexibility. These queues\ncan be placed at input ports (i.e., input buffered switch), output ports (i.e., output\nbuffered switch), centrally within the switch (i.e., centrally buffered switch), or at\nboth the input and output ports of the switch (i.e., input-output-buffered switch).\nFigure F.21 shows a block diagram of an input-output-buffered switch.\nRouting can be implemented using a finite-state machine or forwarding table\nwithin the routing control unit of switches. In the former case, the routing infor-\nmation given in the packet header is processed by a finite-state machine that deter-\nmines the allowed switch output port (or ports if routing is adaptive), according to\nthe routing algorithm. Portions of the routing information in the header are usually\nF-56\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1005,
        "text": "Company\nSystem\n[network]\nname\nMax.\nnumber\nof nodes\n[\u00d7 #\nCPUs]\nBasic\nnetwork\ntopology\nSwitch\nqueuing\n(buffers)\nNetwork\nrouting\nalgorithm\nSwitch\narbitration\ntechnique\nNetwork\nswitching\ntechnique\nIntel\nASCI Red\nParagon\n4510\n[\u00032]\n2D mesh\n(64\u000364)\nInput\nbuffered\n(1 flit)\nDistributed\ndimension-\norder routing\n2-phased\nRR,\ndistributed\nacross\nswitch\nWormhole\nwith no\nvirtual\nchannels\nIBM\nASCI White\nSP Power3\n[Colony]\n512\n[\u000316]\nBidirectional\nMIN with 8-\nport\nbidirectional\nswitches\n(typically a fat\ntree or\nOmega)\nInput and\ncentral\nbuffer\nwith\noutput\nqueuing\n(8-way\nspeedup)\nSource-based\nLCA\nadaptive,\nshortest-path\nrouting, and\ntable-based\nmulticast\nrouting\n2-phased\nRR,\ncentralized\nand\ndistributed\nat outputs\nfor bypass\npaths\nBuffered\nwormhole\nand virtual\ncut-through\nfor\nmulticasting,\nno virtual\nchannels\nIntel\nThunder\nItanium2\nTiger4\n[QsNetII]\n1024\n[\u00034]\nFat tree with\n8-port\nbidirectional\nswitches\nInput\nbuffered\nSource-based\nLCA\nadaptive,\nshortest-path\nrouting\n2-phased\nRR,\npriority,\naging,\ndistributed\nat output\nports\nWormhole\nwith 2 virtual\nchannels\nCray\nXT3\n[SeaStar]\n30,508\n[\u00031]\n3D torus\n(40\u000332\u000324)\nInput with\nstaging\noutput\nDistributed\ntable-based\ndimension-\norder routing\n2-phased\nRR,\ndistributed\nat output\nports\nVirtual cut-\nthrough with\n4 virtual\nchannels\nCray\nX1E\n1024\n[\u00031]\n4-way bristled\n2D torus\n(\u000423\u000311)\nwith express\nlinks\nInput with\nvirtual\noutput\nqueuing\nDistributed\ntable-based\ndimension-\norder routing\n2-phased\nwavefront\n(pipelined)\nglobal\narbiter\nVirtual cut-\nthrough with\n4 virtual\nchannels\nIBM\nASC Purple\npSeries 575\n[Federation]\n>1280\n[\u00038]\nBidirectional\nMIN with 8-\nport\nbidirectional\nswitches\n(typically a fat\ntree or\nOmega)\nInput and\ncentral\nbuffer\nwith\noutput\nqueuing\n(8-way\nspeedup)\nSource and\ndistributed\ntable-based\nLCA\nadaptive,\nshortest-path\nrouting, and\nmulticast\n2-phased\nRR,\ncentralized\nand\ndistributed\nat outputs\nfor bypass\npaths\nBuffered\nwormhole\nand virtual\ncut-through\nfor\nmulticasting\nwith 8 virtual\nchannels\nIBM\nBlue Gene/\nL eServer\nSolution\n[Torus Net.]\n65,536\n[\u00032]\n3D torus\n(32\u000332\u000364)\nInput-\noutput\nbuffered\nDistributed,\nadaptive with\nbubble escape\nvirtual\nchannel\n2-phased\nSLQ,\ndistributed\nat input and\noutput\nVirtual cut-\nthrough with\n4 virtual\nchannels\nFigure F.20 Routing, arbitration, and switching characteristics of interconnections networks in commercial\nmachines.\nF.6\nSwitch Microarchitecture\n\u25a0\nF-57"
    },
    {
        "page": 1006,
        "text": "stripped off or modified by the routing control unit after use to simplify processing\nat the next switch along the path. When routing is implemented using forwarding\ntables, the routing information given in the packet header is used as an address to\naccess a forwarding table entry that contains the allowed switch output port(s) pro-\nvided by the routing algorithm. Forwarding tables must be preloaded into the\nswitches at the outset of network operation. Hybrid approaches also exist where\nthe forwarding table is reduced to a small set of routing bits and combined with\na small logic block. Those routing bits are used by the routing control unit to know\nwhat paths are allowed and decide the output ports the packets need to take. The\ngoal with those approaches is to build flexible yet compact routing control units,\neliminating the area and power wastage of a large forwarding table and thus being\nsuitable for OCNs. The routing control unit is usually implemented as a centralized\nresource, although it could be replicated at every input port so as not to become a\nbottleneck. Routing is done only once for every packet, and packets typically are\nlarge enough to take several cycles to flow through the switch, so a centralized\nrouting control unit rarely becomes a bottleneck. Figure F.21 assumes a centralized\nrouting control unit within the switch.\nArbitration is required when two or more packets concurrently request the\nsame output port, as described in the previous section. Switch arbitration can be\nimplemented in a centralized or distributed way. In the former case, all of the\nrequests and status information are transmitted to the central switch arbitration\nunit; in the latter case, the arbiter is distributed across the switch, usually among\nthe input and/or output ports. Arbitration may be performed multiple times on\npackets, and there may be multiple queues associated with each input port,\nLink\ncontrol\nPhysical\nchannel\nInput\nbuffers\nDemux\nMux\nCrossbar\nDemux\nDemux\nLink\ncontrol\nPhysical\nchannel\nLink\ncontrol\nLink\ncontrol\nInput\nbuffers\nDemux\nRouting control and\narbitration unit\nMux\nPhysical\nchannel\nPhysical\nchannel\nOutput\nbuffers\nMux\nOutput\nbuffers\nMux\nFigure F.21 Basic microarchitectural components of an input-output-buffered switch.\nF-58\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1007,
        "text": "increasing the number of arbitration requests that must be processed. Thus, many\nimplementations use a hierarchical arbitration approach, where arbitration is first\nperformed locally at every input port to select just one request among the corre-\nsponding packets and queues, and later arbitration is performed globally to process\nthe requests made by each of the local input port arbiters. Figure F.21 assumes a\ncentralized arbitration unit within the switch.\nThe basic switch microarchitecture depicted in Figure F.21 functions in the fol-\nlowing way. When a packet starts to arrive at a switch input port, the link controller\ndecodes the incoming signal and generates a sequence of bits, possibly deserializ-\ning data to adapt them to the width of the internal data path if different from the\nexternal link width. Information is also extracted from the packet header or link\ncontrol signals to determine the queue to which the packet should be buffered.\nAs the packet is being received and buffered (or after the entire packet has been\nbuffered, depending on the switching technique), the header is sent to the routing\nunit. This unit supplies a request for one or more output ports to the arbitration unit.\nArbitration for the requested output port succeeds if the port is free and has enough\nspace to buffer the entire packet or flit, depending on the switching technique. If\nwormhole switching with virtual channels is implemented, additional arbitration\nand allocation steps may be required for the transmission of each individual flit.\nOnce the resources are allocated, the packet is transferred across the internal cross-\nbar to the corresponding output buffer and link if no other packets are ahead of it\nand the link is free. Link-level flow control implemented by the link controller pre-\nvents input queue overflow at the neighboring switch on the other end of the link. If\nvirtual channel switching is implemented, several packets may be time-\nmultiplexed across the link on a flit-by-flit basis. As the various input and output\nports operate independently, several incoming packets may be processed concur-\nrently in the absence of contention.\nBuffer Organizations\nAs mentioned above, queues can be located at the switch input, output, or both\nsides. Output-buffered switches have the advantage of completely eliminating\nhead-of-line blocking. Head-of-line (HOL) blocking occurs when two or more\npackets are buffered in a queue, and a blocked packet at the head of the queue\nblocks other packets in the queue that would otherwise be able to advance if they\nwere at the queue head. This cannot occur in output-buffered switches as all the\npackets in a given queue have the same status; they require the same output port.\nHowever, it may be the case that all the switch input ports simultaneously receive a\npacket for the same output port. As there are no buffers at the input side, output\nbuffers must be able to store all those incoming packets at the same time. This\nrequires implementing output queues with an internal switch speedup of k. That\nis, output queues must have a write bandwidth k times the link bandwidth, where\nk is the number of switch ports. This oftentimes is too expensive. Hence, this solu-\ntion by itself has rarely been implemented in lossless networks. As the probability\nof concurrently receiving many packets for the same output port is usually small,\nF.6\nSwitch Microarchitecture\n\u25a0\nF-59"
    },
    {
        "page": 1008,
        "text": "commercial systems that use output-buffered switches typically implement only\nmoderate switch speedup, dropping packets on rare buffer overflow.\nSwitches with buffers on the input side are able to receive packets without hav-\ning any switch speedup; however, HOL blocking can occur within input port\nqueues, as illustrated in Figure F.22(a). This can reduce switch output port utiliza-\ntion to less than 60% even when packet destinations are uniformly distributed. As\nshown in Figure F.22(b), the use of virtual channels (two in this case) can mitigate\nHOL blocking but does not eliminate it. A more effective solution is to organize the\ninput queues as virtual output queues (VOQs), shown in Figure F.22(c). With this,\neach input port implements as many queues as there are output ports, thus provid-\ning separate buffers for packets destined to different output ports. This is a popular\ntechnique widely used in ATM switches and IP routers. The main drawbacks of\nInput buffers\nCrossbar\n(A)\nInput port i\nOutput port X+\nDemux\nY\u2013\nY+\nY+\nX\u2013 X+\nX+\nOutput port X\u2013\nOutput port Y+\nOutput port Y\u2013\nInput buffers\nCrossbar\n(B)\nInput port i\nOutput port X+\nX\u2013 X+\nY+\nY\u2013\nY+\nX+\nOutput port X\u2013\nOutput port Y+\nOutput port Y\u2013\nDemux\nInput buffers\nCrossbar\n(C)\nInput port i\nOutput port X+\nX+\nX\u2013\nY+\nY+\nY\u2013\nX+\nOutput port X\u2013\nOutput port Y+\nOutput port Y\u2013\nFigure F.22 (a) Head-of-line blocking in an input buffer, (b) the use of two virtual channels to reduce HOL block-\ning, and (c) the use of virtual output queuing to eliminate HOL blocking within a switch. The shaded input buffer is\nthe one to which the crossbar is currently allocated. This assumes each input port has only one access port to the\nswitch\u2019s internal crossbar.\nF-60\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1009,
        "text": "VOQs, however, are cost and lack of scalability: The number of VOQs grows qua-\ndratically with switch ports. Moreover, although VOQs eliminate HOL blocking\nwithin a switch, HOL blocking occurring at the network level end-to-end is not\nsolved. Of course, it is possible to design a switch with VOQ support at the network\nlevel also\u2014that is, to implement as many queues per switch input port as there are\noutput ports across the entire network\u2014but this is extremely expensive. An alter-\nnative is to dynamically assign only a fraction of the queues to store (cache) sep-\narately only those packets headed for congested destinations.\nCombined input-output-buffered switches minimize HOL blocking when there\nis sufficient buffer space at the output side to buffer packets, and they minimize the\nswitch speedup required due to buffers being at the input side. This solution has the\nfurther benefit of decoupling packet transmission through the internal crossbar of\nthe switch from transmission through the external links. This is especially useful\nfor cut-through switching implementations that use virtual channels, where flit\ntransmissions are time-multiplexed over the links. Many designs used in commer-\ncial systems implement input-output-buffered switches.\nRouting Algorithm Implementation\nIt is important to distinguish between the routing algorithm and its implementation.\nWhile the routing algorithm describes the rules to forward packets across the net-\nwork and affects packet latency and network throughput, its implementation affects\nthedelaysufferedbypacketswhenreachinganode,therequiredsiliconarea,andthe\npower consumption associated with the routing computation. Several techniques\nhave been proposed to pre-compute the routing algorithm and/or hide the routing\ncomputation delay. However, significantly less effort has been devoted to reduce\nsilicon area and power consumption without significantly affecting routing flexibil-\nity. Both issues have become very important, particularly for OCNs. Many existing\ndesigns address these issues by implementing relatively simple routing algorithms,\nbut more sophisticated routing algorithms will likely be needed in the future to deal\nwith increasing manufacturing defects, process variability, and other complications\narising from continued technology scaling, as discussed briefly below.\nAs mentioned in a previous section, depending on where the routing algorithm\nis computed, two basic forms of routing exist: source and distributed routing. In\nsource routing, the complexity of implementation is moved to the end nodes where\npaths need to be stored in tables, and the path for a given packet is selected based on\nthe destination end node identifier. In distributed routing, however, the complexity\nis moved to the switches where, at each hop along the path of a packet, a selection\nof the output port to take is performed. In distributed routing, two basic implemen-\ntations exist. The first one consists of using a logic block that implements a fixed\nrouting algorithm for a particular topology. The most common example of such an\nimplementation is dimension-order routing, where dimensions are offset in an\nestablished order. Alternatively, distributed routing can be implemented with for-\nwarding tables, where each entry encodes the output port to be used for a particular\nF.6\nSwitch Microarchitecture\n\u25a0\nF-61"
    },
    {
        "page": 1010,
        "text": "destination. Therefore, in the worst case, as many entries as destination nodes are\nrequired.\nBoth methods for implementing distributed routing have their benefits and\ndrawbacks. Logic-based routing features a very short computation delay, usually\nrequires a small silicon area, and has low power consumption. However, logic-\nbased routing needs to be designed with a specific topology in mind and, therefore,\nis restricted to that topology. Table-based distributed routing is quite flexible and\nsupports any topology and routing algorithm. Simply, tables need to be filled with\nthe proper contents based on the applied routing algorithm (e.g., the up*/down*\nrouting algorithm can be defined for any irregular topology). However, the down\nside of table-based distributed routing is its non-negligible area and power cost.\nAlso, scalability is problematic in table-based solutions as, in the worst case, a sys-\ntem with N end nodes (and switches) requires as many as N tables each with N\nentries, thus having quadratic cost.\nDepending on the network domain, one solution is more suitable than the other.\nFor instance, in SANs, it is usual to find table-based solutions as is the case with\nInfiniBand. In other environments, like OCNs, table-based implementations are\navoided due to the aforementioned costs in power and silicon area. In such envi-\nronments, it is more advisable to rely on logic-based implementations. Herein lies\nsome of the challenges OCN designers face: ever continuing technology scaling\nthrough device miniaturization leads to increases in the number of manufacturing\ndefects, higher failure rates (either transient or permanent), significant process var-\niations (transistors behaving differently from design specs), the need for different\nclock frequency and voltage domains, and tight power and energy budgets. All of\nthese challenges translate to the network needing support for heterogeneity. Dif-\nferent\u2014possibly irregular\u2014regions of the network will be created owing to failed\ncomponents, powered down switches and links, disabled components (due to\nunacceptable variations in performance) and so on. Hence, heterogeneous systems\nmay emerge from a homogeneous design. In this framework, it is important to effi-\nciently implement routing algorithms designed to provide enough flexibility to\naddress these new challenges.\nA well-known solution for providing a certain degree of flexibility while being\nmuch more compact than traditional table-based approaches is interval routing\n[Leeuwen 1987], where a range of destinations is defined for each output port.\nAlthough this approach is not flexible enough, it provides a clue on how to address\nemerging challenges. A more recent approach provides a plausible implementation\ndesign point that lies between logic-based implementation (efficiency) and table-\nbased implementation (flexibility). Logic-Based Distributed Routing (LBDR) is a\nhybrid approach that takes as a reference a regular 2D mesh but allows an irregular\nnetwork to be derived from it due to changes in topology induced by manufactur-\ning defects, failures, and other anomalies. Due to the faulty, disabled, and powered-\ndown components, regularity is compromised and the dimension-order routing\nalgorithm can no longer be used. To support such topologies, LBDR defines a\nset of configuration bits at each switch. Four connectivity bits are used at each\nswitch to indicate the connectivity of the switch to the neighbor switches in the\nF-62\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1011,
        "text": "topology. Thus, one connectivity bit per port is used. Those connectivity bits are\nused, for instance, to disable an output port leading to a faulty component. Addi-\ntionally, eight routing bits are used, two per output port, to define the available\nrouting options. The value of the routing bits is set at power-on and is computed\nfrom the routing algorithm to be implemented in the network. Basically, when a\nrouting bit is set, it indicates that a packet can leave the switch through the asso-\nciated output port and is allowed to perform a certain turn at the next switch. In this\nrespect, LBDR is similar to interval routing, but it defines geographical areas\ninstead of ranges of destinations. Figure F.23 shows an example where a\ntopology-agnostic routing algorithm is implemented with LBDR on an irregular\ntopology. The figure shows the computed configuration bits.\nThe connectivity and routing bits are used to implement the routing algorithm.\nFor that purpose, a small set of logic gates are used in combination with the con-\nfiguration bits. Basically, the LBDR approach takes as a reference the initial topol-\nogy (a 2D mesh), and makes a decision based on the current coordinates of the\nrouter, the coordinates of the destination router, and the configuration bits.\nFigure F.24 shows the required logic, and Figure F.25 shows an example of where\na packet is forwarded from its source to its destination with the use of the config-\nuration bits. As can be noticed, routing restrictions are enforced by preventing the\nuse of the west port at switch 10.\nLBDR represents a method for efficient routing implementation in OCNs.\nThis mechanism has been recently extended to support non-minimal paths,\ncollective communication operations, and traffic isolation. All of these improve-\nments have been made while maintaining a compact and efficient implementation\nwith the use of a small set of configuration bits. A detailed description of\nLBDR and its extensions, and the current research on OCNs can be found in\nFlich [2010].\n0\n1\n2\n3\n7\n6\n5\n4\n8\n9\n13\nBidirectional routing restriction\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nRouter\nCn\nCe\nCw\nCs\nRne\nRnw\nRen\nRes\nRwn\nRws\nRse\nRsw\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n12\nFigure F.23 Shown is an example of an irregular network that uses LBDR to implement the routing algorithm.\nFor each router, connectivity and routing bits are defined.\nF.6\nSwitch Microarchitecture\n\u25a0\nF-63"
    },
    {
        "page": 1012,
        "text": "Pipelining the Switch Microarchitecture\nPerformance can be enhanced by pipelining the switch microarchitecture. Pipe-\nlined processing of packets in a switch has similarities with pipelined execution\nof instructions in a vector processor. In a vector pipeline, a single instruction indi-\ncates what operation to apply to all the vector elements executed in a pipelined\nway. Similarly, in a switch pipeline, a single packet header indicates how to pro-\ncess all of the internal data path physical transfer units (or phits) of a packet, which\nare processed in a pipelined fashion. Also, as packets at different input ports are\nindependent of each other, they can be processed in parallel similar to the way mul-\ntiple independent instructions or threads of pipelined instructions can be executed\nin parallel.\n0\n1\n2\n3\n7\n6\n5\n4\n8\n9\n10\n11\n15\n14\n13\n12\nMessage\nBidirectional routing restriction\n0\n1\n2\n3\n7\n6\n5\n4\n8\n9\n10\n11\n15\n14\n13\n12\n0\n1\n2\n3\n7\n6\n5\n4\n8\n9\n10\n11\n15\n14\n13\n12\nFigure F.25 Example of routing a message from Router 14 to Router 5 using LBDR at each router.\nComparator\nXdst\nXcurr\nYcurr\nYdst\nE =Ce\u00b7(E'\u00b7!N'\u00b7!S'+E'\u00b7N'\u00b7Ren+E'\u00b7S'\u00b7Res)\nW =Cw\u00b7(W'\u00b7!N'\u00b7!S'+W'\u00b7N'\u00b7Rwn+ W'\u00b7S'\u00b7Rws)\nS = Cs\u00b7(S'\u00b7!E'\u00b7!W'+ S'\u00b7E'\u00b7Rse+S'\u00b7W'\u00b7Rsw)\nL =!N'\u00b7!E'\u00b7!W'\u00b7!S'\nW'\nW'\nE'\nN'\nE'\nN'\nN'\nRne\nCn\nN\nRnw\n1st stage\nN'\nE'\nW'\nS'\n2nd stage\nFigure F.24 LBDR logic at each input port of the router.\nF-64\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1013,
        "text": "The switch microarchitecture can be pipelined by analyzing the basic functions\nperformed within the switch and organizing them into several stages. Figure F.26\nshows a block diagram of a five-stage pipelined organization for the basic switch\nmicroarchitecture given in Figure F.21, assuming cut-through switching and the\nuse of a forwarding table to implement routing. After receiving the header portion\nof the packet in the first stage, the routing information (i.e., destination address) is\nused in the second stage to look up the allowed routing option(s) in the forwarding\ntable. Concurrent with this, other portions of the packet are received and buffered\nin the input port queue at the first stage. Arbitration is performed in the third stage.\nThe crossbar is configured to allocate the granted output port for the packet in the\nfourth stage, and the packet header is buffered in the switch output port and ready\nfor transmission over the external link in the fifth stage. Note that the second and\nLink\ncontrol\nPhysical\nchannel\nInput\nbuffers\n5\n \ne\ng\na\nt\nS\n4\n \ne\ng\na\nt\nS\n3\n \ne\ng\na\nt\nS\n2\n \ne\ng\na\nt\nS\n1\n \ne\ng\na\nt\nS\nDemux\nMux\nCrossbar\nDemux\nArbitration\nunit\nCrossbar\ncontrol\nOutput\nport #\nForwarding\ntable\nHeader\nfill\nDemux\nLink\ncontrol\nPhysical\nchannel\nPacket header\nPayload fragment\nPayload fragment\nPayload fragment\nIB\nLink\ncontrol\nLink\ncontrol\nInput\nbuffers\nDemux\nRouting \ncontrol unit\nMux\nPhysical\nchannel\nPhysical\nchannel\nOutput\nbuffers\nMux\nOutput\nbuffers\nMux\nRC\nSA\nST\nOB\nIB\nIB\nIB\nST\nOB\nIB\nIB\nIB\nST\nOB\nIB\nIB\nIB\nST\nOB\nFigure F.26 Pipelined version of the basic input-output-buffered switch. The notation in the figure is as follows: IB\nis the input link control and buffer stage, RC is the route computation stage, SA is the crossbar switch arbitration stage,\nST is the crossbar switch traversal stage, and OB is the output buffer and link control stage. Packet fragments (flits)\ncoming after the header remain in the IB stage until the header is processed and the crossbar switch resources are\nprovided.\nF.6\nSwitch Microarchitecture\n\u25a0\nF-65"
    },
    {
        "page": 1014,
        "text": "third stages are used only by the packet header; the payload and trailer portions of\nthe packet use only three of the stages\u2014those used for data flow-thru once the\ninternal data path of the switch is set up.\nA virtual channel switch usually requires an additional stage for virtual channel\nallocation. Moreover, arbitration is required for every flit before transmission\nthrough the crossbar. Finally, depending on the complexity of the routing and arbi-\ntration algorithms, several clock cycles may be required for these operations.\nOther Switch Microarchitecture Enhancements\nAs mentioned earlier, internal switch speedup is sometimes implemented to\nincrease switch output port utilization. This speedup is usually implemented by\nincreasing the clock frequency and/or the internal data path width (i.e., phit size)\nof the switch. An alternative solution consists of implementing several parallel data\npaths from each input port\u2019s set of queues to the output ports. One way of doing this\nis by increasing the number of crossbar input ports. When implementing several\nphysical queues per input port, this can be achieved by devoting a separate crossbar\nport to each input queue. For example, the IBM Blue Gene/L implements two\ncrossbar access ports and two read ports per switch input port.\nAnother way of implementing parallel data paths between input and output\nports is to move the buffers to the crossbar crosspoints. This switch architecture\nis usually referred to as a buffered crossbar switch. A buffered crossbar provides\nindependent data paths from each input port to the different output ports, thus mak-\ning it possible to send up to k packets at a time from a given input port to k different\noutput ports. By implementing independent crosspoint memories for each input-\noutput port pair, HOL blocking is eliminated at the switch level. Moreover, arbi-\ntration is significantly simpler than in other switch architectures. Effectively, each\noutput port can receive packets from only a disjoint subset of the crosspoint mem-\nories. Thus, a completely independent arbiter can be implemented at each switch\noutput port, each of those arbiters being very simple.\nA buffered crossbar would be the ideal switch architecture if it were not so\nexpensive. The number of crosspoint memories increases quadratically with the\nnumber of switch ports, dramatically increasing its cost and reducing its scalability\nwith respect to the basic switch architecture. In addition, each crosspoint memory\nmust be large enough to efficiently implement link-level flow control. To reduce\ncost, most designers prefer input-buffered or combined input-output-buffered\nswitches enhanced with some of the mechanisms described previously.\nF.7\nPractical Issues for Commercial Interconnection\nNetworks\nThere are practical issues in addition to the technical issues described thus far that\nare important considerations for interconnection networks within certain domains.\nWe mention a few of these below.\nF-66\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1015,
        "text": "Connectivity\nThetypeandnumberofdevicesthatcommunicateandtheircommunicationrequire-\nments affect the complexity of the interconnection network and its protocols. The\nprotocols must target the largest network size and handle the types of anomalous\nsystemwide events that might occur. Among some of the issues are the following:\nHow lightweight should the network interface hardware/software be? Should it\nattach to the memory network or the I/O network? Should it support cache coher-\nence? If the operating system must get involved for every network transaction,\nthe sending and receiving overhead becomes quite large. If the network interface\nattaches to the I/O network (PCI-Express or HyperTransport interconnect), the\ninjection and reception bandwidth will be limited to that of the I/O network. This\nis the case for the Cray XT3 SeaStar, Intel Thunder Tiger 4 QsNetII, and many other\nsupercomputer and cluster networks. To support coherence, the sender may have to\nflush the cache before each send, and the receiver may have to flush its cache before\neach receivetoprevent the stale-dataproblem. Such flushesfurther increase sending\nand receiving overhead, often causing the network interface to be the network\nbottleneck.\nComputer systems typically have a multiplicity of interconnects with different\nfunctions and cost-performance objectives. For example, processor-memory inter-\nconnects usually provide higher bandwidth and lower latency than I/O interconnects\nand are more likely to support cache coherence, but they are less likely to follow or\nbecome standards. Personal computers typically have a processormemory intercon-\nnect and an I/O interconnect (e.g., PCI-X 2.0, PCIe or Hyper-Transport) designed to\nconnect both fast and slow devices (e.g., USB 2.0, Gigabit Ethernet LAN, Firewire\n800). The Blue Gene/L supercomputer uses five interconnection networks, only\none of which is the 3D torus used for most of the interprocessor application traffic.\nThe others include a tree-based collective communication network for broadcast\nand multicast; a tree-based barrier network for combining results (scatter, gather);\nacontrol networkfordiagnostics,debugging,andinitialization;andaGigabitEthernet\nnetworkforI/Obetweenthenodesanddisk.TheUniversityofTexasatAustin\u2019sTRIPS\nEdge processor has eight specialized on-chip networks\u2014some with bidirectional\nchannels as wide as 128bits and some with 168 bits in each direction\u2014to interconnect\nthe106 heterogeneoustilescomposingthe twoprocessorcoreswith L2on-chip cache.\nIt also has a chip-to-chip switched network to interconnect multiple chips in a multi-\nprocessor configuration. Two of the on-chip networks are switched networks: One is\nused for operand transport and the other is used for on-chip memory communication.\nThe others are essentially fan-out trees or recombination dedicated link networks\nused for status and control. The portion of chip area allocated to the interconnect is\nsubstantial, with five of the seven metal layers used for global network wiring.\nStandardization: Cross-Company Interoperability\nStandards are useful in many places in computer design, including interconnection\nnetworks. Advantages of successful standards include low cost and stability.\nF.7\nPractical Issues for Commercial Interconnection Networks\n\u25a0\nF-67"
    },
    {
        "page": 1016,
        "text": "The customer has many vendors to choose from, which keeps price close to cost\ndue to competition. It makes the viability of the interconnection independent of the\nstability of a single company. Components designed for a standard interconnection\nmay also have a larger market, and this higher volume can reduce the vendors\u2019\ncosts, further benefiting the customer. Finally, a standard allows many companies\nto build products with interfaces to the standard, so the customer does not have to\nwait for a single company to develop interfaces to all the products of interest.\nOne drawback of standards is the time it takes for committees and special-\ninterest groups to agree on the definition of standards, which is a problem when\ntechnology is changing rapidly. Another problem is when to standardize: On\nthe one hand, designers would like to have a standard before anything is built;\non the other hand, it would be better if something were built before standardization\nto avoid legislating useless features or omitting important ones. When done too\nearly, it is often done entirely by committee, which is like asking all of the chefs\nin France to prepare a single dish of food\u2014masterpieces are rarely served. Stan-\ndards can also suppress innovation at that level, since standards fix the interfaces\u2014\nat least until the next version of the standards surface, which can be every few years\nor longer. More often, we are seeing consortiums of companies getting together to\ndefine and agree on technology that serve as \u201cde facto\u201d industry standards. This\nwas the case for InfiniBand.\nLANs and WANs use standards and interoperate effectively. WANs involve\nmany types of companies and must connect to many brands of computers, so it\nis difficult to imagine a proprietary WAN ever being successful. The ubiquitous\nnature of the Ethernet shows the popularity of standards for LANs as well as\nWANs, and it seems unlikely that many customers would tie the viability of their\nLAN to the stability of a single company. Some SANs are standardized such as\nFibre Channel, but most are proprietary. OCNs for the most part are proprietary\ndesigns, with a few gaining widespread commercial use in system-on-chip\n(SoC) applications, such as IBM\u2019s CoreConnect and ARM\u2019s AMBA.\nCongestion Management\nCongestion arises when too many packets try to use the same link or set of links.\nThis leads to a situation in which the bandwidth required exceeds the bandwidth\nsupplied. Congestion by itself does not degrade network performance: simply, the\ncongested links are running at their maximum capacity. Performance degradation\noccurs in the presence of HOL blocking where, as a consequence of packets going\nto noncongested destinations getting blocked by packets going to congested des-\ntinations, some link bandwidth is wasted and network throughput drops, as illus-\ntrated in the example given at the end of Section F.4. Congestion control refers to\nschemes that reduce traffic when the collective traffic of all nodes is too large for\nthe network to handle.\nOne advantage of a circuit-switched network is that, once a circuit is estab-\nlished, it ensures that there is sufficient bandwidth to deliver all the information\nF-68\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1017,
        "text": "sent along that circuit. Interconnection bandwidth is reserved as circuits are estab-\nlished, and if the network is full, no more circuits can be established. Other switch-\ning techniques generally do not reserve interconnect bandwidth in advance, so the\ninterconnection network can become clogged with too many packets. Just as with\npoor rush-hour commuters, a traffic jam of packets increases packet latency and, in\nextreme cases, fewer packets per second get delivered by the interconnect. In order\nto handle congestion in packet-switched networks, some form of congestion man-\nagement must be implemented. The two kinds of mechanisms used are those that\ncontrol congestion and those that eliminate the performance degradation intro-\nduced by congestion.\nThere are three basic schemes used for congestion control in interconnection\nnetworks, each with its own weaknesses: packet discarding, flow control, and\nchoke packets. The simplest scheme is packet discarding, which we discussed\nbriefly in Section F.2. If a packet arrives at a switch and there is no room in the\nbuffer, the packet is discarded. This scheme relies on higher-level software that\nhandles errors in transmission to resend lost packets. This leads to significant band-\nwidth wastage due to (re)transmitted packets that are later discarded and, therefore,\nis typically used only in lossy networks like the Internet.\nThe second scheme relies on flow control, also discussed previously. When\nbuffers become full, link-level flow control provides feedback that prevents the\ntransmission of additional packets. This backpressure feedback rapidly propagates\nbackward until it reaches the sender(s) of the packets producing congestion,\nforcing a reduction in the injection rate of packets into the network. The main draw-\nbacks of this scheme are that sources become aware of congestion too late when the\nnetwork is already congested, and nothing is done to alleviate congestion. Back-\npressure flow control is common in lossless networks like SANs used in supercom-\nputers and enterprise systems.\nA more elaborate way of using flow control is by implementing it directly\nbetween the sender and the receiver end nodes, generically called end-to-end flow\ncontrol. Windowing is one version of end-to-end credit-based flow control where\nthe window size should be large enough to efficiently pipeline packets through the\nnetwork. The goal of the window is to limit the number of unacknowledged\npackets, thus bounding the contribution of each source to congestion, should it\narise. The TCP protocol uses a sliding window. Note that end-to-end flow control\ndescribes the interaction between just two nodes of the interconnection network,\nnot the entire interconnection network between all end nodes. Hence, flow control\nhelps congestion control, but it is not a global solution.\nChoke packets are used in the third scheme, which is built upon the premise that\ntraffic injection should be throttled only when congestion exists across the net-\nwork. The idea is for each switch to see how busy it is and to enter into a warning\nstate when it passes a threshold. Each packet received by a switch in the warning\nstate is sent back to the source via a choke packet that includes the intended des-\ntination. The source is expected to reduce traffic to that destination by a fixed per-\ncentage. Since it likely will have already sent other packets along that path, the\nsource node waits for all the packets in transit to be returned before acting on\nF.7\nPractical Issues for Commercial Interconnection Networks\n\u25a0\nF-69"
    },
    {
        "page": 1018,
        "text": "the choke packets. In this scheme, congestion is controlled by reducing the packet\ninjection rate until traffic reduces, just as metering lights that guard on-ramps con-\ntrol the rate of cars entering a freeway. This scheme works efficiently when the\nfeedback delay is short. When congestion notification takes a long time, usually\ndue to long time of flight, this congestion control scheme may become unsta-\nble\u2014reacting too slowly or producing oscillations in packet injection rate, both\nof which lead to poor network bandwidth utilization.\nAn alternative to congestion control consists of eliminating the negative\nconsequences of congestion. This can be done by eliminating HOL blocking at\nevery switch in the network as discussed previously. Virtual output queues can\nbe used for this purpose; however, it would be necessary to implement as many\nqueues at every switch input port as devices attached to the network. This solution\nis very expensive, and not scalable at all. Fortunately, it is possible to achieve\ngood results by dynamically assigning a few set-aside queues to store only\nthe congested packets that travel through some hot-spot regions of the network,\nvery much like caches are intended to store only the more frequently accessed\nmemory locations. This strategy is referred to as regional explicit congestion\nnotification (RECN).\nFault Tolerance\nThe probability of system failures increases as transistor integration density and the\nnumber of devices in the system increases. Consequently, system reliability and\navailability have become major concerns and will be even more important in future\nsystems with the proliferation of interconnected devices. A practical issue arises,\ntherefore, as to whether or not the interconnection network relies on all the devices\nbeing operational in order for the network to work properly. Since software failures\nare generally much more frequent than hardware failures, another question sur-\nfaces as to whether a software crash on a single device can prevent the rest of\nthe devices from communicating. Although some hardware designers try to build\nfault-free networks, in practice, it is only a question of the rate of failures, not\nwhether they can be prevented. Thus, the communication subsystem must have\nmechanisms for dealing with faults when\u2014not if\u2014they occur.\nThere are two main kinds of failure in an interconnection network: transient\nand permanent. Transient failures are usually produced by electromagnetic inter-\nference and can be detected and corrected using the techniques described in\nSection F.2. Oftentimes, these can be dealt with simply by retransmitting the\npacket either at the link level or end-to-end. Permanent failures occur when some\ncomponent stops working within specifications. Typically, these are produced by\noverheating, overbiasing, overuse, aging, and so on and cannot be recovered from\nsimply by retransmitting packets with the help of some higher-layer software pro-\ntocol. Either an alternative physical path must exist in the network and be supplied\nby the routing algorithm to circumvent the fault or the network will be crippled,\nunable to deliver packets whose only paths are through faulty resources.\nThree major categories of techniques are used to deal with permanent failures:\nresource sparing, fault-tolerant routing, and network reconfiguration. In the first\nF-70\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1019,
        "text": "technique, faulty resources are switched off or bypassed, and some spare resources\nare switched in to replace the faulty ones. As an example, the ServerNet intercon-\nnection network is designed with two identical switch fabrics, only one of which is\nusable at any given time. In case of failure in one fabric, the other is used. This\ntechnique can also be implemented without switching in spare resources, leading\nto a degraded mode of operation after a failure. The IBM Blue Gene/L supercom-\nputer, for instance, has the facility to bypass failed network resources while retain-\ning its base topological structure and routing algorithm. The main drawback of this\ntechnique is the relatively large number of healthy resources (e.g., midplane node\nboards) that may need to be switched off after a failure in order to retain the base\ntopological structure (e.g., a 3D torus).\nFault-tolerant routing, on the other hand, takes advantage of the multiple paths\nalready existing in the network topology to route messages in the presence of fail-\nures without requiring spare resources. Alternative paths for each supported fault\ncombination are identified at design time and incorporated into the routing algo-\nrithm. When a fault is detected, a suitable alternative path is used. The main dif-\nficulty when using this technique is guaranteeing that the routing algorithm will\nremain deadlock-free when using the alternative paths, given that arbitrary fault\npatterns may occur. This is especially difficult in direct networks whose regularity\ncan be compromised by the fault pattern. The Cray T3E is an example system that\nsuccessfully applies this technique on its 3D torus direct network. There are many\nexamples of this technique in systems using indirect networks, such as with the\nbidirectional multistage networks in the ASCI White and ASC Purple. Those net-\nworks provide multiple minimal paths between end nodes and, inherently, have no\nrouting deadlock problems (see Section F.5). In these networks, alternative paths\nare selected at the source node in case of failure.\nNetwork reconfiguration is yet another, more general technique to handle vol-\nuntary and involuntary changes in the network topology due either to failures or to\nsome other cause. In order for the network to be reconfigured, the nonfaulty por-\ntions of the topology must first be discovered, followed by computation of the new\nrouting tables and distribution of the routing tables to the corresponding network\nlocations (i.e., switches and/or end node devices). Network reconfiguration\nrequires the use of programmable switches and/or network interfaces, depending\non how routing is performed. It may also make use of generic routing algorithms\n(e.g., up*/down* routing) that can be configured for all the possible network topol-\nogies that may result after faults. This strategy relieves the designer from having to\nsupply alternative paths for each possible fault combination at design time. Pro-\ngrammable network components provide a high degree of flexibility but at the\nexpense of higher cost and latency. Most standard and proprietary interconnection\nnetworks for clusters and SANs\u2014including Myrinet, Quadrics, InfiniBand,\nAdvanced Switching, and Fibre Channel\u2014incorporate software for (re)configur-\ning the network routing in accordance with the prevailing topology.\nAnother practical issue ties to node failure tolerance. If an interconnection net-\nwork can survive a failure, can it also continue operation while a new node is added\nto or removed from the network, usually referred to as hot swapping? If not, each\naddition or removal of a new node disables the interconnection network, which is\nF.7\nPractical Issues for Commercial Interconnection Networks\n\u25a0\nF-71"
    },
    {
        "page": 1020,
        "text": "impractical for WANs and LANs and is usually intolerable for most SANs. Online\nsystem expansion requires hot swapping, so most networks allow for it. Hot swap-\nping is usually supported by implementing dynamic network reconfiguration, in\nwhich the network is reconfigured without having to stop user traffic. The main\ndifficulty with this is guaranteeing deadlock-free routing while routing tables\nfor switches and/or end node devices are dynamically and asynchronously updated\nas more than one routing algorithm may be alive (and, perhaps, clashing) in the\nnetwork at the same time. Most WANs solve this problem by dropping packets\nwhenever required, but dynamic network reconfiguration is much more complex\nin lossless networks. Several theories and practical techniques have recently been\ndeveloped to address this problem efficiently.\nExample\nFigure F.27 shows the number of failures of 58 desktop computers on a local\narea network for a period of just over one year. Suppose that one local area net-\nwork is based on a network that requires all machines to be operational for the\ninterconnection network to send data; if a node crashes, it cannot accept mes-\nsages, so the interconnection becomes choked with data waiting to be delivered.\nAn alternative is the traditional local area network, which can operate in the\npresence of node failures; the interconnection simply discards messages for a\nnode that decides not to accept them. Assuming that you need to have both your\nworkstation and the connecting LAN to get your work done, how much greater\nare your chances of being prevented from getting your work done using the\nfailure-intolerant LAN versus traditional LANs? Assume the downtime for a\ncrash is less than 30 minutes. Calculate using the one-hour intervals from this\nfigure.\nAnswer\nAssuming the numbers for Figure F.27, the percentage of hours that you can\u2019t get\nyour work done using the failure-intolerant network is\nIntervals with failures\nTotal intervals\n\u00bc Total intervals\u0005Intervals with no failures\nTotal intervals\n\u00bc 8974\u00058605\n8974\n\u00bc 369\n8974 \u00bc 4:1%\nThe percentage of hours that you can\u2019t get your work done using the traditional\nnetwork is just the time your workstation has crashed. If these failures are equally\ndistributed among workstations, the percentage is\nFailures=Machines\nTotal intervals\n\u00bc 654=58\n8974 \u00bc 11:28\n8974 \u00bc 0:13%\nHence, you are more than 30 times more likely to be prevented from getting your\nwork done with the failure-intolerant LAN than with the traditional LAN, accord-\ning to the failure statistics in Figure F.27. Stated alternatively, the person respon-\nsible for maintaining the LAN would receive a 30-fold increase in phone calls from\nirate users!\nF-72\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1021,
        "text": "F.8\nExamples of Interconnection Networks\nTo further provide mass to the concepts described in the previous sections, we look\nat five example networks from the four interconnection network domains consid-\nered in this appendix. In addition to one for each of the OCN, LAN, and WAN\nareas, we look at two examples from the SAN area: one for system area networks\nFailed\nmachines per\ntime interval\nOne-hour intervals with\nnumber of failed machines\nin first column\nTotal failures\nper one-hour\ninterval\nOne-day intervals with\nnumber of failed machines\nin first column\nTotal failures\nper one-day\ninterval\n0\n8605\n0\n184\n0\n1\n264\n264\n105\n105\n2\n50\n100\n35\n70\n3\n25\n75\n11\n33\n4\n10\n40\n6\n24\n5\n7\n35\n9\n45\n6\n3\n18\n6\n36\n7\n1\n7\n4\n28\n8\n1\n8\n4\n32\n9\n2\n18\n2\n18\n10\n2\n20\n11\n1\n11\n2\n22\n12\n1\n12\n17\n1\n17\n20\n1\n20\n21\n1\n21\n1\n21\n31\n1\n31\n38\n1\n38\n58\n1\n58\nTotal\n8974\n654\n373\n573\nFigure F.27 Measurement of reboots of 58 DECstation 5000 s running Ultrix over a 373-day period. These reboots\nare distributed into time intervals of one hour and one day. The first column sorts the intervals according to the num-\nber of machines that failed in that interval. The next two columns concern one-hour intervals, and the last two col-\numns concern one-day intervals. The second and fourth columns show the number of intervals for each number of\nfailed machines. The third and fifth columns are just the product of the number of failed machines and the number of\nintervals. For example, there were 50 occurrences of one-hour intervals with 2 failed machines, for a total of 100 failed\nmachines, and there were 35 days with 2 failed machines, for a total of 70 failures. As we would expect, the number of\nfailures per interval changes with the size of the interval. For example, the day with 31 failures might include one hour\nwith 11 failures and one hour with 20 failures. The last row shows the total number of each column; the number of\nfailures doesn\u2019t agree because multiple reboots of the same machine in the same interval do not result in separate\nentries. (Randy Wang of the University of California\u2013Berkeley collected these data.)\nF.8\nExamples of Interconnection Networks\n\u25a0\nF-73"
    },
    {
        "page": 1022,
        "text": "and one for system/storage area networks. The first two examples are proprietary\nnetworks used in high-performance systems; the latter three examples are network\nstandards widely used in commercial systems.\nOn-Chip Network: Intel Single-Chip Cloud Computer\nWith continued increases in transistor integration as predicted by Moore\u2019s law,\nprocessor designers are under the gun to find ways of combating chip-crossing\nwire delay and other problems associated with deep submicron technology scaling.\nMulticore microarchitectures have gained popularity, given their advantages of\nsimplicity, modularity, and ability to exploit parallelism beyond that which can\nbe achieved through aggressive pipelining and multiple instruction/data issuing\non a single core. No matter whether the processor consists of a single core or mul-\ntiple cores, higher and higher demands are being placed on intrachip communica-\ntion bandwidth to keep pace\u2014not to mention interchip bandwidth. This has\nspurred a great amount of interest in OCN designs that efficiently support commu-\nnication of instructions, register operands, memory, and I/O data within and\nbetween processor cores both on and off the chip. Here we focus on one such\non-chip network: The Intel Single-chip Cloud Computer prototype.\nThe Single-chip Cloud Computer (SCC) is a prototype chip multiprocessor\nwith 48 Intel IA-32 architecture cores. Cores are laid out (see Figure F.28) on a\nnetwork with a 2D mesh topology (6\u00034). The network connects 24 tiles, 4 on-\ndie memory controllers, a voltage regulator controller (VRC), and an external\nsystem interface controller (SIF). In each tile two cores are connected to a router.\nThe four memory controllers are connected at the boundaries of the mesh, two on\neach side, while the VRC and SIF controllers are connected at the bottom border of\nthe mesh.\nEach memory controller can address two DDR3 DIMMS, each up to 8 GB of\nmemory, thus resulting in a maximum of 64 GB of memory. The VRC controller\nallows any core or the system interface to adjust the voltage in any of the six pre-\ndefined regions configuring the network (two 2-tile regions). The clock can also\nbe adjusted at a finer granularity with each tile having its own operating frequency.\nTheseregionscanbeturnedofforscaleddownforlarge powersavings.Thismethod\nallows full application control of the power state of the cores. Indeed, applications\nhaveanAPIavailabletodefinethevoltageandthefrequencyofeachregion.TheSIF\ncontroller is used to communicate the network from outside the chip.\nEach of the tiles includes two processor cores (P54C-based IA) with associated\nL1 16 KB data cache and 16 KB instruction cache and a 256 KB L2 cache (with\nthe associated controller), a 5-port router, traffic generator (for testing purposes\nonly), a mesh interface unit (MIU) handling all message passing requests, memory\nlook-up tables (with configuration registers to set the mapping of a core\u2019s physical\naddresses to the extended memory map of the system), a message-passing buffer,\nand circuitry for the clock generation and synchronization for crossing asynchro-\nnous boundaries.\nF-74\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1023,
        "text": "Focusing on the OCN, the MIU unit is in charge of interfacing the cores to the\nnetwork, including the packetization and de-packetization of large messages; com-\nmand translation and address decoding/lookup; link-level flow control and credit\nmanagement; and arbiter decisions following a round-robin scheme. A credit-\nbased flow control mechanism is used together with virtual cut-through switching\n(thus making it necessary to split long messages into packets). The routers are con-\nnected in a 2D mesh layout, each on its own power supply and clock source. Links\nconnecting routers have 16B+2B side bands running at 2 GHz. Zero-load latency\nis set to 4 cycles, including link traversal. Eight virtual channels are used for per-\nformance (6 VCs) and protocol-level deadlock handling (2 VCs). A message-level\narbitration is implemented by a wrapped wave-front arbiter. The dimension-order\nXY routing algorithm is used and pre-computation of the output port is performed\nat every router.\nBesides the tiles having regions defined for voltage and frequency, the network\n(made of routers and links) has its own single region. Thus, all the network com-\nponents run at the same speed and use the same power supply. An asynchronous\nclock transition is required between the router and the tile.\nOne of the distinctive features of the SCC architecture is the support for a\nmessaging-based communication protocol rather than hardware cache-coherent\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nR\nTile\nMC\n(x, y) =(0, 3)\n(x, y) = (5, 0)\n(x, y) = (5, 3)\nDIMM\nMC\nMC\nMC\nDIMM\nDIMM\nDIMM\nVRC\nSystem interface\nSystem\nFPGA\nPCIe\nSCC die\nManagement console PC\n(x, y) =(0, 0)\n(x, y)= (3, 0)\nFigure F.28 SCC Top-level architecture. From Howard, J. et al., IEEE International Solid-State Circuits Conference\nDigest of Technical Papers, pp. 58\u201359.\nF.8\nExamples of Interconnection Networks\n\u25a0\nF-75"
    },
    {
        "page": 1024,
        "text": "memory for inter-core communication. Message passing buffers are located on\nevery router and APIs are provided to take full control of MPI structures. Cache\ncoherency can be implemented by software.\nThe SCC router represents a significant improvement over the Teraflops pro-\ncessor chip in the implementation of a 2D on-chip interconnect. Contrasted with\nthe 2D mesh implemented in the Teraflops processor, this implementation is tuned\nfor a wider data path in a multiprocessor interconnect and is more latency, area, and\npower optimized for such a width. It targets a lower 2-GHz frequency of operation\ncompared to the 5 GHz of its predecessor Teraflops processor, yet with a higher-\nperformance interconnect architecture.\nSystem Area Network: IBM Blue Gene/L 3D Torus Network\nThe IBM BlueGene/L was the largest-scaled, highest-performing computer system\nin the world in 2005, according to www.top500.org. With 65,536 dual-processor\ncompute nodes and 1024 I/O nodes, this 360 TFLOPS (peak) supercomputer has a\nsystem footprint of approximately 2500 square feet. Both processors at each node\ncan be used for computation and can handle their own communication protocol\nprocessing in virtual mode or, alternatively, one of the processors can be used\nfor computation and the other for network interface processing. Packets range\nin size from 32 bytes to a maximum of 256 bytes, and 8 bytes are used for the\nheader. The header includes routing, virtual channel, link-level flow control,\npacket size, and other such information, along with 1 byte for CRC to protect\nthe header. Three bytes are used for CRC at the packet level, and 1 byte serves\nas a valid indicator.\nThe main interconnection network is a proprietary 32\u000332\u000364 3D torus SAN\nthat interconnects all 64 K nodes. Each node switch has six 350 MB/sec bidirec-\ntional links to neighboring torus nodes, an injection bandwidth of 612.5 MB/sec\nfrom the two node processors, and a reception bandwidth of 1050 MB/sec to\nthe two node processors. The reception bandwidth from the network equals the\ninbound bandwidth across all switch ports, which prevents reception links from\nbottlenecking network performance. Multiple packets can be sunk concurrently\nat each destination node because of the higher reception link bandwidth.\nTwo nodes are implemented on a 2\u00031\u00031 compute card, 16 compute cards\nand 2 I/O cards are implemented on a 4\u00034\u00032 node board, 16 node boards are\nimplemented on an 8\u00038\u00038 midplane, and 2 midplanes form a 1024-node rack\nwith physical dimensions of 0.9\u00030.9\u00031.9 cubic meters. Links have a maximum\nphysical length of 8.6 meters, thus enabling efficient link-level flow control with\nreasonably low buffering requirements. Low latency is achieved by implementing\nvirtual cut-through switching, distributing arbitration at switch input and output\nports, and precomputing the current routing path at the previous switch using a\nfinite-state machine so that part of the routing delay is removed from the critical\npath in switches. High effective bandwidth is achieved using input-buffered\nF-76\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1025,
        "text": "switches with dual read ports, virtual cut-through switching with four virtual chan-\nnels, and fully adaptive deadlock-free routing based on bubble flow control.\nA key feature in networks of this size is fault tolerance. Failure rate is reduced\nby using a relatively low link clock frequency of 700 MHz (same as processor\nclock) on which both edges of the clock are used (i.e., 1.4 Gbps or 175 MB/sec\ntransfer rate is supported for each bit-serial network link in each direction), but\nfailures may still occur in the network. In case of failure, the midplane node boards\ncontaining the fault(s) are switched off and bypassed to isolate the fault, and com-\nputation resumes from the last checkpoint. Bypassing is done using separate\nbypass switch boards associated with each midplane that are additional to the\nset of torus node boards. Each bypass switch board can be configured to connect\neither to the corresponding links in the midplane node boards or to the next bypass\nboard, effectively removing the corresponding set of midplane node boards.\nAlthough the number of processing nodes is reduced to some degree in some net-\nwork dimensions, the machine retains its topological structure and routing\nalgorithm.\nSome collective communication operations such as barrier synchronization,\nbroadcast/multicast, reduction, and so on are not performed well on the 3D\ntorus as the network would be flooded with traffic. To remedy this, two separate\ntree networks with higher per-link bandwidth are used to implement collective\nand combining operations more efficiently. In addition to providing support for\nefficient synchronization and broadcast/multicast, hardware is used to perform\nsome arithmetic reduction operations in an efficient way (e.g., to compute the\nsum or the maximum value of a set of values, one from each processing node).\nIn addition to the 3D torus and the two tree networks, the Blue Gene/L imple-\nments an I/O Gigabit Ethernet network and a control system Fast Ethernet net-\nwork of lower bandwidth to provide for parallel I/O, configuration, debugging,\nand maintenance.\nSystem/Storage Area Network: InfiniBand\nInfiniBand is an industrywide de facto networking standard developed in October\n2000 by a consortium of companies belonging to the InfiniBand Trade Associa-\ntion. InfiniBand can be used as a system area network for interprocessor commu-\nnication or as a storage area network for server I/O. It is a switch-based\ninterconnect technology that provides flexibility in the topology, routing algo-\nrithm, and arbitration technique implemented by vendors and users. InfiniBand\nsupports data transmission rates of 2 to 120 Gbp/link per direction across distances\nof 300 meters. It uses cut-through switching, 16 virtual channels and service levels,\ncredit-based link-level flow control, and weighted round-robin fair scheduling and\nimplements programmable forwarding tables. It also includes features useful for\nincreasing reliability and system availability, such as communication subnet man-\nagement, end-to-end path establishment, and virtual destination naming.\nF.8\nExamples of Interconnection Networks\n\u25a0\nF-77"
    },
    {
        "page": 1026,
        "text": "Figure F.30 shows the packet format for InfiniBand juxtaposed with two other net-\nwork standards from the LAN and WAN areas. Figure F.31 compares various char-\nacteristics of the InfiniBand standard with two proprietary system area networks\nwidely used in research and commercial high-performance computer systems.\nInstitution\nand\nprocessor\n[network]\nname\nYear\nbuilt\nNumber of\nnetwork\nports [cores\nor tiles\n+other ports]\nBasic\nnetwork\ntopology\n# of data\nbits per\nlink per\ndirection\nLink\nbandwidth\n[link clock\nspeed]\nRouting;\narbitration;\nswitching\n# of chip\nmetal layers;\nflow control;\n#virtual\nchannels\nMIT Raw\n[General\nDynamic\nNetwork]\n2002\n16 ports [16\ntiles]\n2D mesh\n(4\u00034)\n32 bits\n0.9 GB/sec\n[225 MHz,\nclocked at\nproc speed]\nXY DOR with\nrequest-reply\ndeadlock\nrecovery; RR\narbitration;\nwormhole\n6 layers;\ncredit-based\nno virtual\nchannels\nIBM Power5\n2004\n7 ports [2 PE\ncores+5 other\nports]\nCrossbar\n256 bits\nInst fetch;\n64 bits for\nstores;\n256 bits\nLDs\n[1.9 GHz,\nclocked at\nproc speed]\nShortest-path;\nnonblocking;\ncircuit switch\n7 layers;\nhandshaking;\nno virtual\nchannels\nU.T. Austin\nTRIP Edge\n[Operand\nNetwork]\n2005\n25 ports [25\nexecution unit\ntiles]\n2D mesh\n(5\u00035)\n110 bits\n5.86 GB/sec\n[533 MHz\nclock scaled\nby 80%]\nYX DOR;\ndistributed RR\narbitration;\nwormhole\n7 layers; on/\noff flow\ncontrol; no\nvirtual\nchannels\nU.T. Austin\nTRIP Edge\n[On-Chip\nNetwork]\n2005\n40 ports\n[16 L2 tiles\n+24 network\ninterface tile]\n2D mesh\n(10\u00034)\n128 bits\n6.8 GB/sec\n[533 MHz\nclock scaled\nby 80%]\nYX DOR;\ndistributed RR\narbitration;\nVCT switched\n7 layers;\ncredit-based\nflow control;\n4 virtual\nchannels\nSony, IBM,\nToshiba Cell\nBE [Element\nInterconnect\nBus]\n2005\n12 ports [1\nPPE and\n8 SPEs+3\nother ports for\nmemory, I/O\ninterface]\nRing (4\ntotal, 2 in\neach\ndirection)\n128 bits\ndata (+16\nbits tag)\n25.6 GB/sec\n[1.6 GHz,\nclocked at\nhalf the proc\nspeed]\nShortest-path;\ntree-based RR\narbitration\n(centralized);\npipelined circuit\nswitch\n8 layers;\ncredit-based\nflow control;\nno virtual\nchannels\nSun\nUltraSPARC\nT1 processor\n2005\nUp to 13 ports\n[8 PE cores\n+4 L2 banks\n+1 shared I/O]\nCrossbar\n128 bits\nboth for\nthe 8 cores\nand the\n4 L2\nbanks\n19.2 GB/sec\n[1.2 GHz,\nclocked at\nproc speed]\nShortest-path;\nage-based\narbitration;\nVCT switched\n9 layers;\nhandshaking;\nno virtual\nchannels\nFigure F.29 Characteristics of on-chip networks implemented in recent research and commercial processors.\nSome processors implement multiple on-chip networks (not all shown)\u2014for example, two in the MIT Raw and eight\nin the TRIP Edge.\nF-78\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1027,
        "text": "ATM\nData (48)\nDestination\nChecksum\nT\nInfiniBand\nSequence number\nT\nVersion\n32 bits\nEthernet\nPreamble\nPreamble\nPad (0\u201346)\nChecksum\nChecksum\nChecksum\n32 bits\nDestination\nDestination\nSource\nDestination\nPartition key\nDestination queue\nType\nLength\nLength\nSource\nSource\nData (0\u20131500)\nData (0\u20134096)\n32 bits\nFigure F.30 Packet format for InfiniBand, Ethernet, and ATM. ATM calls their messages \u201ccells\u201d instead of packets, so\nthe proper name is ATM cell format. The width of each drawing is 32 bits. All three formats have destination addres-\nsing fields, encoded differently for each situation. All three also have a checksum field to catch transmission errors,\nalthough the ATM checksum field is calculated only over the header; ATM relies on higher-level protocols to catch\nerrors in the data. Both InfiniBand and Ethernet have a length field, since the packets hold a variable amount of data,\nwith the former counted in 32-bit words and the latter in bytes. InfiniBand and ATM headers have a type field (T) that\ngives the type of packet. The remaining Ethernet fields are a preamble to allow the receiver to recover the clock from\nthe self-clocking code used on the Ethernet, the source address, and a pad field to make sure the smallest packet is 64\nbytes (including the header). InfiniBand includes a version field for protocol version, a sequence number to allow in-\norder delivery, a field to select the destination queue, and a partition key field. Infiniband has many more small fields\nnot shown and many other packet formats; above is a simplified view. ATM\u2019s short, fixed packet is a good match to\nreal-time demand of digital voice.\nF.8\nExamples of Interconnection Networks\n\u25a0\nF-79"
    },
    {
        "page": 1028,
        "text": "InfiniBand offers two basic mechanisms to support user-level communica-\ntion: send/receive and remote DMA (RDMA). With send/receive, the receiver\nhas to explicitly post a receive buffer (i.e., allocate space in its channel adapter\nnetwork interface) before the sender can transmit data. With RDMA, the sender\ncan remotely DMA data directly into the receiver device\u2019s memory. For exam-\nple, for a nominal packet size of 4 bytes measured on a Mellanox MHEA28-XT\nchannel adapter connected to a 3.4 GHz Intel Xeon host device, sending and\nreceiving overhead is 0.946 and 1.423 \u03bcs, respectively, for the send/receive\nmechanism, whereas it is 0.910 and 0.323 \u03bcs, respectively, for the RDMA\nmechanism.\nAs discussed in Section F.2, the packet size is important in getting full benefit\nof the network bandwidth. One might ask, \u201cWhat is the natural size of messages?\u201d\nFigure F.32(a) shows the size of messages for a commercial fluid dynamics sim-\nulation application, called Fluent, collected on an InfiniBand network at The Ohio\nState University\u2019s Network-Based Computer Laboratory. One plot is cumulative in\nmessages sent and the other is cumulative in data bytes sent. Messages in this graph\nare message passing interface (MPI) units of information, which gets divided into\nInfiniBand maximum transfer units (packets) transferred over the network. As\nshown, the maximum message size is over 512 KB, but approximately 90% of\nthe messages are less than 512 bytes. Messages of 2 KB represent approximately\n50% of the bytes transferred. An Integer Sort application kernel in the NAS Parallel\nNetwork\nname\n[vendors]\nUsed in top 10\nsupercomputer\nclusters (2005)\nNumber\nof nodes\nBasic\nnetwork\ntopology\nRaw link\nbidirectional\nBW\nRouting\nalgorithm\nArbitration\ntechnique\nSwitching\ntechnique;\nflow control\nInfiniBand\n[Mellanox,\nVoltair]\nSGI Altrix and\nDell Poweredge\nThunderbird\n>Millions\n(2128\nGUID\naddresses,\nlike IPv6)\nCompletely\nconfigurable\n(arbitrary)\n4\u2013240 Gbps\nArbitrary\n(table-\ndriven),\ntypically\nup*/down*\nWeighted\nRR fair\nscheduling\n(2-level\npriority)\nCut-through,\n16 virtual\nchannels (15\nfor data);\ncredit-based\nMyrinet-\n2000\n[Myricom]\nBarcelona\nSupercomputer\nCenter in Spain\n8192\nnodes\nBidirectional\nMIN with 16-\nport\nbidirectional\nswitches\n(Clos net.)\n4 Gbps\nSource-\nbased\ndispersive\n(adaptive)\nminimal\nrouting\nRound-\nrobin\narbitration\nCut-through\nswitching with\nno virtual\nchannels; Xon/\nXoff flow\ncontrol\nQsNetII\n[Quadrics]\nIntel Thunder\nItanium2 Tiger4\n>Tens of\nthousands\nFat tree with\n8-port\nbidirectional\nswitches\n21.3 Gbps\nSource-\nbased LCA\nadaptive\nshortest-\npath\nrouting\n2-phased\nRR, priority,\naging,\ndistributed\nat output\nports\nWormhole with\n2 virtual\nchannels;\ncredit-based\nFigure F.31 Characteristics of system area networks implemented in various top 10 supercomputer clusters in\n2005.\nF-80\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1029,
        "text": "Benchmark suite is also measured to have about 75% of its messages below 512\nbytes (plots not shown). Many applications send far more small messages than\nlarge ones, particularly since requests and acknowledgments are more frequent\nthan data responses and block writes.\nInfiniBand reduces protocol processing overhead by allowing it to be off-\nloaded from the host computer to a controller on the InfiniBand network inter-\nface card. The benefits of protocol offloading and bypassing the operating\nsystem are shown in Figure F.32(b) for MVAPICH, a widely used implemen-\ntation of MPI over InfiniBand. Effective bandwidth is plotted against message\nsize for MVAPICH configured in two modes and two network speeds. One\nmode runs IPoIB, in which InfiniBand communication is handled by the IP\nlayer implemented by the host\u2019s operating system (i.e., no OS bypass). The\nother mode runs MVAPICH directly over VAPI, which is the native Mellanox\nInfiniBand interface that offloads transport protocol processing to the channel\nadapter hardware (i.e., OS bypass). Results are shown for 10 Gbps single data\nrate (SDR) and 20 Gbps double data rate (DDR) InfiniBand networks. The\nresults clearly show that offloading the protocol processing and bypassing\nthe OS significantly reduce sending and receiving overhead to allow near\nwire-speed effective bandwidth to be achieved.\nPercentage\n100%\n90%\n80%\n70%\n60%\n50%\n40%\n30%\n20%\n10%\n64\n0%\nMessage size (bytes)\n256K\n64K\n16K\n4K\n1K\n256\nMeasured effective bandwidth (MB/sec)\n1600\n1400\n1200\n1000\n800\n600\n400\n200\n4\n0\nMessage size (bytes)\n(A)\n(B)\n4M\n256K\n16K\n1K\n64\nNumber of messages\nData volume\nMVAPICH native DDR\nMVAPICH native SDR\nMVAPICH 1PoIB SDR\nMVAPICH 1PoIB DDR\nFigure F.32 Data collected by D.K. Panda, S. Sur, and L. Chai (2005) in the Network-Based Computing Laboratory\nat The Ohio State University. (a) Cumulative percentage of messages and volume of data transferred as message size\nvaries for the Fluent application (www.fluent.com). Each x-axis entry includes all bytes up to the next one; for example,\n128 represents 1 byte to 128 bytes. About 90% of the messages are less than 512 bytes, which represents about 40%\nof the total bytes transferred. (b) Effective bandwidth versus message size measured on SDR and DDR InfiniBand\nnetworks running MVAPICH (http://nowlab.cse.ohio-state.edu/projects/mpi-iba) with OS bypass (native) and\nwithout (IPoIB).\nF.8\nExamples of Interconnection Networks\n\u25a0\nF-81"
    },
    {
        "page": 1030,
        "text": "Ethernet: The Local Area Network\nEthernet has been extraordinarily successful as a LAN\u2014from the 10 Mbit/sec stan-\ndard proposed in 1978 used practically everywhere today to the more recent 10\nGbit/sec standard that will likely be widely used. Many classes of computers\ninclude Ethernet as a standard communication interface. Ethernet, codified as IEEE\nstandard 802.3, is a packet-switched network that routes packets using the desti-\nnation address. It was originally designed for coaxial cable but today uses primarily\nCat5E copper wire, with optical fiber reserved for longer distances and higher\nbandwidths. There is even a wireless version (802.11), which is testimony to its\nubiquity.\nOver a 20-year span, computers became thousands of times faster than they\nwere in 1978, but the shared media Ethernet network remained the same. Hence,\nengineers had to invent temporary solutions until a faster, higher-bandwidth net-\nwork became available. One solution was to use multiple Ethernets to interconnect\nmachines and to connect those Ethernets with internetworking devices that could\ntransfer traffic from one Ethernet to another, as needed. Such devices allow indi-\nvidual Ethernets to operate in parallel, thereby increasing the aggregate intercon-\nnection bandwidth of a collection of computers. In effect, these devices provide\nsimilar functionality to the switches described previously for point-to-point\nnetworks.\nFigure F.33 shows the potential parallelism that can be gained. Depending on\nhow they pass traffic and what kinds of interconnections they can join together,\nthese devices have different names:\nSingle Ethernet: one packet at a time\nMultiple Ethernets: multiple packets at a time\nNode\nNode\nNode\nNode\nNode\nNode\nNode\nNode\nNode\nNode\nNode\nBridge\nBridge\nNode\nNode\nNode\nNode\nNode\nNode\nNode\nNode\nNode\nNode\nNode\nFigure F.33 The potential increased bandwidth of using many Ethernets and bridges.\nF-82\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1031,
        "text": "\u25a0\nBridges\u2014These devices connect LANs together, passing traffic from one side\nto another depending on the addresses in the packet. Bridges operate at the\nEthernet protocol level and are usually simpler and cheaper than routers, dis-\ncussed next. Using the notation of the OSI model described in the next section\n(see Figure F.36 on page F-85), bridges operate at layer 2, the data link layer.\n\u25a0\nRouters or gateways\u2014These devices connect LANs to WANs, or WANs to\nWANs, and resolve incompatible addressing. Generally slower than bridges,\nthey operate at OSI layer 3, the network layer. WAN routers divide the network\ninto separate smaller subnets, which simplifies manageability and improves\nsecurity.\nThe final internetworking devices are hubs, but they merely extend multiple seg-\nments into a single LAN. Thus, hubs do not help with performance, as only one\nmessage can transmit at a time. Hubs operate at OSI layer 1, called the physical\nUCB1.\nBARRNet.net\n192.31.161.4\nmojave.\nStanford.edu\n36.22.0.120\nCIS-Gateway.\nStanford.edu\n36.1.0.22\nSU-CM.\nBARRNet.net\n131.119.5.3\nEthernet\nFDDI\nT1 line\nT3 line\ninr-108-eecs.\nBerkeley.edu\n128.32.120.108\n128.32.120.111\n inr-111-cs2.\nBerkeley.edu\n128.32.149.13\n mammoth.\nBerkeley.edu\n128.32.149.78\nFDDI\nFDDI\nEthernet\nEthernet\nInternet\nfd-0.enss128.t3.\nans.net\n192.31.48.244\nStanford,\nCalifornia\nBerkeley,\nCalifornia\nFigure F.34 The connection established between mojave.stanford.edu and mammoth.berkeley.edu (1995). FDDI\nis a 100 Mbit/sec LAN, while a T1 line is a 1.5 Mbit/sec telecommunications line and a T3 is a 45 Mbit/sec telecom-\nmunications line. BARRNet stands for Bay Area Research Network. Note that inr-111-cs2.Berkeley.edu is a router with\ntwo Internet addresses, one for each port.\nF.8\nExamples of Interconnection Networks\n\u25a0\nF-83"
    },
    {
        "page": 1032,
        "text": "layer. Since these devices were not planned as part of the Ethernet standard, their\nad hoc nature has added to the difficulty and cost of maintaining LANs.\nAs of 2011, Ethernet link speeds are available at 10, 100, 10,000, and 100,000\nMbits/sec. Although 10 and 100 Mbits/sec Ethernets share the media with multiple\ndevices, 1000 Mbits/sec and above Ethernets rely on point-to-point links and\nswitches. Ethernet switches normally use some form of store-and-forward.\nEthernet has no real flow control, dating back to its first instantiation. It orig-\ninally used carrier sensing with exponential back-off (see page F-23) to arbitrate for\nthe shared media. Some switches try to use that interface to retrofit their version of\nflow control, but flow control is not part of the Ethernet standard.\nWide Area Network: ATM\nAsynchronous Transfer Mode (ATM) is a wide area networking standard set by the\ntelecommunications industry. Although it flirted as competition to Ethernet as a\nLAN in the 1990s, ATM has since retreated to its WAN stronghold.\nApplications\nNetworks\nInternetworking\nFigure F.35 The role of internetworking. The width indicates the relative number of\nitems at each level.\nLayer\nnumber\nLayer\nname\nMain function\nExample\nprotocol\nNetwork\ncomponent\n7\nApplication\nUsed for applications specifically written to run over the\nnetwork\nFTP, DNS,\nNFS, http\nGateway, smart\nswitch\n6\nPresentation\nTranslates from application to network format, and vice\nversa\nGateway\n5\nSession\nEstablishes, maintains, and ends sessions across the\nnetwork\nNamed\npipes, RPC\nGateway\n4\nTransport\nAdditional connection below the session layer\nTCP\nGateway\n3\nNetwork\nTranslates logical network address and names to their\nphysical address (e.g., computer name to MAC address)\nIP\nRouter, ATM\nswitch\n2\nData Link\nTurns packets into raw bits and at the receiving end turns\nbits into packets\nEthernet\nBridge, network\ninterface card\n1\nPhysical\nTransmits raw bit stream over physical cable\nIEEE 802\nHub\nFigure F.36 The OSI model layers. Based on www.geocities.com/SiliconValley/Monitor/3131/ne/osimodel.html.\nF-84\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1033,
        "text": "Thetelecommunicationsstandardhasscalablebandwidthbuiltin.Itstarts at155\nMbits/sec and scales by factors of 4 to 620 Mbits/sec, 2480 Mbits/sec, and so on.\nSince it is a WAN, ATM\u2019s medium is fiber, both single mode and multimode.\nAlthough it is a switched medium, unlike the other examples it relies on virtual con-\nnectionsforcommunication.ATMusesvirtualchannelsforroutingtomultiplexdif-\nferent connections on a single network segment, thereby avoiding the inefficiencies\nof conventional connection-based networking. The WAN focus also led to store-\nand-forward switching. Unlike the other protocols, Figure F.30 shows ATM has\na small, fixed-sized packet with 48 bytes of payload. It uses a credit-based flow con-\ntrol scheme as opposed to IP routers that do not implement flow control.\nThe reason for virtual connections and small packets is quality of service. Since\nthe telecommunications industry is concerned about voice traffic, predictability\nmatters as well as bandwidth. Establishing a virtual connection has less variability\nthan connectionless networking, and it simplifies store-and-forward switching.\nThe small, fixed packet also makes it simpler to have fast routers and switches.\nToward that goal, ATM even offers its own protocol stack to compete with\nTCP/IP. Surprisingly, even though the switches are simple, the ATM suite of pro-\ntocols is large and complex. The dream was a seamless infrastructure from LAN to\nWAN, avoiding the hodgepodge of routers common today. That dream has faded\nfrom inspiration to nostalgia.\nF.9\nInternetworking\nUndoubtedly one of the most important innovations in the communications com-\nmunity has been internetworking. It allows computers on independent and incom-\npatible networks to communicate reliably and efficiently. Figure F.34 illustrates\nthe need to traverse between networks. It shows the networks and machines\ninvolved in transferring a file from Stanford University to the University of Cal-\nifornia at Berkeley, a distance of about 75 km.\nThe low cost of internetworking is remarkable. For example, it is vastly less\nexpensive to send electronic mail than to make a coast-to-coast telephone call\nand leave a message on an answering machine. This dramatic cost improvement\nis achieved using the same long-haul communication lines as the telephone call,\nwhich makes the improvement even more impressive.\nThe enabling technologies for internetworking are software standards that\nallow reliable communication without demanding reliable networks. The underly-\ning principle of these successful standards is that they were composed as a hierar-\nchy of layers, each layer taking responsibility for a portion of the overall\ncommunication task. Each computer, network, and switch implements its layer\nof the standards, relying on the other components to faithfully fulfill their respon-\nsibilities. These layered software standards are called protocol families or protocol\nsuites. They enable applications to work with any interconnection without extra\nwork by the application programmer. Figure F.35 suggests the hierarchical model\nof communication.\nF.9\nInternetworking\n\u25a0\nF-85"
    },
    {
        "page": 1034,
        "text": "The most popular internetworking standard is TCP/IP (Transmission Control\nProtocol/Internet Protocol). This protocol family is the basis of the humbly named\nInternet, which connects hundreds of millions of computers around the world.\nThis popularity means TCP/IP is used even when communicating locally across\ncompatible networks; for example, the network file system (NFS) uses IP even\nthough it is very likely to be communicating across a homogenous LAN such\nas Ethernet. We use TCP/IP as our protocol family example; other protocol\nfamilies follow similar lines. Section F.13 gives the history of TCP/IP.\nThe goal of a family of protocols is to simplify the standard by dividing respon-\nsibilities hierarchically among layers, with each layer offering services needed by\nthe layer above. The application program is at the top, and at the bottom is the phys-\nical communication medium, which sends the bits. Just as abstract data types sim-\nplify the programmer\u2019s task by shielding the programmer from details of the\nimplementation of the data type, this layered strategy makes the standard easier\nto understand.\nThere were many efforts at network protocols, which led to confusion in terms.\nHence, Open Systems Interconnect (OSI) developed a model that popularized\ndescribing networks as a series of layers. Figure F.36 shows the model. Although\nall protocols do not exactly follow this layering, the nomenclature for the different\nlayers is widely used. Thus, you can hear discussions about a simple layer 3 switch\nversus a layer 7 smart switch.\nThe key to protocol families is that communication occurs logically at the same\nlevel of the protocol in both sender and receiver, but services of the lower level\nimplement it. This style of communication is called peer-to-peer. As an analogy,\nimagine that General A needs to send a message to General B on the battlefield.\nGeneral A writes the message, puts it in an envelope addressed to General B, and\ngives it to a colonel with orders to deliver it. This colonel puts it in an envelope, and\nwrites the name of the corresponding colonel who reports to General B, and gives it\nto a major with instructions for delivery. The major does the same thing and gives it\nto a captain, who gives it to a lieutenant, who gives it to a sergeant. The sergeant\ntakes the envelope from the lieutenant, puts it into an envelope with the name of a\nsergeant who is in General B\u2019s division, and finds a private with orders to take the\nlarge envelope. The private borrows a motorcycle and delivers the envelope to the\nother sergeant. Once it arrives, it is passed up the chain of command, with each\nperson removing an outer envelope with his name on it and passing on the inner\nenvelope to his superior. As far as General B can tell, the note is from another gen-\neral. Neither general knows who was involved in transmitting the envelope, nor\nhow it was transported from one division to the other.\nProtocol families follow this analogy more closely than you might think, as\nFigure F.37 shows. The original message includes a header and possibly a trailer\nsent by the lower-level protocol. The next-lower protocol in turn adds its own\nheader to the message, possibly breaking it up into smaller messages if it is too\nlarge for this layer. Reusing our analogy, a long message from the general is\ndivided and placed in several envelopes if it could not fit in one. This division\nof the message and appending of headers and trailers continues until the message\nF-86\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1035,
        "text": "descends to the physical transmission medium. The message is then sent to the des-\ntination. Each level of the protocol family on the receiving end will check the mes-\nsage at its level and peel off its headers and trailers, passing it on to the next higher\nlevel and putting the pieces back together. This nesting of protocol layers for a spe-\ncific message is called a protocol stack, reflecting the last in, first out nature of the\naddition and removal of headers and trailers.\nAs in our analogy, the danger in this layered approach is the considerable\nlatency added to message delivery. Clearly, one way to reduce latency is to reduce\nthe number of layers, but keep in mind that protocol families define a standard but\ndo not force how to implement the standard. Just as there are many ways to imple-\nment an instruction set architecture, there are many ways to implement a protocol\nfamily.\nOur protocol stack example is TCP/IP. Let\u2019s assume that the bottom protocol\nlayer is Ethernet. The next level up is the Internet Protocol or IP layer; the official\nterm for an IP packet is a datagram. The IP layer routes the datagram to the des-\ntination machine, which may involve many intermediate machines or switches. IP\nmakes a best effort to deliver the packets but does not guarantee delivery, content,\nor order of datagrams. The TCP layer above IP makes the guarantee of reliable, in-\norder delivery and prevents corruption of datagrams.\nFollowing the example in Figure F.37, assume an application program wants to\nsend a message to a machine via an Ethernet. It starts with TCP. The largest number\nof bytes that can be sent at once is 64 KB. Since the data may be much larger than\n64 KB, TCP must divide them into smaller segments and reassemble them in\nproper order upon arrival. TCP adds a 20-byte header (Figure F.38) to every data-\ngram and passes them down to IP. The IP layer above the physical layer adds a\n20-byte header, also shown in Figure F.38. The data sent down from the IP level\nT\nMessage\nH\nT\nH\nH\nT T\nH\nH\nT T\nH\nH\nT T\nH\nH\nT T\nH\nH\nT T\nT\nH\nT\nH\nT\nMessage\nH\nT\nH\nT\nH\nT\nla\nu\ntc\nA\nla\nu\ntc\nA\nActual\nActual\nLogical\nLogical\nActual\nFigure F.37 A generic protocol stack with two layers. Note that communication is\npeer-to-peer, with headers and trailers for the peer added at each sending layer and\nremoved by each receiving layer. Each layer offers services to the one above to shield\nit from unnecessary details.\nF.9\nInternetworking\n\u25a0\nF-87"
    },
    {
        "page": 1036,
        "text": "IP header\nIP data\nTCP data\nIdentifier\nFragment\nHeader checksum\nSource\nSource\nSequence number (length)\nDestination\nDestination\nLength\nType\nTime\nProtocol\nV\nL\nTCP header\nUrgent  pointer\nWindow\nTCP data\n32 bits\nPiggyback acknowledgment\nFlags\nChecksum\nL\n (0\u201365,516 bytes)\nFigure F.38 The headers for IP and TCP. This drawing is 32 bits wide. The standard headers for both are 20 bytes,\nbut both allow the headers to optionally lengthen for rarely transmitted information. Both headers have a length of\nheader field (L) to accommodate the optional fields, as well as source and destination fields. The length field of the\nwhole datagram is in a separate length field in IP, while TCP combines the length of the datagram with the sequence\nnumber of the datagram by giving the sequence number in bytes. TCP uses the checksum field to be sure that the\ndatagram is not corrupted, and the sequence number field to be sure the datagrams are assembled into the proper\norder when they arrive. IP provides checksum error detection only for the header, since TCP has protected the rest of\nthe packet. One optimization is that TCP can send a sequence of datagrams before waiting for permission to send\nmore. The number of datagrams that can be sent without waiting for approval is called the window, and the window\nfield tells how many bytes may be sent beyond the byte being acknowledged by this datagram. TCP will adjust the\nsize of the window depending on the success of the IP layer in sending datagrams; the more reliable and faster it is,\nthe larger TCP makes the window. Since the window slides forward as the data arrive and are acknowledged, this\ntechnique is called a sliding window protocol. The piggyback acknowledgment field of TCP is another optimization.\nSince some applications send data back and forth over the same connection, it seems wasteful to send a datagram\ncontaining only an acknowledgment. This piggyback field allows a datagram carrying data to also carry the acknowl-\nedgment for a previous transmission, \u201cpiggybacking\u201d on top of a data transmission. The urgent pointer field of TCP\ngives the address within the datagram of an important byte, such as a break character. This pointer allows the appli-\ncation software to skip over data so that the user doesn\u2019t have to wait for all prior data to be processed before seeing a\ncharacter that tells the software to stop. The identifier field and fragment field of IP allow intermediary machines to\nbreak the original datagram into many smaller datagrams. A unique identifier is associated with the original datagram\nand placed in every fragment, with the fragment field saying which piece is which. The time-to-live field allows a\ndatagram to be killed off after going through a maximum number of intermediate switches no matter where it is\nin the network. Knowing the maximum number of hops that it will take for a datagram to arrive\u2014if it ever\narrives\u2014simplifies the protocol software. The protocol field identifies which possible upper layer protocol sent\nthe IP datagram; in our case, it is TCP. The V (for version) and type fields allow different versions of the IP protocol\nsoftware for the network. Explicit version numbering is included so that software can be upgraded gracefully machine\nby machine, without shutting down the entire network. Nowadays, version six of the Internet protocol (IPv6) was\nwidely used."
    },
    {
        "page": 1037,
        "text": "to the Ethernet are sent in packets with the format shown in Figure F.30. Note that\nthe TCP packet appears inside the data portion of the IP datagram, just as\nFigure F.37 suggests.\nF.10\nCrosscutting Issues for Interconnection Networks\nThis section describes five topics discussed in other chapters that are fundamen-\ntally impacted by interconnection networks, and vice versa.\nDensity-Optimized Processors versus SPEC-Optimized\nProcessors\nGiven that people all over the world are accessing Web sites, it doesn\u2019t really mat-\nter where servers are located. Hence, many servers are kept at collocation sites,\nwhich charge by network bandwidth reserved and used and by space occupied\nand power consumed. Desktop microprocessors in the past have been designed\nto be as fast as possible at whatever heat could be dissipated, with little regard\nfor the size of the package and surrounding chips. In fact, some desktop micropro-\ncessors from Intel and AMD as recently as 2006 burned as much as 130 watts!\nFloor space efficiency was also largely ignored. As a result of these priorities,\npower is a major cost for collocation sites, and processor density is limited by\nthe power consumed and dissipated, including within the interconnect!\nWith the proliferation of portable computers (notebook sales exceeded desktop\nsales for the first time in 2005) and their reduced power consumption and cooling\ndemands, the opportunity exists for using this technology to create considerably\ndenser computation. For instance, the power consumption for the Intel Pentium\nM in 2006 was 25 watts, yet it delivered performance close to that of a desktop\nmicroprocessor for a wide set of applications. It is therefore conceivable that per-\nformance per watt or performance per cubic foot could replace performance per\nmicroprocessor as the important figure of merit. The key is that many applications\nalready make use of large clusters, so it is possible that replacing 64 power-hungry\nprocessors with, say, 256 power-efficient processors could be cheaper yet be soft-\nware compatible. This places greater importance on power- and performance-\nefficient interconnection network design.\nThe Google cluster is a prime example of this migration to many \u201ccooler\u201d\nprocessors versus fewer \u201chotter\u201d processors. It uses racks of up to 80 Intel Pen-\ntium III 1 GHz processors instead of more power-hungry high-end processors.\nOther examples include blade servers consisting of 1-inch-wide by 7-inch-high\nrack unit blades designed based on mobile processors. The HP ProLiant BL10e\nG2 blade server supports up to 20 1-GHz ultra-low-voltage Intel Pentium M\nprocessors with a 400-MHz front-side bus, 1-MB L2 cache, and up to 1 GB\nmemory. The Fujitsu Primergy BX300 blade server supports up to 20 1.4- or\n1.6-GHz Intel Pentium M processors, each with 512 MB of memory expandable\nto 4 GB.\nF.10\nCrosscutting Issues for Interconnection Networks\n\u25a0\nF-89"
    },
    {
        "page": 1038,
        "text": "Smart Switches versus Smart Interface Cards\nFigure F.39 shows a trade-off as to where intelligence can be located within a net-\nwork. Generally, the question is whether to have either smarter network interfaces\nor smarter switches. Making one smarter generally makes the other simpler and\nless expensive. By having an inexpensive interface, it was possible for Ethernet\nto become standard as part of most desktop and server computers. Lower-cost\nswitches were made available for people with small configurations, not needing\nsophisticated forwarding tables and spanning-tree protocols of larger Ethernet\nswitches.\nMyrinet followed the opposite approach. Its switches are dumb components\nthat, other than implementing flow control and arbitration, simply extract the first\nbyte from the packet header and use it to directly select the output port. No routing\ntables are implemented, so the intelligence is in the network interface cards (NICs).\nThe NICs are responsible for providing support for efficient communication and\nfor implementing a distributed protocol for network (re)configuration. InfiniBand\ntakes a hybrid approach by offering lower-cost, less sophisticated interface cards\ncalled target channel adapters (or TCAs) for less demanding devices such as\ndisks\u2014in the hope that it can be included within some I/O devices\u2014and by offer-\ning more expensive, powerful interface cards for hosts called host channel adapters\n(or HCAs). The switches implement routing tables.\nSwitch\nInterface\ncard\nSmall-scale\nEthernet switch\nLarge-scale\nEthernet switch\nt\ne\nnir\ny\nM\nt\ne\nn\nr\ne\nh\nt\nE\nMyrinet\nInfiniBand\nInfiniBand target \nchannel adapter\nInfiniBand host\nchannel adapter\nMore \nintelligence\nFigure F.39 Intelligence in a network: switch versus network interface card. Note\nthat Ethernet switches come in two styles, depending on the size of the network,\nand that InfiniBand network interfaces come in two styles, depending on whether they\nare attached to a computer or to a storage device. Myrinet is a proprietary system area\nnetwork.\nF-90\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1039,
        "text": "Protection and User Access to the Network\nA challenge is to ensure safe communication across a network without invoking\nthe operating system in the common case. The Cray Research T3D supercomputer\noffers an interesting case study. Like the more recent Cray X1E, the T3D supports a\nglobal address space, so loads and stores can access memory across the network.\nProtection is ensured because each access is checked by the TLB. To support trans-\nfer of larger objects, a block transfer engine (BLT) was added to the hardware. Pro-\ntection of access requires invoking the operating system before using the BLT to\ncheck the range of accesses to be sure there will be no protection violations.\nFigure F.40 compares the bandwidth delivered as the size of the object varies\nfor reads and writes. For very large reads (e.g., 512 KB), the BLT achieves the\nhighest performance: 140 MB/sec. But simple loads get higher performance for\n8 KB or less. For the write case, both achieve a peak of 90 MB/sec, presumably\nbecause of the limitations of the memory bus. But, for writes, the BLT can only\nmatch the performance of simple stores for transfers of 2 MB; anything smaller\nand it\u2019s faster to send stores. Clearly, a BLT that can avoid invoking the operating\nsystem in the common case would be more useful.\nEfficient Interface to the Memory Hierarchy versus the Network\nTraditional evaluations of processor performance, such as SPECint and SPECfp,\nencourage integration of the memory hierarchy with the processor as the efficiency\nof the memory hierarchy translates directly into processor performance. Hence,\n128\n256\n512\n1024\n2048\n4096\n8192\n16,384\n32,768\n65,536\n131,072\n262,144\n524,288\n1,048,576\n2,097,152\n4,194,304\n8,388,608\nTransfer size (bytes)\n0\n20\n40\n60\n80\n100\n120\n140\n160\nCPU write\nBLT read\nBLT write\nCPU read\nBandwidth (MB/sec)\nFigure F.40 Bandwidth versus transfer size for simple memory access instructions\nversus a block transfer device on the Cray Research T3D. (From Arpaci et al. [1995].)\nF.10\nCrosscutting Issues for Interconnection Networks\n\u25a0\nF-91"
    },
    {
        "page": 1040,
        "text": "microprocessors have multiple levels of caches on chip along with buffers for\nwrites. Because benchmarks such as SPECint and SPECfp do not reward good\ninterfaces to interconnection networks, many machines make the access time to\nthe network delayed by the full memory hierarchy. Writes must lumber their\nway through full write buffers, and reads must go through the cycles of first-,\nsecond-, and often third-level cache misses before reaching the interconnection\nnetwork. This hierarchy results in newer systems having higher latencies to the\ninterconnect than older machines.\nLet\u2019s compare three machines from the past: a 40-MHz SPARCstation-2, a 50-\nMHz SPARCstation-20 without an external cache, and a 50-MHz SPARCstation-\n20 with an external cache. According to SPECint95, this list is in order of increas-\ning performance. The time to access the I/O bus (S-bus), however, increases in this\nsequence: 200 ns, 500 ns, and 1000 ns. The SPARCstation-2 is fastest because it\nhas a single bus for memory and I/O, and there is only one level to the cache. The\nSPARCstation-20 memory access must first go over the memory bus (M-bus) and\nthen to the I/O bus, adding 300 ns. Machines with a second-level cache pay an\nextra penalty of 500 ns before accessing the I/O bus.\nCompute-Optimized Processors versus Receiver Overhead\nThe overhead to receive a message likely involves an interrupt, which bears the\ncost of flushing and then restarting the processor pipeline, if not offloaded. As\nmentioned earlier, reading network status and receiving data from the network\ninterface likely operate at cache miss speeds. If microprocessors become more\nsuperscalar and go to even faster clock rates, the number of missed instruction\nissue opportunities per message reception will likely rise to unacceptable\nlevels.\nF.11\nFallacies and Pitfalls\nMyths and hazards are widespread with interconnection networks. This section\nmentions several warnings, so proceed carefully.\nFallacy\nThe interconnection network is very fast and does not need to be improved\nThe interconnection network provides certain functionality to the system, very\nmuch like the memory and I/O subsystems. It should be designed to allow proces-\nsors to execute instructions at the maximum rate. The interconnection network sub-\nsystem should provide high enough bandwidth to keep from continuously entering\nsaturation and becoming an overall system bottleneck.\nIn the 1980s, when wormhole switching was introduced, it became feasible\nto design large-diameter topologies with single-chip switches so that the band-\nwidth capacity of the network was not the limiting factor. This led to the\nflawed belief that interconnection networks need no further improvement.\nF-92\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1041,
        "text": "Since the 1980s, much attention has been placed on improving processor per-\nformance, but comparatively less has been focused on interconnection net-\nworks. As technology advances, the interconnection network tends to\nrepresent an increasing fraction of system resources, cost, power consumption,\nand various other attributes that impact functionality and performance. Scaling\nthe bandwidth simply by overdimensioning certain network parameters is no\nlonger a cost-viable option. Designers must carefully consider the end-to-\nend interconnection network design in concert with the processor, memory,\nand I/O subsystems in order to achieve the required cost, power, functionality,\nand performance objectives of the entire system. An obvious case in point is\nmulticore processors with on-chip networks.\nFallacy\nBisection bandwidth is an accurate cost constraint of a network\nDespite being very popular, bisection bandwidth has never been a practical con-\nstraint on the implementation of an interconnection network, although it may be\none in future designs. It is more useful as a performance measure than as a cost\nmeasure. Chip pin-outs are the more realistic bandwidth constraint.\nPitfall\nUsing bandwidth (in particular, bisection bandwidth) as the only measure of\nnetwork performance\nIt seldom is the case that aggregate network bandwidth (likewise, network bisec-\ntion bandwidth) is the end-to-end bottlenecking point across the network. Even if it\nwere the case, networks are almost never 100% efficient in transporting packets\nacross the bisection (i.e., \u03c1<100%) nor at receiving them at network endpoints\n(i.e., \u03c3<100%). The former is highly dependent upon routing, switching, arbitra-\ntion, and other such factors while both the former and the latter are highly depen-\ndent\nupon\ntraffic\ncharacteristics.\nIgnoring\nthese\nimportant\nfactors\nand\nconcentrating only on raw bandwidth can give very misleading performance pre-\ndictions. For example, it is perfectly conceivable that a network could have higher\naggregate bandwidth and/or bisection bandwidth relative to another network but\nalso have lower measured performance!\nApparently, given sophisticated protocols like TCP/IP that maximize delivered\nbandwidth, many network companies believe that there is only one figure of merit\nfor networks. This may be true for some applications, such as video streaming,\nwhere there is little interaction between the sender and the receiver. Many appli-\ncations, however, are of a request-response nature, and so for every large message\nthere must be one or more small messages. One example is NFS.\nFigure F.41 compares a shared 10-Mbit/sec Ethernet LAN to a switched 155-\nMbit/sec ATM LAN for NFS traffic. Ethernet drivers were better tuned than the\nATM drivers, such that 10-Mbit/sec Ethernet was faster than 155-Mbit/sec\nATM for payloads of 512 bytes or less. Figure F.41 shows the overhead time, trans-\nmission time, and total time to send all the NFS messages over Ethernet and ATM.\nThe peak link speed of ATM is 15 times faster, and the measured link speed for 8-\nKB messages is almost 9 times faster. Yet, the higher overheads offset the benefits\nso that ATM would transmit NFS traffic only 1.2 times faster.\nF.11\nFallacies and Pitfalls\n\u25a0\nF-93"
    },
    {
        "page": 1042,
        "text": "Pitfall\nNot providing sufficient reception link bandwidth, which causes the network end\nnodes to become even more of a bottleneck to performance\nUnless the traffic pattern is a permutation, several packets will concurrently\narrive at some destinations when most source devices inject traffic, thus pro-\nducing contention. If this problem is not addressed, contention may turn into\ncongestion that will spread across the network. This can be dealt with by ana-\nlyzing traffic patterns and providing extra reception bandwidth. For example, it\nis possible to implement more reception bandwidth than injection bandwidth.\nThe IBM Blue Gene/L, for example, implements an on-chip switch with 7-bit\nSize\nNumber of\nmessages\nOverhead (sec)\nNumber of data\nbytes\nTransmission\n(sec)\nTotal time (sec)\nATM\nEthernet\nATM\nEthernet\nATM\nEthernet\n32\n771,060\n532\n389\n33,817,052\n4\n48\n536\n436\n64\n56,923\n39\n29\n4,101,088\n0\n5\n40\n34\n96\n4,082,014\n2817\n2057\n428,346,316\n46\n475\n2863\n2532\n128\n5,574,092\n3846\n2809\n779,600,736\n83\n822\n3929\n3631\n160\n328,439\n227\n166\n54,860,484\n6\n56\n232\n222\n192\n16,313\n11\n8\n3,316,416\n0\n3\n12\n12\n224\n4820\n3\n2\n1,135,380\n0\n1\n3\n4\n256\n24,766\n17\n12\n9,150,720\n1\n9\n18\n21\n512\n32,159\n22\n16\n25,494,920\n3\n23\n25\n40\n1024\n69,834\n48\n35\n70,578,564\n8\n72\n56\n108\n1536\n8842\n6\n4\n15,762,180\n2\n14\n8\n19\n2048\n9170\n6\n5\n20,621,760\n2\n19\n8\n23\n2560\n20,206\n14\n10\n56,319,740\n6\n51\n20\n61\n3072\n13,549\n9\n7\n43,184,992\n4\n39\n14\n46\n3584\n4200\n3\n2\n16,152,228\n2\n14\n5\n17\n4096\n67,808\n47\n34\n285,606,596\n29\n255\n76\n290\n5120\n6143\n4\n3\n35,434,680\n4\n32\n8\n35\n6144\n5858\n4\n3\n37,934,684\n4\n34\n8\n37\n7168\n4140\n3\n2\n31,769,300\n3\n28\n6\n30\n8192\n287,577\n198\n145\n2,390,688,480\n245\n2132\n444\n2277\nTotal\n11,387,913\n7858\n5740\n4,352,876,316\n452\n4132\n8310\n9872\nFigure F.41 Total time on a 10-Mbit Ethernet and a 155-Mbit ATM, calculating the total overhead and transmis-\nsion time separately. Note that the size of the headers needs to be added to the data bytes to calculate transmission\ntime. The higher overhead of the software driver for ATM offsets the higher bandwidth of the network. These mea-\nsurements were performed in 1994 using SPARCstation 10s, the ForeSystems SBA-200 ATM interface card, and the\nFore Systems ASX-200 switch. (NFS measurements taken by Mike Dahlin of the University of California\u2013Berkeley.)\nF-94\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1043,
        "text": "injection and 12-bit reception links, where the reception BW equals the aggre-\ngate switch input link BW.\nPitfall\nUsing high-performance network interface cards but forgetting about the I/O sub-\nsystem that sits between the network interface and the host processor\nThis issue is related to the previous one. Messages are usually composed in user\nspace buffers and later sent by calling a send function from the communications\nlibrary. Alternatively, a cache controller implementing a cache coherence protocol\nmay compose a message in some SANs and in OCNs. In both cases, messages have\nto be copied to the network interface memory before transmission. If the I/O band-\nwidth is lower than the link bandwidth or introduces significant overhead, this is\ngoing to affect communication performance significantly. As an example, the first\n10-Gigabit Ethernet cards in the market had a PCI-X bus interface for the system\nwith a significantly lower bandwidth than 10 Gbps.\nFallacy\nZero-copy protocols do not require copying messages or fragments from one\nbuffer to another\nTraditional communication protocols for computer networks allow access to com-\nmunication devices only through system calls in supervisor mode. As a conse-\nquence of this, communication routines need to copy the corresponding\nmessage from the user buffer to a kernel buffer when sending a message. Note that\nthe communication protocol may need to keep a copy of the message for retrans-\nmission in case of error, and the application may modify the contents of the user\nbuffer once the system call returns control to the application. This buffer-to-buffer\ncopy is eliminated in zero-copy protocols because the communication routines are\nexecuted in user space and protocols are much simpler.\nHowever, messages still need to be copied from the application buffer to\nthe memory in the network interface card (NIC) so that the card hardware\ncan transmit it from there through to the network. Although it is feasible to\neliminate this copy by allocating application message buffers directly in the\nNIC memory (and, indeed, this is done in some protocols), this may not be\nconvenient in current systems because access to the NIC memory is usually\nperformed through the I/O subsystem, which usually is much slower than\naccessing main memory. Thus, it is generally more efficient to compose the\nmessage in main memory and let DMA devices take care of the transfer to\nthe NIC memory.\nMoreover, what few people count is the copy from where the message frag-\nments are computed (usually the ALU, with results stored in some processor reg-\nister) to main memory. Some systolic-like architectures in the 1980s, like the\niWarp, were able to directly transmit message fragments from the processor to\nthe network, effectively eliminating all the message copies. This is the approach\ntaken in the Cray X1E shared-memory multiprocessor supercomputer.\nSimilar comments can be made regarding the reception side; however, this\ndoes not mean that zero-copy protocols are inefficient. These protocols represent\nthe most efficient kind of implementation used in current systems.\nF.11\nFallacies and Pitfalls\n\u25a0\nF-95"
    },
    {
        "page": 1044,
        "text": "Pitfall\nIgnoring software overhead when determining performance\nLow software overhead requires cooperation with the operating system as well\nas with the communication libraries, but even with protocol offloading it con-\ntinues to dominate the hardware overhead and must not be ignored.\nFigures F.32 and F.41 give two examples, one for a SAN standard and the\nother for a WAN standard. Other examples come from proprietary SANs for\nsupercomputers. The Connection Machine CM-5 supercomputer in the early\n1990s had a software overhead of 20 \u03bcs to send a message and a hardware\noverhead of only 0.5 \u03bcs. The first Intel Paragon supercomputer built in the\nearly 1990s had a hardware overhead of just 0.2 \u03bcs, but the initial release of\nthe software had an overhead of 250 \u03bcs. Later releases reduced this overhead\ndown to 25 \u03bcs and, more recently, down to only a few microseconds, but this\nstill dominates the hardware overhead. The IBM Blue Gene/L has an MPI\nsending/receiving overhead of approximately 3 \u03bcs, only a third of which (at\nmost) is attributed to the hardware.\nThis pitfall is simply Amdahl\u2019s law applied to networks: Faster network\nhardware is superfluous if there is not a corresponding decrease in software\noverhead. The software overhead is much reduced these days with OS\nbypass, lightweight protocols, and protocol offloading down to a few micro-\nseconds or less, typically, but it remains a significant factor in determining\nperformance.\nFallacy\nMINs are more cost-effective than direct networks\nA MIN is usually implemented using significantly fewer switches than the number\nof devices that need to be connected. On the other hand, direct networks usually\ninclude a switch as an integral part of each node, thus requiring as many switches as\nnodes to interconnect. However, nothing prevents the implementation of nodes\nwith multiple computing devices on it (e.g., a multicore processor with an on-chip\nswitch) or with several devices attached to each switch (i.e., bristling). In these\ncases, a direct network may be as (or even more) cost-effective as a MIN. Note\nthat, for a MIN, several network interfaces may be required at each node to match\nthe bandwidth delivered by the multiple links per node provided by the direct\nnetwork.\nFallacy\nLow-dimensional\ndirect\nnetworks\nachieve\nhigher\nperformance\nthan\nhigh-dimensional networks such as hypercubes\nThis conclusion was drawn by several studies that analyzed the optimal number of\ndimensions under the main physical constraint of bisection bandwidth. However,\nmost of those studies did not consider link pipelining, considered only very short\nlinks, and/or did not consider switch architecture design constraints. The misplaced\nassumption that bisection bandwidth serves as the main limit did not help matters.\nNowadays, most researchers and designers believe that high-radix switches are\nmore cost-effective than low-radix switches, including some who concluded the\nopposite before.\nF-96\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1045,
        "text": "Fallacy\nWormhole\nswitching\nachieves\nbetter\nperformance\nthan\nother\nswitching\ntechniques\nWormhole switching delivers the same no-load latency as other pipelined switch-\ning techniques, like virtual cut-through switching. The introduction of wormhole\nswitches in the late 1980s coinciding with a dramatic increase in network band-\nwidth led many to believe that wormhole switching was the main reason for the\nperformance boost. Instead, most of the performance increase came from a drastic\nincrease in link bandwidth, which, in turn, was enabled by the ability of wormhole\nswitching to buffer packet fragments using on-chip buffers, instead of using the\nnode\u2019s main memory or some other off-chip source for that task. More recently,\nmuch larger on-chip buffers have become feasible, and virtual cutthrough achieved\nthe same no-load latency as wormhole while delivering much higher throughput.\nThis did not mean that wormhole switching was dead. It continues to be the switch-\ning technique of choice for applications in which only small buffers should be used\n(e.g., perhaps for on-chip networks).\nFallacy\nImplementing a few virtual channels always increases throughput by allowing\npackets to pass through blocked packets ahead\nIn general, implementing a few virtual channels in a wormhole switch is a\ngood idea because packets are likely to pass blocked packets ahead of them,\nthus reducing latency and significantly increasing throughput. However, the\nimprovements are not as dramatic for virtual cut-through switches. In virtual\ncut-through, buffers should be large enough to store several packets. As a con-\nsequence, each virtual channel may introduce HOL blocking, possibly degrad-\ning performance at high loads. Adding virtual channels increases cost, but it\nmay deliver little additional performance unless there are as many virtual chan-\nnels as switch ports and packets are mapped to virtual channels according to\ntheir destination (i.e., virtual output queueing). It is certainly the case that vir-\ntual channels can be useful in virtual cut-through networks to segregate differ-\nent traffic classes, which can be very beneficial. However, multiplexing the\npackets over a physical link on a flit-by-flit basis causes all the packets from\ndifferent virtual channels to get delayed. The average packet delay is signifi-\ncantly shorter if multiplexing takes place on a packet-by-packet basis, but in\nthis case packet size should be bounded to prevent any one packet from\nmonopolizing the majority of link bandwidth.\nFallacy\nAdaptive routing causes out-of-order packet delivery, thus introducing too much\noverhead needed to reorder packets at the destination device\nAdaptive routing allows packets to follow alternative paths through the network\ndepending on network traffic; therefore, adaptive routing usually introduces\noutof-order packet delivery. However, this does not necessarily imply that reorder-\ning packets at the destination device is going to introduce a large overhead, making\nadaptive routing not useful. For example, the most efficient adaptive routing algo-\nrithms to date support fully adaptive routing in some virtual channels but required\nF.11\nFallacies and Pitfalls\n\u25a0\nF-97"
    },
    {
        "page": 1046,
        "text": "deterministic routing to be implemented in some other virtual channels in order to\nprevent deadlocks (\u00e0 la the IBM Blue Gene/L). In this case, it is very easy to select\nbetween adaptive and deterministic routing for each individual packet. A single bit\nin the packet header can indicate to the switches whether all the virtual channels\ncan be used or only those implementing deterministic routing. This hardware sup-\nport can be used as indicated below to eliminate packet reordering overhead at the\ndestination.\nMost communication protocols for parallel computers and clusters implement\ntwo different protocols depending on message size. For short messages, an eager\nprotocol is used in which messages are directly transmitted, and the receiving\nnodes use some preallocated buffer to temporarily store the incoming message.\nOn the other hand, for long messages, a rendezvous protocol is used. In this case,\na control message is sent first, requesting the destination node to allocate a buffer\nlarge enough to store the entire message. The destination node confirms buffer\nallocation by returning an acknowledgment, and the sender can proceed with frag-\nmenting the message into bounded-size packets, transmitting them to the\ndestination.\nIf eager messages use only deterministic routing, it is obvious that they do\nnot introduce any reordering overhead at the destination. On the other hand,\npackets belonging to a long message can be transmitted using adaptive routing.\nAs every packet contains the sequence number within the message (or the off-\nset from the beginning of the message), the destination node can store every\nincoming packet directly in its correct location within the message buffer, thus\nincurring no overhead with respect to using deterministic routing. The only\nthing that differs is the completion condition. Instead of checking that the last\npacket in the message has arrived, it is now necessary to count the arrived\npackets, notifying the end of reception when the count equals the message size.\nTaking into account that long messages, even if not frequent, usually consume\nmost of the network bandwidth, it is clear that most packets can benefit from\nadaptive routing without introducing reordering overhead when using the pro-\ntocol described above.\nFallacy\nAdaptive routing by itself always improves network fault tolerance because it\nallows packets to follow alternative paths\nAdaptive routing by itself is not enough to tolerate link and/or switch failures.\nSome mechanism is required to detect failures and notify them, so that the routing\nlogic could exclude faulty paths and use the remaining ones. Moreover, while a\ngiven link or switch failure affects a certain number of paths when using determin-\nistic routing, many more source/destination pairs could be affected by the same\nfailure when using adaptive routing. As a consequence of this, some switches\nimplementing adaptive routing transition to deterministic routing in the presence\nof failures. In this case, failures are usually tolerated by sending messages through\nalternative paths from the source node. As an example, the Cray T3E implements\ndirection-order routing to tolerate a few failures. This fault-tolerant routing\ntechnique avoids cycles in the use of resources by crossing directions in order\nF-98\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1047,
        "text": "(e.g., X+, Y+, Z+, Z\u0005, Y\u0005, then X\u0005). At the same time, it provides an easy way to\nsend packets through nonminimal paths, if necessary, to avoid crossing faulty com-\nponents. For instance, a packet can be initially forwarded a few hops in the X+\ndirection even if it has to go in the X\u0005 direction at some point later.\nPitfall\nTrying to provide features only within the network versus end-to-end\nThe concern is that of providing at a lower level the features that can only be\naccomplished at the highest level, thus only partially satisfying the communication\ndemand. Saltzer, Reed, and Clark [1984] gave the end-to-end argument as follows:\nThe function in question can completely and correctly be specified only with\nthe knowledge and help of the application standing at the endpoints of the\ncommunication system. Therefore, providing that questioned function as a fea-\nture of the communication system itself is not possible. [page 278]\nTheir example of the pitfall was a network at MIT that used several gateways, each\nof which added a checksum from one gateway to the next. The programmers of the\napplication assumed that the checksum guaranteed accuracy, incorrectly believing\nthat the message was protected while stored in the memory of each gateway. One\ngateway developed a transient failure that swapped one pair of bytes per million\nbytes transferred. Over time, the source code of one operating system was repeat-\nedly passed through the gateway, thereby corrupting the code. The only solution\nwas to correct infected source files by comparing them to paper listings and repair-\ning code by hand! Had the checksums been calculated and checked by the appli-\ncation running on the end systems, safety would have been ensured.\nThere is a useful role for intermediate checks at the link level, however, pro-\nvided that end-to-end checking is available. End-to-end checking may show that\nsomething is broken between two nodes, but it doesn\u2019t point to where the problem\nis. Intermediate checks can discover the broken component.\nA second issue regards performance using intermediate checks. Although it is\nsufficient to retransmit the whole in case of failures from the end point, it can be\nmuch faster to retransmit a portion of the message at an intermediate point rather\nthan wait for a time-out and a full message retransmit at the end point.\nPitfall\nRelying on TCP/IP for all networks, regardless of latency, bandwidth, or software\nrequirements\nThe network designers on the first workstations decided it would be elegant to use a\nsingle protocol stack no matter where the destination of the message: Across a\nroom or across an ocean, the TCP/IP overhead must be paid. This might have been\na wise decision back then, especially given the unreliability of early Ethernet hard-\nware, but it sets a high software overhead barrier for commercial systems of today.\nSuch an obstacle lowers the enthusiasm for low-latency network interface hard-\nware and low-latency interconnection networks if the software is just going to\nwaste hundreds of microseconds when the message must travel only dozens of\nmeters or less. It also can use significant processor resources. One rough rule of\nF.11\nFallacies and Pitfalls\n\u25a0\nF-99"
    },
    {
        "page": 1048,
        "text": "thumb is that each Mbit/sec of TCP/IP bandwidth needs about 1 MHz of processor\nspeed, so a 1000-Mbit/sec link could saturate a processor with an 800- to 1000-\nMHz clock.\nThe flip side is that, from a software perspective, TCP/IP is the most desirable\ntarget since it is the most connected and, hence, provides the largest number of\nopportunities. The downside of using software optimized to a particular LAN or\nSAN is that it is limited. For example, communication from a Java program\ndepends on TCP/IP, so optimization for another protocol would require creation\nof glue software to interface Java to it.\nTCP/IP advocates point out that the protocol itself is theoretically not as bur-\ndensome as current implementations, but progress has been modest in commercial\nsystems. There are also TCP/IP offloading engines in the market, with the hope of\npreserving the universal software model while reducing processor utilization and\nmessage latency. If processors continue to improve much faster than network\nspeeds, or if multiple processors become ubiquitous, software TCP/IP may become\nless significant for processor utilization and message latency.\nF.12\nConcluding Remarks\nInterconnection network design is one of the most exciting areas of computer archi-\ntecture development today. With the advent of new multicore processor paradigms\nand advances in traditional multiprocessor/cluster systems and the Internet, many\nchallenges and opportunities exist for interconnect architecture innovation. These\napply to all levels of computer systems: communication between cores on a chip,\nbetween chips on a board, between boards in a system, and between computers in a\nmachine room, over a local area and across the globe. Irrespective of their domain\nof application, interconnection networks should transfer the maximum amount of\ninformation within the least amount of time for given cost and power constraints so\nas not to bottleneck the system. Topology, routing, arbitration, switching, and flow\ncontrol are among some of the key concepts in realizing such high-performance\ndesigns.\nThe design of interconnection networks is end-to-end: It includes injection\nlinks, reception links, and the interfaces at network end points as much as it\ndoes the topology, switches, and links within the network fabric. It is often\nthe case that the bandwidth and overhead at the end node interfaces are the\nbottleneck, yet many mistakenly think of the interconnection network to mean\nonly the network fabric. This is as bad as processor designers thinking of com-\nputer architecture to mean only the instruction set architecture or only the\nmicroarchitecture! End-to-end issues and understanding of the traffic charac-\nteristics make the design of interconnection networks challenging and very\nmuch relevant even today. For instance, the need for low end-to-end latency\nis driving the development of efficient network interfaces located closer to\nthe processor/memory controller. We may soon see most multicore processors\nused in multiprocessor systems implementing network interfaces on-chip,\nF-100\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1049,
        "text": "devoting some core(s) to execute communication tasks. This is already the case\nfor the IBM Blue Gene/L supercomputer, which uses one of its two cores on\neach processor chip for this purpose.\nNetworking has a long way to go from its humble shared-media beginnings. It\nis in \u201ccatch-up\u201d mode, with switched-media point-to-point networks only recently\ndisplacing traditional bus-based networks in many networking domains, including\non chip, I/O, and the local area. We are not near any performance plateaus, so we\nexpect rapid advancement of WANs, LANs, SANs, and especially OCNs in the\nnear future. Greater interconnection network performance is key to the\ninformation- and communication-centric vision of the future of our field, which,\nso far, has benefited many millions of people around the world in various ways.\nAs the quotes at the beginning of this appendix suggest, this revolution in two-\nway communication is at the heart of changes in the form of our human associa-\ntions and actions.\nAcknowledgments\nWe express our sincere thanks to the following persons who, in some way, have\ncontributed to the contents of the previous edition of the appendix: Lei Chai, Scott\nClark, Jos\u0001e Flich, Jose Manuel Garcia, Paco Gilabert, Rama Govindaraju, Manish\nGupta, Wai Hong Ho, Siao Jer, Steven Keckler, Dhabaleswar (D.K.) Panda, Fab-\nrizio Petrini, Steve Scott, Jeonghee Shin, Craig Stunkel, Sayantan Sur, Michael B.\nTaylor, and Bilal Zafar. We especially appreciate the new contributions of Jose\nFlich to this edition of the appendix.\nF.13\nHistorical Perspective and References\nThis appendix has taken the perspective that interconnection networks for very\ndifferent domains\u2014from on-chip networks within a processor chip to wide\narea networks connecting computers across the globe\u2014share many of the same\nconcerns. With this, interconnection network concepts are presented in a uni-\nfied way, irrespective of their application; however, their histories are vastly\ndifferent, as evidenced by the different solutions adopted to address similar\nproblems. The lack of significant interaction between research communities\nfrom the different domains certainly contributed to the diversity of implemen-\nted solutions. Highlighted below are relevant readings on each topic. In addi-\ntion, good general texts featuring WAN and LAN networking have been\nwritten by Davie, Peterson, and Clark [1999] and by Kurose and Ross\n[2001]. Good texts focused on SANs for multiprocessors and clusters have\nbeen written by Duato, Yalamanchili, and Ni [2003] and by Dally and\nTowles [2004]. An informative chapter devoted to dead-lock resolution in\ninterconnection networks was written by Pinkston [2004]. Finally, an edited\nwork by Jantsch and Tenhunen [2003] on OCNs for multicore processors\nand system-on-chips is also interesting reading.\nF.13\nHistorical Perspective and References\n\u25a0\nF-101"
    },
    {
        "page": 1050,
        "text": "Wide Area Networks\nWide area networks are the earliest of the data interconnection networks. The fore-\nrunner of the Internet is the ARPANET, which in 1969 connected computer sci-\nence departments across the United States that had research grants funded by the\nAdvanced Research Project Agency (ARPA), a U.S. government agency. It was\noriginally envisioned as using reliable communications at lower levels. Practical\nexperience with failures of the underlying technology led to the failure-tolerant\nTCP/IP, which is the basis for the Internet today. Vint Cerf and Robert Kahn\nare credited with developing the TCP/IP protocols in the mid-1970s, winning\nthe ACM Software Award in recognition of that achievement. Kahn [1972] is\nan early reference on the ideas of ARPANET. For those interested in learning more\nabout TPC/IP, Stevens [1994\u20131996] has written classic books on the topic.\nIn 1975, there were roughly 100 networks in the ARPANET; in 1983, only\n200. In 1995, the Internet encompassed 50,000 networks worldwide, about half\nof which were in the United States. That number is hard to calculate now, but\nthe number of IP hosts grew by a factor of 15 from 1995 to 2000, reaching 100\nmillion Internet hosts by the end of 2000. It has grown much faster since then. With\nmost service providers assigning dynamic IP addresses, many local area networks\nusing private IP addresses, and with most networks allowing wireless connections,\nthe total number of hosts in the Internet is nearly impossible to compute. In July\n2005, the Internet Systems Consortium (www.isc.org) estimated more than 350\nmillion Internet hosts, with an annual increase of about 25% projected. Although\nkey government networks made the Internet possible (i.e., ARPANET and\nNSFNET), these networks have been taken over by the commercial sector, allow-\ning the Internet to thrive. But major innovations to the Internet are still likely to\ncome from government-sponsored research projects rather than from the commer-\ncial sector. The National Science Foundation\u2019s Global Environment for Network\nInnovation (GENI) initiative is an example of this.\nThe most exciting application of the Internet is the World Wide Web, devel-\noped in 1989 by Tim Berners-Lee, a programmer at the European Center for Par-\nticle Research (CERN), for information access. In 1992, a young programmer at\nthe University of Illinois, Marc Andreessen, developed a graphical interface for the\nWeb called Mosaic. It became immensely popular. He later became a founder of\nNetscape, which popularized commercial browsers. In May 1995, at the time of the\nsecond edition of this book, there were over 30,000 Web pages, and the number\nwas doubling every two months. During the writing of the third edition of this text,\nthere were more than 1.3 billion Web pages. In December 2005, the number of\nWeb servers approached 75 million, having increased by 30% during that\nsame year.\nAsynchronous Transfer Mode (ATM) was an attempt to design the definitive\ncommunication standard. It provided good support for data transmission as well as\ndigital voice transmission (i.e., phone calls). From a technical point of view, it\ncombined the best from packet switching and circuit switching, also providing\nexcellent support for providing quality of service (QoS). Alles [1995] offers a good\nF-102\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1051,
        "text": "survey on ATM. In 1995, no one doubted that ATM was going to be the future for\nthis community. Ten years later, the high equipment and personnel training costs\nbasically killed ATM, and we returned back to the simplicity of TCP/IP. Another\nimportant blow to ATM was its defeat by the Ethernet family in the LAN domain,\nwhere packet switching achieved significantly lower latencies than ATM, which\nrequired establishing a connection before data transmission. ATM connectionless\nservers were later introduced in an attempt to fix this problem, but they were expen-\nsive and represented a central bottleneck in the LAN.\nFinally, WANs today rely on optical fiber. Fiber technology has made so\nmany advances that today WAN fiber bandwidth is often underutilized. The\nmain reason for this is the commercial introduction of wavelength division\nmultiplexing (WDM), which allows each fiber to transmit many data streams\nsimultaneously over different wavelengths, thus allowing three orders of mag-\nnitude bandwidth increase in just one generation, that is, 3 to 5 years (a good\ntext by Senior [1993] discusses optical fiber communications). However, IP\nrouters may still become a bottleneck. At 10- to 40-Gbps link rates, and with\nthousands of ports in large core IP routers, packets must be processed very\nquickly\u2014that is, within a few tens of nanoseconds. The most time-consuming\noperation is routing. The way IP addresses have been defined and assigned to\nInternet hosts makes routing very complicated, usually requiring a complex\nsearch in a tree structure for every packet. Network processors have become\npopular as a cost-effective solution for implementing routing and other\npacket-filtering operations. They usually are RISC-like and highly multi-\nthreaded and implement local stores instead of caches.\nLocal Area Networks\nARPA\u2019s success with wide area networks led directly to the most popular local area\nnetworks. Many researchers at Xerox Palo Alto Research Center had been funded\nby ARPA while working at universities, so they all knew the value of networking.\nIn 1974, this group invented the Alto, the forerunner of today\u2019s desktop computers\n[Thacker et al. 1982], and the Ethernet [Metcalfe and Boggs 1976], today\u2019s LAN.\nThis group\u2014David Boggs, Butler Lampson, Ed McCreight, Bob Sprowl, and\nChuck Thacker\u2014became luminaries in computer science and engineering, collect-\ning a treasure chest of awards among them.\nThis first Ethernet provided a 3-Mbit/sec interconnection, which seemed like\nan unlimited amount of communication bandwidth with computers of that era. It\nrelied on the interconnect technology developed for the cable television industry.\nSpecial microcode support gave a round-trip time of 50 \u03bcs for the Alto over Ether-\nnet, which is still a respectable latency. It was Boggs\u2019 experience as a ham radio\noperator that led to a design that did not need a central arbiter, but instead listened\nbefore use and then varied back-off times in case of conflicts.\nThe announcement by Digital Equipment Corporation, Intel, and Xerox of a\nstandard for 10-Mbit/sec Ethernet was critical to the commercial success of\nF.13\nHistorical Perspective and References\n\u25a0\nF-103"
    },
    {
        "page": 1052,
        "text": "Ethernet. This announcement short-circuited a lengthy IEEE standards effort,\nwhich eventually did publish IEEE 802.3 as a standard for Ethernet.\nThere have been several unsuccessful candidates that have tried to replace the\nEthernet. The Fiber Data Distribution Interconnect (FDDI) committee, unfortu-\nnately, took a very long time to agree on the standard, and the resulting interfaces\nwere expensive. It was also a shared medium when switches were becoming\naffordable. ATM also missed the opportunity in part because of the long time\nto standardize the LAN version of ATM, and in part because of the high latency\nand poor behavior of ATM connectionless servers, as mentioned above. Infini-\nBand for the reasons discussed below has also faltered. As a result, Ethernet con-\ntinues to be the absolute leader in the LAN environment, and it remains a strong\nopponent in the high-performance computing market as well, competing against\nthe SANs by delivering high bandwidth at low cost. The main drawback of Ether-\nnet for high-end systems is its relatively high latency and lack of support in most\ninterface cards to implement the necessary protocols.\nBecause of failures of the past, LAN modernization efforts have been centered\non extending Ethernet to lower-cost media such as unshielded twisted pair (UTP),\nswitched interconnects, and higher link speeds as well as to new domains such as\nwireless communication. Practically all new PC motherboards and laptops imple-\nment a Fast/Gigabit Ethernet port (100/1000 Mbps), and most laptops implement a\n54 Mbps Wireless Ethernet connection. Also, home wired or wireless LANs con-\nnecting all the home appliances, set-top boxes, desktops, and laptops to a shared\nInternet connection are very common. Spurgeon [2006] has provided a nice online\nsummary of Ethernet technology, including some of its history.\nSystem Area Networks\nOne of the first nonblocking multistage interconnection networks was proposed by\nClos [1953] for use in telephone exchange offices. Building on this, many early\ninventions for system area networks came from their use in massively parallel pro-\ncessors (MPPs). One of the first MPPs was the Illiac IV, a SIMD array built in the\nearly 1970s with 64 processing elements (\u201cmassive\u201d at that time) interconnected\nusing a topology based on a 2D torus that provided neighbor-to-neighbor commu-\nnication. Another representative of early MPP was the Cosmic Cube, which used\nEthernet interface chips to connect 64 processors in a 6-cube. Communication\nbetween nonneighboring nodes was made possible by store-and-forwarding of\npackets at intermediate nodes toward their final destination. A much larger and\ntruly \u201cmassive\u201d MPP built in the mid-1980s was the Connection Machine, a SIMD\nmultiprocessor consisting of 64 K 1-bit processing elements, which also used a\nhypercube with store-and-forwarding. Since these early MPP machines, intercon-\nnection networks have improved considerably.\nIn the 1970s through the 1990s, considerable research went into trying to opti-\nmize the topology and, later, the routing algorithm, switching, arbitration, and flow\ncontrol techniques. Initially, research focused on maximizing performance with\nF-104\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1053,
        "text": "little attention paid to implementation constraints or crosscutting issues. Many\nexotic topologies were proposed having very interesting properties, but most of\nthem complicated the routing. Rising from the fray was the hypercube, a very pop-\nular network in the 1980s that has all but disappeared from MPPs since the 1990s.\nWhat contributed to this shift was a performance model by Dally [1990] that\nshowed that if the implementation is wire limited, lower-dimensional topologies\nachieve better performance than higher-dimensional ones because of their wider\nlinks for a given wire budget. Many designers followed that trend assuming their\ndesigns to be wire limited, even though most implementations were (and still are)\npin limited. Several supercomputers since the 1990s have implemented low-\ndimensional topologies, including the Intel Paragon, Cray T3D, Cray T3E, HP\nAlphaServer, Intel ASCI Red, and IBM Blue Gene/L.\nMeanwhile, other designers followed a very different approach, implementing\nbidirectional MINs in order to reduce the number of required switches below the\nnumber of network nodes. The most popular bidirectional MIN was the fat tree\ntopology, originally proposed by Leiserson [1985] and first used in the Connection\nMachine CM-5 supercomputer and, later, the IBM ASCI White and ASC Purple\nsupercomputers. This indirect topology was also used in several European parallel\ncomputers based on the Transputer. The Quadrics network has inherited character-\nistics from some of those Transputer-based networks. Myrinet has also evolved\nsignificantly from its first version, with Myrinet 2000 incorporating the fat tree\nas its principal topology. Indeed, most current implementations of SANs, including\nMyrinet, InfiniBand, and Quadrics as well as future implementations such as PCI-\nExpress Advanced Switching, are based on fat trees.\nAlthough the topology is the most visible aspect of a network, other features\nalso have a significant impact on performance. A seminal work that raised aware-\nness of deadlock properties in computer systems was published by Holt [1972].\nEarly techniques for avoiding deadlock in store-and-forward networks were pro-\nposed by Merlin and Schweitzer [1980] and by Gunther [1981]. Pipelined switch-\ning techniques were first introduced by Kermani and Kleinrock [1979] (virtual cut-\nthrough) and improved upon by Dally and Seitz [1986] (wormhole), which signif-\nicantly reduced low-load latency and the topology\u2019s impact on message latency\nover previously proposed techniques. Wormhole switching was initially better\nthan virtual cut-through largely because flow control could be implemented at a\ngranularity smaller than a packet, allowing high-bandwidth links that were not\nas constrained by available switch memory bandwidth. Today, virtual cut-through\nis usually preferred over wormhole because it achieves higher throughput due to\nless HOL blocking effects and is enabled by current integration technology that\nallows the implementation of many packet buffers per link.\nTamir and Frazier [1992] laid the groundwork for virtual output queuing with\nthe notion of dynamically allocated multiqueues. Around this same time, Dally\n[1992] contributed the concept of virtual channels, which was key to the develop-\nment of more efficient deadlock-free routing algorithms and congestion-reducing\nflow control techniques for improved network throughput. Another highly relevant\ncontribution to routing was a new theory proposed by Duato [1993] that allowed\nF.13\nHistorical Perspective and References\n\u25a0\nF-105"
    },
    {
        "page": 1054,
        "text": "the implementation of fully adaptive routing with just one \u201cescape\u201d virtual channel\nto avoid deadlock. Previous to this, the required number of virtual channels to\navoid deadlock increased exponentially with the number of network dimensions.\nPinkston and Warnakulasuriya [1997] went on to show that deadlock actually can\noccur very infrequently, giving credence to deadlock recovery routing approaches.\nScott and Goodman [1994] were among the first to analyze the usefulness of pipe-\nlined channels for making link bandwidth independent of the time of flight. These\nand many other innovations have become quite popular, finding use in most high-\nperformance interconnection networks, both past and present. The IBM Blue\nGene/L, for example, implements virtual cut-through switching, four virtual chan-\nnels per link, fully adaptive routing with one escape channel, and pipelined links.\nMPPs represent a very small (and currently shrinking) fraction of the informa-\ntion technology market, giving way to bladed servers and clusters. In the United\nStates, government programs such as the Advanced Simulation and Computing\n(ASC) program (formerly known as the Accelerated Strategic Computing Initia-\ntive, or ASCI) have promoted the design of those machines, resulting in a series\nof increasingly powerful one-of-a-kind MPPs costing $50 million to $100 million.\nThese days, many are basically lower-cost clusters of symmetric multiprocessors\n(SMPs) (see Pfister [1998] and Sterling [2001] for two perspectives on clustering).\nIn fact, in 2005, nearly 75% of the TOP500 supercomputers were clusters. Nev-\nertheless, the design of each generation of MPPs and even clusters pushes inter-\nconnection network research forward to confront new problems arising due to\nshear size and other scaling factors. For instance, source-based routing\u2014the sim-\nplest form of routing\u2014does not scale well to large systems. Likewise, fat trees\nrequire increasingly longer links as the network size increases, which led IBM Blue\nGene/L designers to adopt a 3D torus network with distributed routing that can be\nimplemented with bounded-length links.\nStorage Area Networks\nSystem area networks were originally designed for a single room or single floor\n(thus their distances are tens to hundreds of meters) and were for use in MPPs\nand clusters. In the intervening years, the acronym SAN has been co-opted to also\nmean storage area networks, whereby networking technology is used to connect\nstorage devices to compute servers. Today, many refer to \u201cstorage\u201d when they\nsay SAN. The most widely used SAN example in 2006 was Fibre Channel\n(FC), which comes in many varieties, including various versions of Fibre Channel\nArbitrated Loop (FC-AL) and Fibre Channel Switched (FC-SW). Not only are disk\narrays attached to servers via FC links, but there are even some disks with FC links\nattached to switches so that storage area networks can enjoy the benefits of greater\nbandwidth and interconnectivity of switching.\nIn October 2000, the InfiniBand Trade Association announced the version 1.0\nspecification of InfiniBand [InfiniBand Trade Association 2001]. Led by Intel, HP,\nIBM, Sun, and other companies, it was targeted to the high-performance\nF-106\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1055,
        "text": "computing market as a successor to the PCI bus by having point-to-point links and\nswitches with its own set of protocols. Its characteristics are desirable potentially\nboth for system area networks to connect clusters and for storage area networks to\nconnect disk arrays to servers. Consequently, it has had strong competition from\nboth fronts. On the storage area networking side, the chief competition for Infini-\nBand has been the rapidly improving Ethernet technology widely used in LANs.\nThe Internet Engineering Task Force proposed a standard called iSCSI to send\nSCSI commands over IP networks [Satran et al. 2001]. Given the cost advantages\nof the higher-volume Ethernet switches and interface cards, Gigabit Ethernet dom-\ninates the low-end and medium range for this market. What\u2019s more, the slow intro-\nduction of InfiniBand and its small market share delayed the development of chip\nsets incorporating native support for InfiniBand. Therefore, network interface\ncards had to be plugged into the PCI or PCI-X bus, thus never delivering on the\npromise of replacing the PCI bus.\nIt was another I/O standard, PCI-Express, that finally replaced the PCI bus.\nLike InfiniBand, PCI-Express implements a switched network but with point-\nto-point serial links. To its credit, it maintains software compatibility with the\nPCI bus, drastically simplifying migration to the new I/O interface. Moreover,\nPCI-Express benefited significantly from mass market production and has found\napplication in the desktop market for connecting one or more high-end graphics\ncards, making gamers very happy. Every PC motherboard now implements one\nor more 16x PCI-Express interfaces. PCI-Express absolutely dominates the I/O\ninterface, but the current standard does not provide support for interprocessor\ncommunication.\nYet another standard, Advanced Switching Interconnect (ASI), may emerge as\na complementary technology to PCI-Express. ASI is compatible with PCI-Express,\nthus linking directly to current motherboards, but it also implements support for\ninterprocessor communication as well as I/O. Its defenders believe that it will even-\ntually replace both SANs and LANs with a unified network in the data center mar-\nket, but ironically this was also said of InfiniBand. The interested reader is referred\nto Pinkston et al. [2003] for a detailed discussion on this. There is also a new disk\ninterface standard called Serial Advanced Technology Attachment (SATA) that is\nreplacing parallel Integrated Device Electronics (IDE) with serial signaling tech-\nnology to allow for increased bandwidth. Most disks in the market use this new\ninterface, but keep in mind that Fibre Channel is still alive and well. Indeed, most\nof the promises made by InfiniBand in the SAN market were satisfied by Fibre\nChannel first, thus increasing their share of the market.\nSome believe that Ethernet, PCI-Express, and SATA have the edge in the\nLAN, I/O interface, and disk interface areas, respectively. But the fate of the\nremaining storage area networking contenders depends on many factors. A won-\nderful characteristic of computer architecture is that such issues will not remain\nendless academic debates, unresolved as people rehash the same arguments repeat-\nedly. Instead, the battle is fought in the marketplace, with well-funded and talented\ngroups giving their best efforts at shaping the future. Moreover, constant changes\nto technology reward those who are either astute or lucky. The best combination of\nF.13\nHistorical Perspective and References\n\u25a0\nF-107"
    },
    {
        "page": 1056,
        "text": "technology and follow-through has often determined commercial success. Time\nwill tell us who will win and who will lose, at least for the next round!\nOn-Chip Networks\nRelative to the other network domains, on-chip networks are in their infancy. As\nrecently as the late 1990s, the traditional way of interconnecting devices such as\ncaches, register files, ALUs, and other functional units within a chip was to use\ndedicated links aimed at minimizing latency or shared buses aimed at simplicity.\nBut with subsequent increases in the volume of interconnected devices on a single\nchip, the length and delay of wires to cross a chip, and chip power consumption, it\nhas become important to share on-chip interconnect bandwidth in a more struc-\ntured way, giving rise to the notion of a network on-chip. Among the first to rec-\nognize this were Agarwal [Waingold et al. 1997] and Dally [Dally 1999; Dally and\nTowles 2001]. They and others argued that on-chip networks that route packets\nallow efficient sharing of burgeoning wire resources between many communica-\ntion flows and also facilitate modularity to mitigate chip-crossing wire delay prob-\nlems identified by Ho, Mai, and Horowitz [2001]. Switched on-chip networks were\nalso viewed as providing better fault isolation and tolerance. Challenges in\ndesigning these networks were later described by Taylor et al. [2005], who also\nproposed a 5-tuple model for characterizing the delay of OCNs. A design process\nfor OCNs that provides a complete synthesis flow was proposed by Bertozzi et al.\n[2005]. Following these early works, much research and development has gone\ninto on-chip network design, making this a very hot area of microarchitecture\nactivity.\nMulticore and tiled designs featuring on-chip networks have become very pop-\nular since the turn of the millennium. Pinkston and Shin [2005] provide a survey of\non-chip networks used in early multicore/tiled systems. Most designs exploit the\nreduced wiring complexity of switched OCNs as the paths between cores/tiles can\nbe precisely defined and optimized early in the design process, thus enabling\nimproved power and performance characteristics. With typically tens of thousands\nof wires attached to the four edges of a core or tile as \u201cpinouts,\u201d wire resources can\nbe traded off for improved network performance by having very wide channels\nover which data can be sent broadside (and possibly scaled up or down according\nto the power management technique), as opposed to serializing the data over fixed\nnarrow channels.\nRings, meshes, and crossbars are straightforward to implement in planar chip\ntechnology and routing is easily defined on them, so these were popular topolog-\nical choices in early switched OCNs. It will be interesting to see if this trend con-\ntinues in the future when several tens to hundreds of heterogeneous cores and tiles\nwill likely be interconnected within a single chip, possibly using 3D integration\ntechnology. Considering that processor microarchitecture has evolved signifi-\ncantly from its early beginnings in response to application demands and technolog-\nical advancements, we would expect to see vast architectural improvements to on-\nchip networks as well.\nF-108\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1057,
        "text": "References\nAgarwal, A., 1991. Limits on interconnection network performance. IEEE Trans. on Parallel and Dis-\ntributed Systems 2 (4 (April)), 398\u2013412.\nAlles, A., 1995. \u201cATM internetworking\u201d (May). www.cisco.com/warp/public/614/12.html.\nAnderson, T.E., Culler, D.E., Patterson, D., 1995. A case for NOW (networks of workstations). IEEE\nMicro 15 (1 (February)), 54\u201364.\nAnjan, K.V., Pinkston, T.M., 1995. An efficient, fully-adaptive deadlock recovery scheme: Disha.\nIn: Proc. 22nd Annual Int\u2019l. Symposium on Computer Architecture, June 22\u201324, 1995. Santa Mar-\ngherita Ligure, Italy.\nArpaci, R.H., Culler, D.E., Krishnamurthy, A., Steinberg, S.G., Yelick, K., 1995. Empirical evaluation\nof the Cray-T3D: A compiler perspective. In: Proc. 22nd Annual Int\u2019l. Symposium on Computer\nArchitecture, June 22\u201324, 1995. Santa Margherita Ligure, Italy.\nBell, G., Gray, J., 2001. Crays, Clusters and Centers. Microsoft Corporation, Redmond, Wash. MSR-\nTR-2001-76.\nBenes, V.E., 1962. Rearrangeable three stage connecting networks. Bell Syst. Tech. J. 41, 1481\u20131492.\nBertozzi, D., Jalabert, A., Murali, S., Tamhankar, R., Stergiou, S., Benini, L., De Micheli, G., 2005.\nNoC synthesis flow for customized domain specific multiprocessor systems-on-chip. IEEE Trans.\non Parallel and Distributed Systems 16 (2 (February)), 113\u2013130.\nBhuyan, L.N., Agrawal, D.P., 1984. Generalized hypercube and hyperbus structures for a computer\nnetwork. IEEE Trans. on Computers 32 (4 (April)), 322\u2013333.\nBrewer, E.A., Kuszmaul, B.C., 1994. How to get good performance from the CM-5 data network.\nIn: Proc. Eighth Int\u2019l Parallel Processing Symposium, April 26\u201329, 1994. Cancun, Mexico.\nClos, C., 1953. A study of non-blocking switching networks. Bell Systems Technical Journal\n32 (March), 406\u2013424.\nDally, W.J., 1990. Performance analysis of k-ary n-cube interconnection networks. IEEE Trans. on\nComputers 39 (6 (June)), 775\u2013785.\nDally, W.J., 1992. Virtual channel flow control. IEEE Trans. on Parallel and Distributed Systems 3 (2\n(March)), 194\u2013205.\nDally, W.J., 1999. Interconnect limited VLSI architecture. In: Proc. of the Int\u2019l. Interconnect Technol-\nogy Conference, May 24\u201326, 1999. San Francisco, Calif.\nDally, W.J., Seitz, C.I., 1986. The torus routing chip. Distributed Computing 1 (4), 187\u2013196.\nDally, W.J., Towles, B., 2001. Route packets, not wires: On-chip interconnection networks. In: Proc. of\nthe 38th Design Automation Conference, June 18\u201322, 2001. Las Vegas, Nev.\nDally, W.J., Towles, B., 2004. Principles and Practices of Interconnection Networks. Morgan Kauf-\nmann Publishers, San Francisco.\nDavie, B.S., Peterson, L.L., Clark, D., 1999. Computer Networks: A Systems Approach, second ed.\nMorgan Kaufmann Publishers, San Francisco.\nDuato, J., 1993. A new theory of deadlock-free adaptive routing in wormhole networks. IEEE Trans. on\nParallel and Distributed Systems 4 (12 (December)), 1320\u20131331.\nDuato, J., Pinkston, T.M., 2001. A general theory for deadlock-free adaptive routing using a mixed set of\nresources. IEEE Trans. on Parallel and Distributed Systems 12 (12 (December)), 1219\u20131235.\nDuato, J., Yalamanchili, S., Ni, L., 2003. Interconnection Networks: An Engineering Approach. Mor-\ngan Kaufmann Publishers, San Francisco. 2nd printing.\nDuato, J., Johnson, I., Flich, J., Naven, F., Garcia, P., Nachiondo, T., 2005a. A new scalable and cost-\neffective congestion management strategy for lossless multistage interconnection networks.\nIn: Proc. 11th Int\u2019l. Symposium on High Performance Computer Architecture, February 12\u201316,\n2005 San Francisco.\nDuato, J., Lysne, O., Pang, R., Pinkston, T.M., 2005b. Part I: A theory for deadlock-free dynamic recon-\nfiguration of interconnection networks. IEEE Trans. on Parallel and Distributed Systems 16 (5\n(May)), 412\u2013427.\nFlich, J., Bertozzi, D., 2010. Designing Network-on-Chip Architectures in the Nanoscale Era. CRC\nPress, Boca Raton, FL.\nGlass, C.J., Ni, L.M., 1992. The Turn Model for adaptive routing. In: Proc. 19th Int\u2019l. Symposium on\nComputer Architecture. May, Gold Coast, Australia.\nGunther, K.D., 1981. Prevention of deadlocks in packet-switched data transport systems. IEEE Trans.\non Communications, 512\u2013524. COM\u201329:4 (April).\nHo, R., Mai, K.W., Horowitz, M.A., 2001. The future of wires. In: Proc. of the IEEE 89:4 (April),\npp. 490\u2013504.\nHolt, R.C., 1972. Some deadlock properties of computer systems. ACM Computer Surveys\n4 (3 (September)), 179\u2013196.\nF.13\nHistorical Perspective and References\n\u25a0\nF-109"
    },
    {
        "page": 1058,
        "text": "Hoskote, Y., Vangal, S., Singh, A., Borkar, N., Borkar, S., 2007. A 5-ghz mesh interconnect for a tera-\nflops processor. IEEE Micro 27 (5), 51\u201361.\nHoward, J., Dighe, S., Hoskote, Y., Vangal, S., Finan, S., Ruhl, G., Jenkins, D., Wilson, H., Borka, N.,\nSchrom, G., Pailet, F., Jain, S., Jacob, T., Yada, S., Marella, S., Salihundam, P., Erraguntla, V.,\nKonow, M., Riepen, M., Droege, G., Lindemann, J., Gries, M., Apel, T., Henriss, K., Lund-\nLarsen, T., Steibl, S., Borkar, S., De, V., Van Der Wijngaart, R., Mattson, T., 2010. A 48-core\nIA-32 message-passing processor with DVFS in 45 nm CMOS. In: IEEE International Solid-State\nCircuits Conference Digest of Technical Papers, pp. 58\u201359.\nInfiniBand Trade Association, 2001. InfiniBand Architecture Specifications Release 1.0.a. www.\ninfinibandta.org.\nJantsch, A., Tenhunen, H. (Eds.), 2003. Networks on Chips. Kluwer Academic Publishers, The\nNetherlands.\nKahn, R.E., 1972. Resource-sharing computer communication networks. In: Proc. IEEE 60:11 (Novem-\nber), pp. 1397\u20131407.\nKermani, P., Kleinrock, L., 1979. Virtual cut-through: A new computer communication switching tech-\nnique. Computer Networks 3 (January), 267\u2013286.\nKurose, J.F., Ross, K.W., 2001. Computer Networking: A Top-Down Approach Featuring the Internet.\nAddison-Wesley, Boston.\nLeiserson, C.E., 1985. Fat trees: Universal networks for hardware-efficient supercomputing. IEEE\nTrans. on Computers, 892\u2013901. C\u201334:10 (October).\nMerlin, P.M., Schweitzer, P.J., 1980. Deadlock avoidance in store-and-forward networks. I. Store-and-\nforward deadlock. IEEE Trans. on Communications, 345\u2013354. COM\u201328:3 (March).\nMetcalfe, R.M., 1993. Computer/network interface design: Lessons from Arpanet and Ethernet. IEEE J.\non Selected Areas in Communications 11 (2 (February)), 173\u2013180.\nMetcalfe, R.M., Boggs, D.R., 1976. Ethernet: Distributed packet switching for local computer networks.\nComm. ACM 19 (7 (July)), 395\u2013404.\nPartridge, C., 1994. Gigabit Networking. Addison-Wesley, Reading, Mass.\nPeh, L.S., Dally, W.J., 2001. A delay model and speculative architecture for pipelined routers. In: Proc.\n7th Int\u2019l. Symposium on High Performance Computer Architecture, January 20\u201324, 2001. Monter-\nrey, Mexico.\nPfister, G.F., 1998. In Search of Clusters, second ed. Prentice Hall, Upper Saddle River, N.J.\nPinkston, T.M., 2004. Deadlock characterization and resolution in interconnection networks.\nIn: Zhu, M.C., Fanti, M.P. (Eds.), Deadlock Resolution in Computer-Integrated Systems. CRC\nPress, Boca Raton, Fl, pp. 445\u2013492.\nPinkston, T.M., Shin, J., 2005. Trends toward on-chip networked microsystems. Int\u2019l. J. of High Per-\nformance Computing and Networking 3 (1), 3\u201318.\nPinkston, T.M., Warnakulasuriya, S., 1997. On deadlocks in interconnection networks. In: Proc. 24th\nInt\u2019l. Symposium on Computer Architecture, June 2\u20134, 1997. Denver, Colo.\nPinkston, T.M., Benner, A., Krause, M., Robinson, I., Sterling, T., 2003. InfiniBand: The \u2018de facto\u2019\nfuture standard for system and local area networks or just a scalable replacement for PCI buses?\u201d\nSpecial Issue on Communication Architecture for Clusters 6:2 (April). Cluster Computing, 95\u2013104.\nPuente, V., Beivide, R., Gregorio, J.A., Prellezo, J.M., Duato, J., Izu, C., 1999. Adaptive bubble router:\nA design to improve performance in torus networks. In: Proc. 28th Int\u2019l. Conference on Parallel\nProcessing, September 21\u201324, 1999. Aizu-Wakamatsu, Japan.\nRodrigo, S., Flich, J., Duato, J., Hummel, M., 2008. Efficient unicast and multicast support for CMPs.\nIn: Proc. 41st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-41),\nNovember 8\u201312, 2008. Lake Como, Italy, pp. 364\u2013375.\nSaltzer, J.H., Reed, D.P., Clark, D.D., 1984. End-to-end arguments in system design. ACM Trans. on\nComputer Systems 2 (4 (November)), 277\u2013288.\nSatran, J., Smith, D., Meth, K., Sapuntzakis, C., Wakeley, M., Von Stamwitz, P., Haagens, R.,\nZeidner, E., Dalle Ore, L., Klein, Y., 2001. \u201ciSCSI\u201d, IPS working group of IETF, Internet draft.\nwww.ietf.org/internet-drafts/draft-ietf-ips-iscsi-07.txt.\nScott, S.L., Goodman, J., 1994. The impact of pipelined channels on k-ary n-cube networks. IEEE\nTrans. on Parallel and Distributed Systems 5 (1 (January)), 1\u201316.\nSenior, J.M., 1993. Optical Fiber Commmunications: Principles and Practice, second ed. Prentice Hall,\nHertfordshire, U.K..\nSpurgeon, C., 2006. Charles Spurgeon\u2019s Ethernet Web Site. www.etherman-age.com/ethernet/ethernet.\nhtml.\nF-110\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1059,
        "text": "Sterling, T., 2001. Beowulf PC Cluster Computing with Windows and Beowulf PC Cluster Computing\nwith Linux. MIT Press, Cambridge, Mass.\nStevens, W.R., 1994\u20131996. TCP/IP Illustrated (three volumes). Addison-Wesley, Reading, Mass.\nTamir, Y., Frazier, G., 1992. Dynamically-allocated multi-queue buffers for VLSI communication\nswitches. IEEE Trans. on Computers 41 (6 (June)), 725\u2013734.\nTanenbaum, A.S., 1988. Computer Networks, second ed. Prentice Hall, Englewood Cliffs, N.J.\nTaylor, M.B., Lee, W., Amarasinghe, S.P., Agarwal, A., 2005. Scalar operand networks. IEEE Trans. on\nParallel and Distributed Systems 16 (2 (February)), 145\u2013162.\nThacker, C.P., McCreight, E.M., Lampson, B.W., Sproull, R.F., Boggs, D.R., 1982. Alto: A personal\ncomputer. In: Siewiorek, D.P., Bell, C.G., Newell, A. (Eds.), Computer Structures: Principles and\nExamples. McGraw-Hill, New York, pp. 549\u2013572.\nTILE-GX, http://www.tilera.com/sites/default/files/productbriefs/PB025_TILE-Gx_Processor_A_v3.\npdf.\nVaidya, A.S., Sivasubramaniam, A., Das, C.R., 1997. Performance benefits of virtual channels and\nadaptive routing: An application-driven study. In: Proc. 11th ACM Int\u2019l Conference on Supercom-\nputing, July 7\u201311, 1997. Vienna, Austria.\nVan Leeuwen, J., Tan, R.B., 1987. Interval Routing. The Computer Journal 30 (4), 298\u2013307.\nvon Eicken, T., Culler, D.E., Goldstein, S.C., Schauser, K.E., 1992. Active messages: A mechanism for\nintegrated communication and computation. In: Proc. 19th Annual Int\u2019l. Symposium on Computer\nArchitecture, May 19\u201321, 1992. Gold Coast, Australia.\nWaingold, E., Taylor, M., Srikrishna, D., Sarkar, V., Lee, W., Lee, V., Kim, J., Frank, M., Finch, P.,\nBarua, R., Babb, J., Amarasinghe, S., Agarwal, A., 1997. Baring it all to software: Raw Machines.\nIEEE Computer 30 (September), 86\u201393.\nYang, Y., Mason, G., 1991. Nonblocking broadcast switching networks. IEEE Trans. on Computers\n40 (9 (September)), 1005\u20131015.\nExercises\nSolutions to \u201cstarred\u201d exercises are available for instructors who register at text-\nbooks.elsevier.com.\n\u272aF.1\n[15]<F.2, F.3>Is electronic communication always faster than nonelectronic\nmeans for longer distances? Calculate the time to send 1000 GB using 25 8-mm\ntapes and an overnight delivery service versus sending 1000 GB by FTP over\nthe Internet. Make the following four assumptions:\n\u25a0\nThe tapes are picked up at 4 P.M. Pacific time and delivered 4200 km away at\n10 A.M. Eastern time (7 A.M. Pacific time).\n\u25a0\nOn one route the slowest link is a T3 line, which transfers at 45 Mbits/sec.\n\u25a0\nOn another route the slowest link is a 100-Mbit/sec Ethernet.\n\u25a0\nYou can use 50% of the slowest link between the two sites.\nWill all the bytes sent by either Internet route arrive before the overnight delivery\nperson arrives?\n\u272aF.2\n[10]<F.2, F.3>For the same assumptions as Exercise F.1, what is the bandwidth\nof overnight delivery for a 1000-GB package?\n\u272aF.3\n[10]<F.2, F.3>For the same assumptions as Exercise F.1, what is the minimum\nbandwidth of the slowest link to beat overnight delivery? What standard network\noptions match that speed?\nExercises\n\u25a0\nF-111"
    },
    {
        "page": 1060,
        "text": "\u272aF.4\n[15]<F.2, F.3>The original Ethernet standard was for 10 Mbits/sec and a max-\nimum distance of 2.5 km. How many bytes could be in flight in the original Ether-\nnet? Assume you can use 90% of the peak bandwidth.\n\u272aF.5\n[15]<F.2, F.3>Flow control is a problem for WANs due to the long time of\nflight, as the example on page F-14 illustrates. Ethernet did not include flow\ncontrol when it was first standardized at 10 Mbits/sec. Calculate the number of\nbytes in flight for a 10-Gbit/sec Ethernet over a 100 meter link, assuming you\ncan use 90% of peak bandwidth. What does your answer mean for network\ndesigners?\n\u272aF.6\n[15]<F.2, F.3>Assume the total overhead to send a zero-length data packet on an\nEthernet is 100 \u03bcs and that an unloaded network can transmit at 90% of the peak\n1000-Mbit/sec rating. For the purposes of this question, assume that the size of the\nEthernet header and trailer is 56 bytes. Assume a continuous stream of packets of\nthe same size. Plot the delivered bandwidth of user data in Mbits/sec as the payload\ndata size varies from 32 bytes to the maximum size of 1500 bytes in 32-byte\nincrements.\n\u272aF.7\n[10]<F.2, F.3>Exercise F.6 suggests that the delivered Ethernet bandwidth to a\nsingle user may be disappointing. Making the same assumptions as in that exercise,\nby how much would the maximum payload size have to be increased to deliver half\nof the peak bandwidth?\n\u272aF.8\n[10]<F.2, F.3>One reason that ATM has a fixed transfer size is that when a\nshort message is behind a long message, a node may need to wait for an entire\ntransfer to complete. For applications that are time sensitive, such as when\ntransmitting voice or video, the large transfer size may result in transmission\ndelays that are too long for the application. On an unloaded interconnection,\nwhat is the worstcase delay in microseconds if a node must wait for one\nfull-size Ethernet packet versus an ATM transfer? See Figure F.30 (page F-\n78) to find the packet sizes. For this question assume that you can transmit\nat 100% of the 622-Mbits/sec ATM network and 100% of the 1000-Mbit/\nsec Ethernet.\n\u272aF.9\n[10]<F.2, F.3>Exercise F.7 suggests the need for expanding the maximum\npay-load to increase the delivered bandwidth, but Exercise F.8 suggests the\nimpact on worst-case latency of making it longer. What would be the impact\non latency of increasing the maximum payload size by the answer to Exercise\nF.7?\n\u272aF.10\n[12/12/20]<F.4>The Omega network shown in Figure F.11 on page F-31 con-\nsists of three columns of four switches, each with two inputs and two outputs. Each\nswitch can be set to straight, which connects the upper switch input to the upper\nswitch output and the lower input to the lower output, and to exchange, which con-\nnects the upper input to the lower output and vice versa for the lower input. For\neach column of switches, label the inputs and outputs 0, 1, \u2026, 7 from top to bottom,\nto correspond with the numbering of the processors.\nF-112\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1061,
        "text": "a. [12]<F.4>When a switch is set to exchange and a message passes through,\nwhat is the relationship between the label values for the switch input and output\nused by the message? (Hint: Think in terms of operations on the digits of the\nbinary representation of the label number.)\nb. [12]<F.4>Between any two switches in adjacent columns that are connected\nby a link, what is the relationship between the label of the output connected to\nthe input?\nc. [20]<F.4>Based on your results in parts (a) and (b), design and describe a\nsimple routing scheme for distributed control of the Omega network. A message\nwill carry a routing tag computed by the sending processor. Describe how the\nprocessor computes the tag and how each switch can set itself by examining a bit\nof the routing tag.\n\u272aF.11\n[12/12/12/12/12/12]<F.4>Prove whether or not it is possible to realize the fol-\nlowing permutations (i.e., communication patterns) on the eight-node Omega net-\nwork shown in Figure F.11 on page F-31:\na. [12]<F.4>Bit-reversal permutation\u2014the node with binary coordinates an\u00051,\nan\u00052, \u2026, a1, a0 communicates with the node a0, a1, \u2026, an\u00052, an\u00051.\nb. [12]<F.4>Perfect shuffle permutation\u2014the node with binary coordinates\nan\u00051, an\u00052, \u2026, a1, a0 communicates with the node an\u00052, an\u00053, \u2026, a0, an\u00051\n(i.e., rotate left 1 bit).\nc. [12]<F.4>Bit-complement permutation\u2014the node with binary coordinates\nan\u00051, an\u00052, \u2026, a1, a0 communicates with the node an\u00051, an\u00052,\u2026, a1, a0\n(i.e., complement each bit).\nd. [12]<F.4>Butterfly permutation\u2014the node with binary coordinates an\u00051,\nan\u00052, \u2026, a1, a0 communicates with the node a0, an\u00052, \u2026, a1, an\u00051 (i.e., swap\nthe most and least significant bits).\ne. [12]<F.4>Matrix transpose permutation\u2014the node with binary coordinates\nan\u00051, an\u00052, \u2026, a1, a0 communicates with the node an/2\u00051, \u2026, a0, an\u00051, \u2026,\nan/2 (i.e., transpose the bits in positions approximately halfway around).\nf. [12]<F.4>Barrel-shift permutation\u2014node i communicates with node i+1\nmodulo N\u00051, where N is the total number of nodes and 0\u0001i.\n\u272aF.12\n[12]<F.4>Design a network topology using 18-port crossbar switches that has\nthe minimum number of switches to connect 64 nodes. Each switch port supports\ncommunication to and from one device.\n\u272aF.13\n[15]<F.4>Design a network topology that has the minimum latency through the\nswitches for 64 nodes using 18-port crossbar switches. Assume unit delay in the\nswitches and zero delay for wires.\n\u272aF.14\n[15]<F.4>Design a switch topology that balances the bandwidth required for all\nlinks for 64 nodes using 18-port crossbar switches. Assume a uniform traffic\npattern.\nExercises\n\u25a0\nF-113"
    },
    {
        "page": 1062,
        "text": "\u272aF.15\n[15]<F.4>Compare the interconnection latency of a crossbar, Omega network,\nand fat tree with eight nodes. Use Figure F.11 on page F-31, Figure F.12 on page F-\n33, and Figure F.14 on page F-37. Assume that the fat tree is built entirely from\ntwo-input, two-output switches so that its hardware resources are more comparable\nto that of the Omega network. Assume that each switch costs a unit time delay.\nAssume that the fat tree randomly picks a path, so give the best case and worst\ncase for each example. How long will it take to send a message from node 0 to\nnode 6? How long will it take node 1 and node 7 to communicate?\n\u272aF.16\n[15]<F.4>Draw the topology of a 6-cube after the same manner of the 4-cube in\nFigure F.14 on page F-37. What is the maximum and average number of hops\nneeded by packets assuming a uniform distribution of packet destinations?\n\u272aF.17\n[15]<F.4>Complete a table similar to Figure F.15 on page F-40 that captures the\nperformance and cost of various network topologies, but do it for the general case\nof N nodes using k\u0003k switches instead of the specific case of 64 nodes.\n\u272aF.18\n[20]<F.4>Repeat the example given on page F-41, but use the bit-complement\ncommunication pattern given in Exercise F.11 instead of NEWS communication.\n\u272aF.19\n[15]<F.5>Give the four specific conditions necessary for deadlock to exist in an\ninterconnection network. Which of these are removed by dimension-order routing?\nWhich of these are removed in adaptive routing with the use of \u201cescape\u201d routing\npaths? Which of these are removed in adaptive routing with the technique of dead-\nlock recovery (regressive or progressive)? Explain your answer.\n\u272aF.20\n[12/12/12/12]<F.5>Prove whether or not the following routing algorithms based\non prohibiting dimensional turns are suitable to be used as escape paths for 2D\nmeshes by analyzing whether they are both connected and deadlock-free. Explain\nyour answer. (Hint: You may wish to refer to the Turn Model algorithm and/or to\nprove your answer by drawing a directed graph for a 4\u00034 mesh that depicts depen-\ndencies between channels and verifying the channel dependency graph is free of\ncycles.) The routing algorithms are expressed with the following abbreviations:\nW\u00bcwest, E\u00bceast, N\u00bcnorth, and S\u00bcsouth.\na. [12]<F.5>Allowed turns are from W to N, E to N, S to W, and S to E.\nb. [12]<F.5>Allowed turns are from W to S, E to S, N to E, and S to E.\nc. [12]<F.5>Allowed turns are from W to S, E to S, N to W, S to E, W to N, and\nS to W.\nd. [12]<F.5>Allowed turns are from S to E, E to S, S to W, N to W, N to E, and E\nto N.\n\u272aF.21\n[15]<F.5>Compute and compare the upper bound for the efficiency factor, \u03c1, for\ndimension-order routing and up*/down* routing assuming uniformly distributed\ntraffic on a 64-node 2D mesh network. For up*/down* routing, assume optimal\nplacement of the root node (i.e., a node near the middle of the mesh). (Hint:\nYou will have to find the loading of links across the network bisection that carries\nthe global load as determined by the routing algorithm.)\nF-114\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1063,
        "text": "\u272aF.22\n[15]<F.5>For the same assumptions as Exercise F.21, find the efficiency factor\nfor up*/down* routing on a 64-node fat tree network using 4\u00034 switches. Com-\npare this result with the \u03c1 found for up*/down* routing on a 2D mesh. Explain.\n\u272aF.23\n[15]<F.5>Calculate the probability of matching two-phased arbitration requests\nfrom all k input ports of a switch simultaneously to the k output ports assuming a\nuniform distribution of requests and grants to/from output ports. How does this\ncompare to the matching probability for three-phased arbitration in which each\nof the k input ports can make two simultaneous requests (again, assuming a uni-\nform random distribution of requests and grants)?\n\u272aF.24\n[15]<F.5>The equation on page F-52 shows the value of cut-through switching.\nEthernet switches used to build clusters often do not support cut-through switching.\nCompare the time to transfer 1500 bytes over a 1000-Mbit/sec Ethernet with and\nwithout cut-through switching for a 64-node cluster. Assume that each Ethernet\nswitch takes 1.0 \u03bcs and that a message goes through seven intermediate switches.\n\u272aF.25\n[15]<F.5>Making the same assumptions as in Exercise F.24, what is the differ-\nence between cut-through and store-and-forward switching for 32 bytes?\n\u272aF.26\n[15]<F.5>One way to reduce latency is to use larger switches. Unlike Exercise\nF.24, let\u2019s assume we need only three intermediate switches to connect any two\nnodes in the cluster. Make the same assumptions as in Exercise F.24 for the remain-\ning parameters. What is the difference between cut-through and store-and-forward\nfor 1500 bytes? For 32 bytes?\n\u272aF.27\n[20]<F.5>Using FlexSim 1.2 (http://ceng.usc.edu/smart/FlexSim/flexsim.html)\nor some other cycle-accurate network simulator, simulate a 256-node 2D torus net-\nwork assuming wormhole routing, 32-flit packets, uniform (random) communica-\ntion pattern, and four virtual channels. Compare the performance of deterministic\nrouting using DOR, adaptive routing using escape paths (i.e., Duato\u2019s Protocol),\nand true fully adaptive routing using progressive deadlock recovery (i.e., Disha\nrouting). Do so by plotting latency versus applied load and through-put versus\napplied load for each, as is done in Figure F.19 for the example on page F-53. Also\nrun simulations and plot results for two and eight virtual channels for each. Com-\npare and explain your results by addressing how/why the number and use of virtual\nchannels by the various routing algorithms affect network performance. (Hint: Be\nsure to let the simulation reach steady state by allowing a warm-up period of a sev-\neral thousand network cycles before gathering results.)\n\u272aF.28\n[20]<F.5>Repeat Exercise F.27 using bit-reversal communication instead of the\nuniform random communication pattern. Compare and explain your results by\naddressing how/why the communication pattern affects network performance.\n\u272aF.29\n[40]<F.5>Repeat Exercises F.27 and F.28 using 16-flit packets and 128-flit\npackets. Compare and explain your results by addressing how/why the packet size\nalong with the other design parameters affect network performance.\nF.30\n[20]<F.2, F.4, F.5, F.8>Figures F.7, F.16, and F.20 show interconnection\nnetwork characteristics of several of the top 500 supercomputers by machine type\nExercises\n\u25a0\nF-115"
    },
    {
        "page": 1064,
        "text": "as of the publication of the fourth edition. Update that figure to the most recent top\n500. How have the systems and their networks changed since the data in the orig-\ninal figure? Do similar comparisons for OCNs used in microprocessors and SANs\ntargeted for clusters using Figures F.29 and F.31.\n\u272aF.31\n[12/12/12/15/15/18]<F.8>Use the M/M/1 queuing model to answer this exer-\ncise. Measurements of a network bridge show that packets arrive at 200 packets\nper second and that the gateway forwards them in about 2 ms.\na. [12]<F.8>What is the utilization of the gateway?\nb. [12]<F.8>What is the mean number of packets in the gateway?\nc. [12]<F.8>What is the mean time spent in the gateway?\nd. [15]<F.8>Plot response time versus utilization as you vary the arrival rate.\ne. [15]<F.8>For an M/M/1 queue, the probability of finding n or more tasks in\nthe system is Utilizationn. What is the chance of an overflow of the FIFO if it can\nhold 10 messages?\nf. [18]<F.8>How big must the gateway be to have packet loss due to FIFO over-\nflow less than one packet per million?\n\u272aF.32\n[20]<F.8>The imbalance between the time of sending and receiving can cause\nproblems in network performance. Sending too fast can cause the network to back\nup and increase the latency of messages, since the receivers will not be able to pull\nout the message fast enough. A technique called bandwidth matching proposes a\nsimple solution: Slow down the sender so that it matches the performance of the\nreceiver [Brewer and Kuszmaul 1994]. If two machines exchange an equal number\nof messages using a protocol like UDP, one will get ahead of the other, causing it to\nsend all its messages first. After the receiver puts all these messages away, it will\nthen send its messages. Estimate the performance for this case versus a bandwidth-\nmatched case. Assume that the send overhead is 200 \u03bcs, the receive overhead is\n300 \u03bcs, time of flight is 5 \u03bcs, latency is 10 \u03bcs, and that the two machines want\nto exchange 100 messages.\nF.33\n[40]<F.8>Compare the performance of UDP with and without bandwidth\nmatching by slowing down the UDP send code to match the receive code as\nadvised by bandwidth matching [Brewer and Kuszmaul 1994]. Devise an exper-\niment to see how much performance changes as a result. How should you change\nthe send rate when two nodes send to the same destination? What if one sender\nsends to two destinations?\n\u272aF.34\n[40]<F.6, F.8>If you have access to an SMP and a cluster, write a program to\nmeasure latency of communication and bandwidth of communication between pro-\ncessors, as was plotted in Figure F.32 on page F-80.\nF.35\n[20/20/20]<F.9>If you have access to a UNIX system, use ping to explore the\nInternet. First read the manual page. Then use ping without option flags to be sure\nyou can reach the following sites. It should say that X is alive. Depending on\nyour system, you may be able to see the path by setting the flags to verbose mode\nF-116\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1065,
        "text": "(-v) and trace route mode (-R) to see the path between your machine and the\nexample machine. Alternatively, you may need to use the program trace route\nto see the path. If so, try its manual page. You may want to use the UNIX command\nscript to make a record of your session.\na. [20]<F.9>Trace the route to another machine on the same local area network.\nWhat is the latency?\nb. [20]<F.9>Trace the route to another machine on your campus that is not on\nthe same local area network.What is the latency?\nc. [20]<F.9>Trace the route to another machine off campus. For example, if you\nhave a friend you send email to, try tracing that route. See if you can discover\nwhat types of networks are used along that route.What is the latency?\nF.36\n[15]<F.9>Use FTP to transfer a file from a remote site and then between local\nsites on the same LAN. What is the difference in bandwidth for each transfer? Try\nthe transfer at different times of day or days of the week. Is the WAN or LAN the\nbottleneck?\n\u272aF.37\n[10/10]<F.9, F.11>Figure F.41 on page F-93 compares latencies for a high-\nbandwidth network with high overhead and a low-bandwidth network with low\noverhead for different TCP/IP message sizes.\na. [10]<F.9, F.11>For what message sizes is the delivered bandwidth higher for\nthe high-bandwidth network?\nb. [10]<F.9, F.11>For your answer to part (a), what is the delivered bandwidth\nfor each network?\n\u272aF.38\n[15]<F.9, F.11>Using the statistics in Figure F.41 on page F-93, estimate the\nper-message overhead for each network.\n\u272aF.39\n[15]<F.9, F.11>Exercise F.37 calculates which message sizes are faster for two\nnetworks with different overhead and peak bandwidth. Using the statistics in\nFigure F.41 on page F-93, what is the percentage of messages that are transmitted\nmore quickly on the network with low overhead and bandwidth? What is the per-\ncentage of data transmitted more quickly on the network with high overhead and\nbandwidth?\n\u272aF.40\n[15]<F.9, F.11>One interesting measure of the latency and bandwidth of an\ninter-connection is to calculate the size of a message needed to achieve one-half\nof the peak bandwidth. This halfway point is sometimes referred to as n1/2, taken\nfrom the terminology of vector processing. Using Figure F.41 on page F-93, esti-\nmate n1/2 for TCP/IP message using 155-Mbit/sec ATM and 10-Mbit/sec Ethernet.\nF.41\n[Discussion]<F.10>The Google cluster used to be constructed from 1 rack unit\n(RU) PCs, each with one processor and two disks. Today there are considerably\ndenser options. How much less floor space would it take if we were to replace\nthe 1 RU PCs with modern alternatives? Go to the Compaq or Dell Web sites\nto find the densest alternative. What would be the estimated impact on cost of\nthe equipment? What would be the estimated impact on rental cost of floor space?\nExercises\n\u25a0\nF-117"
    },
    {
        "page": 1066,
        "text": "What would be the impact on interconnection network design for achieving power/\nperformance efficiency?\nF.42\n[Discussion]<F.13>At the time of the writing of the fourth edition, it was unclear\nwhat would happen with Ethernet versus InfiniBand versus Advanced Switching\nin the machine room. What are the technical advantages of each? What are the eco-\nnomic advantages of each? Why would people maintaining the system prefer one\nto the other? How popular is each network today? How do they compare to\nproprietary commercial networks such as Myrinet and Quadrics?\nF-118\n\u25a0\nAppendix F Interconnection Networks"
    },
    {
        "page": 1067,
        "text": "G.1\nIntroduction\nG-2\nG.2\nVector Performance in More Depth\nG-2\nG.3\nVector Memory Systems in More Depth\nG-9\nG.4\nEnhancing Vector Performance\nG-11\nG.5\nEffectiveness of Compiler Vectorization\nG-14\nG.6\nPutting It All Together: Performance of Vector Processors\nG-15\nG.7\nA Modern Vector Supercomputer: The Cray X1\nG-21\nG.8\nConcluding Remarks\nG-25\nG.9\nHistorical Perspective and References\nG-26\nExercises\nG-29"
    },
    {
        "page": 1068,
        "text": "G\nVector Processors in\nMore Depth\nRevised by Krste Asanovic\nMassachusetts Institute of Technology\nI\u2019m certainly not inventing vector processors. There are three\nkinds that I know of existing today. They are represented by the\nIlliac-IV, the (CDC) Star processor, and the TI (ASC) processor. Those\nthree were all pioneering processors.\u2026One of the problems of\nbeing a pioneer is you always make mistakes and I never, never\nwant to be a pioneer. It\u2019s always best to come second when you\ncan look at the mistakes the pioneers made.\nSeymour Cray\nPublic lecture at Lawrence Livermore Laboratorieson on the\nintroduction of the Cray-1 (1976)"
    },
    {
        "page": 1069,
        "text": "G.1\nIntroduction\nChapter 4 introduces vector architectures and places Multimedia SIMD extensions\nand GPUs in proper context to vector architectures.\nIn this appendix, we go into more detail on vector architectures, including more\naccurate performance models and descriptions of previous vector architectures.\nFigure G.1 shows the characteristics of some typical vector processors, including\nthe size and count of the registers, the number and types of functional units, the\nnumber of load-store units, and the number of lanes.\nG.2\nVector Performance in More Depth\nThe chime approximation is reasonably accurate for long vectors. Another source\nof overhead is far more significant than the issue limitation.\nThe most important source of overhead ignored by the chime model is vector\nstart-up time. The start-up time comes from the pipelining latency of the vector\noperation and is principally determined by how deep the pipeline is for the func-\ntional unit used. The start-up time increases the effective time to execute a convoy\nto more than one chime. Because of our assumption that convoys do not overlap in\ntime, the start-up time delays the execution of subsequent convoys. Of course, the\ninstructions in successive convoys either have structural conflicts for some\nfunctional unit or are data dependent, so the assumption of no overlap is reason-\nable. The actual time to complete a convoy is determined by the sum of the vector\nlength and the start-up time. If vector lengths were infinite, this start-up overhead\nwould be amortized, but finite vector lengths expose it, as the following example\nshows.\nExample\nAssume that the start-up overhead for functional units is shown in Figure G.2.\nShow the time that each convoy can begin and the total number of cycles\nneeded. How does the time compare to the chime approximation for a vector of\nlength 64?\nAnswer\nFigure G.3 provides the answer in convoys, assuming that the vector length is n.\nOne tricky question is when we assume the vector sequence is done; this deter-\nmines whether the start-up time of the SV is visible or not. We assume that the\ninstructions following cannot fit in the same convoy, and we have already assumed\nthat convoys do not overlap. Thus, the total time is given by the time until the last\nvector instruction in the last convoy completes. This is an approximation, and the\nstart-up time of the last vector instruction may be seen in some sequences and not in\nothers. For simplicity, we always include it.\nThe time per result for a vector of length 64 is 4+(42/64)\u00bc4.65 clock cycles,\nwhile the chime approximation would be 4. The execution time with startup\noverhead is 1.16 times higher.\nG-2\n\u25a0\nAppendix G Vector Processors in More Depth"
    },
    {
        "page": 1070,
        "text": "Processor (year)\nVector\nclock rate\n(MHz)\nVector\nregisters\nElements per\nregister (64-bit\nelements)\nVector arithmetic units\nVector\nload-store\nunits\nLanes\nCray-1 (1976)\n80\n8\n64\n6: FP add, FP multiply, FP reciprocal, integer\nadd, logical, shift\n1\n1\nCray X-MP (1983)\n118\n8\n64\n8: FP add, FP multiply, FP reciprocal, integer\nadd, 2 logical, shift, population count/parity\n2 loads\n1 store\n1\nCray Y-MP (1988)\n166\nCray-2 (1985)\n244\n8\n64\n5: FP add, FP multiply, FP reciprocal/sqrt,\ninteger add/shift/population count, logical\n1\n1\nFujitsu VP100/\nVP200 (1982)\n133\n8\u2013256\n32\u20131024\n3: FP or integer add/logical, multiply, divide\n2\n1 (VP100)\n2 (VP200)\nHitachi S810/S820\n(1983)\n71\n32\n256\n4: FP multiply-add, FP multiply/divide-add\nunit, 2 integer add/logical\n3 loads\n1 store\n1 (S810)\n2 (S820)\nConvex C-1 (1985)\n10\n8\n128\n2: FP or integer multiply/divide, add/logical\n1\n1 (64 bit)\n2 (32 bit)\nNEC SX/2 (1985)\n167\n8+32\n256\n4: FP multiply/divide, FP add, integer add/\nlogical, shift\n1\n4\nCray C90 (1991)\n240\n8\n128\n8: FP add, FP multiply, FP reciprocal, integer\nadd, 2 logical, shift, population count/parity\n2 loads\n1 store\n2\nCray T90 (1995)\n460\nNEC SX/5 (1998)\n312\n8+64\n512\n4: FP or integer add/shift, multiply, divide,\nlogical\n1\n16\nFujitsu VPP5000\n(1999)\n300\n8\u2013256\n128\u20134096\n3: FP or integer multiply, add/logical, divide\n1 load\n1 store\n16\nCray SV1 (1998)\n300\n8\n64 (MSP)\n8: FP add, FP multiply, FP reciprocal, integer\nadd, 2 logical, shift, population count/parity\n1 load-store\n1 load\n2\n8 (MSP)\nSV1ex (2001)\n500\nVMIPS (2001)\n500\n8\n64\n5: FP multiply, FP divide, FP add, integer add/\nshift, logical\n1 load-store\n1\nNEC SX/6 (2001)\n500\n8+64\n256\n4: FP or integer add/shift, multiply, divide,\nlogical\n1\n8\nNEC SX/8 (2004)\n2000\n8+64\n256\n4: FP or integer add/shift, multiply, divide,\nlogical\n1\n4\nCray X1 (2002)\n800\n32\n64\n256 (MSP)\n3: FP or integer, add/logical, multiply/shift,\ndivide/square root/logical\n1 load\n1 store\n2\n8 (MSP)\nCray XIE (2005)\n1130\nFigure G.1 Characteristics of several vector-register architectures. If the machine is a multiprocessor, the entries\ncorrespond to the characteristics of one processor. Several of the machines have different clock rates in the vector\nand scalar units; the clock rates shown are for the vector units. The Fujitsu machines\u2019 vector registers are configurable:\nThe size and count of the 8K 64-bit entries may be varied inversely to one another (e.g., on the VP200, from eight\nregisters each 1K elements long to 256 registers each 32 elements long). The NEC machines have eight foreground\nvector registers connected to the arithmetic units plus 32 to 64 background vector registers connected between the\nmemory system and the foreground vector registers. Add pipelines perform add and subtract. The multiply/divide-\nadd unit on the Hitachi S810/820 performs an FP multiply or divide followed by an add or subtract (while the multiply-\nadd unit performs a multiply followed by an add or subtract). Note that most processors use the vector FP multiply\nand divide units for vector integer multiply and divide, and several of the processors use the same units for FP scalar\nand FP vector operations. Each vector load-store unit represents the ability to do an independent, overlapped transfer\nto or from the vector registers. The number of lanes is the number of parallel pipelines in each of the functional units\nas described in Section G.4. For example, the NEC SX/5 can complete 16 multiplies per cycle in the multiply functional\nunit. Several machines can split a 64-bit lane into two 32-bit lanes to increase performance for applications that\nrequire only reduced precision. The Cray SV1 and Cray X1 can group four CPUs with two lanes each to act in unison\nas a single larger CPU with eight lanes, which Cray calls a Multi-Streaming Processor (MSP)."
    },
    {
        "page": 1071,
        "text": "For simplicity, we will use the chime approximation for running time, incorporat-\ning start-up time effects only when we want performance that is more detailed or to\nillustrate the benefits of some enhancement. For long vectors, a typical situation,\nthe overhead effect is not that large. Later in the appendix, we will explore ways to\nreduce start-up overhead.\nStart-up time for an instruction comes from the pipeline depth for the functional\nunit implementing that instruction. If the initiation rate is to be kept at 1 clock cycle\nper result, then\nPipeline depth \u00bc Total functional unit time\nClock cycle time\n\u0001\n\u0003\nFor example, if an operation takes 10 clock cycles, it must be pipelined 10 deep to\nachieve an initiation rate of one per clock cycle. Pipeline depth, then, is deter-\nmined by the complexity of the operation and the clock cycle time of the proces-\nsor. The pipeline depths of functional units vary widely\u20142 to 20 stages are\ncommon\u2014although the most heavily used units have pipeline depths of 4 to\n8 clock cycles.\nFor VMIPS, we will use the same pipeline depths as the Cray-1, although laten-\ncies in more modern processors have tended to increase, especially for loads. All\nfunctional units are fully pipelined. From Chapter 4, pipeline depths are 6 clock\ncycles for floating-point add and 7 clock cycles for floating-point multiply. On\nVMIPS, as on most vector processors, independent vector operations using\ndifferent functional units can issue in the same convoy.\nIn addition to the start-up overhead, we need to account for the overhead of\nexecuting the strip-mined loop. This strip-mining overhead, which arises from\nUnit\nStart-up overhead (cycles)\nLoad and store unit\n12\nMultiply unit\n7\nAdd unit\n6\nFigure G.2 Start-up overhead.\nConvoy\nStarting time\nFirst-result time\nLast-result time\n1. LV\n0\n12\n11+n\n2. MULVS.D LV\n12+n\n12+n+12\n23+2n\n3. ADDV.D\n24+2n\n24+2n+6\n29+3n\n4. SV\n30+3n\n30+3n+12\n41+4n\nFigure G.3 Starting times and first- and last-result times for convoys 1 through 4.\nThe vector length is n.\nG-4\n\u25a0\nAppendix G Vector Processors in More Depth"
    },
    {
        "page": 1072,
        "text": "the need to reinitiate the vector sequence and set the Vector Length Register (VLR)\neffectively adds to the vector start-up time, assuming that a convoy does not over-\nlap with other instructions. If that overhead for a convoy is 10 cycles, then the\neffective overhead per 64 elements increases by 10 cycles, or 0.15 cycles per\nelement.\nTwo key factors contribute to the running time of a strip-mined loop consisting\nof a sequence of convoys:\n1. The number of convoys in the loop, which determines the number of chimes.\nWe use the notation Tchime for the execution time in chimes.\n2. The overhead for each strip-mined sequence of convoys. This overhead consists\nof the cost of executing the scalar code for strip-mining each block, Tloop, plus\nthe vector start-up cost for each convoy, Tstart.\nThere may also be a fixed overhead associated with setting up the vector sequence\nthe first time. In recent vector processors, this overhead has become quite small, so\nwe ignore it.\nThe components can be used to state the total running time for a vector\nsequence operating on a vector of length n, which we will call Tn:\nTn \u00bc\nn\nMVL\nh\ni\n Tloop + Tstart\n\u0004\n\u0005\n+ nTchime\nThe values of Tstart, Tloop, and Tchime are compiler and processor dependent. The\nregister allocation and scheduling of the instructions affect both what goes in a con-\nvoy and the start-up overhead of each convoy.\nFor simplicity, we will use a constant value for Tloop on VMIPS. Based on a\nvariety of measurements of Cray-1 vector execution, the value chosen is 15 for\nTloop. At first glance, you might think that this value is too small. The overhead\nin each loop requires setting up the vector starting addresses and the strides, incre-\nmenting counters, and executing a loop branch. In practice, these scalar instruc-\ntions can be totally or partially overlapped with the vector instructions,\nminimizing the time spent on these overhead functions. The value of Tloop of\ncourse depends on the loop structure, but the dependence is slight compared with\nthe connection between the vector code and the values of Tchime and Tstart.\nOperation\nStart-up penalty\nVector add\n6\nVector multiply\n7\nVector divide\n20\nVector load\n12\nFigure G.4 Start-up penalties on VMIPS. These are the start-up penalties in clock\ncycles for VMIPS vector operations.\nG.2\nVector Performance in More Depth\n\u25a0\nG-5"
    },
    {
        "page": 1073,
        "text": "Example\nWhat is the execution time on VMIPS for the vector operation A\u00bcBs, where s is\na scalar and the length of the vectors A and B is 200?\nAnswer\nAssume that the addresses of A and B are initially in Ra and Rb, s is in Fs, and\nrecall that for MIPS (and VMIPS) R0 always holds 0. Since (200 mod 64)\u00bc8, the\nfirst iteration of the strip-mined loop will execute for a vector length of 8 elements,\nand the following iterations will execute for a vector length of 64 elements. The\nstarting byte addresses of the next segment of each vector is eight times the vector\nlength. Since the vector length is either 8 or 64, we increment the address registers\nby 88\u00bc64 after the first segment and 864\u00bc512 for later segments. The total\nnumber of bytes in the vector is 8200\u00bc1600, and we test for completion by\ncomparing the address of the next vector segment to the initial address plus\n1600. Here is the actual code:\nDADDUI\nR2,R0,#1600 ;total # bytes in vector\nDADDU\nR2,R2,Ra\n;address of the end of A vector\nDADDUI\nR1,R0,#8\n;loads length of 1st segment\nMTC1\nVLR,R1\n;load vector length in VLR\nDADDUI\nR1,R0,#64\n;length in bytes of 1st segment\nDADDUI\nR3,R0,#64\n;vector length of other segments\nLoop: LV\nV1,Rb\n;load B\nMULVS.D\nV2,V1,Fs\n;vector * scalar\nSV\nRa,V2\n;store A\nDADDU\nRa,Ra,R1\n;address of next segment of A\nDADDU\nRb,Rb,R1\n;address of next segment of B\nDADDUI\nR1,R0,#512\n;load byte offset next segment\nMTC1\nVLR,R3\n;set length to 64 elements\nDSUBU\nR4,R2,Ra\n;at the end of A?\nBNEZ\nR4,Loop\n;if not, go back\nThe three vector instructions in the loop are dependent and must go into three\nconvoys, hence Tchime\u00bc3. Let\u2019s use our basic formula:\nTn \u00bc\nn\nMVL\nh\ni\n Tloop + Tstart\n\u0004\n\u0005\n+ nTchime\nT200 \u00bc 4 15 + Tstart\n\u00f0\n\u00de + 2003\nT200 \u00bc 60 + 4Tstart\n\u00f0\n\u00de + 600 \u00bc 660 + 4Tstart\n\u00f0\n\u00de\nThe value of Tstart is the sum of:\n\u25a0\nThe vector load start-up of 12 clock cycles\n\u25a0\nA 7-clock-cycle start-up for the multiply\n\u25a0\nA 12-clock-cycle start-up for the store\nThus, the value of Tstart is given by:\nTstart \u00bc 12 + 7 + 12 \u00bc 31\nG-6\n\u25a0\nAppendix G Vector Processors in More Depth"
    },
    {
        "page": 1074,
        "text": "So, the overall value becomes:\nT200 \u00bc 660 + 431 \u00bc 784\nThe execution time per element with all start-up costs is then 784/200\u00bc3.9,\ncompared with a chime approximation of three. In Section G.4, we will be more\nambitious\u2014allowing overlapping of separate convoys.\nFigure G.5 shows the overhead and effective rates per element for the previous\nexample (A\u00bcBs) with various vector lengths. A chime-counting model would\nlead to 3 clock cycles per element, while the two sources of overhead add 0.9 clock\ncycles per element in the limit.\nPipelined Instruction Start-Up and Multiple Lanes\nAdding multiple lanes increases peak performance but does not change start-up\nlatency, and so it becomes critical to reduce start-up overhead by allowing the start\nof one vector instruction to be overlapped with the completion of preceding vector\ninstructions. The simplest case to consider is when two vector instructions access a\ndifferent set of vector registers. For example, in the code sequence\nADDV.D V1,V2,V3\nADDV.D V4,V5,V6\nTotal time\nper element\nTotal\noverhead\nper element\n10\nClock\ncycles\n30\n50\n70\n90\n110\n130\n150\n170\n190\n0\n1\n2\n3\n4\n5\n6\n7\n8\nVector size\n9\nFigure G.5 The total execution time per element and the total overhead time per\nelement versus the vector length for the example on page F-6. For short vectors,\nthe total start-up time is more than one-half of the total time, while for long vectors\nit reduces to about one-third of the total time. The sudden jumps occur when the vector\nlength crosses a multiple of 64, forcing another iteration of the strip-mining code and\nexecution of a set of vector instructions. These operations increase Tn by Tloop+Tstart.\nG.2\nVector Performance in More Depth\n\u25a0\nG-7"
    },
    {
        "page": 1075,
        "text": "An implementation can allow the first element of the second vector instruction to\nfollow immediately the last element of the first vector instruction down the FP\nadder pipeline. To reduce the complexity of control logic, some vector machines\nrequire some recovery time or dead time in between two vector instructions dis-\npatched to the same vector unit. Figure G.6 is a pipeline diagram that shows both\nstart-up latency and dead time for a single vector pipeline.\nThe following example illustrates the impact of this dead time on achievable\nvector performance.\nExample\nThe Cray C90 has two lanes but requires 4 clock cycles of dead time between any\ntwo vector instructions to the same functional unit, even if they have no data depen-\ndences. For the maximum vector length of 128 elements, what is the reduction in\nachievable peak performance caused by the dead time? What would be the reduc-\ntion if the number of lanes were increased to 16?\nAnswer\nA maximum length vector of 128 elements is divided over the two lanes and occupies\na vector functional unit for 64 clock cycles. The dead time adds another 4 cycles of\noccupancy, reducing the peak performance to 64/(64+4)\u00bc94.1% of the value with-\nout dead time. If the number of lanes is increased to 16, maximum length vector\ninstructions will occupy a functional unit for only 128/16\u00bc8 cycles, and the dead\ntime will reduce peak performance to 8/(8+4)\u00bc66.6% of the value without dead\ntime. In this second case, the vector units can never be more than 2/3 busy!\nFigure G.6 Start-up latency and dead time for a single vector pipeline. Each element\nhas a 5-cycle latency: 1 cycle to read the vector-register file, 3 cycles in execution, then 1\ncycle to write the vector-register file. Elements from the same vector instruction can\nfollow each other down the pipeline, but this machine inserts 4 cycles of dead time\nbetween two different vector instructions. The dead time can be eliminated with more\ncomplex control logic. (Reproduced with permission from Asanovic [1998].)\nG-8\n\u25a0\nAppendix G Vector Processors in More Depth"
    },
    {
        "page": 1076,
        "text": "Pipelining instruction start-up becomes more complicated when multiple instruc-\ntions can be reading and writing the same vector register and when some instructions\nmay stall unpredictably\u2014for example, a vector load encountering memory bank\nconflicts. However, as both the number of lanes and pipeline latencies increase, it\nbecomes increasingly important to allow fully pipelined instruction start-up.\nG.3\nVector Memory Systems in More Depth\nTo maintain an initiation rate of one word fetched or stored per clock, the memory\nsystem must be capable of producing or accepting this much data. As we saw in\nChapter 4, this usually done by spreading accesses across multiple independent\nmemory banks. Having significant numbers of banks is useful for dealing with vec-\ntor loads or stores that access rows or columns of data.\nThe desired access rate and the bank access time determined how many banks\nwere needed to access memory without stalls. This example shows how these tim-\nings work out in a vector processor.\nExample\nSuppose we want to fetch a vector of 64 elements starting at byte address 136, and a\nmemory access takes 6 clocks. How many memory banks must we have to support\none fetch per clock cycle? With what addresses are the banks accessed? When will\nthe various elements arrive at the CPU?\nAnswer\nSix clocks per access require at least 6 banks, but because we want the number of\nbanks to be a power of 2, we choose to have 8 banks. Figure G.7 shows the timing\nfor the first few sets of accesses for an 8-bank system with a 6-clock-cycle access\nlatency.\nThe timing of real memory banks is usually split into two different components, the\naccess latency and the bank cycle time (or bank busy time). The access latency is the\ntime from when the address arrives at the bank until the bank returns a data value,\nwhile the busy time is the time the bank is occupied with one request. The access\nlatency adds to the start-up cost of fetching a vector from memory (the total memory\nlatency also includes time to traverse the pipelined interconnection networks that\ntransfer addresses and data between the CPU and memory banks). The bank busy\ntime governs the effective bandwidth of a memory system because a processor can-\nnot issue a second request to the same bank until the bank busy time has elapsed.\nFor simple unpipelined SRAM banks as used in the previous examples, the\naccess latency and busy time are approximately the same. For a pipelined\nSRAM bank, however, the access latency is larger than the busy time because\neach element access only occupies one stage in the memory bank pipeline. For a\nDRAM bank, the access latency is usually shorter than the busy time because a\nDRAM needs extra time to restore the read value after the destructive read oper-\nation. For memory systems that support multiple simultaneous vector accesses\nG.3\nVector Memory Systems in More Depth\n\u25a0\nG-9"
    },
    {
        "page": 1077,
        "text": "or allow nonsequential accesses in vector loads or stores, the number of mem-\nory banks should be larger than the minimum; otherwise, memory bank con-\nflicts will exist.\nMemory bank conflicts will not occur within a single vector memory instruc-\ntion if the stride and number of banks are relatively prime with respect to each other\nand there are enough banks to avoid conflicts in the unit stride case. When there are\nno bank conflicts, multiword and unit strides run at the same rates. Increasing the\nnumber of memory banks to a number greater than the minimum to prevent stalls\nwith a stride of length 1 will decrease the stall frequency for some other strides. For\nexample, with 64 banks, a stride of 32 will stall on every other access, rather than\nevery access. If we originally had a stride of 8 and 16 banks, every other access\nwould stall; with 64 banks, a stride of 8 will stall on every eighth access. If we have\nmultiple memory pipelines and/or multiple processors sharing the same memory\nsystem, we will also need more banks to prevent conflicts. Even machines with\na single memory pipeline can experience memory bank conflicts on unit stride\nBank\nCycle no.\n0\n1\n2\n3\n4\n5\n6\n7\n0\n136\n1\nBusy\n144\n2\nBusy\nBusy\n152\n3\nBusy\nBusy\nBusy\n160\n4\nBusy\nBusy\nBusy\nBusy\n168\n5\nBusy\nBusy\nBusy\nBusy\nBusy\n176\n6\nBusy\nBusy\nBusy\nBusy\nBusy\n184\n7\n192\nBusy\nBusy\nBusy\nBusy\nBusy\n8\nBusy\n200\nBusy\nBusy\nBusy\nBusy\n9\nBusy\nBusy\n208\nBusy\nBusy\nBusy\n10\nBusy\nBusy\nBusy\n216\nBusy\nBusy\n11\nBusy\nBusy\nBusy\nBusy\n224\nBusy\n12\nBusy\nBusy\nBusy\nBusy\nBusy\n232\n13\nBusy\nBusy\nBusy\nBusy\nBusy\n240\n14\nBusy\nBusy\nBusy\nBusy\nBusy\n248\n15\n256\nBusy\nBusy\nBusy\nBusy\nBusy\n16\nBusy\n264\nBusy\nBusy\nBusy\nBusy\nFigure G.7 Memory addresses (in bytes) by bank number and time slot at which\naccess begins. Each memory bank latches the element address at the start of an access\nand is then busy for 6 clock cycles before returning a value to the CPU. Note that the CPU\ncannot keep all 8 banks busy all the time because it is limited to supplying one new\naddress and receiving one data item each cycle.\nG-10\n\u25a0\nAppendix G Vector Processors in More Depth"
    },
    {
        "page": 1078,
        "text": "accesses between the last few elements of one instruction and the first few elements\nof the next instruction, and increasing the number of banks will reduce the prob-\nability of these inter-instruction conflicts. In 2011, most vector supercomputers\nspread the accesses from each CPU across hundreds of memory banks. Because\nbank conflicts can still occur in non-unit stride cases, programmers favor unit stride\naccesses whenever possible.\nA modern supercomputer may have dozens of CPUs, each with multiple mem-\nory pipelines connected to thousands of memory banks. It would be impractical to\nprovide a dedicated path between each memory pipeline and each memory bank,\nso, typically, a multistage switching network is used to connect memory pipelines\nto memory banks. Congestion can arise in this switching network as different vec-\ntor accesses contend for the same circuit paths, causing additional stalls in the\nmemory system.\nG.4\nEnhancing Vector Performance\nIn this section, we present techniques for improving the performance of a vector\nprocessor in more depth than we did in Chapter 4.\nChaining in More Depth\nEarly implementations of chaining worked like forwarding, but this restricted the\ntiming of the source and destination instructions in the chain. Recent implementa-\ntions use flexible chaining, which allows a vector instruction to chain to essentially\nany other active vector instruction, assuming that no structural hazard is generated.\nFlexible chaining requires simultaneous access to the same vector register by dif-\nferent vector instructions, which can be implemented either by adding more read\nand write ports or by organizing the vector-register file storage into interleaved\nbanks in a similar way to the memory system. We assume this type of chaining\nthroughout the rest of this appendix.\nEven though a pair of operations depends on one another, chaining allows the\noperations to proceed in parallel on separate elements of the vector. This permits\nthe operations to be scheduled in the same convoy and reduces the number of chimes\nrequired. For the previous sequence, a sustained rate (ignoring start-up) of two\nfloating-point operations per clock cycle, or one chime, can be achieved, even though\nthe operations are dependent! The total running time for the above sequence becomes:\nVector length + Start-up timeADDV + Start-up timeMULV\nFigure G.8 shows the timing of a chained and an unchained version of the above\npair of vector instructions with a vector length of 64. This convoy requires one\nchime; however, because it uses chaining, the start-up overhead will be seen in\nthe actual timing of the convoy. In Figure G.8, the total time for chained operation\nis 77 clock cycles, or 1.2 cycles per result. With 128 floating-point operations done\nin that time, 1.7 FLOPS per clock cycle are obtained. For the unchained version,\nthere are 141 clock cycles, or 0.9 FLOPS per clock cycle.\nG.4\nEnhancing Vector Performance\n\u25a0\nG-11"
    },
    {
        "page": 1079,
        "text": "Although chaining allows us to reduce the chime component of the execution\ntime by putting two dependent instructions in the same convoy, it does not eliminate\nthe start-up overhead. If we want an accurate running time estimate, we must count\nthe start-up time both within and across convoys. With chaining, the number of\nchimesfor asequenceisdeterminedbythenumberofdifferent vectorfunctionalunits\navailable in the processor and the number required by the application. In particular,\nno convoy can contain a structural hazard. This means, for example, that a sequence\ncontaining two vector memory instructions must take at least two convoys, and hence\ntwo chimes, on a processor like VMIPS with only one vector load-store unit.\nChaining is so important that every modern vector processor supports flexible\nchaining.\nSparse Matrices in More Depth\nChapter 4 shows techniques to allow programs with sparse matrices to execute in\nvector mode. Let\u2019s start with a quick review. In a sparse matrix, the elements of a\nvector are usually stored in some compacted form and then accessed indirectly.\nAssuming a simplified sparse structure, we might see code that looks like this:\ndo\n100 i = 1,n\n100\nA(K(i)) = A(K(i)) + C(M(i))\nThis code implements a sparse vector sum on the arrays A and C, using index\nvectors K and M to designate the nonzero elements of A and C. (A and C must have\nthe same number of nonzero elements\u2014n of them.) Another common representa-\ntion for sparse matrices uses a bit vector to show which elements exist and a dense\nvector for the nonzero elements. Often both representations exist in the same pro-\ngram. Sparse matrices are found in many codes, and there are many ways to imple-\nment them, depending on the data structure used in the program.\nA simple vectorizing compiler could not automatically vectorize the source\ncode above because the compiler would not know that the elements of K are distinct\nvalues and thus that no dependences exist. Instead, a programmer directive would\ntell the compiler that it could run the loop in vector mode.\nMore sophisticated vectorizing compilers can vectorize the loop automatically\nwithout programmer annotations by inserting run time checks for data\nUnchained\nChained\nTotal = 77\nTotal = 141\n7\n64\n7\n64\nMULV\n64\nADDV\n64\nMULV\nADDV\n6\n6\nFigure G.8 Timings for a sequence of dependent vector operations ADDV and MULV,\nboth unchained and chained. The 6- and 7-clock-cycle delays are the latency of the\nadder and multiplier.\nG-12\n\u25a0\nAppendix G Vector Processors in More Depth"
    },
    {
        "page": 1080,
        "text": "dependences. These run time checks are implemented with a vectorized software\nversion of the advanced load address table (ALAT) hardware described in Appen-\ndix H for the Itanium processor. The associative ALAT hardware is replaced with a\nsoftware hash table that detects if two element accesses within the same stripmine\niteration are to the same address. If no dependences are detected, the stripmine iter-\nation can complete using the maximum vector length. If a dependence is detected,\nthe vector length is reset to a smaller value that avoids all dependency violations,\nleaving the remaining elements to be handled on the next iteration of the strip-\nmined loop. Although this scheme adds considerable software overhead to the\nloop, the overhead is mostly vectorized for the common case where there are\nno dependences; as a result, the loop still runs considerably faster than scalar code\n(although much slower than if a programmer directive was provided).\nA scatter-gather capability is included on many of the recent supercomputers.\nThese operations often run more slowly than strided accesses because they are\nmore complex to implement and are more susceptible to bank conflicts, but they\nare still much faster than the alternative, which may be a scalar loop. If the sparsity\nproperties of a matrix change, a new index vector must be computed. Many pro-\ncessors provide support for computing the index vector quickly. The CVI (create\nvector index) instruction in VMIPS creates an index vector given a stride (m),\nwhere the values in the index vector are 0, m, 2m, \u2026, 63m. Some processors\nprovide an instruction to create a compressed index vector whose entries corre-\nspond to the positions with a one in the mask register. Other vector architectures\nprovide a method to compress a vector. In VMIPS, we define the CVI instruction\nto always create a compressed index vector using the vector mask. When the vector\nmask is all ones, a standard index vector will be created.\nThe indexed loads-stores and the CVI instruction provide an alternative method\nto support conditional vector execution. Let us first recall code from Chapter 4:\nlow = 1\nVL = (n mod MVL) /*find the odd-size piece*/\ndo 1 j = 0,(n/MVL) /*outer loop*/\ndo 10 i = low, low + VL - 1 /*runs for length VL*/\nY(i) = a * X(i) + Y(i) /*main operation*/\n10\ncontinue\nlow = low + VL /*start of next vector*/\nVL = MVL /*reset the length to max*/\n1\ncontinue\nHere is a vector sequence that implements that loop using CVI:\nLV\nV1,Ra\n;load vector A into V1\nL.D\nF0,#0\n;load FP zero into F0\nSNEVS.D\nV1,F0\n;sets the VM to 1 if V1(i)!=F0\nCVI\nV2,#8\n;generates indices in V2\nPOP\nR1,VM\n;find the number of 1\u2019s in VM\nMTC1\nVLR,R1\n;load vector-length register\nCVM\n;clears the mask\nG.4\nEnhancing Vector Performance\n\u25a0\nG-13"
    },
    {
        "page": 1081,
        "text": "LVI\nV3,(Ra+V2)\n;load the nonzero A elements\nLVI\nV4,(Rb+V2)\n;load corresponding B elements\nSUBV.D\nV3,V3,V4\n;do the subtract\nSVI\n(Ra+V2),V3\n;store A back\nWhether the implementation using scatter-gather is better than the condition-\nally executed version depends on the frequency with which the condition holds and\nthe cost of the operations. Ignoring chaining, the running time of the original ver-\nsion is 5n+c1. The running time of the second version, using indexed loads and\nstores with a running time of one element per clock, is 4n+4fn+c2, where f is\nthe fraction of elements for which the condition is true (i.e., A(i) \u00a6 0). If we assume\nthat the values of c1 and c2 are comparable, or that they are much smaller than n, we\ncan find when this second technique is better.\nTime1 \u00bc 5 n\n\u00f0 \u00de\nTime2 \u00bc 4n + 4fn\nWe want Time1>Time2, so\n5n > 4n + 4fn\n1\n4 > f\nThat is, the second method is faster if less than one-quarter of the elements are non-\nzero. In many cases, the frequency of execution is much lower. If the index vector\ncan be reused, or if the number of vector statements within the if statement grows,\nthe advantage of the scatter-gather approach will increase sharply.\nG.5\nEffectiveness of Compiler Vectorization\nTwo factors affect the success with which a program can be run in vector mode.\nThe first factor is the structure of the program itself: Do the loops have true data\ndependences, or can they be restructured so as not to have such dependences? This\nfactor is influenced by the algorithms chosen and, to some extent, by how they are\ncoded. The second factor is the capability of the compiler. While no compiler can\nvectorize a loop where no parallelism among the loop iterations exists, there is tre-\nmendous variation in the ability of compilers to determine whether a loop can be\nvectorized. The techniques used to vectorize programs are the same as those\ndiscussed in Chapter 3 for uncovering ILP; here, we simply review how well these\ntechniques work.\nThere is tremendous variation in how well different compilers do in vectorizing\nprograms. As a summary of the state of vectorizing compilers, consider the data in\nFigure G.9, which shows the extent of vectorization for different processors using a\ntest suite of 100 handwritten FORTRAN kernels. The kernels were designed to test\nvectorization capability and can all be vectorized by hand; we will see several\nexamples of these loops in the exercises.\nG-14\n\u25a0\nAppendix G Vector Processors in More Depth"
    },
    {
        "page": 1082,
        "text": "G.6\nPutting It All Together: Performance of Vector\nProcessors\nIn this section, we look at performance measures for vector processors and what they\ntell us about the processors. To determine the performance of a processor on a vector\nproblem we must look at the start-up cost and the sustained rate. The simplest and\nbest way to report the performance of a vector processor on a loop is to give the\nexecution time of the vector loop. For vector loops, people often give the MFLOPS\n(millions of floating-point operations per second) rating rather than execution time.\nWe use the notation Rn for the MFLOPS rating on a vector of length n. Using the\nmeasurements Tn (time) or Rn (rate) is equivalent if the number of FLOPS is agreed\nupon. In any event, either measurement should include the overhead.\nIn this section, we examine the performance of VMIPS on a DAXPY loop (see\nChapter 4) by looking at performance from different viewpoints. We will continue\nto compute the execution time of a vector loop using the equation developed in\nSection G.2. At the same time, we will look at different ways to measure perfor-\nmance using the computed time. The constant values for Tloop used in this section\nintroduce some small amount of error, which will be ignored.\nMeasures of Vector Performance\nBecause vector length is so important in establishing the performance of a proces-\nsor, length-related measures are often applied in addition to time and MFLOPS.\nThese length-related measures tend to vary dramatically across different processors\nProcessor\nCompiler\nCompletely\nvectorized\nPartially\nvectorized\nNot\nvectorized\nCDC CYBER 205 VAST-2 V2.21\n62\n5\n33\nConvex C-series\nFC5.0\n69\n5\n26\nCray X-MP\nCFT77 V3.0\n69\n3\n28\nCray X-MP\nCFT V1.15\n50\n1\n49\nCray-2\nCFT2 V3.1a\n27\n1\n72\nETA-10\nFTN 77 V1.0\n62\n7\n31\nHitachi S810/820\nFORT77/HAP V20-2B\n67\n4\n29\nIBM 3090/VF\nVS FORTRAN V2.4\n52\n4\n44\nNEC SX/2\nFORTRAN77 / SX V.040\n66\n5\n29\nFigure G.9 Result of applying vectorizing compilers to the 100 FORTRAN test\nkernels. For each processor we indicate how many loops were completely vectorized,\npartially vectorized, and unvectorized. These loops were collected by Callahan,\nDongarra, and Levine [1988]. Two different compilers for the Cray X-MP show the large\ndependence on compiler technology.\nG.6\nPutting It All Together: Performance of Vector Processors\n\u25a0\nG-15"
    },
    {
        "page": 1083,
        "text": "and are interesting to compare. (Remember, though, that time is always the mea-\nsure of interest when comparing the relative speed of two processors.) Three of the\nmost important length-related measures are\n\u25a0\nR\u221e\u2014The MFLOPS rate on an infinite-length vector. Although this measure\nmay be of interest when estimating peak performance, real problems have lim-\nited vector lengths, and the overhead penalties encountered in real problems\nwill be larger.\n\u25a0\nN1/2\u2014The vector length needed to reach one-half of R\u221e. This is a good mea-\nsure of the impact of overhead.\n\u25a0\nNv\u2014The vector length needed to make vector mode faster than scalar mode.\nThis measures both overhead and the speed of scalars relative to vectors.\nLet\u2019s look at these measures for our DAXPY problem running on VMIPS. When\nchained, the inner loop of the DAXPY code in convoys looks like Figure G.10\n(assuming that Rx and Ry hold starting addresses).\nRecall our performance equation for the execution time of a vector loop with n\nelements, Tn:\nTn \u00bc\nn\nMVL\nh\ni\n Tloop + Tstart\n\u0004\n\u0005\n+ nTchime\nChaining allows the loop to run in three chimes (and no less, since there is one\nmemory pipeline); thus, Tchime\u00bc3. If Tchime were a complete indication of per-\nformance, the loop would run at an MFLOPS rate of 2/3clock rate (since there\nare 2 FLOPS per iteration). Thus, based only on the chime count, a 500 MHz\nVMIPS would run this loop at 333 MFLOPS assuming no strip-mining or\nstart-up overhead. There are several ways to improve the performance: Add addi-\ntional vector load-store units, allow convoys to overlap to reduce the impact of\nstart-up overheads, and decrease the number of loads required by vector-register\nallocation. We will examine the first two extensions in this section. The last\noptimization is actually used for the Cray-1, VMIPS\u2019s cousin, to boost the per-\nformance by 50%. Reducing the number of loads requires an interprocedural\noptimization; we examine this transformation in Exercise G.6. Before we exam-\nine the first two extensions, let\u2019s see what the real performance, including\noverhead, is.\nLV V1,Rx\nMULVS.D V2,V1,F0\nConvoy 1: chained load and multiply\nLV V3,Ry\nADDV.D V4,V2,V3\nConvoy 2: second load and add, chained\nSV Ry,V4\nConvoy 3: store the result\nFigure G.10 The inner loop of the DAXPY code in chained convoys.\nG-16\n\u25a0\nAppendix G Vector Processors in More Depth"
    },
    {
        "page": 1084,
        "text": "The Peak Performance of VMIPS on DAXPY\nFirst, we should determine what the peak performance, R\u221e, really is, since we\nknow it must differ from the ideal 333 MFLOPS rate. For now, we continue to\nuse the simplifying assumption that a convoy cannot start until all the instructions\nin an earlier convoy have completed; later we will remove this restriction. Using\nthis simplification, the start-up overhead for the vector sequence is simply the sum\nof the start-up times of the instructions:\nTstart \u00bc 12 + 7 + 12 + 6 + 12 \u00bc 49\nUsing MVL\u00bc64, Tloop\u00bc15, Tstart\u00bc49, and Tchime\u00bc3 in the performance\nequation, and assuming that n is not an exact multiple of 64, the time for an n-\nelement operation is\nTn \u00bc n\n64\nh\ni\n 15 + 49\n\u00f0\n\u00de + 3n\n\u0003 n + 64\n\u00f0\n\u00de + 3n\n\u00bc 4n + 64\nThe sustained rate is actually over 4 clock cycles per iteration, rather than the the-\noretical rate of 3 chimes, which ignores overhead. The major part of the difference\nis the cost of the start-up overhead for each block of 64 elements (49 cycles versus\n15 for the loop overhead).\nWe can now compute R\u221efor a 500 MHz clock as:\nR\u221e\u00bc lim\nn!\u221e\nOperations per iterationC1ock rate\nC1ock cyc1es per iteration\n\u0006\n\u0007\nThe numerator is independent of n, hence\nR\u221e\u00bc Operations per iterationC1ock rate\nlim\nn!\u221eC1ock cyc1es per iteration\n\u00f0\n\u00de\nlim\nn!\u221eClock cycles per iteration\n\u00f0\n\u00de \u00bc lim\nn!\u221e\nTn\nn\n\u0006\n\u0007\n\u00bc lim\nn!\u221e\n4n + 64\nn\n\u0006\n\u0007\n\u00bc 4\nR\u221e\u00bc 2500 MHz\n4\n\u00bc 250 MFLOPS\nThe performance without the start-up overhead, which is the peak performance\ngiven the vector functional unit structure, is now 1.33 times higher. In actuality,\nthe gap between peak and sustained performance for this benchmark is even\nlarger!\nSustained Performance of VMIPS on the Linpack Benchmark\nThe Linpack benchmark is a Gaussian elimination on a 100100 matrix. Thus, the\nvector element lengths range from 99 down to 1. A vector of length k is used k\ntimes. Thus, the average vector length is given by:\nG.6\nPutting It All Together: Performance of Vector Processors\n\u25a0\nG-17"
    },
    {
        "page": 1085,
        "text": "X\n99\ni\u00bc1\ni2\nX\n99\ni\u00bc1\ni\n\u00bc 66:3\nNow we can obtain an accurate estimate of the performance of DAXPY using a\nvector length of 66:\nT66 \u00bc 2 15 + 49\n\u00f0\n\u00de + 663 \u00bc 128 + 198 \u00bc 326\nR66 \u00bc 266500\n326\nMFLOPS \u00bc 202 MFLOPS\nThe peak number, ignoring start-up overhead, is 1.64 times higher than this\nestimate of sustained performance on the real vector lengths. In actual practice,\nthe Linpack benchmark contains a nontrivial fraction of code that cannot be\nvectorized. Although this code accounts for less than 20% of the time before\nvectorization, it runs at less than one-tenth of the performance when counted\nas FLOPS. Thus, Amdahl\u2019s law tells us that the overall performance will be\nsignificantly lower than the performance estimated from analyzing the\ninner loop.\nSince vector length has a significant impact on performance, the N1/2 and Nv\nmeasures are often used in comparing vector machines.\nExample\nWhat is N1/2 for just the inner loop of DAXPY for VMIPS with a 500 MHz clock?\nAnswer\nUsing R\u221eas the peak rate, we want to know the vector length that will achieve\nabout 125 MFLOPS. We start with the formula for MFLOPS assuming that the\nmeasurement is made for N1/2 elements:\nMFLOPS \u00bc\nFLOPS executed in N1=2 iterations\nC1ock cyc1es to execute N1=2 iterationsC1ock cycles\nSecond\n10\u00046\n125 \u00bc 2N1=2\nTN1=2\n500\nSimplifying this and then assuming N1/2<64, so that TN1=2<64 \u00bc 64 + 3n, yields:\nTN1=2 \u00bc 8N1=2\n64 + 3N1=2 \u00bc 8N1=2\n5N1=2 \u00bc 64\nN1=2 \u00bc 12:8\nSo N1/2\u00bc13; that is, a vector of length 13 gives approximately one-half the peak\nperformance for the DAXPY loop on VMIPS.\nG-18\n\u25a0\nAppendix G Vector Processors in More Depth"
    },
    {
        "page": 1086,
        "text": "Example\nWhatisthevectorlength,Nv,suchthatthevectoroperationrunsfasterthanthescalar?\nAnswer\nAgain, we know that Nv<64. The time to do one iteration in scalar mode can be\nestimated as 10+12+12+7+6+12\u00bc59 clocks, where 10 is the estimate of the\nloop overhead, known to be somewhat less than the strip-mining loop overhead.\nIn the last problem, we showed that this vector loop runs in vector mode in time\nTn\u000364\u00bc64+3n clock cycles. Therefore,\n64 + 3Nv \u00bc 59Nv\nNv \u00bc 64\n56\n\u0001\n\u0003\nNv \u00bc 2\nFor the DAXPY loop, vector mode is faster than scalar as long as the vector has at\nleast two elements. This number is surprisingly small.\nDAXPY Performance on an Enhanced VMIPS\nDAXPY, like many vector problems, is memory limited. Consequently, perfor-\nmance could be improved by adding more memory access pipelines. This is the\nmajor architectural difference between the Cray X-MP (and later processors)\nand the Cray-1. The Cray X-MP has three memory pipelines, compared with\nthe Cray-1\u2019s single memory pipeline, and the X-MP has more flexible chaining.\nHow does this affect performance?\nExample\nWhat would be the value of T66 for DAXPY on VMIPS if we added two more\nmemory pipelines?\nAnswer\nWith three memory pipelines, all the instructions fit in one convoy and take one\nchime. The start-up overheads are the same, so\nT66 \u00bc 66\n64\n\u0001\n\u0003\n Tloop + Tstart\n\u0004\n\u0005\n+ 66Tchime\nT66 \u00bc 2 15 + 49\n\u00f0\n\u00de + 661 \u00bc 194\nWith three memory pipelines, we have reduced the clock cycle count for sustained\nperformance from 326 to 194, a factor of 1.7. Note the effect of Amdahl\u2019s law: We\nimproved the theoretical peak rate as measured by the number of chimes by a factor\nof 3, but only achieved an overall improvement of a factor of 1.7 in sustained\nperformance.\nG.6\nPutting It All Together: Performance of Vector Processors\n\u25a0\nG-19"
    },
    {
        "page": 1087,
        "text": "Another improvement could come from allowing different convoys to overlap\nand also allowing the scalar loop overhead to overlap with the vector instructions.\nThis requires that one vector operation be allowed to begin using a functional unit\nbefore another operation has completed, which complicates the instruction issue\nlogic. Allowing this overlap eliminates the separate start-up overhead for every\nconvoy except the first and hides the loop overhead as well.\nTo achieve the maximum hiding of strip-mining overhead, we need to be able\nto overlap strip-mined instances of the loop, allowing two instances of a convoy as\nwell as possibly two instances of the scalar code to be in execution simulta-\nneously. This requires the same techniques we looked at in Chapter 3 to avoid\nWAR hazards, although because no overlapped read and write of a single vector\nelement is possible, copying can be avoided. This technique, called tailgating,\nwas used in the Cray-2. Alternatively, we could unroll the outer loop to create\nseveral instances of the vector sequence using different register sets (assuming\nsufficient registers), just as we did in Chapter 3. By allowing maximum overlap\nof the convoys and the scalar loop overhead, the start-up and loop overheads will\nonly be seen once per vector sequence, independent of the number of convoys and\nthe instructions in each convoy. In this way, a processor with vector registers can\nhave both low start-up overhead for short vectors and high peak performance for\nvery long vectors.\nExample\nWhat would be the values of R\u221eand T66 for DAXPY on VMIPS if we added two\nmore memory pipelines and allowed the strip-mining and start-up overheads to be\nfully overlapped?\nAnswer\nR\u221e\u00bc lim\nn!\u221e\nOperations per iterationC1ock rate\nC1ock cyc1es per iteration\n\u0006\n\u0007\nlim\nn!\u221eClock cycles per iteration\n\u00f0\n\u00de \u00bc lim\nn!\u221e\nTn\nn\n\u0006\n\u0007\nSince the overhead is only seen once, Tn\u00bcn+49+15\u00bcn+64. Thus,\nlim\nn!\u221e\nTn\nn\n\u0006\n\u0007\n\u00bc lim\nn!\u221e\nn + 64\nn\n\u0006\n\u0007\n\u00bc 1\nR\u221e\u00bc 2500 MHz\n1\n\u00bc 1000 MFLOPS\nAdding the extra memory pipelines and more flexible issue logic yields an\nimprovement in peak performance of a factor of 4. However, T66\u00bc130, so for\nshorter\nvectors\nthe\nsustained\nperformance\nimprovement\nis\nabout\n326/\n130\u00bc2.5 times.\nG-20\n\u25a0\nAppendix G Vector Processors in More Depth"
    },
    {
        "page": 1088,
        "text": "In summary, we have examined several measures of vector performance.\nTheoretical peak performance can be calculated based purely on the value of\nTchime as:\nNumber of FLOPS per iterationClock rate\nTchime\nBy including the loop overhead, we can calculate values for peak performance for\nan infinite-length vector (R\u221e) and also for sustained performance, Rn for a vector\nof length n, which is computed as:\nRn \u00bc Number of FLOPS per iterationnClock rate\nTn\nUsing these measures we also can find N1/2 and Nv, which give us another way of\nlooking at the start-up overhead for vectors and the ratio of vector to scalar speed. A\nwide variety of measures of performance of vector processors is useful in under-\nstanding the range of performance that applications may see on a vector processor.\nG.7\nA Modern Vector Supercomputer: The Cray X1\nThe Cray X1 was introduced in 2002, and, together with the NEC SX/8, represents\nthe state of the art in modern vector supercomputers. The X1 system architecture\nsupports thousands of powerful vector processors sharing a single global memory.\nThe Cray X1 has an unusual processor architecture, shown in Figure G.11. A\nlarge Multi-Streaming Processor (MSP) is formed by ganging together four Single-\nStreaming Processors (SSPs). Each SSP is a complete single-chip vector micropro-\ncessor, containing a scalar unit, scalar caches, and a two-lane vector unit. The SSP\nscalar unit is a dual-issue out-of-order superscalar processor with a 16 KB instruc-\ntion cache and a 16 KB scalar write-through data cache, both two-way set associa-\ntive with 32-byte cache lines. The SSP vector unit contains a vector register file,\nthree vector arithmetic units, and one vector load-store unit. It is much easier to\npipeline deeply a vector functional unit than a superscalar issue mechanism, so\nthe X1 vector unit runs at twice the clock rate (800 MHz) of the scalar unit\n(400 MHz). Each lane can perform a 64-bit floating-point add and a 64-bit\nfloating-point multiply each cycle, leading to a peak performance of 12.8 GFLOPS\nper MSP.\nAll previous Cray machines could trace their instruction set architecture (ISA)\nlineage back to the original Cray-1 design from 1976, with 8 primary registers each\nfor addresses, scalar data, and vector data. For the X1, the ISA was redesigned from\nscratch to incorporate lessons learned over the last 30 years of compiler and micro-\narchitecture research. The X1 ISA includes 64 64-bit scalar address registers and\n64 64-bit scalar data registers, with 32 vector data registers (64 bits per element)\nand 8 vector mask registers (1 bit per element). The large increase in the number of\nregisters allows the compiler to map more program variables into registers to\nreduce memory traffic and also allows better static scheduling of code to improve\nG.7\nA Modern Vector Supercomputer: The Cray X1\n\u25a0\nG-21"
    },
    {
        "page": 1089,
        "text": "run time overlap of instruction execution. Earlier Crays had a compact variable-\nlength instruction set, but the X1 ISA has fixedlength instructions to simplify\nsuperscalar fetch and decode.\nFour SSP chips are packaged on a multichip module together with four cache\nchips implementing an external 2 MB cache (Ecache) shared by all the SSPs. The\nEcache is two-way set associative with 32-byte lines and a write-back policy. The\nEcache can be used to cache vectors, reducing memory traffic for codes that exhibit\ntemporal locality. The ISA also provides vector load and store instruction variants\nthat do not allocate in cache to avoid polluting the Ecache with data that is known\nto have low locality. The Ecache has sufficient bandwidth to supply one 64-bit\nword per lane per 800 MHz clock cycle, or over 50 GB/sec per MSP.\nAt the next level of the X1 packaging hierarchy, shown in Figure G.12, four\nMSPs are placed on a single printed circuit board together with 16 memory con-\ntroller chips and DRAM to form an X1 node. Each memory controller chip has\neight separate Rambus DRAM channels, where each channel provides 1.6 GB/\nsec of memory bandwidth. Across all 128 memory channels, the node has over\n200 GB/sec of main memory bandwidth.\nAn X1 system can contain up to 1024 nodes (4096 MSPs or 16,384 SSPs),\nconnected via a very high-bandwidth global network. The network connections\nare made via the memory controller chips, and all memory in the system is directly\naccessible from any processor using load and store instructions. This provides\nmuch faster global communication than the message-passing protocols used in\ncluster-based systems. Maintaining cache coherence across such a large number\nof high-bandwidth shared-memory nodes would be challenging. The approach\ntaken in the X1 is to restrict each Ecache to cache data only from the local node\nDRAM. The memory controllers implement a directory scheme to maintain\nMSP\nSSP\nS\nS\nSuperscalar unit\nV\nVector unit\nV\nV\nSSP\nS\nV\nV\nSSP\nS\nV\nV\nSSP\nS\nV\nV\n0.5 MB\nEcache\n0.5 MB\nEcache\n0.5 MB\nEcache\n0.5 MB\nEcache\nFigure G.11 Cray MSP module. (From Dunnigan et al. [2005].)\nG-22\n\u25a0\nAppendix G Vector Processors in More Depth"
    },
    {
        "page": 1090,
        "text": "coherency between the four Ecaches on a node. Accesses from remote nodes will\nobtain the most recent version of a location, and remote stores will invalidate local\nEcaches before updating memory, but the remote node cannot cache these local\nlocations.\nVector loads and stores are particularly useful in the presence of long-latency\ncache misses and global communications, as relatively simple vector hardware can\ngenerate and track a large number of in-flight memory requests. Contemporary\nsuperscalar microprocessors support only 8 to 16 outstanding cache misses,\nwhereas each MSP processor can have up to 2048 outstanding memory requests\n(512 per SSP). To compensate, superscalar microprocessors have been moving\nto larger cache line sizes (128 bytes and above) to bring in more data with each\ncache miss, but this leads to significant wasted bandwidth on non-unit stride\naccesses over large datasets. The X1 design uses short 32-byte lines throughout\nto reduce bandwidth waste and instead relies on supporting many independent\ncache misses to sustain memory bandwidth. This latency tolerance together with\nthe huge memory bandwidth for non-unit strides explains why vector machines can\nprovide large speedups over superscalar microprocessors for certain codes.\nMulti-Streaming Processors\nThe Multi-Streaming concept was first introduced by Cray in the SV1, but has been\nconsiderably enhanced in the X1. The four SSPs within an MSP share Ecache, and\nthere is hardware support for barrier synchronization across the four SSPs within\nan MSP. Each X1 SSP has a two-lane vector unit with 32 vector registers each\nholding 64 elements. The compiler has several choices as to how to use the SSPs\nwithin an MSP.\nM\nmem\nM\nmem\nM\nmem\nM\nmem\nM\nmem\nM\nmem\nM\nmem\nM\nmem\nM\nmem\nM\nmem\nM\nmem\nM\nmem\nM\nmem\nM\nmem\nM\nmem\nM\nmem\n51 GFLOPS, 200 GB/sec\nIO\nIO\nIO\nP\nP\nP\nP\nS\nS\nS\nS\nP\nP\nP\nP\nS\nS\nS\nS\nP\nP\nP\nP\nS\nS\nS\nS\nP\nP\nP\nP\nS\nS\nS\nS\nFigure G.12 Cray X1 node. (From Tanqueray [2002].)\nG.7\nA Modern Vector Supercomputer: The Cray X1\n\u25a0\nG-23"
    },
    {
        "page": 1091,
        "text": "The simplest use is to gang together four two-lane SSPs to emulate a single\neight-lane vector processor. The X1 provides efficient barrier synchronization\nprimitives between SSPs on a node, and the compiler is responsible for generating\nthe MSP code. For example, for a vectorizable inner loop over 1000 elements, the\ncompiler will allocate iterations 0\u2013249 to SSP0, iterations 250\u2013499 to SSP1, iter-\nations 500\u2013749 to SSP2, and iterations 750\u2013999 to SSP3. Each SSP can process its\nloop iterations independently but must synchronize back with the other SSPs\nbefore moving to the next loop nest.\nIf inner loops do not have many iterations, the eight-lane MSP will have low\nefficiency, as each SSP will have only a few elements to process and execution\ntime will be dominated by start-up time and synchronization overheads. Another\nway to use an MSP is for the compiler to parallelize across an outer loop, giving\neach SSP a different inner loop to process. For example, the following nested loops\nscale the upper triangle of a matrix by a constant:\n/* Scale upper triangle by constant K. */\nfor (row = 0; row < MAX_ROWS; row++)\nfor (col = row; col < MAX_COLS; col++)\nA[row][col] = A[row][col] * K;\nConsider the case where MAX_ROWS and MAX_COLS are both 100 elements.\nThe vector length of the inner loop steps down from 100 to 1 over the iterations of\nthe outer loop. Even for the first inner loop, the loop length would be much less\nthan the maximum vector length (256) of an eight-lane MSP, and the code would\ntherefore be inefficient. Alternatively, the compiler can assign entire inner loops to\na single SSP. For example, SSP0 might process rows 0, 4, 8, and so on, while SSP1\nprocesses rows 1, 5, 9, and so on. Each SSP now sees a longer vector. In effect, this\napproach parallelizes the scalar overhead and makes use of the individual scalar\nunits within each SSP.\nMost application code uses MSPs, but it is also possible to compile code to use\nall the SSPs as individual processors where there is limited vector parallelism but\nsignificant thread-level parallelism.\nCray X1E\nIn 2004, Cray announced an upgrade to the original Cray X1 design. The X1E\nuses newer fabrication technology that allows two SSPs to be placed on a single\nchip, making the X1E the first multicore vector microprocessor. Each physical\nnode now contains eight MSPs, but these are organized as two logical nodes\nof four MSPs each to retain the same programming model as the X1. In addition,\nthe clock rates were raised from 400 MHz scalar and 800 MHz vector to\n565 MHz scalar and 1130 MHz vector, giving an improved peak performance\nof 18 GFLOPS.\nG-24\n\u25a0\nAppendix G Vector Processors in More Depth"
    },
    {
        "page": 1092,
        "text": "G.8\nConcluding Remarks\nDuring the 1980s and 1990s, rapid performance increases in pipelined scalar pro-\ncessors led to a dramatic closing of the gap between traditional vector supercom-\nputers and fast, pipelined, superscalar VLSI microprocessors. In 2011, it is possible\nto buy a laptop computer for under $1000 that has a higher CPU clock rate than any\navailable vector supercomputer, even those costing tens of millions of dollars.\nAlthough the vector supercomputers have lower clock rates, they support greater\nparallelism using multiple lanes (up to 16 in the Japanese designs) versus the lim-\nited multiple issue of the superscalar microprocessors. Nevertheless, the peak\nfloating-point performance of the low-cost microprocessors is within a factor of\ntwo of the leading vector supercomputer CPUs. Of course, high clock rates and\nhigh peak performance do not necessarily translate into sustained application\nperformance. Main memory bandwidth is the key distinguishing feature between\nvector supercomputers and superscalar microprocessor systems.\nProviding this large non-unit stride memory bandwidth is one of the major\nexpenses in a vector supercomputer, and traditionally SRAM was used as main\nmemory to reduce the number of memory banks needed and to reduce vector\nstart-up penalties. While SRAM has an access time several times lower than that\nof DRAM, it costs roughly 10 times as much per bit! To reduce main memory costs\nand to allow larger capacities, all modern vector supercomputers now use DRAM\nfor main memory, taking advantage of new higher-bandwidth DRAM interfaces\nsuch as synchronous DRAM.\nThis adoption of DRAM for main memory (pioneered by Seymour Cray in the\nCray-2) is one example of how vector supercomputers have adapted commodity\ntechnology to improve their price-performance. Another example is that vector\nsupercomputers are now including vector data caches. Caches are not effective\nfor all vector codes, however, so these vector caches are designed to allow high\nmain memory bandwidth even in the presence of many cache misses. For example,\nthe Cray X1 MSP can have 2048 outstanding memory loads; for microprocessors,\n8 to 16 outstanding cache misses per CPU are more typical maximum numbers.\nAnother example is the demise of bipolar ECL or gallium arsenide as technol-\nogies of choice for supercomputer CPU logic. Because of the huge investment in\nCMOS technology made possible by the success of the desktop computer, CMOS\nnow offers competitive transistor performance with much greater transistor density\nand much reduced power dissipation compared with these more exotic technolo-\ngies. As a result, all leading vector supercomputers are now built with the same\nCMOS technology as superscalar microprocessors. The primary reason why vector\nsupercomputers have lower clock rates than commodity microprocessors is that\nthey are developed using standard cell ASIC techniques rather than full custom\ncircuit design to reduce the engineering design cost. While a microprocessor\ndesign may sell tens of millions of copies and can amortize the design cost over\nthis large number of units, a vector supercomputer is considered a success if over\na hundred units are sold!\nG.8\nConcluding Remarks\n\u25a0\nG-25"
    },
    {
        "page": 1093,
        "text": "Conversely, via superscalar microprocessor designs have begun to absorb\nsome of the techniques made popular in earlier vector computer systems, such\nas with the Multimedia SIMD extensions. As we showed in Chapter 4, the invest-\nment in hardware for SIMD performance is increasing rapidly, perhaps even more\nthan for multiprocessors. If the even wider SIMD units of GPUs become well inte-\ngrated with the scalar cores, including scatter-gather support, we may well con-\nclude that vector architectures have won the architecture wars!\nG.9\nHistorical Perspective and References\nThis historical perspective adds some details and references that were left out of the\nversion in Chapter 4.\nThe CDC STAR processor and its descendant, the CYBER 205, were memory-\nmemory vector processors. To keep the hardware simple and support the high\nbandwidth requirements (up to three memory references per floating-point opera-\ntion), these processors did not efficiently handle non-unit stride. While most loops\nhave unit stride, a non-unit stride loop had poor performance on these processors\nbecause memory-to-memory data movements were required to gather together\n(and scatter back) the nonadjacent vector elements; these operations used special\nscatter-gather instructions. In addition, there was special support for sparse vectors\nthat used a bit vector to represent the zeros and nonzeros and a dense vector of\nnonzero values. These more complex vector operations were slow because of\nthe long memory latency, and it was often faster to use scalar mode for sparse\nor non-unit stride operations. Schneck [1987] described several of the early pipe-\nlined processors (e.g., Stretch) through the first vector processors, including the\n205 and Cray-1. Dongarra [1986] did another good survey, focusing on more\nrecent processors.\nThe 1980s also saw the arrival of smaller-scale vector processors, called\nmini-supercomputers. Priced at roughly one-tenth the cost of a supercomputer\n($0.5 to $1 million versus $5 to $10 million), these processors caught on quickly.\nAlthough many companies joined the market, the two companies that were most\nsuccessful were Convex and Alliant. Convex started with the uniprocessor C-1\nvector processor and then offered a series of small multiprocessors, ending with\nthe C-4 announced in 1994. The keys to the success of Convex over this period\nwere their emphasis on Cray software capability, the effectiveness of their com-\npiler (see Figure G.9), and the quality of their UNIX OS implementation. The\nC-4 was the last vector machine Convex sold; they switched to making large-scale\nmultiprocessors using Hewlett-Packard RISC microprocessors and were bought by\nHP in 1995. Alliant [1987] concentrated more on the multiprocessor aspects; they\nbuilt an eight-processor computer, with each processor offering vector capability.\nAlliant ceased operation in the early 1990s.\nIn the early 1980s, CDC spun out a group, called ETA, to build a new super-\ncomputer, the ETA-10, capable of 10 GFLOPS. The ETA processor was delivered\nin the late 1980s (see Fazio [1987]) and used low-temperature CMOS in a\nG-26\n\u25a0\nAppendix G Vector Processors in More Depth"
    },
    {
        "page": 1094,
        "text": "configuration with up to 10 processors. Each processor retained the memory-\nmemory architecture based on the CYBER 205. Although the ETA-10 achieved\nenormous peak performance, its scalar speed was not comparable. In 1989,\nCDC, the first supercomputer vendor, closed ETA and left the supercomputer\ndesign business.\nIn 1986, IBM introduced the System/370 vector architecture (see Moore et al.\n[1987]) and its first implementation in the 3090 Vector Facility. The architecture\nextended the System/370 architecture with 171 vector instructions. The 3090/VF\nwas integrated into the 3090 CPU. Unlike most other vector processors of the time,\nthe 3090/VF routed its vectors through the cache. The IBM 370 machines contin-\nued to evolve over time and are now called the IBM zSeries. The vector extensions\nhave been removed from the architecture and some of the opcode space was reused\nto implement 64-bit address extensions.\nIn late 1989, Cray Research was split into two companies, both aimed at build-\ning high-end processors available in the early 1990s. Seymour Cray headed the\nspin-off, Cray Computer Corporation, until its demise in 1995. Their initial pro-\ncessor, the Cray-3, was to be implemented in gallium arsenide, but they were\nunable to develop a reliable and cost-effective implementation technology. A sin-\ngle Cray-3 prototype was delivered to the National Center for Atmospheric\nResearch (NCAR) for evaluation purposes in 1993, but no paying customers were\nfound for the design. The Cray-4 prototype, which was to have been the first pro-\ncessor to run at 1 GHz, was close to completion when the company filed for bank-\nruptcy. Shortly before his tragic death in a car accident in 1996, Seymour Cray\nstarted yet another company, SRC Computers, to develop high-performance sys-\ntems but this time using commodity components. In 2000, SRC announced the\nSRC-6 system, which combined 512 Intel microprocessors, 5 billion gates of\nreconfigurable logic, and a high-performance vector-style memory system.\nCray Research focused on the C90, a new high-end processor with up to 16\nprocessors and a clock rate of 240 MHz. This processor was delivered in 1991.\nThe J90 was a CMOS-based vector machine using DRAM memory starting at\n$250,000, but with typical configurations running about $1 million. In mid-\n1995, Cray Research was acquired by Silicon Graphics, and in 1998 released\nthe SV1 system, which grafted considerably faster CMOS processors onto the\nJ90 memory system, and which also added a data cache for vectors to each\nCPU to help meet the increased memory bandwidth demands. The SV1 also intro-\nduced the MSP concept, which was developed to provide competitive single-CPU\nperformance by ganging together multiple slower CPUs. Silicon Graphics sold\nCray Research to Tera Computer in 2000, and the joint company was renamed\nCray Inc.\nThe basis for modern vectorizing compiler technology and the notion of data\ndependence was developed by Kuck and his colleagues [1974] at the University of\nIllinois. Banerjee [1979] developed the test named after him. Padua and Wolfe\n[1986] gave a good overview of vectorizing compiler technology.\nBenchmark studies of various supercomputers, including attempts to under-\nstand the performance differences, have been undertaken by Lubeck, Moore,\nG.9\nHistorical Perspective and References\n\u25a0\nG-27"
    },
    {
        "page": 1095,
        "text": "and Mendez [1985], Bucher [1983], and Jordan [1987]. There are several bench-\nmark suites aimed at scientific usage and often employed for supercomputer\nbenchmarking, including Linpack and the Lawrence Livermore Laboratories FOR-\nTRAN kernels. The University of Illinois coordinated the collection of a set of\nbenchmarks for supercomputers, called the Perfect Club. In 1993, the Perfect Club\nwas integrated into SPEC, which released a set of benchmarks, SPEChpc96, aimed\nat high-end scientific processing in 1996. The NAS parallel benchmarks developed\nat the NASA Ames Research Center [Bailey et al. 1991] have become a popular set\nof kernels and applications used for supercomputer evaluation. A new benchmark\nsuite, HPC Challenge, was introduced consisting of a few kernels that stress\nmachine memory and interconnect bandwidths in addition to floating-point perfor-\nmance [Luszczek et al. 2005]. Although standard supercomputer benchmarks are\nuseful as a rough measure of machine capabilities, large supercomputer purchases\nare generally preceded by a careful performance evaluation on the actual mix of\napplications required at the customer site.\nReferences\nAlliant Computer Systems Corp, 1987. Alliant FX/Series: Product Summary. Mass, Acton (June).\nAsanovic, K., 1998. Vector microprocessors,\u201d Ph.D. thesis, Computer Science Division. University of\nCalifornia at Berkeley (May).\nBailey, D.H., Barszcz, E., Barton, J.T., Browning, D.S., Carter, R.L., Dagum, L., Fatoohi, R.A.,\nFrederickson, P.O., Lasinski, T.A., Schreiber, R.S., Simon, H.D., Venkatakrishnan, V.,\nWeeratunga, S.K., 1991. The NAS parallel benchmarks. Int\u2019l. J. Supercomputing Applications\n5, 63\u201373.\nBanerjee, U., 1979. Speedup of ordinary programs,\u201d Ph.D. thesis, Department of Computer Science.\nUniversity of Illinois at Urbana-Champaign (October).\nBaskett, F., Keller, T.W., 1977. An Evaluation of the Cray-1 Processor. In: Kuck, D.J., Lawrie, D.H.,\nSameh, A.H. (Eds.), High Speed Computer and Algorithm Organization. Academic Press, San\nDiego, pp. 71\u201384.\nBrandt, M., Brooks, J., Cahir, M., Hewitt, T., Lopez-Pineda, E., Sandness, D., 2000. The Benchmarker\u2019s\nGuide for Cray SV1 Systems. Cray Inc., Seattle, Wash.\nBucher, I.Y., 1983. The computational speed of supercomputers. In: Proc. ACM SIGMETRICS Conf.\non Measurement and Modeling of Computer Systems, August 29\u201331, 1983. Minneapolis, Minn,\npp. 151\u2013165.\nCallahan, D., Dongarra, J., Levine, D., 1988. Vectorizing compilers: A test suite and results.\nIn: Supercomputing \u201988: Proceedings of the 1988 ACM/IEEE Conference on Supercomputing,\nNovember 12\u201317, pp. 98\u2013105. Orlando, FL.\nChen, S., 1983. Large-scale and high-speed multiprocessor system for scientific applications.\nIn: Hwang, K. (Ed.), Superprocessors: Design and applications. Proc. NATO Advanced Research\nWorkshop on High-Speed Computing, June 20\u201322, 1983, Julich, Kernforschungsanlage, Federal\nRepublic of Germany. IEEE, (August), 1984.\nDongarra, J.J., 1986. A survey of high performance processors. COMPCON, IEEE, 8\u201311 (March).\nDunnigan, T.H., Vetter, J.S., White III, J.B., Worley, P.H., 2005. Performance evaluation of the Cray\nX1 distributed shared-memory architecture. IEEE Micro 25 (1 (January\u2013February)), 30\u201340.\nFazio, D., 1987. It\u2019s really much more fun building a supercomputer than it is simply inventing one.\nCOMPCON, IEEE, 102\u2013105 (February).\nFlynn, M.J., 1966. Very high-speed computing systems. In: Proc. IEEE 54:12 (December),\npp. 1901\u20131909.\nHintz, R.G., Tate, D.P., 1972. Control data STAR-100 processor design. COMPCON, IEEE 1\u20134\n(September).\nJordan, K.E., 1987. Performance comparison of large-scale scientific processors: Scalar mainframes,\nmainframes with vector facilities, and supercomputers. Computer 20 (3 (March)), 10\u201323.\nG-28\n\u25a0\nAppendix G Vector Processors in More Depth"
    },
    {
        "page": 1096,
        "text": "Kitagawa, K., Tagaya, S., Hagihara, Y., Kanoh, Y., 2003. A hardware overview of SX-6 and SX-7\nsupercomputer. NEC Research & Development J 44 (1 (January)), 2\u20137.\nKuck, D., Budnik, P.P., Chen, S.-C., Lawrie, D.H., Towle, R.A., Strebendt, R.E., Davis Jr., E.W.,\nHan, J., Kraska, P.W., Muraoka, Y., 1974. Measurements of parallelism in ordinary FORTRAN\nprograms. Computer 7 (1 (January)), 37\u201346.\nLincoln, N.R., 1982. Technology and design trade offs in the creation of a modern supercomputer. IEEE\nTrans. on Computers, 363\u2013376. C-31:5 (May).\nLubeck, O., Moore, J., Mendez, R., 1985. A benchmark comparison of three supercomputers: Fujitsu\nVP-200, Hitachi S810/20, and Cray X-MP/2. Computer 18 (1 (January)), 10\u201329.\nLuszczek, P., Dongarra, J.J., Koester, D., Rabenseifner, R., Lucas, B., Kepner, J., McCalpin, J., Bailey, D.,\nTakahashi, D., 2005. In: Introduction to the HPC challenge benchmark suite,\u201d Lawrence Berkeley\nNational Laboratory, Paper LBNL-57493 (April 25). http://repositories.cdlib.org/lbnl/LBNL-57493.\nMiranker, G.S., Rubenstein, J., Sanguinetti, J., 1988. Squeezing a Cray-class supercomputer into a\nsingle-user package. COMPCON, IEEE, 452\u2013456 (March).\nMiura, K., Uchida, K., 1983. FACOM vector processing system: VP100/200. In: Proc. NATO\nAdvanced Research Workshop on High-Speed Computing. June 20\u201322, 1983, Julich, Ker-\nnforschungsanlage, Federal Republic of Germany; also in K. Hwang, ed., \u201cSuperprocessors: Design\nand applications,\u201d IEEE (August), 1984, 59\u201373.\nMoore, B., Padegs, A., Smith, R., Bucholz, W., 1987. Concepts of the System/370 vector architecture.\nIn: Proc. 14th Int\u2019l. Symposium on Computer Architecture, June 3\u20136, 1987. Pittsburgh, Penn,\npp. 282\u2013292.\nPadua, D., Wolfe, M., 1986. Advanced compiler optimizations for supercomputers. Comm. ACM\n29 (12 (December)), 1184\u20131201.\nRussell, R.M., 1978. The Cray-1 processor system. Comm. of the ACM 21 (1 (January)), 63\u201372.\nSchneck, P.B., 1987. Superprocessor Architecture. Kluwer Academic Publishers, Norwell, Mass.\nSmith, B.J., 1981. Architecture and applications of the HEP multiprocessor system. Real-Time Signal\nProcessing IV 298, 241\u2013248. August.\nSporer, M., Moss, F.H., Mathais, C.J., 1988. An introduction to the architecture of the Stellar Graphics\nsupercomputer. COMPCON, IEEE 464 (March).\nTanqueray, D., 2002. The Cray X1 and supercomputer road map. In: Proc. 13th Daresbury Machine\nEvaluation Workshop, December 11\u201312. Cheshire, England.\nVajapeyam, S., 1991. Instruction-level characterization of the Cray Y-MP processor. Ph.D. thesis, Com-\nputer Sciences Department. University of Wisconsin-Madison.\nWatanabe, T., 1987. Architecture and performance of the NEC supercomputer SX system. Parallel\nComputing 5, 247\u2013255.\nWatson, W.J., 1972. The TI ASC\u2014a highly modular and flexible super processor architecture. In: Proc.\nAFIPS Fall Joint Computer Conf, pp. 221\u2013228.\nExercises\nIn these exercises assume VMIPS has a clock rate of 500 MHz and that Tloop\u00bc15.\nUse the start-up times from Figure G.2, and assume that the store latency is always\nincluded in the running time.\nG.1\n[10]<G.1, G.2>Write a VMIPS vector sequence that achieves the peak MFLOPS\nperformance of the processor (use the functional unit and instruction description in\nSection G.2). Assuming a 500-MHz clock rate, what is the peak MFLOPS?\nG.2\n[20/15/15]<G.1\u2013G.6>Consider the following vector code run on a 500 MHz\nversion of VMIPS for a fixed vector length of 64:\nLV\nV1,Ra\nMULV.D\nV2,V1,V3\nADDV.D\nV4,V1,V3\nSV\nRb,V2\nSV\nRc,V4\nExercises\n\u25a0\nG-29"
    },
    {
        "page": 1097,
        "text": "Ignore all strip-mining overhead, but assume that the store latency must be\nincluded in the time to perform the loop. The entire sequence produces 64 results.\na. [20]<G.1\u2013G.4>Assuming no chaining and a single memory pipeline, how\nmany chimes are required? How many clock cycles per result (including both\nstores as one result) does this vector sequence require, including start-up\noverhead?\nb. [15]<G.1\u2013G.4>If the vector sequence is chained, how many clock cycles per\nresult does this sequence require, including overhead?\nc. [15]<G.1\u2013G.6>Suppose VMIPS had three memory pipelines and chaining.\nIf there were no bank conflicts in the accesses for the above loop, how many\nclock cycles are required per result for this sequence?\nG.3\n[20/20/15/15/20/20/20]<G.2\u2013G.6>Consider the following FORTRAN code:\ndo 10 i=1,n\nA(i)=A(i)+B(i)\nB(i)=x * B(i)\n10\ncontinue\nUse the techniques of Section G.6 to estimate performance throughout this\nexercise, assuming a 500 MHz version of VMIPS.\na. [20]<G.2\u2013G.6>Write the best VMIPS vector code for the inner portion of the\nloop. Assume x is in F0 and the addresses of A and B are in Ra and Rb,\nrespectively.\nb. [20]<G.2\u2013G.6>Find the total time for this loop on VMIPS (T100). What is the\nMFLOPS rating for the loop (R100)?\nc. [15]<G.2\u2013G.6>Find R\u221efor this loop.\nd. [15]<G.2\u2013G.6>Find N1/2 for this loop.\ne. [20]<G.2\u2013G.6>Find Nv for this loop. Assume the scalar code has been pipe-\nline scheduled so that each memory reference takes six cycles and each FP oper-\nation takes three cycles. Assume the scalar overhead is also Tloop.\nf. [20]<G.2\u2013G.6>Assume VMIPS has two memory pipelines. Write vector code\nthat takes advantage of the second memory pipeline. Show the layout in\nconvoys.\ng. [20]<G.2\u2013G.6>Compute T100 and R100 for VMIPS with two memory\npipelines.\nG.4\n[20/10]<G.2>Suppose we have a version of VMIPS with eight memory banks\n(each a double word wide) and a memory access time of eight cycles.\na. [20]<G.2>If a load vector of length 64 is executed with a stride of 20 double\nwords, how many cycles will the load take to complete?\nb. [10]<G.2>What percentage of the memory bandwidth do you achieve on a\n64-element load at stride 20 versus stride 1?\nG-30\n\u25a0\nAppendix G Vector Processors in More Depth"
    },
    {
        "page": 1098,
        "text": "G.5\n[12/12]<G.5\u2013G.6>Consider the following loop:\nC=0.0\ndo 10 i=1,64\nA(i)=A(i)+B(i)\nC=C+A(i)\n10\ncontinue\na. [12]<G.5\u2013G.6>Split the loop into two loops: one with no dependence and one\nwith a dependence. Write these loops in FORTRAN\u2014as a source-to-source\ntransformation. This optimization is called loop fission.\nb. [12]<G.5\u2013G.6>Write the VMIPS vector code for the loop without a\ndependence.\nG.6\n[20/15/20/20]<G.5\u2013G.6>The compiled Linpack performance of the Cray-1\n(designed in 1976) was almost doubled by a better compiler in 1989.\nLet\u2019s look at a simple example of how this might occur. Consider\nthe DAXPY-like loop (where k is a parameter to the procedure containing\nthe loop):\ndo 10 i=1,64\ndo 10 j=1,64\nY(k,j)=a*X(i,j)+Y(k,j)\n10\ncontinue\na. [20]<G.5\u2013G.6>Write the straightforward code sequence for just the inner\nloop in VMIPS vector instructions.\nb. [15]<G.5\u2013G.6>Using the techniques of Section G.6, estimate the perfor-\nmance of this code on VMIPS by finding T64 in clock cycles. You may assume\nthat Tloop of overhead is incurred for each iteration of the outer loop. What limits\nthe performance?\nc. [20]<G.5\u2013G.6>Rewrite the VMIPS code to reduce the performance limita-\ntion; show the resulting inner loop in VMIPS vector instructions. (Hint: Think\nabout what establishes Tchime; can you affect it?) Find the total time for the\nresulting sequence.\nd. [20]<G.5\u2013G.6>Estimate the performance of your new version, using the\ntechniques of Section G.6 and finding T64.\nG.7\n[15/15/25]<G.4>Consider the following code:\ndo 10 i=1,64\nif (B(i) .ne. 0) then\nA(i)=A(i)/B(i)\n10\ncontinue\nAssume that the addresses of A and B are in Ra and Rb, respectively, and that F0\ncontains 0.\nExercises\n\u25a0\nG-31"
    },
    {
        "page": 1099,
        "text": "a. [15]<G.4>Write the VMIPS code for this loop using the vector-mask capability.\nb. [15]<G.4>Write the VMIPS code for this loop using scatter-gather.\nc. [25]<G.4>Estimate the performance (T100 in clock cycles) of these two vector\nloops, assuming a divide latency of 20 cycles. Assume that all vector instruc-\ntions run at one result per clock, independent of the setting of the vector-mask\nregister. Assume that 50% of the entries of B are 0. Considering hardware costs,\nwhich would you build if the above loop were typical?\nG.8\n[15/20/15/15]<G.1\u2013G.6>The difference between peak and sustained perfor-\nmance can be large. For one problem, a Hitachi S810 had a peak speed twice as\nhigh as that of the Cray X-MP, while for another more realistic problem, the Cray\nX-MP was twice as fast as the Hitachi processor. Let\u2019s examine why this might\noccur using two versions of VMIPS and the following code sequences:\nC\nCode sequence 1\ndo 10 i=1,10000\nA(i)=x * A(i)+y * A(i)\n10\ncontinue\nC\nCode sequence 2\ndo 10 i=1,100\nA(i)=x * A(i)\n10\ncontinue\nAssume there is a version of VMIPS (call it VMIPS-II) that has two copies of every\nfloating-point functional unit with full chaining among them. Assume that both\nVMIPS and VMIPS-II have two load-store units. Because of the extra functional\nunits and the increased complexity of assigning operations to units, all the over-\nheads (Tloop and Tstart) are doubled for VMIPS-II.\na. [15]<G.1\u2013G.6>Find the number of clock cycles on code sequence 1\non VMIPS.\nb. [20]<G.1\u2013G.6>Find the number of clock cycles on code sequence 1 for\nVMIPS-II. How does this compare to VMIPS?\nc. [15]<G.1\u2013G.6>Find the number of clock cycles on code sequence 2 for VMIPS.\nd. [15]<G.1\u2013G.6>Find the number of clock cycles on code sequence 2 for\nVMIPS-II. How does this compare to VMIPS?\nG.9\n[20]<G.5>Here is a tricky piece of code with two-dimensional arrays. Does this\nloop have dependences? Can these loops be written so they are parallel? If so, how?\nRewrite the source code so that it is clear that the loop can be vectorized, if\npossible.\ndo 290 j=2,n\ndo 290 i=2,j\naa(i,j)=aa(i-1,j)*aa(i-1,j)+bb(i,j)\n290\ncontinue\nG-32\n\u25a0\nAppendix G Vector Processors in More Depth"
    },
    {
        "page": 1100,
        "text": "G.10\n[12/15]<G.5>Consider the following loop:\ndo 10 i=2,n\nA(i)=B\n10\nC(i)=A(i - 1)\na. [12]<G.5>Show there is a loop-carried dependence in this code fragment.\nb. [15]<G.5>Rewrite the code in FORTRAN so that it can be vectorized as two\nseparate vector sequences.\nG.11\n[15/25/25]<G.5>As we saw in Section G.5, some loop structures are not easily\nvectorized. One common structure is a reduction\u2014a loop that reduces an array to a\nsingle value by repeated application of an operation. This is a special case of a\nrecurrence. A common example occurs in dot product:\ndot=0.0\ndo 10 i=1,64\n10\ndot=dot+A(i) * B(i)\nThis loop has an obvious loop-carried dependence (on dot) and cannot be vec-\ntorized in a straightforward fashion. The first thing a good vectorizing compiler\nwould do is split the loop to separate out the vectorizable portion and the recurrence\nand perhaps rewrite the loop as:\ndo 10 i=1,64\n10\ndot(i)=A(i) * B(i)\ndo 20 i=2,64\n20\ndot(1)=dot(1)+dot(i)\nThe variable dot has been expanded into a vector; this transformation is called\nscalar expansion. We can try to vectorize the second loop either relying strictly on\nthe compiler (part (a)) or with hardware support as well (part (b)). There is an\nimportant caveat in the use of vector techniques for reduction. To make reduction\nwork, we are relying on the associativity of the operator being used for the reduc-\ntion. Because of rounding and finite range, however, floating-point arithmetic is\nnot strictly associative. For this reason, most compilers require the programmer\nto indicate whether associativity can be used to more efficiently compile\nreductions.\na. [15]<G.5>One simple scheme for compiling the loop with the recurrence is to\nadd sequences of progressively shorter vectors\u2014two 32-element vectors, then\ntwo 16-element vectors, and so on. This technique has been called recursive\ndoubling. It is faster than doing all the operations in scalar mode. Show how\nthe FORTRAN code would look for execution of the second loop in the preced-\ning code fragment using recursive doubling.\nb. [25]<G.5>In some vector processors, the vector registers are addressable, and\nthe operands to a vector operation may be two different parts of the same vector\nregister. This allows another solution for the reduction, called partial sums.\nExercises\n\u25a0\nG-33"
    },
    {
        "page": 1101,
        "text": "The key idea in partial sums is to reduce the vector to m sums where m is the\ntotal latency through the vector functional unit, including the operand read and\nwrite times. Assume that the VMIPS vector registers are addressable (e.g., you\ncan initiate a vector operation with the operand V1(16), indicating that the input\noperand began with element 16). Also, assume that the total latency for adds,\nincluding operand read and write, is eight cycles. Write a VMIPS code\nsequence that reduces the contents of V1 to eight partial sums. It can be done\nwith one vector operation.\nc. [25]<G.5>Discuss how adding the extension in part (b) would affect a\nmachine that had multiple lanes.\nG.12\n[40]<G.3\u2013G.4>Extend the MIPS simulator to be a VMIPS simulator, including\nthe ability to count clock cycles. Write some short benchmark programs in MIPS\nand VMIPS assembly language. Measure the speedup on VMIPS, the percentage\nof vectorization, and usage of the functional units.\nG.13\n[50]<G.5>Modify the MIPS compiler to include a dependence checker. Run\nsome scientific code and loops through it and measure what percentage of the state-\nments could be vectorized.\nG.14\n[Discussion] Some proponents of vector processors might argue that the vector\nprocessors have provided the best path to ever-increasing amounts of processor\npower by focusing their attention on boosting peak vector performance. Others\nwould argue that the emphasis on peak performance is misplaced because an\nincreasing percentage of the programs are dominated by nonvector performance.\n(Remember Amdahl\u2019s law?) The proponents would respond that programmers\nshould work to make their programs vectorizable. What do you think about this\nargument?\nG-34\n\u25a0\nAppendix G Vector Processors in More Depth"
    },
    {
        "page": 1102,
        "text": "H.1\nIntroduction: Exploiting Instruction-Level Parallelism Statically\nH-2\nH.2\nDetecting and Enhancing Loop-Level Parallelism\nH-2\nH.3\nScheduling and Structuring Code for Parallelism\nH-12\nH.4\nHardware Support for Exposing Parallelism: Predicated Instructions\nH-23\nH.5\nHardware Support for Compiler Speculation\nH-27\nH.6\nThe Intel IA-64 Architecture and Itanium Processor\nH-32\nH.7\nConcluding Remarks\nH-43"
    },
    {
        "page": 1103,
        "text": "H\nHardware and Software for\nVLIW and EPIC\nThe EPIC approach is based on the application of massive\nresources. These resources include more load-store, computational,\nand branch units, as well as larger, lower-latency caches than would\nbe required for a superscalar processor. Thus, IA-64 gambles that, in\nthe future, power will not be the critical limitation, and that massive\nresources, along with the machinery to exploit them, will not\npenalize performance with their adverse effect on clock speed,\npath length, or CPI factors.\nM. Hopkins\nin a commentary on the EPIC\napproach and the IA-64 architecture (2000)"
    },
    {
        "page": 1104,
        "text": "H.1\nIntroduction: Exploiting Instruction-Level Parallelism\nStatically\nIn this chapter, we discuss compiler technology for increasing the amount of par-\nallelism that we can exploit in a program as well as hardware support for these\ncompiler techniques. The next section defines when a loop is parallel, how a depen-\ndence can prevent a loop from being parallel, and techniques for eliminating some\ntypes of dependences. The following section discusses the topic of scheduling code\nto improve parallelism. These two sections serve as an introduction to these\ntechniques.\nWe do not attempt to explain the details of ILP-oriented compiler techniques,\nsince that would take hundreds of pages, rather than the 20 we have allotted.\nInstead, we view this material as providing general background that will enable\nthe reader to have a basic understanding of the compiler techniques used to exploit\nILP in modern computers.\nHardware support for these compiler techniques can greatly increase their\neffectiveness, and Sections H.4 and H.5 explore such support. The IA-64 repre-\nsents the culmination of the compiler and hardware ideas for exploiting\nparallelism statically and includes support for many of the concepts proposed\nby researchers during more than a decade of research into the area of compiler-\nbased instruction-level parallelism. Section H.6 provides a description and perfor-\nmance analyses of the Intel IA-64 architecture and its second-generation imple-\nmentation, Itanium 2.\nThe core concepts that we exploit in statically based techniques\u2014finding par-\nallelism, reducing control and data dependences, and using speculation\u2014are the\nsame techniques we saw exploited in Chapter 3 using dynamic techniques. The\nkey difference is that the techniques in this appendix are applied at compile time\nby the compiler, rather than at runtime by the hardware. The advantages of compile\ntime techniques are primarily two: They do not burden runtime execution with any\ninefficiency, and they can take into account a wider range of the program than a\nruntime approach might be able to incorporate. As an example of the latter, the next\nsection shows how a compiler might determine that an entire loop can be executed\nin parallel; hardware techniques might or might not be able to find such parallel-\nism. The major disadvantage of static approaches is that they can use only compile\ntime information. Without runtime information, compile time techniques must\noften be conservative and assume the worst case.\nH.2\nDetecting and Enhancing Loop-Level Parallelism\nLoop-level parallelism is normally analyzed at the source level or close to it, while\nmost analysis of ILP is done once instructions have been generated by the\ncompiler. Loop-level analysis involves determining what dependences exist\namong the operands in a loop across the iterations of that loop. For now, we will\nH-2\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1105,
        "text": "consider only data dependences, which arise when an operand is written at some\npoint and read at a later point. Name dependences also exist and may be removed\nby renaming techniques like those we explored in Chapter 3.\nThe analysis of loop-level parallelism focuses on determining whether data\naccesses in later iterations are dependent on data values produced in earlier\niterations; such a dependence is called a loop-carried dependence. Most of the\nexamples we considered in Section 3.2 have no loop-carried dependences and,\nthus, are loop-level parallel. To see that a loop is parallel, let us first look at the\nsource representation:\nfor (i=1000; i>0; i=i-1)\nx[i] =\nx[i] + s;\nIn this loop, there is a dependence between the two uses of x[i], but this depen-\ndence is within a single iteration and is not loop carried. There is a dependence\nbetween successive uses of i in different iterations, which is loop carried, but this\ndependence involves an induction variable and can be easily recognized and elim-\ninated. We saw examples of how to eliminate dependences involving induction\nvariables during loop unrolling in Section 3.2, and we will look at additional exam-\nples later in this section.\nBecause finding loop-level parallelism involves recognizing structures such as\nloops, array references, and induction variable computations, the compiler can do\nthis analysis more easily at or near the source level, as opposed to the machine-code\nlevel. Let\u2019s look at a more complex example.\nExample\nConsider a loop like this one:\nfor (i=1; i<=100; i=i+1) {\nA[i+1] = A[i] + C[i];\n/* S1 */\nB[i+1] = B[i] + A[i+1]; /* S2 */\n}\nAssume that A, B, and C are distinct, nonoverlapping arrays. (In practice, the\narrays may sometimes be the same or may overlap. Because the arrays may be\npassed as parameters to a procedure, which includes this loop, determining whether\narrays overlap or are identical often requires sophisticated, interprocedural analysis\nof the program.) What are the data dependences among the statements S1 and S2 in\nthe loop?\nAnswer\nThere are two different dependences:\n1. S1 uses a value computed by S1 in an earlier iteration, since iteration i com-\nputes A[i+1], which is read in iteration i+1. The same is true of S2 for B[i]\nand B[i+1].\n2. S2 uses the value, A[i+1], computed by S1 in the same iteration.\nH.2\nDetecting and Enhancing Loop-Level Parallelism\n\u25a0\nH-3"
    },
    {
        "page": 1106,
        "text": "These two dependences are different and have different effects. To see how\nthey differ, let\u2019s assume that only one of these dependences exists at a time.\nBecause the dependence of statement S1 is on an earlier iteration of S1, this depen-\ndence is loop carried. This dependence forces successive iterations of this loop to\nexecute in series.\nThe second dependence (S2 depending on S1) is within an iteration and is not\nloop carried. Thus, if this were the only dependence, multiple iterations of the loop\ncould execute in parallel, as long as each pair of statements in an iteration were kept\nin order. We saw this type of dependence in an example in Section 3.2, where\nunrolling was able to expose the parallelism.\nIt is also possible to have a loop-carried dependence that does not prevent par-\nallelism, as the next example shows.\nExample\nConsider a loop like this one:\nfor (i=1; i<=100; i=i+1) {\nA[i] = A[i] + B[i];\n/* S1 */\nB[i+1] = C[i] + D[i]; /* S2 */\n}\nWhat are the dependences between S1 and S2? Is this loop parallel? If not, show\nhow to make it parallel.\nAnswer\nStatement S1 uses the value assigned in the previous iteration by statement S2, so\nthere is a loop-carried dependence between S2 and S1. Despite this loop-carried\ndependence, this loop can be made parallel. Unlike the earlier loop, this depen-\ndence is not circular: Neither statement depends on itself, and, although S1\ndepends on S2, S2 does not depend on S1. A loop is parallel if it can be written\nwithout a cycle in the dependences, since the absence of a cycle means that the\ndependences give a partial ordering on the statements.\nAlthough there are no circular dependences in the above loop, it must be trans-\nformed to conform to the partial ordering and expose the parallelism. Two obser-\nvations are critical to this transformation:\n1. There is no dependence from S1 to S2. If there were, then there would be a\ncycle in the dependences and the loop would not be parallel. Since this other\ndependence is absent, interchanging the two statements will not affect the exe-\ncution of S2.\n2. On the first iteration of the loop, statement S1 depends on the value of B[1]\ncomputed prior to initiating the loop.\nThese two observations allow us to replace the loop above with the following code\nsequence:\nH-4\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1107,
        "text": "A[1] = A[1] + B[1];\nfor (i=1; i<=99; i=i+1) {\nB[i+1] = C[i] + D[i];\nA[i+1] = A[i+1] + B[i+1];\n}\nB[101] = C[100] + D[100];\nThe dependence between the two statements is no longer loop carried, so iter-\nations of the loop may be overlapped, provided the statements in each iteration are\nkept in order.\nOur analysis needs to begin by finding all loop-carried dependences. This\ndependence information is inexact, in the sense that it tells us that such a depen-\ndence may exist. Consider the following example:\nfor (i=1;i<=100;i=i+1) {\nA[i] = B[i] + C[i]\nD[i] = A[i] * E[i]\n}\nThe second reference to A in this example need not be translated to a load instruc-\ntion, since we know that the value is computed and stored by the previous state-\nment; hence, the second reference to A can simply be a reference to the register into\nwhich A was computed. Performing this optimization requires knowing that the\ntwo references are always to the same memory address and that there is no inter-\nvening access to the same location. Normally, data dependence analysis only tells\nthat one reference may depend on another; a more complex analysis is required to\ndetermine that two references must be to the exact same address. In the example\nabove, a simple version of this analysis suffices, since the two references are in the\nsame basic block.\nOften loop-carried dependences are in the form of a recurrence:\nfor (i=2;i<=100;i=i+1) {\nY[i] = Y[i-1] + Y[i];\n}\nA recurrence is when a variable is defined based on the value of that variable in\nan earlier iteration, often the one immediately preceding, as in the above fragment.\nDetecting a recurrence can be important for two reasons: Some architectures (espe-\ncially vector computers) have special support for executing recurrences, and some\nrecurrences can be the source of a reasonable amount of parallelism. To see how\nthe latter can be true, consider this loop:\nfor (i=6;i<=100;i=i+1) {\nY[i] = Y[i-5] + Y[i];\n}\nH.2\nDetecting and Enhancing Loop-Level Parallelism\n\u25a0\nH-5"
    },
    {
        "page": 1108,
        "text": "On the iteration i, the loop references element i \u0001 5. The loop is said to have a\ndependence distance of 5. Many loops with carried dependences have a depen-\ndence distance of 1. The larger the distance, the more potential parallelism can\nbe obtained by unrolling the loop. For example, if we unroll the first loop, with\na dependence distance of 1, successive statements are dependent on one another;\nthere is still some parallelism among the individual instructions, but not much. If\nwe unroll the loop that has a dependence distance of 5, there is a sequence of five\nstatements that have no dependences, and thus much more ILP. Although many\nloops with loop-carried dependences have a dependence distance of 1, cases with\nlarger distances do arise, and the longer distance may well provide enough paral-\nlelism to keep a processor busy.\nFinding Dependences\nFinding the dependences in a program is an important part of three tasks: (1) good\nscheduling of code, (2) determining which loops might contain parallelism, and (3)\neliminating name dependences. The complexity of dependence analysis arises\nbecause of the presence of arrays and pointers in languages like C or C++, or\npass-by-reference parameter passing in FORTRAN. Since scalar variable refer-\nences explicitly refer to a name, they can usually be analyzed quite easily, with\naliasing because of pointers and reference parameters causing some complications\nand uncertainty in the analysis.\nHow does the compiler detect dependences in general? Nearly all dependence\nanalysis algorithms work on the assumption that array indices are affine. In sim-\nplest terms, a one-dimensional array index is affine if it can be written in the form\na \u0003 i + b, where a and b are constants and i is the loop index variable. The index of\na multidimensional array is affine if the index in each dimension is affine. Sparse\narray accesses, which typically have the form x[y[i]], are one of the major\nexamples of nonaffine accesses.\nDetermining whether there is a dependence between two references to the same\narray in a loop is thus equivalent to determining whether two affine functions can\nhave the same value for different indices between the bounds of the loop. For\nexample, suppose we have stored to an array element with index value a \u0003 i +\nb and loaded from the same array with index value c \u0003 i + d, where i is the\nfor-loop index variable that runs from m to n. A dependence exists if two condi-\ntions hold:\n1. There are two iteration indices, j and k, both within the limits of the for loop.\nThat is, m \u0004 j \u0004 n, m \u0004 k \u0004 n.\n2. The loop stores into an array element indexed by a \u0003 j + b and later fetches from\nthat same array element when it is indexed by c \u0003 k + d. That is, a \u0003 j + b \u00bc c \u0003\nk + d.\nH-6\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1109,
        "text": "In general, we cannot determine whether a dependence exists at compile time.\nFor example, the values of a, b, c, and d may not be known (they could be values in\nother arrays), making it impossible to tell if a dependence exists. In other cases, the\ndependence testing may be very expensive but decidable at compile time. For\nexample, the accesses may depend on the iteration indices of multiple nested loops.\nMany programs, however, contain primarily simple indices where a, b, c, and d are\nall constants. For these cases, it is possible to devise reasonable compile time tests\nfor dependence.\nAs an example, a simple and sufficient test for the absence of a dependence is\nthe greatest common divisor (GCD) test. It is based on the observation that if a\nloop-carried dependence exists, then GCD (c,a) must divide (d \u0001 b). (Recall that\nan integer, x, divides another integer, y, if we get an integer quotient when we do\nthe division y/x and there is no remainder.)\nExample\nUse the GCD test to determine whether dependences exist in the following loop:\nfor (i=1; i<=100; i=i+1) {\nX[2*i+3] = X[2*i] * 5.0;\n}\nAnswer\nGiven the values a \u00bc 2, b \u00bc 3, c \u00bc 2, and d \u00bc 0, then GCD(a,c) \u00bc 2, and d \u0001 b \u00bc\n\u00013. Since 2 does not divide \u00013, no dependence is possible.\nThe GCD test is sufficient to guarantee that no dependence exists; however,\nthere are cases where the GCD test succeeds but no dependence exists. This\ncan arise, for example, because the GCD test does not take the loop bounds into\naccount.\nIn general, determining whether a dependence actually exists is NP complete.\nIn practice, however, many common cases can be analyzed precisely at low cost.\nRecently, approaches using a hierarchy of exact tests increasing in generality\nand cost have been shown to be both accurate and efficient. (A test is exact if\nit precisely determines whether a dependence exists. Although the general case\nis NP complete, there exist exact tests for restricted situations that are much\ncheaper.)\nIn addition to detecting the presence of a dependence, a compiler wants to\nclassify the type of dependence. This classification allows a compiler to recog-\nnize name dependences and eliminate them at compile time by renaming and\ncopying.\nExample\nThe following loop has multiple types of dependences. Find all the true depen-\ndences, output dependences, and antidependences, and eliminate the output depen-\ndences and antidependences by renaming.\nH.2\nDetecting and Enhancing Loop-Level Parallelism\n\u25a0\nH-7"
    },
    {
        "page": 1110,
        "text": "for (i=1; i<=100; i=i+1) {\nY[i] = X[i] / c; /* S1 */\nX[i] = X[i] + c; /* S2 */\nZ[i] = Y[i] + c; /* S3 */\nY[i] = c - Y[i]; /* S4 */\n}\nAnswer\nThe following dependences exist among the four statements:\n1. There are true dependences from S1 to S3 and from S1 to S4 because of Y[i].\nThese are not loop carried, so they do not prevent the loop from being consid-\nered parallel. These dependences will force S3 and S4 to wait for S1 to\ncomplete.\n2. There is an antidependence from S1 to S2, based on X[i].\n3. There is an antidependence from S3 to S4 for Y[i].\n4. There is an output dependence from S1 to S4, based on Y[i].\nThe following version of the loop eliminates these false (or pseudo) dependences:\nfor (i=1; i<=100; i=i+1 {\n/* Y renamed to T to remove output dependence */\nT[i] = X[i] / c;\n/* X renamed to X1 to remove antidependence */\nX1[i] = X[i] + c;\n/* Y renamed to T to remove antidependence */\nZ[i] = T[i] + c;\nY[i] = c - T[i];\n}\nAfter the loop, the variable X has been renamed X1. In code that follows the loop,\nthe compiler can simply replace the name X by X1. In this case, renaming does not\nrequire an actual copy operation but can be done by substituting names or by reg-\nister allocation. In other cases, however, renaming will require copying.\nDependence analysis is a critical technology for exploiting parallelism. At the\ninstruction level, it provides information needed to interchange memory references\nwhen scheduling, as well as to determine the benefits of unrolling a loop. For\ndetecting loop-level parallelism, dependence analysis is the basic tool. Effectively\ncompiling programs to either vector computers or multiprocessors depends criti-\ncally on this analysis. The major drawback of dependence analysis is that it applies\nonly under a limited set of circumstances\u2014namely, among references within a sin-\ngle loop nest and using affine index functions. Thus, there is a wide variety of sit-\nuations in which array-oriented dependence analysis cannot tell us what we might\nwant to know, including the following:\nH-8\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1111,
        "text": "\u25a0\nWhen objects are referenced via pointers rather than array indices (but see\ndiscussion below)\n\u25a0\nWhen array indexing is indirect through another array, which happens with\nmany representations of sparse arrays\n\u25a0\nWhen a dependence may exist for some value of the inputs but does not exist in\nactuality when the code is run since the inputs never take on those values\n\u25a0\nWhen an optimization depends on knowing more than just the possibility of a\ndependence but needs to know on which write of a variable does a read of that\nvariable depend\nTo deal with the issue of analyzing programs with pointers, another type of anal-\nysis,oftencalledpoints-to analysis,isrequired(seeWilsonand Lam[1995]).Thekey\nquestion that we want answered from dependence analysis of pointers is whether two\npointers can designate the same address. In the case of complex dynamic data struc-\ntures,thisproblem isextremely difficult.For example,we may wantto knowwhether\ntwopointerscanreferencethesamenodeinalistatagivenpointinaprogram,whichin\ngeneral is undecidable and in practice is extremely difficult to answer. We may, how-\never, be able to answer a simpler question: Can two pointers designate nodes in the\nsame list, even if they may be separate nodes? This more restricted analysis can still\nbe quite useful in scheduling memory accesses performed through pointers.\nThe basic approach used in points-to analysis relies on information from three\nmajor sources:\n1. Type information, which restricts what a pointer can point to.\n2. Information derived when an object is allocated or when the address of an object\nis taken, which can be used to restrict what a pointer can point to. For example,\nif p always points to an object allocated in a given source line and q never points\nto that object, then p and q can never point to the same object.\n3. Information derived from pointer assignments. For example, if p may be\nassigned the value of q, then p may point to anything q points to.\nThere are several cases where analyzing pointers has been successfully applied\nand is extremely useful:\n\u25a0\nWhen pointers are used to pass the address of an object as a parameter, it is\npossible to use points-to analysis to determine the possible set of objects refer-\nenced by a pointer. One important use is to determine if two pointer parameters\nmay designate the same object.\n\u25a0\nWhen a pointer can point to one of several types, it is sometimes possible to\ndetermine the type of the data object that a pointer designates at different parts\nof the program.\n\u25a0\nIt is often possible to separate out pointers that may only point to a local object\nversus a global one.\nH.2\nDetecting and Enhancing Loop-Level Parallelism\n\u25a0\nH-9"
    },
    {
        "page": 1112,
        "text": "There are two different types of limitations that affect our ability to do accu-\nrate dependence analysis for large programs. The first type of limitation arises\nfrom restrictions in the analysis algorithms. Often, we are limited by the lack\nof applicability of the analysis rather than a shortcoming in dependence analysis\nper se. For example, dependence analysis for pointers is essentially impossible\nfor programs that use pointers in arbitrary fashion\u2014such as by doing arithmetic\non pointers.\nThe second limitation is the need to analyze behavior across procedure bound-\naries to get accurate information. For example, if a procedure accepts two param-\neters that are pointers, determining whether the values could be the same requires\nanalyzing across procedure boundaries. This type of analysis, called interproce-\ndural analysis, is much more difficult and complex than analysis within a single\nprocedure. Unlike the case of analyzing array indices within a single loop nest,\npoints-to analysis usually requires an interprocedural analysis. The reason for this\nis simple. Suppose we are analyzing a program segment with two pointers; if the\nanalysis does not know anything about the two pointers at the start of the program\nsegment, it must be conservative and assume the worst case. The worst case is that\nthe two pointers may designate the same object, but they are not guaranteed to\ndesignate the same object. This worst case is likely to propagate through the anal-\nysis, producing useless information. In practice, getting fully accurate interproce-\ndural information is usually too expensive for real programs. Instead, compilers\nusually use approximations in interprocedural analysis. The result is that the infor-\nmation may be too inaccurate to be useful.\nModern programming languages that use strong typing, such as Java, make the\nanalysis of dependences easier. At the same time the extensive use of procedures to\nstructure programs, as well as abstract data types, makes the analysis more diffi-\ncult. Nonetheless, we expect that continued advances in analysis algorithms, com-\nbined with the increasing importance of pointer dependency analysis, will mean\nthat there is continued progress on this important problem.\nEliminating Dependent Computations\nCompilers can reduce the impact of dependent computations so as to achieve more\ninstruction-level parallelism (ILP). The key technique is to eliminate or reduce a\ndependent computation by back substitution, which increases the amount of par-\nallelism and sometimes increases the amount of computation required. These tech-\nniques can be applied both within a basic block and within loops, and we describe\nthem differently.\nWithin a basic block, algebraic simplifications of expressions and an optimi-\nzation called copy propagation, which eliminates operations that copy values, can\nbe used to simplify sequences like the following:\nDADDUI\nR1,R2,#4\nDADDUI\nR1,R1,#4\nH-10\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1113,
        "text": "to\nDADDUI\nR1,R2,#8\nassuming this is the only use of R1. In fact, the techniques we used to reduce\nmultiple increments of array indices during loop unrolling and to move the incre-\nments across memory addresses in Section 3.2 are examples of this type of\noptimization.\nIn these examples, computations are actually eliminated, but it is also pos-\nsible that we may want to increase the parallelism of the code, possibly even\nincreasing the number of operations. Such optimizations are called tree height\nreduction because they reduce the height of the tree structure representing a\ncomputation, making it wider but shorter. Consider the following code\nsequence:\nADD\nR1,R2,R3\nADD\nR4,R1,R6\nADD\nR8,R4,R7\nNotice that this sequence requires at least three execution cycles, since all the\ninstructions depend on the immediate predecessor. By taking advantage of asso-\nciativity, we can transform the code and rewrite it as\nADD\nR1,R2,R3\nADD\nR4,R6,R7\nADD\nR8,R1,R4\nThis sequence can be computed in two execution cycles. When loop unrolling is\nused, opportunities for these types of optimizations occur frequently.\nAlthough arithmetic with unlimited range and precision is associative, com-\nputer arithmetic is not associative, for either integer arithmetic, because of limited\nrange, or floating-point arithmetic, because of both range and precision. Thus,\nusing these restructuring techniques can sometimes lead to erroneous behavior,\nalthough such occurrences are rare. For this reason, most compilers require that\noptimizations that rely on associativity be explicitly enabled.\nWhen loops are unrolled, this sort of algebraic optimization is important to\nreduce the impact of dependences arising from recurrences. Recurrences are\nexpressions whose value on one iteration is given by a function that depends on\nthe previous iterations. When a loop with a recurrence is unrolled, we may be able\nto algebraically optimize the unrolled loop, so that the recurrence need only be\nevaluated once per unrolled iteration. One common type of recurrence arises from\nan explicit program statement, such as:\nsum = sum + x;\nH.2\nDetecting and Enhancing Loop-Level Parallelism\n\u25a0\nH-11"
    },
    {
        "page": 1114,
        "text": "Assume we unroll a loop with this recurrence five times. If we let the value of x on\nthese five iterations be given by x1, x2, x3, x4, and x5, then we can write the\nvalue of sum at the end of each unroll as:\nsum = sum + x1 + x2 + x3 + x4 + x5;\nIf unoptimized, this expression requires five dependent operations, but it can be\nrewritten as:\nsum = ((sum + x1) + (x2 + x3)) + (x4 + x5);\nwhich can be evaluated in only three dependent operations.\nRecurrences also arise from implicit calculations, such as those associated with\narray indexing. Each array index translates to an address that is computed based on\nthe loop index variable. Again, with unrolling and algebraic optimization, the\ndependent computations can be minimized.\nH.3\nScheduling and Structuring Code for Parallelism\nWe have already seen that one compiler technique, loop unrolling, is useful to\nuncover parallelism among instructions by creating longer sequences of straight-\nline code. There are two other important techniques that have been developed for\nthis purpose: software pipelining and trace scheduling.\nSoftware Pipelining: Symbolic Loop Unrolling\nSoftware pipelining is a technique for reorganizing loops such that each iteration in\nthe software-pipelined code is made from instructions chosen from different iter-\nations of the original loop. This approach is most easily understood by looking at\nthe scheduled code for the unrolled loop, which appeared in the example in\nSection 2.2. The scheduler in this example essentially interleaves instructions from\ndifferent loop iterations, so as to separate the dependent instructions that occur\nwithin a single loop iteration. By choosing instructions from different iterations,\ndependent computations are separated from one another by an entire loop body,\nincreasing the possibility that the unrolled loop can be scheduled without stalls.\nA software-pipelined loop interleaves instructions from different iterations\nwithout unrolling the loop, as illustrated in Figure H.1. This technique is the soft-\nware counterpart to what Tomasulo\u2019s algorithm does in hardware. The software-\npipelined loop for the earlier example would contain one load, one add, and one\nstore, each from a different iteration. There is also some start-up code that is needed\nbefore the loop begins as well as code to finish up after the loop is completed. We\nwill ignore these in this discussion, for simplicity.\nH-12\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1115,
        "text": "Example\nShow a software-pipelined version of this loop, which increments all the elements\nof an array whose starting address is in R1 by the contents of F2:\nLoop:\nL.D\nF0,0(R1)\nADD.D\nF4,F0,F2\nS.D\nF4,0(R1)\nDADDUI\nR1,R1,#-8\nBNE\nR1,R2,Loop\nYou may omit the start-up and clean-up code.\nAnswer\nSoftware pipelining symbolically unrolls the loop and then selects instructions\nfrom each iteration. Since the unrolling is symbolic, the loop overhead instructions\n(the DADDUI and BNE) need not be replicated. Here\u2019s the body of the unrolled loop\nwithout overhead instructions, highlighting the instructions taken from each\niteration:\nIteration i:\nL.D\nF0,0(R1)\nADD.D\nF4,F0,F2\nS.D\nF4,0(R1)\nIteration i+1:\nL.D\nF0,0(R1)\nADD.D\nF4,F0,F2\nS.D\nF4,0(R1)\nIteration i+2:\nL.D\nF0,0(R1)\nADD.D\nF4,F0,F2\nS.D\nF4,0(R1)\nSoftware-\npipelined\niteration\nIteration\n0\nIteration\n1\nIteration\n2\nIteration\n3\nIteration\n4\nFigure H.1 A software-pipelined loop chooses instructions from different loop iter-\nations, thus separating the dependent instructions within one iteration of the\noriginal loop. The start-up and finish-up code will correspond to the portions above\nand below the software-pipelined iteration.\nH.3\nScheduling and Structuring Code for Parallelism\n\u25a0\nH-13"
    },
    {
        "page": 1116,
        "text": "The selected instructions from different iterations are then put together in the loop\nwith the loop control instructions:\nLoop:\nS.D\nF4,16(R1)\n;stores into M[i]\nADD.D\nF4,F0,F2\n;adds to M[i-1]\nL.D\nF0,0(R1)\n;loads M[i-2]\nDADDUI\nR1,R1,#-8\nBNE\nR1,R2,Loop\nThis loop can be run at a rate of 5 cycles per result, ignoring the start-up and clean-\nup portions, and assuming that DADDUI is scheduled before the ADD.D and that\nthe L.D instruction, with an adjusted offset, is placed in the branch delay slot.\nBecause the load and store are separated by offsets of 16 (two iterations), the loop\nshould run for two fewer iterations. Notice that the reuse of registers (e.g., F4, F0,\nand R1) requires the hardware to avoid the write after read (WAR) hazards that\nwould occur in the loop. This hazard should not be a problem in this case, since\nno data-dependent stalls should occur.\nBy looking at the unrolled version we can see what the start-up code and finish-up\ncode will need to be. For start-up, we will need to execute any instructions that cor-\nrespond to iteration 1 and 2 that will not be executed. These instructions are the L.D\nfor iterations 1 and 2 and the ADD.D for iteration 1. For the finish-up code, we need\nto execute any instructions that will not be executed in the final two iterations. These\ninclude the ADD.D for the last iteration and the S.D for the last two iterations.\nRegister management in software-pipelined loops can be tricky. The previous\nexample is not too hard since the registers that are written on one loop iteration are\nread on the next. In other cases, we may need to increase the number of iterations\nbetween when we issue an instruction and when the result is used. This increase is\nrequired when there are a small number of instructions in the loop body and the\nlatencies are large. In such cases, a combination of software pipelining and loop\nunrolling is needed.\nSoftware pipelining can be thought of as symbolic loop unrolling. Indeed, some\nof the algorithms for software pipelining use loop-unrolling algorithms to figure\nout how to software-pipeline the loop. The major advantage of software pipelining\nover straight loop unrolling is that software pipelining consumes less code space.\nSoftware pipelining and loop unrolling, in addition to yielding a better scheduled\ninner loop, each reduce a different type of overhead. Loop unrolling reduces the\noverhead of the loop\u2014the branch and counter update code. Software pipelining\nreduces the time when the loop is not running at peak speed to once per loop at\nthe beginning and end. If we unroll a loop that does 100 iterations a constant num-\nber of times, say, 4, we pay the overhead 100/4 \u00bc 25 times\u2014every time the inner\nunrolled loop is initiated. Figure H.2 shows this behavior graphically. Because\nthese techniques attack two different types of overhead, the best performance\ncan come from doing both. In practice, compilation using software pipelining is\nquite difficult for several reasons: Many loops require significant transformation\nH-14\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1117,
        "text": "before they can be software pipelined, the trade-offs in terms of overhead versus\nefficiency of the software-pipelined loop are complex, and the issue of register\nmanagement creates additional complexities. To help deal with the last two of\nthese issues, the IA-64 added extensive hardware sport for software pipelining.\nAlthough this hardware can make it more efficient to apply software pipelining,\nit does not eliminate the need for complex compiler support, or the need to make\ndifficult decisions about the best way to compile a loop.\nGlobal Code Scheduling\nIn Section 3.2 we examined the use of loop unrolling and code scheduling to\nimprove ILP. The techniques in Section 3.2 work well when the loop body is\nstraight-line code, since the resulting unrolled loop looks like a single basic block.\nSimilarly, software pipelining works well when the body is a single basic block,\nsince it is easier to find the repeatable schedule. When the body of an unrolled loop\ncontains internal control flow, however, scheduling the code is much more com-\nplex. In general, effective scheduling of a loop body with internal control flow will\nrequire moving instructions across branches, which is global code scheduling.\nIn this section, we first examine the challenge and limitations of global code\n(a) Software pipelining\nProportional\nto number of\nunrolls\nOverlap between\nunrolled iterations\nTime\nWind-down\ncode\nStart-up\ncode\n(b) Loop unrolling\nTime\nNumber\nof\noverlapped\noperations\nNumber\nof\noverlapped\noperations\nFigure H.2 The execution pattern for (a) a software-pipelined loop and (b) an\nunrolled loop. The shaded areas are the times when the loop is not running with max-\nimum overlap or parallelism among instructions. This occurs once at the beginning and\nonce at the end for the software-pipelined loop. For the unrolled loop it occurs m/n\ntimes if the loop has a total of m iterations and is unrolled n times. Each block represents\nan unroll of n iterations. Increasing the number of unrollings will reduce the start-up and\nclean-up overhead. The overhead of one iteration overlaps with the overhead of the\nnext, thereby reducing the impact. The total area under the polygonal region in each\ncase will be the same, since the total number of operations is just the execution rate\nmultiplied by the time.\nH.3\nScheduling and Structuring Code for Parallelism\n\u25a0\nH-15"
    },
    {
        "page": 1118,
        "text": "scheduling. In Section H.4 we examine hardware support for eliminating control\nflow within an inner loop, then we examine two compiler techniques that can be\nused when eliminating the control flow is not a viable approach.\nGlobal code scheduling aims to compact a code fragment with internal control\nstructure into the shortest possible sequence that preserves the data and control\ndependences. The data dependences force a partial order on operations, while\nthe control dependences dictate instructions across which code cannot be easily\nmoved. Data dependences are overcome by unrolling and, in the case of memory\noperations, using dependence analysis to determine if two references refer to the\nsame address. Finding the shortest possible sequence for a piece of code means\nfinding the shortest sequence for the critical path, which is the longest sequence\nof dependent instructions.\nControl dependences arising from loop branches are reduced by unrolling.\nGlobal code scheduling can reduce the effect of control dependences arising from\nconditional nonloop branches by moving code. Since moving code across branches\nwill often affect the frequency of execution of such code, effectively using global\ncode motion requires estimates of the relative frequency of different paths.\nAlthough global code motion cannot guarantee faster code, if the frequency infor-\nmation is accurate, the compiler can determine whether such code movement is\nlikely to lead to faster code.\nGlobal code motion is important since many inner loops contain conditional\nstatements. Figure H.3 shows a typical code fragment, which may be thought of\nas an iteration of an unrolled loop, and highlights the more common control flow.\nA(i) = A(i) + B(i)\nF\nT\nX\nB(i) =\nA(i) = 0?\nC(i) =\nFigure H.3 A code fragment and the common path shaded with gray. Moving the\nassignments to B or C requires a more complex analysis than for straight-line code.\nIn this section we focus on scheduling this code segment efficiently without hardware\nassistance. Predication or conditional instructions, which we discuss in the next section,\nprovide another way to schedule this code.\nH-16\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1119,
        "text": "Effectively scheduling this code could require that we move the assignments to\nB and C to earlier in the execution sequence, before the test of A. Such global code\nmotion must satisfy a set of constraints to be legal. In addition, the movement of the\ncode associated with B, unlike that associated with C, is speculative: It will speed\nthe computation up only when the path containing the code would be taken.\nTo perform the movement of B, we must ensure that neither the data flow nor\nthe exception behavior is changed. Compilers avoid changing the exception behav-\nior by not moving certain classes of instructions, such as memory references, that\ncan cause exceptions. In Section H.5, we will see how hardware support allows for\nmore opportunities for speculative code motion and removes control dependences.\nAlthough such enhanced support for speculation can make it possible to explore\nmore opportunities, the difficulty of choosing how to best compile the code\nremains complex.\nHow can the compiler ensure that the assignments to B and C can be moved\nwithout affecting the data flow? To see what\u2019s involved, let\u2019s look at a typical code\ngeneration sequence for the flowchart in Figure H.3. Assuming that the addresses\nfor A, B, C are in R1, R2, and R3, respectively, here is such a sequence:\nLD\nR4,0(R1)\n;load A\nLD\nR5,0(R2)\n;load B\nDADDU\nR4,R4,R5\n;Add to A\nSD\nR4,0(R1)\n;Store A\n...\nBNEZ\nR4,elsepart\n;Test A\n...\n;then part\nSD\n...,0(R2)\n;Stores to B\n...\nJ\njoin\n;jump over else\nelsepart:\n...\n;else part\nX\n;code for X\n...\njoin:\n...\n;after if\nSD\n...,0(R3)\n;store C[i]\nLet\u2019s first consider the problem of moving the assignment to B to before the\nBNEZ instruction. Call the last instruction to assign to B before the if statement\ni. If B is referenced before it is assigned either in code segment X or after the if\nstatement, call the referencing instruction j. If there is such an instruction j, then\nmoving the assignment to B will change the data flow of the program. In particular,\nmoving the assignment to B will cause j to become data dependent on the moved\nversion of the assignment to B rather than on i, on which j originally depended.\nYou could imagine more clever schemes to allow B to be moved even when\nthe value is used: For example, in the first case, we could make a shadow copy\nof B before the if statement and use that shadow copy in X. Such schemes are usu-\nally avoided, both because they are complex to implement and because they will\nH.3\nScheduling and Structuring Code for Parallelism\n\u25a0\nH-17"
    },
    {
        "page": 1120,
        "text": "slow down the program if the trace selected is not optimal and the operations end\nup requiring additional instructions to execute.\nMoving the assignment to C up to before the first branch requires two steps. First,\nthe assignment is moved over the join point of the else part into the portion corre-\nsponding to the then part. This movement makes the instructions for C control\ndependent on the branch and means that they will not execute if the else path, which\nis the infrequent path, is chosen. Hence, instructions that were data dependent on the\nassignment to C, and which execute after this code fragment, will be affected. To\nensure the correct value is computed for such instructions, a copy is made of the\ninstructions that compute and assign to C on the else path. Second, we can move\nC from the then part of the branch across the branch condition, if it does not affect\nany data flow into the branch condition. If C is moved to before the if test, the copy of\nC in the else branch can usually be eliminated, since it will be redundant.\nWe can see from this example that global code scheduling is subject to many\nconstraints. This observation is what led designers to provide hardware support to\nmake such code motion easier, and Sections H.4 and H.5 explores such support in\ndetail.\nGlobal code scheduling also requires complex trade-offs to make code motion\ndecisions. For example, assuming that the assignment to B can be moved before the\nconditional branch (possibly with some compensation code on the alternative\nbranch), will this movement make the code run faster? The answer is, possibly!\nSimilarly, moving the copies of C into the if and else branches makes the code\ninitially bigger! Only if the compiler can successfully move the computation across\nthe if test will there be a likely benefit.\nConsider the factors that the compiler would have to consider in moving the\ncomputation and assignment of B:\n\u25a0\nWhataretherelativeexecutionfrequenciesofthethencaseandtheelsecaseinthe\nbranch?Ifthethencaseismuchmorefrequent,thecodemotionmaybebeneficial.\nIf not, it is less likely, although not impossible, to consider moving the code.\n\u25a0\nWhat is the cost of executing the computation and assignment to B above the\nbranch? It may be that there are a number of empty instruction issue slots in the\ncode above the branch and that the instructions for B can be placed into these\nslots that would otherwise go empty. This opportunity makes the computation\nof B \u201cfree\u201d at least to first order.\n\u25a0\nHow will the movement of B change the execution time for the then case? If B\nis at the start of the critical path for the then case, moving it may be highly\nbeneficial.\n\u25a0\nIs B the best code fragment that can be moved above the branch? How does it\ncompare with moving C or other statements within the then case?\n\u25a0\nWhat is the cost of the compensation code that may be necessary for the else\ncase? How effectively can this code be scheduled, and what is its impact on\nexecution time?\nH-18\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1121,
        "text": "As we can see from this partial list, global code scheduling is an extremely\ncomplex problem. The trade-offs depend on many factors, and individual\ndecisions to globally schedule instructions are highly interdependent. Even choos-\ning which instructions to start considering as candidates for global code motion is\ncomplex!\nTo try to simplify this process, several different methods for global code sched-\nuling have been developed. The two methods we briefly explore here rely on a\nsimple principle: Focus the attention of the compiler on a straight-line code seg-\nment representing what is estimated to be the most frequently executed code path.\nUnrolling is used to generate the straight-line code, but, of course, the complexity\narises in how conditional branches are handled. In both cases, they are effectively\nstraightened by choosing and scheduling the most frequent path.\nTrace Scheduling: Focusing on the Critical Path\nTrace scheduling is useful for processors with a large number of issues per clock,\nwhere conditional or predicated execution (see Section H.4) is inappropriate or\nunsupported, and where simple loop unrolling may not be sufficient by itself to\nuncover enough ILP to keep the processor busy. Trace scheduling is a way to orga-\nnize the global code motion process, so as to simplify the code scheduling by incur-\nring the costs of possible code motion on the less frequent paths. Because it can\ngenerate significant overheads on the designated infrequent path, it is best used\nwhere profile information indicates significant differences in frequency between\ndifferent paths and where the profile information is highly indicative of program\nbehavior independent of the input. Of course, this limits its effective applicability\nto certain classes of programs.\nThere are two steps to trace scheduling. The first step, called trace selection,\ntries to find a likely sequence of basic blocks whose operations will be put together\ninto a smaller number of instructions; this sequence is called a trace. Loop unrol-\nling is used to generate long traces, since loop branches are taken with high prob-\nability. Additionally, by using static branch prediction, other conditional branches\nare also chosen as taken or not taken, so that the resultant trace is a straight-line\nsequence resulting from concatenating many basic blocks. If, for example, the pro-\ngram fragment shown in Figure H.3 corresponds to an inner loop with the\nhighlighted path being much more frequent, and the loop were unwound four\ntimes, the primary trace would consist of four copies of the shaded portion of\nthe program, as shown in Figure H.4.\nOnce a trace is selected, the second process, called trace compaction, tries to\nsqueeze the trace into a small number of wide instructions. Trace compaction is\ncode scheduling; hence, it attempts to move operations as early as it can in a\nsequence (trace), packing the operations into as few wide instructions (or issue\npackets) as possible.\nThe advantage of the trace scheduling approach is that it simplifies the deci-\nsions concerning global code motion. In particular, branches are viewed as jumps\ninto or out of the selected trace, which is assumed to be the most probable path.\nH.3\nScheduling and Structuring Code for Parallelism\n\u25a0\nH-19"
    },
    {
        "page": 1122,
        "text": "A(i) = A(i) + B(i)\nF\nT\nB(i) =\nA(i) = 0?\nC(i) =\nTrace entrance\nA(i) = A(i) + B(i)\nT\nF\nB(i) =\nA(i) = 0?\nC(i) =\nTrace exit\nTrace exit\nTrace entrance\nA(i) = A(i) + B(i)\nT\nF\nB(i) =\nA(i) = 0?\nC(i) =\nTrace exit\nTrace entrance\nA(i) = A(i) + B(i)\nT\nF\nB(i) =\nC(i) =\nA(i) = 0?\nTrace exit\nFigure H.4 This trace is obtained by assuming that the program fragment in Figure H.3 is the inner loop and\nunwinding it four times, treating the shaded portion in Figure H.3 as the likely path. The trace exits correspond\nto jumps off the frequent path, and the trace entrances correspond to returns to the trace.\nH-20\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1123,
        "text": "When code is moved across such trace entry and exit points, additional bookkeep-\ning code will often be needed on the entry or exit point. The key assumption is that\nthe trace is so much more probable than the alternatives that the cost of the book-\nkeeping code need not be a deciding factor: If an instruction can be moved and\nthereby make the main trace execute faster, it is moved.\nAlthough trace scheduling has been successfully applied to scientific code with\nits intensive loops and accurate profile data, it remains unclear whether this\napproach is suitable for programs that are less simply characterized and less loop\nintensive. In such programs, the significant overheads of compensation code may\nmake trace scheduling an unattractive approach, or, at best, its effective use will be\nextremely complex for the compiler.\nSuperblocks\nOne of the major drawbacks of trace scheduling is that the entries and exits into the\nmiddle of the trace cause significant complications, requiring the compiler to gen-\nerate and track the compensation code and often making it difficult to assess the\ncost of such code. Superblocks are formed by a process similar to that used for\ntraces, but are a form of extended basic blocks, which are restricted to a single entry\npoint but allow multiple exits.\nBecause superblocks have only a single entry point, compacting a superblock\nis easier than compacting a trace since only code motion across an exit need be\nconsidered. In our earlier example, we would form superblocks that contained only\none entrance; hence, moving C would be easier. Furthermore, in loops that have a\nsingle loop exit based on a count (for example, a for loop with no loop exit other\nthan the loop termination condition), the resulting superblocks have only one exit\nas well as one entrance. Such blocks can then be scheduled more easily.\nHow can a superblock with only one entrance be constructed? The answer is to\nuse tail duplication to create a separate block that corresponds to the portion of the\ntrace after the entry. In our previous example, each unrolling of the loop would\ncreate an exit from the superblock to a residual loop that handles the remaining\niterations. Figure H.5 shows the superblock structure if the code fragment from\nFigure H.3 is treated as the body of an inner loop and unrolled four times. The\nresidual loop handles any iterations that occur if the superblock is exited, which,\nin turn, occurs when the unpredicted path is selected. If the expected frequency of\nthe residual loop were still high, a superblock could be created for that loop as well.\nThe superblock approach reduces the complexity of bookkeeping and sched-\nuling versus the more general trace generation approach but may enlarge code size\nmore than a trace-based approach. Like trace scheduling, superblock scheduling\nmay be most appropriate when other techniques (e.g., if conversion) fail. Even\nin such cases, assessing the cost of code duplication may limit the usefulness of\nthe approach and will certainly complicate the compilation process.\nLoop unrolling, software pipelining, trace scheduling, and superblock sched-\nuling all aim at trying to increase the amount of ILP that can be exploited by a pro-\ncessor issuing more than one instruction on every clock cycle. The effectiveness of\nH.3\nScheduling and Structuring Code for Parallelism\n\u25a0\nH-21"
    },
    {
        "page": 1124,
        "text": "C(i) =\nA(i) = A(i) + B(i)\nF\nT\nB(i) =\nA(i) = 0?\nC(i) =\nA(i) = A(i) + B(i)\nT\nF\nB(i) =\nA(i) = 0?\nC(i) =\nA(i) = A(i) + B(i)\nF\nT\nB(i) =\nA(i) = 0?\nC(i) =\nA(i) = A(i) + B(i)\nT\nF\nB(i) =\nA(i) = 0?\nA(i) = A(i) + B(i)\nF\nT\nX\nB(i) =\nA(i) = 0?\nC(i) =\nExecute\nn times\nSuperblock exit\nwith n = 4\nSuperblock exit \nwith n = 3\nSuperblock exit \nwith n = 2\nSuperblock exit \nwith n = 1\nFigure H.5 This superblock results from unrolling the code in Figure H.3 four times and creating a superblock.\nH-22\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1125,
        "text": "each of these techniques and their suitability for various architectural approaches\nare among the hottest topics being actively pursued by researchers and designers of\nhigh-speed processors.\nH.4\nHardware Support for Exposing Parallelism:\nPredicated Instructions\nTechniques such as loop unrolling, software pipelining, and trace scheduling can\nbe used to increase the amount of parallelism available when the behavior of\nbranches is fairly predictable at compile time. When the behavior of branches is\nnot well known, compiler techniques alone may not be able to uncover much\nILP. In such cases, the control dependences may severely limit the amount of par-\nallelism that can be exploited. To overcome these problems, an architect can extend\nthe instruction set to include conditional or predicated instructions. Such instruc-\ntions can be used to eliminate branches, converting a control dependence into a\ndata dependence and potentially improving performance. Such approaches are use-\nful with either the hardware-intensive schemes in Chapter 3 or the software-\nintensive approaches discussed in this appendix, since in both cases predication\ncan be used to eliminate branches.\nThe concept behind conditional instructions is quite simple: An instruction\nrefers to a condition, which is evaluated as part of the instruction execution. If\nthe condition is true, the instruction is executed normally; if the condition is false,\nthe execution continues as if the instruction were a no-op. Many newer architec-\ntures include some form of conditional instructions. The most common example of\nsuch an instruction is conditional move, which moves a value from one register to\nanother if the condition is true. Such an instruction can be used to completely elim-\ninate a branch in simple code sequences.\nExample\nConsider the following code:\nif (A==0) {S=T;}\nAssuming that registers R1, R2, and R3 hold the values of A, S, and T, respec-\ntively, show the code for this statement with the branch and with the\nconditional move.\nAnswer\nThe straightforward code using a branch for this statement is (remember that we are\nassuming normal rather than delayed branches)\nBNEZ\nR1,L\nADDU\nR2,R3,R0\nL:\nUsing a conditional move that performs the move only if the third operand is\nequal to zero, we can implement this statement in one instruction:\nCMOVZ\nR2,R3,R1\nH.4\nHardware Support for Exposing Parallelism: Predicated Instructions\n\u25a0\nH-23"
    },
    {
        "page": 1126,
        "text": "The conditional instruction allows us to convert the control dependence present in\nthe branch-based code sequence to a data dependence. (This transformation is also\nused for vector computers, where it is called if conversion.) For a pipelined pro-\ncessor, this moves the place where the dependence must be resolved from near\nthe front of the pipeline, where it is resolved for branches, to the end of the pipeline,\nwhere the register write occurs.\nOne obvious use for conditional move is to implement the absolute value func-\ntion: A = abs (B), which is implemented as if (B<0) {A=-B;} else\n{A=B;}. This if statement can be implemented as a pair of conditional moves,\nor as one unconditional move (A=B) and one conditional move (A=-B).\nIn the example above or in the compilation of absolute value, conditional\nmoves are used to change a control dependence into a data dependence. This\nenables us to eliminate the branch and possibly improve the pipeline behavior.\nAs issue rates increase, designers are faced with one of two choices: execute\nmultiple branches per clock cycle or find a method to eliminate branches to\navoid this requirement. Handling multiple branches per clock is complex, since\none branch must be control dependent on the other. The difficulty of accurately\npredicting two branch outcomes, updating the prediction tables, and executing\nthe correct sequence has so far caused most designers to avoid processors that\nexecute multiple branches per clock. Conditional moves and predicated instruc-\ntions provide a way of reducing the branch pressure. In addition, a conditional\nmove can often eliminate a branch that is hard to predict, increasing the\npotential gain.\nConditional moves are the simplest form of conditional or predicated\ninstructions and, although useful for short sequences, have limitations. In par-\nticular, using conditional move to eliminate branches that guard the execution\nof large blocks of code can be inefficient, since many conditional moves may\nneed to be introduced.\nTo remedy the inefficiency of using conditional moves, some architectures\nsupport full predication, whereby the execution of all instructions is controlled\nby a predicate. When the predicate is false, the instruction becomes a no-op. Full\npredication allows us to simply convert large blocks of code that are branch depen-\ndent. For example, an if-then-else statement within a loop can be entirely converted\nto predicated execution, so that the code in the then case executes only if the value\nof the condition is true and the code in the else case executes only if the value of the\ncondition is false. Predication is particularly valuable with global code scheduling,\nsince it can eliminate nonloop branches, which significantly complicate instruction\nscheduling.\nPredicated instructions can also be used to speculatively move an instruction\nthat is time critical, but may cause an exception if moved before a guarding branch.\nAlthough it is possible to do this with conditional move, it is more costly.\nH-24\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1127,
        "text": "Example\nHere is a code sequence for a two-issue superscalar that can issue a combination of\none memory reference and one ALU operation, or a branch by itself, every cycle:\nFirst instruction slot\nSecond instruction slot\nLW\nR1,40\n(R2)\nADD R3,R4,R5\nADD R6,R3,R7\nBEQZ\nR10,L\nLW\nR8,0\n(R10)\nLW\nR9,0(R8)\nThis sequence wastes a memory operation slot in the second cycle and will incur a\ndata dependence stall if the branch is not taken, since the second LW after the\nbranch depends on the prior load. Show how the code can be improved using a\npredicated form of LW.\nAnswer\nCall the predicated version load word LWC and assume the load occurs unless the\nthird operand is 0. The LW immediately following the branch can be converted to\nan LWC and moved up to the second issue slot:\nFirst instruction slot\nSecond instruction slot\nLW\nR1,40(R2)\nADD R3,R4,R5\nLWC\nR8,0(R10),R10\nADD R6,R3,R7\nBEQZ\nR10,L\nLW\nR9,0(R8)\nThis improves the execution time by several cycles since it eliminates one instruc-\ntion issue slot and reduces the pipeline stall for the last instruction in the sequence.\nOf course, if the compiler mispredicted the branch, the predicated instruction will\nhave no effect and will not improve the running time. This is why the transforma-\ntion is speculative.\nIf the sequence following the branch were short, the entire block of code might\nbe converted to predicated execution and the branch eliminated.\nWhen we convert an entire code segment to predicated execution or specula-\ntively move an instruction and make it predicted, we remove a control dependence.\nCorrect code generation and the conditional execution of predicated instructions\nensure that we maintain the data flow enforced by the branch. To ensure that\nthe exception behavior is also maintained, a predicated instruction must not gen-\nerate an exception if the predicate is false. The property of not causing exceptions is\nH.4\nHardware Support for Exposing Parallelism: Predicated Instructions\n\u25a0\nH-25"
    },
    {
        "page": 1128,
        "text": "quite critical, as the previous example shows: If register R10 contains zero, the\ninstruction LW R8,0(R10) executed unconditionally is likely to cause a protec-\ntion exception, and this exception should not occur. Of course, if the condition is\nsatisfied (i.e., R10 is not zero), the LW may still cause a legal and resumable excep-\ntion (e.g., a page fault), and the hardware must take the exception when it knows\nthat the controlling condition is true.\nThe major complication in implementing predicated instructions is deciding\nwhen to annul an instruction. Predicated instructions may either be annulled during\ninstruction issue or later in the pipeline before they commit any results or raise an\nexception. Each choice has a disadvantage. If predicated instructions are annulled\nearly in the pipeline, the value of the controlling condition must be known early to\nprevent a stall for a data hazard. Since data-dependent branch conditions, which\ntend to be less predictable, are candidates for conversion to predicated execution,\nthis choice can lead to more pipeline stalls. Because of this potential for data hazard\nstalls, no design with predicated execution (or conditional move) annuls instruc-\ntions early. Instead, all existing processors annul instructions later in the pipeline,\nwhich means that annulled instructions will consume functional unit resources and\npotentially have a negative impact on performance. A variety of other pipeline\nimplementation techniques, such as forwarding, interact with predicated instruc-\ntions, further complicating the implementation.\nPredicated or conditional instructions are extremely useful for implementing\nshort alternative control flows, for eliminating some unpredictable branches,\nand for reducing the overhead of global code scheduling. Nonetheless, the useful-\nness of conditional instructions is limited by several factors:\n\u25a0\nPredicated instructions that are annulled (i.e., whose conditions are false) still\ntake some processor resources. An annulled predicated instruction requires\nfetch resources at a minimum, and in most processors functional unit execution\ntime. Therefore, moving an instruction across a branch and making it condi-\ntional will slow the program down whenever the moved instruction would\nnot have been normally executed. Likewise, predicating a control-dependent\nportion of code and eliminating a branch may slow down the processor if that\ncode would not have been executed. An important exception to these situations\noccurs when the cycles used by the moved instruction when it is not performed\nwould have been idle anyway (as in the earlier superscalar example). Moving\nan instruction across a branch or converting a code segment to predicated exe-\ncution is essentially speculating on the outcome of the branch. Conditional\ninstructions make this easier but do not eliminate the execution time taken\nby an incorrect guess. In simple cases, where we trade a conditional move\nfor a branch and a move, using conditional moves or predication is almost\nalways better. When longer code sequences are made conditional, the benefits\nare more limited.\n\u25a0\nPredicated instructions are most useful when the predicate can be evaluated\nearly. If the condition evaluation and predicated instructions cannot be\nH-26\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1129,
        "text": "separated (because of data dependences in determining the condition), then a\nconditional instruction may result in a stall for a data hazard. With branch pre-\ndiction and speculation, such stalls can be avoided, at least when the branches\nare predicted accurately.\n\u25a0\nThe use of conditional instructions can be limited when the control flow\ninvolves more than a simple alternative sequence. For example, moving an\ninstruction across multiple branches requires making it conditional on both\nbranches, which requires two conditions to be specified or requires additional\ninstructions to compute the controlling predicate. If such capabilities are not\npresent, the overhead of if conversion will be larger, reducing its advantage.\n\u25a0\nConditional instructions may have some speed penalty compared with uncon-\nditional instructions. This may show up as a higher cycle count for such\ninstructions or a slower clock rate overall. If conditional instructions are more\nexpensive, they will need to be used judiciously.\nFor these reasons, many architectures have included a few simple conditional\ninstructions (with conditional move being the most frequent), but only a few archi-\ntectures include conditional versions for the majority of the instructions. The\nMIPS, Alpha, PowerPC, SPARC, and Intel x86 (as defined in the Pentium proces-\nsor) all support conditional move. The IA-64 architecture supports full predication\nfor all instructions, as we will see in Section H.6.\nH.5\nHardware Support for Compiler Speculation\nAs we saw in Chapter 3, many programs have branches that can be accurately pre-\ndicted at compile time either from the program structure or by using a profile. In\nsuch cases, the compiler may want to speculate either to improve the scheduling or\nto increase the issue rate. Predicated instructions provide one method to speculate,\nbut they are really more useful when control dependences can be completely elim-\ninated by if conversion. In many cases, we would like to move speculated instruc-\ntions not only before the branch but also before the condition evaluation, and\npredication cannot achieve this.\nTo speculate ambitiously requires three capabilities:\n1. The ability of the compiler to find instructions that, with the possible use of reg-\nister renaming, can be speculatively moved and not affect the program data flow\n2. The ability to ignore exceptions in speculated instructions, until we know that\nsuch exceptions should really occur\n3. The ability to speculatively interchange loads and stores, or stores and stores,\nwhich may have address conflicts\nThe first of these is a compiler capability, while the last two require hardware sup-\nport, which we explore next.\nH.5\nHardware Support for Compiler Speculation\n\u25a0\nH-27"
    },
    {
        "page": 1130,
        "text": "Hardware Support for Preserving Exception Behavior\nTo speculate ambitiously, we must be able to move any type of instruction and still\npreserve its exception behavior. The key to being able to do this is to observe that\nthe results of a speculated sequence that is mispredicted will not be used in the final\ncomputation, and such a speculated instruction should not cause an exception.\nThere are four methods that have been investigated for supporting more ambi-\ntious speculation without introducing erroneous exception behavior:\n1. The hardware and operating system cooperatively ignore exceptions for spec-\nulative instructions. As we will see later, this approach preserves exception\nbehavior for correct programs, but not for incorrect ones. This approach may\nbe viewed as unacceptable for some programs, but it has been used, under pro-\ngram control, as a \u201cfast mode\u201d in several processors.\n2. Speculative instructions that never raise exceptions are used, and checks are\nintroduced to determine when an exception should occur.\n3. A set of status bits, called poison bits, are attached to the result registers written\nby speculated instructions when the instructions cause exceptions. The poison\nbits cause a fault when a normal instruction attempts to use the register.\n4. A mechanism is provided to indicate that an instruction is speculative, and the\nhardware buffers the instruction result until it is certain that the instruction is no\nlonger speculative.\nTo explain these schemes, we need to distinguish between exceptions that indi-\ncate a program error and would normally cause termination, such as a memory pro-\ntection violation, and those that are handled and normally resumed, such as a page\nfault. Exceptions that can be resumed can be accepted and processed for specula-\ntive instructions just as if they were normal instructions. If the speculative instruc-\ntion should not have been executed, handling the unneeded exception may have\nsome negative performance effects, but it cannot cause incorrect execution. The\ncost of these exceptions may be high, however, and some processors use hardware\nsupport to avoid taking such exceptions, just as processors with hardware specu-\nlation may take some exceptions in speculative mode, while avoiding others until\nan instruction is known not to be speculative.\nExceptions that indicate a program error should not occur in correct programs,\nand the result of a program that gets such an exception is not well defined, except\nperhaps when the program is running in a debugging mode. If such exceptions arise\nin speculated instructions, we cannot take the exception until we know that the\ninstruction is no longer speculative.\nIn the simplest method for preserving exceptions, the hardware and the oper-\nating system simply handle all resumable exceptions when the exception occurs\nand simply return an undefined value for any exception that would cause termina-\ntion. If the instruction generating the terminating exception was not speculative,\nthen the program is in error. Note that instead of terminating the program, the\nH-28\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1131,
        "text": "program is allowed to continue, although it will almost certainly generate incorrect\nresults. If the instruction generating the terminating exception is speculative, then\nthe program may be correct and the speculative result will simply be unused; thus,\nreturning an undefined value for the instruction cannot be harmful. This scheme\ncan never cause a correct program to fail, no matter how much speculation is done.\nAn incorrect program, which formerly might have received a terminating excep-\ntion, will get an incorrect result. This is acceptable for some programs, assuming\nthe compiler can also generate a normal version of the program, which does not\nspeculate and can receive a terminating exception.\nExample\nConsider that the following code fragment from an if-then-else statement of\nthe form\nif (A==0) A = B; else A = A+4;\nwhere A is at 0(R3) and B is at 0(R2):\nLD\nR1,0(R3)\n;load A\nBNEZ\nR1,L1\n;test A\nLD\nR1,0(R2)\n;then clause\nJ\nL2\n;skip else\nL1:\nDADDI\nR1,R1,#4\n;else clause\nL2:\nSD\nR1,0(R3)\n;store A\nAssume that the then clause is almost always executed. Compile the code using\ncompiler-based speculation. Assume R14 is unused and available.\nAnswer\nHere is the new code:\nLD\nR1,0(R3)\n;load A\nLD\nR14,0(R2)\n;speculative load B\nBEQZ\nR1,L3\n;other branch of the if\nDADDI\nR14,R1,#4\n;the else clause\nL3:\nSD\nR14,0(R3)\n;nonspeculative store\nThe then clause is completely speculated. We introduce a temporary register to\navoid destroying R1 when B is loaded; if the load is speculative, R14 will be use-\nless. After the entire code segment is executed, A will be in R14. The else clause\ncould have also been compiled speculatively with a conditional move, but if the\nbranch is highly predictable and low cost, this might slow the code down, since\ntwo extra instructions would always be executed as opposed to one branch.\nIn such a scheme, it is not necessary to know that an instruction is speculative.\nIndeed, it is helpful only when a program is in error and receives a terminating\nexception on a normal instruction; in such cases, if the instruction were not marked\nas speculative, the program could be terminated.\nH.5\nHardware Support for Compiler Speculation\n\u25a0\nH-29"
    },
    {
        "page": 1132,
        "text": "In this method for handling speculation, as in the next one, renaming will often\nbe needed to prevent speculative instructions from destroying live values. Renam-\ning is usually restricted to register values. Because of this restriction, the targets of\nstores cannot be destroyed and stores cannot be speculative. The small number of\nregisters and the cost of spilling will act as one constraint on the amount of spec-\nulation. Of course, the major constraint remains the cost of executing speculative\ninstructions when the compiler\u2019s branch prediction is incorrect.\nA second approach to preserving exception behavior when speculating intro-\nduces speculative versions of instructions that do not generate terminating excep-\ntions and instructions to check for such exceptions. This combination preserves the\nexception behavior exactly.\nExample\nShow how the previous example can be coded using a speculative load (sLD) and a\nspeculation check instruction (SPECCK) to completely preserve exception behav-\nior. Assume R14 is unused and available.\nAnswer\nHere is the code that achieves this:\nLD\nR1,0(R3)\n;load A\nsLD\nR14,0(R2)\n;speculative, no termination\nBNEZ\nR1,L1\n;test A\nSPECCK\n0(R2)\n;perform speculation check\nJ\nL2\n;skip else\nL1:\nDADDI\nR14,R1,#4\n;else clause\nL2:\nSD\nR14,0(R3)\n;store A\nNotice that the speculation check requires that we maintain a basic block for the\nthen case. If we had speculated only a portion of the then case, then a basic block\nrepresenting the then case would exist in any event. More importantly, notice that\nchecking for a possible exception requires extra code.\nA third approach for preserving exception behavior tracks exceptions as they\noccur but postpones any terminating exception until a value is actually used, pre-\nserving the occurrence of the exception, although not in a completely precise fash-\nion. The scheme is simple: A poison bit is added to every register, and another bit is\nadded to every instruction to indicate whether the instruction is speculative. The\npoison bit of the destination register is set whenever a speculative instruction\nresults in a terminating exception; all other exceptions are handled immediately.\nIf a speculative instruction uses a register with a poison bit turned on, the destina-\ntion register of the instruction simply has its poison bit turned on. If a normal\ninstruction attempts to use a register source with its poison bit turned on, the\ninstruction causes a fault. In this way, any program that would have generated\nan exception still generates one, albeit at the first instance where a result is used\nby an instruction that is not speculative. Since poison bits exist only on register\nH-30\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1133,
        "text": "values and not memory values, stores are never speculative and thus trap if either\noperand is \u201cpoison.\u201d\nExample\nConsider the code fragment from page H-29 and show how it would be compiled\nwith speculative instructions and poison bits. Show where an exception for the\nspeculative memory reference would be recognized. Assume R14 is unused and\navailable.\nAnswer\nHere is the code (an s preceding the opcode indicates a speculative instruction):\nLD\nR1,0(R3)\n;load A\nsLD\nR14,0(R2)\n;speculative load B\nBEQZ\nR1,L3\n;\nDADDI\nR14,R1,#4\n;\nL3:\nSD\nR14,0(R3)\n;exception for speculative LW\nIf the speculative sLD generates a terminating exception, the poison bit of R14 will\nbe turned on. When the nonspeculative SW instruction occurs, it will raise an\nexception if the poison bit for R14 is on.\nOne complication that must be overcome is how the OS saves the user registers\non a context switch if the poison bit is set. A special instruction is needed to save\nand reset the state of the poison bits to avoid this problem.\nThe fourth and final approach listed earlier relies on a hardware mechanism\nthat operates like a reorder buffer. In such an approach, instructions are marked\nby the compiler as speculative and include an indicator of how many branches\nthe instruction was speculatively moved across and what branch action (taken/\nnot taken) the compiler assumed. This last piece of information basically tells\nthe hardware the location of the code block where the speculated instruction orig-\ninally was. In practice, most of the benefit of speculation is gained by allowing\nmovement across a single branch; thus, only 1 bit saying whether the speculated\ninstruction came from the taken or not taken path is required. Alternatively, the\noriginal location of the speculative instruction is marked by a sentinel, which tells\nthe hardware that the earlier speculative instruction is no longer speculative and\nvalues may be committed.\nAll instructions are placed in a reorder buffer when issued and are forced to\ncommit in order, as in a hardware speculation approach. (Notice, though, that\nno actual speculative branch prediction or dynamic scheduling occurs.) The reor-\nder buffer tracks when instructions are ready to commit and delays the \u201cwrite-\nback\u201d portion of any speculative instruction. Speculative instructions are not\nallowed to commit until the branches that have been speculatively moved over\nare also ready to commit, or, alternatively, until the corresponding sentinel is\nreached. At that point, we know whether the speculated instruction should have\nbeen executed or not. If it should have been executed and it generated a terminating\nH.5\nHardware Support for Compiler Speculation\n\u25a0\nH-31"
    },
    {
        "page": 1134,
        "text": "exception, then we know that the program should be terminated. If the instruction\nshould not have been executed, then the exception can be ignored. Notice that the\ncompiler, rather than the hardware, has the job of register renaming to ensure cor-\nrect usage of the speculated result, as well as correct program execution.\nHardware Support for Memory Reference Speculation\nMoving loads across stores is usually done when the compiler is certain the\naddresses do not conflict. As we saw with the examples in Section 3.2, such trans-\nformations are critical to reducing the critical path length of a code segment. To\nallow the compiler to undertake such code motion when it cannot be absolutely\ncertain that such a movement is correct, a special instruction to check for address\nconflicts can be included in the architecture. The special instruction is left at the\noriginal location of the load instruction (and acts like a guardian), and the load\nis moved up across one or more stores.\nWhen a speculated load is executed, the hardware saves the address of the\naccessed memory location. If a subsequent store changes the location before the\ncheck instruction, then the speculation has failed. If the location has not been\ntouched, then the speculation is successful. Speculation failure can be handled\nin two ways. If only the load instruction was speculated, then it suffices to redo\nthe load at the point of the check instruction (which could supply the target register\nin addition to the memory address). If additional instructions that depended on the\nload were also speculated, then a fix-up sequence that reexecutes all the speculated\ninstructions starting with the load is needed. In this case, the check instruction\nspecifies the address where the fix-up code is located.\nIn this section, we have seen a variety of hardware assist mechanisms. Such\nmechanisms are key to achieving good support with the compiler-intensive\napproaches of Chapter 3 and this appendix. In addition, several of them can be eas-\nily integrated in the hardware-intensive approaches of Chapter 3 and provide addi-\ntional benefits.\nH.6\nThe Intel IA-64 Architecture and Itanium Processor\nThis section is an overview of the Intel IA-64 architecture, the most advanced\nVLIW-style processor, and its implementation in the Itanium processor.\nThe Intel IA-64 Instruction Set Architecture\nThe IA-64 is a RISC-style, register-register instruction set, but with many novel\nfeatures designed to support compiler-based exploitation of ILP. Our focus here\nis on the unique aspects of the IA-64 ISA. Most of these aspects have been dis-\ncussed already in this appendix, including predication, compiler-based parallelism\ndetection, and support for memory reference speculation.\nH-32\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1135,
        "text": "When they announced the IA-64 architecture, HP and Intel introduced the term\nEPIC (Explicitly Parallel Instruction Computer) to distinguish this new architec-\ntural approach from the earlier VLIW architectures and from other RISC architec-\ntures. Although VLIW and EPIC architectures share many features, the EPIC\napproach includes several concepts that extend the earlier VLIW approach. These\nextensions fall into two main areas:\n1. EPIC has greater flexibility in indicating parallelism among instructions and in\ninstruction formats. Rather than relying on a fixed instruction format where all\noperations in the instruction must be capable of being executed in parallel and\nwhere the format is completely rigid, EPIC uses explicit indicators of possible\ninstruction dependence as well as a variety of instruction formats. This EPIC\napproach can express parallelism more flexibly than the more rigid VLIW\nmethod and can reduce the increases in code size caused by the typically inflex-\nible VLIW instruction format.\n2. EPIC has more extensive support for software speculation than the earlier\nVLIW schemes that had only minimal support.\nIn addition, the IA-64 architecture includes a variety of features to improve perfor-\nmance, such as register windows and a rotating floating-point register (FPR) stack.\nThe IA-64 Register Model\nThe components of the IA-64 register state are\n\u25a0\n128 64-bit general-purpose registers, which as we will see shortly are actually\n65 bits wide\n\u25a0\n128 82-bit floating-point registers, which provide two extra exponent bits over\nthe standard 80-bit IEEE format\n\u25a0\n64 1-bit predicate registers\n\u25a0\n8 64-bit branch registers, which are used for indirect branches\n\u25a0\nA variety of registers used for system control, memory mapping, performance\ncounters, and communication with the OS\nThe integer registers are configured to help accelerate procedure calls using a\nregister stack mechanism similar to that developed in the Berkeley RISC-I proces-\nsor and used in the SPARC architecture. Registers 0 to 31 are always accessible\nand are addressed as 0 to 31. Registers 32 to 128 are used as a register stack,\nand each procedure is allocated a set of registers (from 0 to 96) for its use. The new\nregister stack frame is created for a called procedure by renaming the registers in\nhardware; a special register called the current frame pointer (CFM) points to the set\nof registers to be used by a given procedure. The frame consists of two parts:\nthe local area and the output area. The local area is used for local storage, while\nH.6\nThe Intel IA-64 Architecture and Itanium Processor\n\u25a0\nH-33"
    },
    {
        "page": 1136,
        "text": "the output area is used to pass values to any called procedure. The alloc instruc-\ntion specifies the size of these areas. Only the integer registers have register stack\nsupport.\nOn a procedure call, the CFM pointer is updated so that R32 of the called pro-\ncedure points to the first register of the output area of the called procedure. This\nupdate enables the parameters of the caller to be passed into the addressable reg-\nisters of the callee. The callee executes an alloc instruction to allocate both the\nnumber of required local registers, which include the output registers of the caller,\nand the number of output registers needed for parameter passing to a called pro-\ncedure. Special load and store instructions are available for saving and restoring the\nregister stack, and special hardware (called the register stack engine) handles over-\nflow of the register stack.\nIn addition to the integer registers, there are three other sets of registers: the\nfloating-point registers, the predicate registers, and the branch registers. The\nfloating-point registers are used for floating-point data, and the branch registers\nare used to hold branch destination addresses for indirect branches. The predication\nregisters hold predicates, which control the execution of predicated instructions;\nwe describe the predication mechanism later in this section.\nBoth the integer and floating-point registers support register rotation for reg-\nisters 32 to 128. Register rotation is designed to ease the task of allocating registers\nin software-pipelined loops, a problem that we discussed in Section H.3. In addi-\ntion, when combined with the use of predication, it is possible to avoid the need for\nunrolling and for separate prologue and epilogue code for a software-pipelined\nloop. This capability reduces the code expansion incurred to use software pipelin-\ning and makes the technique usable for loops with smaller numbers of iterations,\nwhere the overheads would traditionally negate many of the advantages.\nInstruction Format and Support for Explicit Parallelism\nThe IA-64 architecture is designed to achieve the major benefits of a VLIW\napproach\u2014implicit parallelism among operations in an instruction and fixed for-\nmatting of the operation fields\u2014while maintaining greater flexibility than a VLIW\nnormally allows. This combination is achieved by relying on the compiler to detect\nILP and schedule instructions into parallel instruction slots, but adding flexibility\nin the formatting of instructions and allowing the compiler to indicate when an\ninstruction cannot be executed in parallel with its successors.\nThe IA-64 architecture uses two different concepts to achieve the benefits of\nimplicit parallelism and ease of instruction decode. Implicit parallelism is achieved\nby placing instructions into instruction groups, while the fixed formatting of mul-\ntiple instructions is achieved through the introduction of a concept called a bundle,\nwhich contains three instructions. Let\u2019s start by defining an instruction group.\nAn instruction group is a sequence of consecutive instructions with no register\ndata dependences among them (there are a few minor exceptions). All the instruc-\ntions in a group could be executed in parallel, if sufficient hardware resources\nexisted and if any dependences through memory were preserved. An instruction\ngroup can be arbitrarily long, but the compiler must explicitly indicate the\nH-34\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1137,
        "text": "boundary between one instruction group and another. This boundary is indicated\nby placing a stop between two instructions that belong to different groups. To\nunderstand how stops are indicated, we must first explain how instructions are\nplaced into bundles.\nIA-64 instructions are encoded in bundles, which are 128 bits wide. Each bun-\ndle consists of a 5-bit template field and three instructions, each 41 bits in length.\n(Actually, the 41-bit quantities are not truly instructions, since they can only be\ninterpreted in conjunction with the template field. The name syllable is sometimes\nused for these operations. For simplicity, we will continue to use the term \u201cinstruc-\ntion.\u201d) To simplify the decoding and instruction issue process, the template field of\na bundle specifies what types of execution units each instruction in the bundle\nrequires. Figure H.6 shows the five different execution unit types and describes\nwhat instruction classes they may hold, together with some examples.\nThe 5-bit template field within each bundle describes both the presence of any\nstops associated with the bundle and the execution unit type required by each\ninstruction within the bundle. Figure H.7 shows the possible formats that the tem-\nplate field encodes and the position of any stops it specifies. The bundle formats\ncan specify only a subset of all possible combinations of instruction types and\nstops. To see how the bundle works, let\u2019s consider an example.\nExample\nUnroll the array increment example, x[i] = x[i] + s, seven times and place the\ninstructions into bundles, first ignoring pipeline latencies (to minimize the number\nof bundles) and then scheduling the code to minimize stalls. In scheduling the code\nassume one bundle executes per clock and that any stalls cause the entire bundle to\nExecution\nunit slot\nInstruction\ntype\nInstruction\ndescription\nExample instructions\nI-unit\nA\nInteger ALU\nAdd, subtract, and, or, compare\nI\nNon-ALU\ninteger Integer and multimedia shifts,\nbit tests, moves\nM-unit\nA\nInteger ALU\nAdd, subtract, and, or, compare\nM\nMemory access\nLoads and stores for integer/FP\nregisters\nF-unit\nF\nFloating point\nFloating-point instructions\nB-unit\nB\nBranches\nConditional branches, calls, loop\nbranches\nL + X\nL + X\nExtended\nExtended immediates, stops and\nno-ops\nFigure H.6 The five execution unit slots in the IA-64 architecture and what instruc-\ntions types they may hold are shown. A-type instructions, which correspond to integer\nALU instructions, may be placed in either an I-unit or M-unit slot. L + X slots are special,\nas they occupy two instruction slots; L + X instructions are used to encode 64-bit imme-\ndiates and a few special instructions. L + X instructions are executed either by the I-unit\nor the B-unit.\nH.6\nThe Intel IA-64 Architecture and Itanium Processor\n\u25a0\nH-35"
    },
    {
        "page": 1138,
        "text": "be stalled. Use the pipeline latencies from Figure 3.2. Use MIPS instruction mne-\nmonics for simplicity.\nAnswer\nThe two different versions are shown in Figure H.8. Although the latencies are dif-\nferent from those in Itanium, the most common bundle, MMF, must be issued by\nitself in Itanium, just as our example assumes.\nFigure H.7 The 24 possible template values (8 possible values are reserved) and the\ninstruction slots and stops for each format. Stops are indicated by heavy lines and may\nappear within and/or at the end of the bundle. For example, template 9 specifies that the\ninstruction slots are M, M, and I (in that order) and that the only stop is between this bun-\ndle and the next. Template 11 has the same type of instruction slots but also includes a\nstop after the first slot. The L + X format is used when slot 1 is L and slot 2 is X.\nH-36\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1139,
        "text": "Figure H.8 The IA-64 instructions, including bundle bits and stops, for the unrolled version of x[i] 5 x[i] + s, when\nunrolled seven times and scheduled (a) to minimize the number of instruction bundles and (b) to minimize the\nnumber of cycles (assuming that a hazard stalls an entire bundle). Blank entries indicate unused slots, which are\nencoded as no-ops. The absence of stops indicates that some bundles could be executed in parallel. Minimizing the\nnumber of bundles yields 9 bundles versus the 11 needed to minimize the number of cycles. The scheduled version\nexecutes in just over half the number of cycles. Version (a) fills 85% of the instruction slots, while (b) fills 70%. The\nnumber of empty slots in the scheduled code and the use of bundles may lead to code sizes that are much larger than\nother RISC architectures. Note that the branch in the last bundle in both sequences depends on the DADD in the same\nbundle. In the IA-64 instruction set, this sequence would be coded as a setting of a predication register and a branch\nthat would be predicated on that register. Normally, such dependent operations could not occur in the same bundle,\nbut this case is one of the exceptions mentioned earlier.\nH.6\nThe Intel IA-64 Architecture and Itanium Processor\n\u25a0\nH-37"
    },
    {
        "page": 1140,
        "text": "Instruction Set Basics\nBefore turning to the special support for speculation, we briefly discuss the major\ninstruction encodings and survey the instructions in each of the five primary\ninstruction classes (A, I, M, F, and B). Each IA-64 instruction is 41 bits in length.\nThe high-order 4 bits, together with the bundle bits that specify the execution unit\nslot, are used as the major opcode. (That is, the 4-bit opcode field is reused across\nthe execution field slots, and it is appropriate to think of the opcode as being 4 bits\nplus the M, F, I, B, L + X designation.) The low-order 6 bits of every instruction are\nused for specifying the predicate register that guards the instruction (see the next\nsubsection).\nFigure H.9 summarizes most of the major instruction formats, other than the\nmultimedia instructions, and gives examples of the instructions encoded for each\nformat.\nPredication and Speculation Support\nThe IA-64 architecture provides comprehensive support for predication: Nearly\nevery instruction in the IA-64 architecture can be predicated. An instruction is\npredicated by specifying a predicate register, whose identity is placed in the lower\n6 bits of each instruction field. Because nearly all instructions can be predicated,\nboth if conversion and code motion have lower overhead than they would with\nonly limited support for conditional instructions. One consequence of full predi-\ncation is that a conditional branch is simply a branch with a guarding predicate!\nPredicate registers are set using compare or test instructions. A compare\ninstruction specifies one of ten different comparison tests and two predicate reg-\nisters as destinations. The two predicate registers are written either with the result\nof the comparison (0 or 1) and the complement, or with some logical function that\ncombines the two tests (such as and) and the complement. This capability allows\nmultiple comparisons to be done in one instruction.\nSpeculation support in the IA-64 architecture consists of separate support for\ncontrol speculation, which deals with deferring exception for speculated instruc-\ntions, and memory reference speculation, which supports speculation of load\ninstructions.\nDeferred exception handling for speculative instructions is supported by pro-\nviding the equivalent of poison bits. For the general-purpose registers (GPRs),\nthese bits are called NaTs (Not a Thing), and this extra bit makes the GPRs effec-\ntively 65 bits wide. For the FP registers this capability is obtained using a special\nvalue, NaTVal (Not a Thing Value); this value is encoded using a significand of\n0 and an exponent outside of the IEEE range. Only speculative load instructions\ngenerate such values, but all instructions that do not affect memory will cause a\nNaT or NaTVal to be propagated to the result register. (There are both speculative\nand non-speculative loads; the latter can only raise immediate exceptions and can-\nnot defer them.) Floating-point exceptions are not handled through this mechanism\nbut instead use floating-point status registers to record exceptions.\nH-38\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1141,
        "text": "Instruction\ntype\nNumber\nof formats\nRepresentative instructions\nExtra\nopcode\nbits\nGPRs/\nFPRs\nImmediate\nbits\nOther/comment\nA\n8\nAdd, subtract, and, or\n9\n3\n0\nShift left and add\n7\n3\n0\n2-bit shift count\nALU immediates\n9\n2\n8\nAdd immediate\n3\n2\n14\nAdd immediate\n0\n2\n22\nCompare\n4\n2\n0\n2 predicate\nregister\ndestinations\nCompare immediate\n3\n1\n8\n2 predicate\nregister\ndestinations\nI\n29\nShift R/L variable\n9\n3\n0\nMany multimedia\ninstructions use\nthis format.\nTest bit\n6\n3\n6-bit field\nspecifier\n2 predicate\nregister\ndestinations\nMove to BR\n6\n1\n9-bit branch\npredict\nBranch register\nspecifier\nM\n46\nInteger/FP load and store, line\nprefetch\n10\n2\n0\nSpeculative/\nnonspeculative\nInteger/FP load and store, and\nline prefetch and post-\nincrement by immediate\n9\n2\n8\nSpeculative/\nnonspeculative\nInteger/FP load prefetch and\nregister postincrement\n10\n3\nSpeculative/\nnonspeculative\nInteger/FP speculation check\n3\n1\n21 in two\nfields\nB\n9\nPC-relative branch, counted\nbranch\n7\n0\n21\nPC-relative call\n4\n0\n21\n1 branch register\nF\n15\nFP arithmetic\n2\n4\nFP compare\n2\n2\n2 6-bit predicate\nregs\nL + X\n4\nMove immediate long\n2\n1\n64\nFigure H.9 A summary of some of the instruction formats of the IA-64 ISA. The major opcode bits and the guarding\npredication register specifier add 10 bits to every instruction. The number of formats indicated for each instruction\nclass in the second column (a total of 111) is a strict interpretation: A different use of a field, even of the same size, is\nconsidered a different format. The number of formats that actually have different field sizes is one-third to one-half as\nlarge. Some instructions have unused bits that are reserved; we have not included those in this table. Immediate bits\ninclude the sign bit. The branch instructions include prediction bits, which are used when the predictor does not have\na valid prediction. Only one of the many formats for the multimedia instructions is shown in this table.\nH.6\nThe Intel IA-64 Architecture and Itanium Processor\n\u25a0\nH-39"
    },
    {
        "page": 1142,
        "text": "A deferred exception can be resolved in two different ways. First, if a non-\nspeculative instruction, such as a store, receives a NaT or NaTVal as a source\noperand, it generates an immediate and unrecoverable exception. Alternatively,\na chk.s instruction can be used to detect the presence of NaT or NaTVal and\nbranch to a routine designed by the compiler to recover from the speculative\noperation. Such a recovery approach makes more sense for memory reference\nspeculation.\nThe inability to store the contents of instructions with a NaT or NaTVal set\nwould make it impossible for the OS to save the state of the processor. Thus,\nIA-64 includes special instructions to save and restore registers that do not\ncause an exception for a NaT or NaTVal and also save and restore the\nNaT bits.\nMemory reference support in the IA-64 uses a concept called advanced loads.\nAn advanced load is a load that has been speculatively moved above store instruc-\ntions on which it is potentially dependent. To speculatively perform a load, the ld.\na (for advanced load) instruction is used. Executing this instruction creates an entry\nin a special table, called the ALAT. The ALAT stores both the register destination\nof the load and the address of the accessed memory location. When a store is exe-\ncuted, an associative lookup against the active ALAT entries is performed. If there\nis an ALAT entry with the same memory address as the store, the ALAT entry is\nmarked as invalid.\nBefore any nonspeculative instruction (i.e., a store) uses the value generated by\nan advanced load or a value derived from the result of an advanced load, an explicit\ncheck is required. The check specifies the destination register of the advanced load.\nIf the ALAT for that register is still valid, the speculation was legal and the only\neffect of the check is to clear the ALAT entry. If the check fails, the action taken\ndepends on which of two different types of checks was employed. The first type of\ncheck is an instruction ld.c, which simply causes the data to be reloaded from\nmemory at that point. An ld.c instruction is used when only the load is advanced.\nThe alternative form of a check, chk.a, specifies the address of a fix-up routine\nthat is used to reexecute the load and any other speculated code that depended on\nthe value of the load.\nThe Itanium 2 Processor\nThe Itanium 2 processor is the second implementation of the IA-64 architecture.\nThe first version, Itanium 1, became available in 2001 with an 800 MHz clock.\nThe Itanium 2, first delivered in 2003, had a maximum clock rate in 2005 of\n1.6 GHz. The two processors are very similar, with some differences in the pipeline\nstructure and greater differences in the memory hierarchies. The Itanium 2 is about\nfour times faster than the Itanium 1. This performance improvement comes from a\ndoubling of the clock rate, a more aggressive memory hierarchy, additional func-\ntional units that improve instruction throughput, more complete bypassing, a\nH-40\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1143,
        "text": "shorter pipeline that reduces some stalls, and a more mature compiler system. Dur-\ning roughly the same period that elapsed from the Itanium 1 to Itanium 2, the Pen-\ntium processors improved by slightly more than a factor of three. The greater\nimprovement for the Itanium is reasonable given the novelty of the architecture\nand software system versus the more established IA-32 implementations.\nThe Itanium 2 can fetch and issue two bundles, or up to six instructions, per\nclock. The Itanium 2 uses a three-level memory hierarchy all on-chip. The first\nlevel uses split instruction and data caches, each 16 KB; floating-point data are\nnot placed in the first-level cache. The second and third levels are unified caches\nof 256 KB and of 3 MB to 9 MB, respectively.\nFunctional Units and Instruction Issue\nThere are 11 functional units in the Itanium 2 processor: two I-units, four M-units\n(two for loads and two for stores), three B-units, and two F-units. All the functional\nunits are pipelined. Figure H.10 gives the pipeline latencies for some typical\ninstructions. In addition, when a result is bypassed from one unit to another, there\nis usually at least one additional cycle of delay.\nItanium 2 can issue up to six instructions per clock from two bundles. In the\nworst case, if a bundle is split when it is issued, the hardware could see as few as\nfour instructions: one from the first bundle to be executed and three from the sec-\nond bundle. Instructions are allocated to functional units based on the bundle bits,\nignoring the presence of no-ops or predicated instructions with untrue predicates.\nIn addition, when issue to a functional unit is blocked because the next instruction\nto be issued needs an already committed unit, the resulting bundle is split. A split\nbundle still occupies one of the two bundle slots, even if it has only one instruction\nremaining.\nInstruction\nLatency\nInteger load\n1\nFloating-point load\n5\u20139\nCorrectly predicted taken branch\n0\u20133\nMispredicted branch\n6\nInteger ALU operations\n0\nFP arithmetic\n4\nFigure H.10 The latency of some typical instructions on Itanium 2. The latency is\ndefined as the smallest number of intervening instructions between two dependent\ninstructions. Integer load latency assumes a hit in the first-level cache. FP loads always\nbypass the primary cache, so the latency is equal to the access time of the second-level\ncache. There are some minor restrictions for some of the functional units, but these pri-\nmarily involve the execution of infrequent instructions.\nH.6\nThe Intel IA-64 Architecture and Itanium Processor\n\u25a0\nH-41"
    },
    {
        "page": 1144,
        "text": "The Itanium 2 processor uses an eight-stage pipeline divided into four major\nparts:\n\u25a0\nFront-end (stages IPG and Rotate)\u2014Prefetches up to 32 bytes per clock (two\nbundles) into a prefetch buffer, which can hold up to eight bundles (24 instruc-\ntions). Branch prediction is done using a multilevel adaptive predictor like\nthose described in Chapter 3.\n\u25a0\nInstruction delivery (stages EXP and REN)\u2014Distributes up to six instructions\nto the 11 functional units. Implements register renaming for both rotation and\nregister stacking.\n\u25a0\nOperand delivery (REG)\u2014Accesses the register file, performs register bypass-\ning, accesses and updates a register scoreboard, and checks predicate depen-\ndences. The scoreboard is used to detect when individual instructions can\nproceed, so that a stall of one instruction (for example, due to an unpredictable\nevent like a cache miss) in a bundle need not cause the entire bundle to stall. (As\nwe saw in Figure H.8, stalling the entire bundle leads to poor performance\nunless the instructions are carefully scheduled.)\n\u25a0\nExecution (EXE, DET, and WRB)\u2014Executes instructions through ALUs and\nload-store units, detects exceptions and posts NaTs, retires instructions, and\nperforms write-back.\nBoth the Itanium 1 and the Itanium 2 have many of the features more com-\nmonly associated with the dynamically scheduled pipelines described in\nChapter 3: dynamic branch prediction, register renaming, scoreboarding, a pipeline\nwith a number of stages before execution (to handle instruction alignment, renam-\ning, etc.), and several stages following execution to handle exception detection.\nAlthough these mechanisms are generally simpler than those in an advanced\ndynamically scheduled superscalar, the overall effect is that the Itanium proces-\nsors, which rely much more on compiler technology, seem to be as complex as\nthe dynamically scheduled processors we saw in Chapter 3!\nOne might ask why such features are included in a processor that relies primar-\nily on compile time techniques for finding and exploiting parallelism. There are\ntwo main motivations. First, dynamic techniques are sometimes significantly bet-\nter, and omitting them would hurt performance significantly. The inclusion of\ndynamic branch prediction is such a case.\nSecond, caches are absolutely necessary to achieve high performance, and with\ncaches come cache misses, which are both unpredictable and which in current pro-\ncessors take a relatively long time. In the early VLIW processors, the entire pro-\ncessor would freeze when a cache miss occurred, retaining the lockstep parallelism\ninitially specified by the compiler. Such an approach is totally unrealistic in a mod-\nern processor where cache misses can cost tens to hundreds of cycles. Allowing\nsome instructions to continue while others are stalled, however, requires the intro-\nduction of some form of dynamic scheduling, in this case scoreboarding. In addi-\ntion, if a stall is likely to be long, then antidependences are likely to prevent much\nH-42\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1145,
        "text": "progress while waiting for the cache miss; hence, the Itanium implementations also\nintroduce register renaming.\nItanium 2 Performance\nFigure H.11 shows the performance of a 1.5 GHz Itanium 2 versus a Pentium 4, an\nAMD Athlon processor, and an IBM Power5 for five SPECint and five SPECfp\nbenchmarks. Overall, the Itanium 2 is slightly slower than the Power5 for the full\nset of SPEC floating-point benchmarks and about 35% faster than the AMD Athlon\nor Pentium 4. On SPECint, the Itanium 2 is 15% faster than the Power5, while both\nthe AMD Athlon and Pentium 4 are about 15% faster than the Itanium 2. The Ita-\nnium 2 and Power5 are much higher power and have larger die sizes. In fact, the\nPower5 contains two processors, only one of which is active during normal SPEC\nbenchmarks, and still it has less than half the transistor count of the Itanium. If we\nwere to reduce the die size, transistor count, and power of the Power5 by eliminat-\ning one of the processors, the Itanium would be by far the largest and highest-\npower processor.\nH.7\nConcluding Remarks\nWhen the design of the IA-64 architecture began, it was a joint effort of Hewlett-\nPackard and Intel and many of the designers had benefited from experience with\nearly VLIW processors as well of years of research building on the early concepts.\nThe clear goal for the IA-64 architecture was to achieve levels of ILP as good or\nwupwise\ngalgel\nmesa\nswim\nsixtrack\ngcc\ngzip\ncrafty\ngap\ntwolf\n0\n5000\n4000\n3000\n2000\n1000\n6000\n7000\nSPEC Ratio\n8000\n9000\nItanium 2\nPentium 4\nAMD Athlon 64\nPower5\nFigure H.11 The performance of four multiple-issue processors for five SPECfp and SPECint benchmarks. The\nclock rates of the four processors are Itanium 2 at 1.5 GHz, Pentium 4 Extreme Edition at 3.8 GHz, AMD Athlon 64\nat 2.8 GHz, and the IBM Power5 at 1.9 GHz.\nH.7\nConcluding Remarks\n\u25a0\nH-43"
    },
    {
        "page": 1146,
        "text": "better than what had been achieved with hardware-based approaches, while also\nallowing a much simpler hardware implementation. With a simpler hardware\nimplementation, designers hoped that much higher clock rates could be achieved.\nIndeed, when the IA-64 architecture and the first Itanium were announced, they\nwere announced as the successor to the RISC approaches with clearly superior\nadvantages.\nUnfortunately, the practical reality has been quite different. The IA-64 and Ita-\nnium implementations appear to be at least as complicated as the dynamically\nbased speculative processors, and neither approach has a significant and consistent\nperformance advantage. The fact that the Itanium designs have also not been more\npower efficient has led to a situation where the Itanium design has been adopted by\nonly a small number of customers primarily interested in FP performance.\nIntel had planned for IA-64 to be its new 64-bit architecture as well. But the\ncombination of its mediocre integer performance (especially in Itanium 1) and\nlarge die size, together with AMD\u2019s introduction of a 64-bit version of the IA-\n32 architecture, forced Intel to extend the address space of IA-32. The availability\nof a larger address space IA-32 processor with strong integer performance has fur-\nther reduced the interest in IA-64 and Itanium. Most recently, Intel has introduced\nthe name IPF to replace IA-64, since the former name made less sense once the\nolder x86 architecture was extended to 64 bits.\nReference\nWilson, R.P., Lam, M.S., 1995. Efficient context-sensitive pointer analysis for C programs. In: Proc.\nACM SIGPLAN\u201995 Conf. on Programming Language Design and Implementation, June 18\u201321,\n1995, La Jolla, Calif, pp. 1\u201312.\nH-44\n\u25a0\nAppendix H Hardware and Software for VLIW and EPIC"
    },
    {
        "page": 1147,
        "text": "I.1\nIntroduction\nI-2\nI.2\nInterprocessor Communication: The Critical Performance Issue\nI-3\nI.3\nCharacteristics of Scientific Applications\nI-6\nI.4\nSynchronization: Scaling Up\nI-12\nI.5\nPerformance of Scientific Applications on Shared-Memory\nMultiprocessors\nI-21\nI.6\nPerformance Measurement of Parallel Processors with Scientific\nApplications\nI-33\nI.7\nImplementing Cache Coherence\nI-34\nI.8\nThe Custom Cluster Approach: Blue Gene/L\nI-41\nI.9\nConcluding Remarks\nI-44"
    },
    {
        "page": 1148,
        "text": "I\nLarge-Scale Multiprocessors\nand Scientific Applications\nHennessy and Patterson should move MPPs to Chapter 11.\nJim Gray, Microsoft Research\nwhen asked about the coverage of massively parallel processors\n(MPPs) for the third edition in 2000\nUnfortunately for companies in the MPP business, the third edition\nhad only ten chapters and the MPP business did not grow as\nanticipated when the first and second edition were written."
    },
    {
        "page": 1149,
        "text": "I.1\nIntroduction\nThe primary application of large-scale multiprocessors is for true parallel program-\nming, as opposed to multiprogramming or transaction-oriented computing where\nindependent tasks are executed in parallel without much interaction. In true parallel\ncomputing, a set of tasks execute in a collaborative fashion on one application.\nThe primary target of parallel computing is scientific and technical applications.\nIn contrast, for loosely coupled commercial applications, such as Web servers\nand most transaction-processing applications, there is little communication among\ntasks. For such applications, loosely coupled clusters are generally adequate and\nmost cost-effective, since intertask communication is rare.\nBecause true parallel computing involves cooperating tasks, the nature of com-\nmunication between those tasks and how such communication is supported in the\nhardware is of vital importance in determining the performance of the application.\nThe next section of this appendix examines such issues and the characteristics of\ndifferent communication models.\nIn comparison to sequential programs, whose performance is largely dictated\nby the cache behavior and issues related to instruction-level parallelism, parallel\nprograms have several additional characteristics that are important to performance,\nincluding the amount of parallelism, the size of parallel tasks, the frequency and\nnature of intertask communication, and the frequency and nature of synchroniza-\ntion. These aspects are affected both by the underlying nature of the application as\nwell as by the programming style. Section I.3 reviews the important characteristics\nof several scientific applications to give a flavor of these issues.\nAs we saw in Chapter 5, synchronization can be quite important in achieving\ngood performance. The larger number of parallel tasks that may need to synchro-\nnize makes contention involving synchronization a much more serious problem\nin large-scale multiprocessors. Section I.4 examines methods of scaling up the\nsynchronization mechanisms of Chapter 5.\nSection I.5 explores the detailed performance of shared-memory parallel appli-\ncations executing on a moderate-scale shared-memory multiprocessor. As we will\nsee, the behavior and performance characteristics are quite a bit more complicated\nthan those in small-scale shared-memory multiprocessors. Section I.6 discusses\nthe general issue of how to examine parallel performance for different sized\nmultiprocessors. Section I.7 explores the implementation challenges of distributed\nshared-memory cache coherence, the key architectural approach used in moderate-\nscale multiprocessors. Sections I.7 and I.8 rely on a basic understanding of inter-\nconnection networks, and the reader should at least quickly review Appendix F\nbefore reading these sections.\nSection I.8 explores the design of one of the newest and most exciting large-\nscale multiprocessors in recent times, Blue Gene. Blue Gene is a cluster-based mul-\ntiprocessor, but it uses a custom, highly dense node designed specifically for this\nfunction, as opposed to the nodes of most earlier cluster multiprocessors that used a\nnode architecture similar to those in a desktop or smaller-scale multiprocessor\nI-2\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1150,
        "text": "node. By using a custom node design, Blue Gene achieves a significant reduction\nin the cost, physical size, and power consumption of a node. Blue Gene/L, a 64 K-\nnode version, was the world\u2019s fastest computer in 2006, as measured by the linear\nalgebra benchmark, Linpack.\nI.2\nInterprocessor Communication: The Critical\nPerformance Issue\nIn multiprocessors with larger processor counts, interprocessor communication\nbecomes more expensive, since the distance between processors increases.\nFurthermore, in truly parallel applications where the threads of the application\nmust communicate, there is usually more communication than in a loosely coupled\nset of distinct processes or independent transactions, which characterize many\ncommercial server applications. These factors combine to make efficient interpro-\ncessor communication one of the most important determinants of parallel perfor-\nmance, especially for the scientific market.\nUnfortunately, characterizing the communication needs of an application and\nthe capabilities of an architecture is complex. This section examines the key hard-\nware characteristics that determine communication performance, while the next\nsection looks at application behavior and communication needs.\nThree performance metrics are critical in any hardware communication\nmechanism:\n1. Communication bandwidth\u2014Ideally, the communication bandwidth is limited\nby processor, memory, and interconnection bandwidths, rather than by some\naspect of the communication mechanism. The interconnection network deter-\nmines the maximum communication capacity of the system. The bandwidth\nin or out of a single node, which is often as important as total system bandwidth,\nis affected both by the architecture within the node and by the communication\nmechanism. How does the communication mechanism affect the communica-\ntion bandwidth of a node? When communication occurs, resources within the\nnodes involved in the communication are tied up or occupied, preventing other\noutgoing or incoming communication. When this occupancy is incurred for\neach word of a message, it sets an absolute limit on the communication band-\nwidth. This limit is often lower than what the network or memory system can\nprovide. Occupancy may also have a component that is incurred for each com-\nmunication event, such as an incoming or outgoing request. In the latter case,\nthe occupancy limits the communication rate, and the impact of the occupancy\non overall communication bandwidth depends on the size of the messages.\n2. Communication latency\u2014Ideally, the latency is as low as possible. As\nAppendix F explains:\nCommunication latency \u00bc Sender overhead + Time of flight\n+ Transmission time + Receiver overhead\nI.2\nInterprocessor Communication: The Critical Performance Issue\n\u25a0\nI-3"
    },
    {
        "page": 1151,
        "text": "assuming no contention. Time of flight is fixed and transmission time is deter-\nmined by the interconnection network. The software and hardware overheads in\nsending and receiving messages are largely determined by the communication\nmechanism and its implementation. Why is latency crucial? Latency affects\nboth performance and how easy it is to program a multiprocessor. Unless\nlatency is hidden, it directly affects performance either by tying up processor\nresources or by causing the processor to wait.\nOverhead and occupancy are closely related, since many forms of overhead also\ntie up some part of the node, incurring an occupancy cost, which in turn limits\nbandwidth. Key features of a communication mechanism may directly affect\noverhead and occupancy. For example, how is the destination address for a\nremote communication named, and how is protection implemented? When\nnaming and protection mechanisms are provided by the processor, as in a shared\naddress space, the additional overhead is small. Alternatively, if these mecha-\nnisms must be provided by the operating system for each communication, this\nincreases the overhead and occupancy costs of communication, which in turn\nreduce bandwidth and increase latency.\n3. Communication latency hiding\u2014How well can the communication mechanism\nhide latency by overlapping communication with computation or with other\ncommunication? Although measuring this is not as simple as measuring the first\ntwo metrics, it is an important characteristic that can be quantified by measuring\nthe running time on multiprocessors with the same communication latency but\ndifferent support for latency hiding. Although hiding latency is certainly a good\nidea, it poses an additional burden on the software system and ultimately on the\nprogrammer. Furthermore, the amount of latency that can be hidden is applica-\ntion dependent. Thus, it is usually best to reduce latency wherever possible.\nEach of these performance measures is affected by the characteristics of the\ncommunications needed in the application, as we will see in the next section.\nThe size of the data items being communicated is the most obvious characteristic,\nsince it affects both latency and bandwidth directly, as well as affecting the efficacy\nof different latency-hiding approaches. Similarly, the regularity in the communi-\ncation patterns affects the cost of naming and protection, and hence the commu-\nnication overhead. In general, mechanisms that perform well with smaller as\nwell as larger data communication requests, and irregular as well as regular com-\nmunication patterns, are more flexible and efficient for a wider class of applica-\ntions. Of course, in considering any communication mechanism, designers must\nconsider cost as well as performance.\nAdvantages of Different Communication Mechanisms\nThe two primary means of communicating data in a large-scale multiprocessor are\nmessage passing and shared memory. Each of these two primary communication\nI-4\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1152,
        "text": "mechanisms has its advantages. For shared-memory communication, the advan-\ntages include\n\u25a0\nCompatibility with the well-understood mechanisms in use in centralized\nmultiprocessors, which all use shared-memory communication. The OpenMP\nconsortium (see www.openmp.org for description) has proposed a standardized\nprogramming interface for shared-memory multiprocessors. Although mes-\nsage passing also uses a standard, MPI or Message Passing Interface, this stan-\ndard is not used either in shared-memory multiprocessors or in loosely coupled\nclusters in use in throughput-oriented environments.\n\u25a0\nEase of programming when the communication patterns among processors are\ncomplex or vary dynamically during execution. Similar advantages simplify\ncompiler design.\n\u25a0\nThe ability to develop applications using the familiar shared-memory model,\nfocusing attention only on those accesses that are performance critical.\n\u25a0\nLower overhead for communication and better use of bandwidth when commu-\nnicating small items. This arises from the implicit nature of communication and\nthe use of memory mapping to implement protection in hardware, rather than\nthrough the I/O system.\n\u25a0\nThe ability to use hardware-controlled caching to reduce the frequency of\nremote communication by supporting automatic caching of all data, both\nshared and private. As we will see, caching reduces both latency and contention\nfor accessing shared data. This advantage also comes with a disadvantage,\nwhich we mention below.\nThe major advantages for message-passing communication include the following:\n\u25a0\nThe hardware can be simpler, especially by comparison with a scalable shared-\nmemory implementation that supports coherent caching of remote data.\n\u25a0\nCommunication is explicit, which means it is simpler to understand. In shared-\nmemory models, it can be difficult to know when communication is occurring\nand when it is not, as well as how costly the communication is.\n\u25a0\nExplicit communication focuses programmer attention on this costly aspect\nof parallel computation, sometimes leading to improved structure in a multi-\nprocessor program.\n\u25a0\nSynchronization is naturally associated with sending messages, reducing the\npossibility for errors introduced by incorrect synchronization.\n\u25a0\nIt makes it easier to use sender-initiated communication, which may have some\nadvantages in performance.\n\u25a0\nIf the communication is less frequent and more structured, it is easier to\nimprove fault tolerance by using a transaction-like structure. Furthermore,\nI.2\nInterprocessor Communication: The Critical Performance Issue\n\u25a0\nI-5"
    },
    {
        "page": 1153,
        "text": "the less tight coupling of nodes and explicit communication make fault isola-\ntion simpler.\n\u25a0\nThe very largest multiprocessors use a cluster structure, which is inherently\nbased on message passing. Using two different communication models may\nintroduce more complexity than is warranted.\nOf course, the desired communication model can be created in software on top of a\nhardware model that supports either of these mechanisms. Supporting message\npassing on top of shared memory is considerably easier: Because messages essen-\ntially send data from one memory to another, sending a message can be implemen-\nted by doing a copy from one portion of the address space to another. The major\ndifficulties arise from dealing with messages that may be misaligned and of arbi-\ntrary length in a memory system that is normally oriented toward transferring\naligned blocks of data organized as cache blocks. These difficulties can be over-\ncome either with small performance penalties in software or with essentially no\npenalties, using a small amount of hardware support.\nSupporting shared memory efficiently on top of hardware for message passing\nis much more difficult. Without explicit hardware support for shared memory, all\nshared-memory references need to involve the operating system to provide address\ntranslation and memory protection, as well as to translate memory references into\nmessage sends and receives. Loads and stores usually move small amounts of data,\nso the high overhead of handling these communications in software severely limits\nthe range of applications for which the performance of software-based shared\nmemory is acceptable. For these reasons, it has never been practical to use message\npassing to implement shared memory for a commercial system.\nI.3\nCharacteristics of Scientific Applications\nThe primary use of scalable shared-memory multiprocessors is for true parallel\nprogramming, as opposed to multiprogramming or transaction-oriented comput-\ning. The primary target of parallel computing is scientific and technical applica-\ntions. Thus, understanding the design issues requires some insight into the\nbehavior of such applications. This section provides such an introduction.\nCharacteristics of Scientific Applications\nOur scientific/technical parallel workload consists of two applications and two\ncomputational kernels. The kernels are fast Fourier transformation (FFT) and an\nLU decomposition, which were chosen because they represent commonly used\ntechniques in a wide variety of applications and have performance characteristics\ntypical of many parallel scientific applications. In addition, the kernels have small\ncode segments whose behavior we can understand and directly track to specific\narchitectural characteristics. Like many scientific applications, I/O is essentially\nnonexistent in this workload.\nI-6\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1154,
        "text": "The two applications that we use in this appendix are Barnes and Ocean, which\nrepresent two important but very different types of parallel computation. We\nbriefly describe each of these applications and kernels and characterize their basic\nbehavior in terms of parallelism and communication. We describe how the prob-\nlem is decomposed for a distributed shared-memory multiprocessor; certain data\ndecompositions that we describe are not necessary on multiprocessors that have\na single, centralized memory.\nThe FFT Kernel\nThe FFT is the key kernel in applications that use spectral methods, which arise in\nfieldsrangingfromsignalprocessingtofluidflowtoclimatemodeling.TheFFTappli-\ncationwestudyhereisaone-dimensionalversionofaparallelalgorithmforacomplex\nnumber FFT. It has a sequential execution time for n data points of n log n. The algo-\nrithm uses a high radix (equal to\n\ufb03\ufb03\ufb03n\np ) that minimizes communication. The measure-\nments shown in this appendix are collected for a million-point input data set.\nThere are three primary data structures: the input and output arrays of the data\nbeing transformed and the roots of unity matrix, which is precomputed and only\nread during the execution. All arrays are organized as square matrices. The six\nsteps in the algorithm are as follows:\n1. Transpose data matrix.\n2. Perform 1D FFT on each row of data matrix.\n3. Multiply the roots of unity matrix by the data matrix and write the result in the\ndata matrix.\n4. Transpose data matrix.\n5. Perform 1D FFT on each row of data matrix.\n6. Transpose data matrix.\nThe data matrices and the roots of unity matrix are partitioned among processors in\ncontiguous chunks of rows, so that each processor\u2019s partition falls in its own local\nmemory. The first row of the roots of unity matrix is accessed heavily by all pro-\ncessors and is often replicated, as we do, during the first step of the algorithm just\nshown. The data transposes ensure good locality during the individual FFT steps,\nwhich would otherwise access nonlocal data.\nThe only communication is in the transpose phases, which require all-to-all\ncommunication of large amounts of data. Contiguous subcolumns in the rows\nassigned to a processor are grouped into blocks, which are transposed and placed\ninto the proper location of the destination matrix. Every processor transposes one\nblock locally and sends one block to each of the other processors in the system.\nAlthough there is no reuse of individual words in the transpose, with long cache\nblocks it makes sense to block the transpose to take advantage of the spatial locality\nafforded by long blocks in the source matrix.\nI.3\nCharacteristics of Scientific Applications\n\u25a0\nI-7"
    },
    {
        "page": 1155,
        "text": "The LU Kernel\nLU is an LU factorization of a dense matrix and is representative of many dense\nlinear algebra computations, such as QR factorization, Cholesky factorization,\nand eigenvalue methods. For a matrix of size nn the running time is n3 and\nthe parallelism is proportional to n2. Dense LU factorization can be performed\nefficiently by blocking the algorithm, using the techniques in Chapter 2, which\nleads to highly efficient cache behavior and low communication. After blocking\nthe algorithm, the dominant computation is a dense matrix multiply that occurs in\nthe innermost loop. The block size is chosen to be small enough to keep the cache\nmiss rate low and large enough to reduce the time spent in the less parallel parts\nof the computation. Relatively small block sizes (88 or 1616) tend to satisfy\nboth criteria.\nTwo details are important for reducing interprocessor communication. First,\nthe blocks of the matrix are assigned to processors using a 2D tiling: The n\nB n\nB\n(where each block is BB) matrix of blocks is allocated by laying a grid of size\npp over the matrix of blocks in a cookie-cutter fashion until all the blocks are\nallocated to a processor. Second, the dense matrix multiplication is performed\nby the processor that owns the destination block. With this blocking and allocation\nscheme, communication during the reduction is both regular and predictable. For\nthe measurements in this appendix, the input is a 512512 matrix and a block of\n1616 is used.\nA natural way to code the blocked LU factorization of a 2D matrix in a shared\naddress space is to use a 2D array to represent the matrix. Because blocks are allo-\ncated in a tiled decomposition, and a block is not contiguous in the address space\nin a 2D array, it is very difficult to allocate blocks in the local memories of the\nprocessors that own them. The solution is to ensure that blocks assigned to a\nprocessor are allocated locally and contiguously by using a 4D array (with the first\ntwo dimensions specifying the block number in the 2D grid of blocks, and the next\ntwo specifying the element in the block).\nThe Barnes Application\nBarnes is an implementation of the Barnes-Hut n-body algorithm solving a\nproblem in galaxy evolution. N-body algorithms simulate the interaction among\na large number of bodies that have forces interacting among them. In this\ninstance, the bodies represent collections of stars and the force is gravity. To\nreduce the computational time required to model completely all the individual\ninteractions among the bodies, which grow as n2, n-body algorithms take advan-\ntage of the fact that the forces drop off with distance. (Gravity, for example,\ndrops off as 1/d2, where d is the distance between the two bodies.) The\nBarnes-Hut algorithm takes advantage of this property by treating a collection\nof bodies that are \u201cfar away\u201d from another body as a single point at the center of\nmass of the collection and with mass equal to the collection. If the body is far\nenough from any body in the collection, then the error introduced will be\nI-8\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1156,
        "text": "negligible. The collections are structured in a hierarchical fashion, which can be\nrepresented in a tree. This algorithm yields an n log n running time with par-\nallelism proportional to n.\nThe Barnes-Hut algorithm uses an octree (each node has up to eight children)\nto represent the eight cubes in a portion of space. Each node then represents the\ncollection of bodies in the subtree rooted at that node, which we call a cell.\nBecause the density of space varies and the leaves represent individual bodies,\nthe depth of the tree varies. The tree is traversed once per body to compute\nthe net force acting on that body. The force calculation algorithm for a body starts\nat the root of the tree. For every node in the tree it visits, the algorithm determines\nif the center of mass of the cell represented by the subtree rooted at the node is\n\u201cfar enough away\u201d from the body. If so, the entire subtree under that node is\napproximated by a single point at the center of mass of the cell, and the force\nthat this center of mass exerts on the body is computed. On the other hand, if\nthe center of mass is not far enough away, the cell must be \u201copened\u201d and each\nof its subtrees visited. The distance between the body and the cell, together with\nthe error tolerances, determines which cells must be opened. This force calcula-\ntion phase dominates the execution time. This appendix takes measurements\nusing 16K bodies; the criterion for determining whether a cell needs to be opened\nis set to the middle of the range typically used in practice.\nObtaining effective parallel performance on Barnes-Hut is challenging because\nthe distribution of bodies is nonuniform and changes over time, making partition-\ning the work among the processors and maintenance of good locality of reference\ndifficult. We are helped by two properties: (1) the system evolves slowly, and (2)\nbecause gravitational forces fall off quickly, with high probability, each cell\nrequires touching a small number of other cells, most of which were used on\nthe last time step. The tree can be partitioned by allocating each processor a subtree.\nMany of the accesses needed to compute the force on a body in the subtree will be\nto other bodies in the subtree. Since the amount of work associated with a subtree\nvaries (cells in dense portions of space will need to access more cells), the size of\nthe subtree allocated to a processor is based on some measure of the work it has to\ndo (e.g., how many other cells it needs to visit), rather than just on the number of\nnodes in the subtree. By partitioning the octree representation, we can obtain good\nload balance and good locality of reference, while keeping the partitioning cost\nlow. Although this partitioning scheme results in good locality of reference, the\nresulting data references tend to be for small amounts of data and are unstructured.\nThus, this scheme requires an efficient implementation of shared-memory\ncommunication.\nThe Ocean Application\nOcean simulates the influence of eddy and boundary currents on large-scale flow\nin the ocean. It uses a restricted red-black Gauss-Seidel multigrid technique to\nsolve a set of elliptical partial differential equations. Red-black Gauss-Seidel is\nan iteration technique that colors the points in the grid so as to consistently update\nI.3\nCharacteristics of Scientific Applications\n\u25a0\nI-9"
    },
    {
        "page": 1157,
        "text": "each point based on previous values of the adjacent neighbors. Multigrid methods\nsolve finite difference equations by iteration using hierarchical grids. Each grid in\nthe hierarchy has fewer points than the grid below and is an approximation to the\nlower grid. A finer grid increases accuracy and thus the rate of convergence,\nwhile requiring more execution time, since it has more data points. Whether to\nmove up or down in the hierarchy of grids used for the next iteration is deter-\nmined by the rate of change of the data values. The estimate of the error at every\ntime step is used to decide whether to stay at the same grid, move to a coarser\ngrid, or move to a finer grid. When the iteration converges at the finest level, a\nsolution has been reached. Each iteration has n2 work for an nn grid and the\nsame amount of parallelism.\nThe arrays representing each grid are dynamically allocated and sized to the\nparticular problem. The entire ocean basin is partitioned into square subgrids\n(as close as possible) that are allocated in the portion of the address space corre-\nsponding to the local memory of the individual processors, which are assigned\nresponsibility for the subgrid. For the measurements in this appendix we use an\ninput that has 130130 grid points. There are five steps in a time iteration. Since\ndata are exchanged between the steps, all the processors present synchronize at the\nend of each step before proceeding to the next. Communication occurs when the\nboundary points of a subgrid are accessed by the adjacent subgrid in nearest-\nneighbor fashion.\nComputation/Communication for the Parallel Programs\nA key characteristic in determining the performance of parallel programs is the\nratio of computation to communication. If the ratio is high, it means the application\nhas lots of computation for each datum communicated. As we saw in Section I.2,\ncommunication is the costly part of parallel computing; therefore, high\ncomputation-to-communication ratios are very beneficial. In a parallel processing\nenvironment, we are concerned with how the ratio of computation to communica-\ntion changes as we increase either the number of processors, the size of the prob-\nlem, or both. Knowing how the ratio changes as we increase the processor count\nsheds light on how well the application can be sped up. Because we are often inter-\nested in running larger problems, it is vital to understand how changing the data set\nsize affects this ratio.\nTo\nunderstand\nwhat\nhappens\nquantitatively\nto\nthe\ncomputation-to-\ncommunication ratio as we add processors, consider what happens separately to\ncomputation and to communication as we either add processors or increase prob-\nlem size. Figure I.1 shows that as we add processors, for these applications, the\namount of computation per processor falls proportionately and the amount of com-\nmunication per processor falls more slowly. As we increase the problem size, the\ncomputation scales as the O( ) complexity of the algorithm dictates. Communica-\ntion scaling is more complex and depends on details of the algorithm; we describe\nthe basic phenomena for each application in the caption of Figure I.1.\nI-10\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1158,
        "text": "The overall computation-to-communication ratio is computed from the indi-\nvidual growth rate in computation and communication. In general, this ratio rises\nslowly with an increase in dataset size and decreases as we add processors. This\nreminds us that performing a fixed-size problem with more processors leads to\nincreasing inefficiencies because the amount of communication among processors\ngrows. It also tells us how quickly we must scale dataset size as we add processors\nto keep the fraction of time in communication fixed. The following example illus-\ntrates these trade-offs.\nExample\nSuppose we know that for a given multiprocessor the Ocean application spends\n20% of its execution time waiting for communication when run on four processors.\nAssume that the cost of each communication event is independent of processor\ncount, which is not true in general, since communication costs rise with processor\ncount. How much faster might we expect Ocean to run on a 32-processor machine\nwith the same problem size? What fraction of the execution time is spent on com-\nmunication in this case? How much larger a problem should we run if we want the\nfraction of time spent communicating to be the same?\nAnswer\nThe computation-to-communication ratio for Ocean is\n\ufb03\ufb03\ufb03n\np =\n\ufb03\ufb03\ufb03p\np , so if the problem\nsize is the same, the communication frequency scales by\n\ufb03\ufb03\ufb03p\np . This means that\ncommunication time increases by\n\ufb03\ufb03\ufb03\n8\np\n. We can use a variation on Amdahl\u2019s law,\nApplication\nScaling of computation\nScaling of communication\nScaling of computation-to-\ncommunication\nFFT\nnlogn\np\nn\np\nlog n\nLU\nn\np\n\ufb03\ufb03\ufb03n\np\n\ufb03\ufb03\ufb03p\np\n\ufb03\ufb03\ufb03n\np\n\ufb03\ufb03\ufb03p\np\nBarnes\nnlogn\np\napproximately\n\ufb03\ufb03\ufb03n\np\nlogn\n\u00f0\n\u00de\n\ufb03\ufb03\ufb03p\np\napproximately\n\ufb03\ufb03\ufb03n\np\n\ufb03\ufb03\ufb03p\np\nOcean\nn\np\n\ufb03\ufb03\ufb03n\np\n\ufb03\ufb03\ufb03p\np\n\ufb03\ufb03\ufb03n\np\n\ufb03\ufb03\ufb03p\np\nFigure I.1 Scaling of computation, of communication, and of the ratio are critical factors in determining perfor-\nmance on parallel multiprocessors. In this table, p is the increased processor count and n is the increased dataset\nsize. Scaling is on a per-processor basis. The computation scales up with n at the rate given by O( ) analysis and scales\ndown linearly as p is increased. Communication scaling is more complex. In FFT, all data points must interact, so com-\nmunication increases with n and decreases with p. In LU and Ocean, communication is proportional to the boundary\nof a block, so it scales with dataset size at a rate proportional to the side of a square with n points, namely,\n\ufb03\ufb03\ufb03n\np ; for the\nsame reason communication in these two applications scales inversely to\n\ufb03\ufb03\ufb03p\np . Barnes has the most complex scaling\nproperties. Because of the fall-off of interaction between bodies, the basic number of interactions among bodies that\nrequire communication scales as\n\ufb03\ufb03\ufb03n\np . An additional factor of log n is needed to maintain the relationships among the\nbodies. As processor count is increased, communication scales inversely to\n\ufb03\ufb03\ufb03p\np .\nI.3\nCharacteristics of Scientific Applications\n\u25a0\nI-11"
    },
    {
        "page": 1159,
        "text": "recognizing that the computation is decreased but the communication time is\nincreased. If T4 is the total execution time for four processors, then the execution\ntime for 32 processors is\nT32 \u00bc Compute time + Communication time\n\u00bc 0:8T4\n8\n+ 0:2T4\n\u00f0\n\u00de\n\ufb03\ufb03\ufb03\n8\np\n\u00bc 0:1T4 + 0:57T4 \u00bc 0:67T4\nHence, the speedup is\nSpeedup \u00bc T4\nT32\n\u00bc\nT4\n0:67T4\n\u00bc 1:49\nand the fraction of time spent in communication goes from 20% to 0.57/\n0.67\u00bc85%.\nFor the fraction of the communication time to remain the same, we must keep\nthe computation-to-communication ratio the same, so the problem size must scale\nat the same rate as the processor count. Notice that, because we have changed the\nproblem size, we cannot fairly compare the speedup of the original problem and the\nscaled problem. We will return to the critical issue of scaling applications for mul-\ntiprocessors in Section I.6.\nI.4\nSynchronization: Scaling Up\nIn this section, we focus first on synchronization performance problems in larger\nmultiprocessors and then on solutions for those problems.\nSynchronization Performance Challenges\nTo understand why the simple spin lock scheme presented in Chapter 5 does not\nscale well, imagine a large multiprocessor with all processors contending for the\nsame lock. The directory or bus acts as a point of serialization for all the processors,\nleading to lots of contention, as well as traffic. The following example shows how\nbad things can be.\nExample\nSuppose there are 10 processors on a bus and each tries to lock a variable simul-\ntaneously. Assume that each bus transaction (read miss or write miss) is 100\nclock cycles long. You can ignore the time of the actual read or write of a lock\nheld in the cache, as well as the time the lock is held (they won\u2019t matter much!).\nDetermine the number of bus transactions required for all 10 processors to\nacquire the lock, assuming they are all spinning when the lock is released at time\n0. About how long will it take to process the 10 requests? Assume that the bus is\nI-12\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1160,
        "text": "totally fair so that every pending request is serviced before a new request and that\nthe processors are equally fast.\nAnswer\nWhen i processes are contending for the lock, they perform the following sequence\nof actions, each of which generates a bus transaction:\ni load linked operations to access the lock\ni store conditional operations to try to lock the lock\n1 store (to release the lock)\nThus, for i processes, there are a total of 2i+1 bus transactions. Note that this\nassumes that the critical section time is negligible, so that the lock is released before\nany other processors whose store conditional failed attempt another load linked.\nThus, for n processes, the total number of bus operations is\nX\nn\ni\u00bc1\n2i + 1\n\u00f0\n\u00de \u00bc n n + 1\n\u00f0\n\u00de + n \u00bc n2 + 2n\nFor 10 processes there are 120 bus transactions requiring 12,000 clock cycles or\n120 clock cycles per lock acquisition!\nThe difficulty in this example arises from contention for the lock and serialization\nof lock access, as well as the latency of the bus access. (The fairness property of the\nbus actually makes things worse, since it delays the processor that claims the lock\nfrom releasing it; unfortunately, for any bus arbitration scheme some worst-case\nscenario does exist.) The key advantages of spin locks\u2014that they have low over-\nhead in terms of bus or network cycles and offer good performance when locks are\nreused by the same processor\u2014are both lost in this example. We will consider\nalternative implementations in the next section, but before we do that, let\u2019s con-\nsider the use of spin locks to implement another common high-level synchroniza-\ntion primitive.\nBarrier Synchronization\nOne additional common synchronization operation in programs with parallel\nloops is a barrier. A barrier forces all processes to wait until all the processes\nreach the barrier and then releases all of the processes. A typical implementation\nof a barrier can be done with two spin locks: one to protect a counter that tallies\nthe processes arriving at the barrier and one to hold the processes until the last\nprocess arrives at the barrier. To implement a barrier, we usually use the ability to\nspin on a variable until it satisfies a test; we use the notation spin(condi-\ntion) to indicate this. Figure I.2 is a typical implementation, assuming that\nlock and unlock provide basic spin locks and total is the number of pro-\ncesses that must reach the barrier.\nI.4\nSynchronization: Scaling Up\n\u25a0\nI-13"
    },
    {
        "page": 1161,
        "text": "In practice, another complication makes barrier implementation slightly\nmore complex. Frequently a barrier is used within a loop, so that processes\nreleased from the barrier would do some work and then reach the barrier again.\nAssume that one of the processes never actually leaves the barrier (it stays at\nthe spin operation), which could happen if the OS scheduled another process,\nfor example. Now it is possible that one process races ahead and gets to the\nbarrier again before the last process has left. The \u201cfast\u201d process then traps\nthe remaining \u201cslow\u201d process in the barrier by resetting the flag release.\nNow all the processes will wait infinitely at the next instance of this barrier\nbecause one process is trapped at the last instance, and the number of processes\ncan never reach the value of total.\nThe important observation in this example is that the programmer did\nnothing wrong. Instead, the implementer of the barrier made some assump-\ntions about forward progress that cannot be assumed. One obvious solution\nto this is to count the processes as they exit the barrier (just as we did on\nentry) and not to allow any process to reenter and reinitialize the barrier until\nall processes have left the prior instance of this barrier. This extra step would\nsignificantly increase the latency of the barrier and the contention, which as\nwe will see shortly are already large. An alternative solution is a sense-\nreversing barrier, which makes use of a private per-process variable,\nlocal_sense, which is initialized to 1 for each process. Figure I.3 shows\nthe code for the sense-reversing barrier. This version of a barrier is safely\nusable; as the next example shows, however, its performance can still be\nquite poor.\nlock (counterlock);/* ensure update atomic */\nif (count==0) release=0;/* first=>reset release */\ncount = count + 1;/* count arrivals */\nunlock(counterlock);/* release lock */\nif (count==total) {/* all arrived */\ncount=0;/* reset counter */\nrelease=1;/* release processes */\n}\nelse {/* more to come */\nspin (release==1);/* wait for arrivals */\n}\nFigure I.2 Code for a simple barrier. The lock counterlock protects the counter so\nthat it can be atomically incremented. The variable count keeps the tally of how many\nprocesses have reached the barrier. The variable release is used to hold the processes\nuntil the last one reaches the barrier. The operation spin (release==1) causes a\nprocess to wait until all processes reach the barrier.\nI-14\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1162,
        "text": "Example\nSuppose there are 10 processors on a bus and each tries to execute a barrier simul-\ntaneously. Assume that each bus transaction is 100 clock cycles, as before. You can\nignore the time of the actual read or write of a lock held in the cache as the time to\nexecute other nonsynchronization operations in the barrier implementation. Deter-\nmine the number of bus transactions required for all 10 processors to reach the bar-\nrier, be released from the barrier, and exit the barrier. Assume that the bus is totally\nfair, so that every pending request is serviced before a new request and that the\nprocessors are equally fast. Don\u2019t worry about counting the processors out of\nthe barrier. How long will the entire process take?\nAnswer\nWe assume that load linked and store conditional are used to implement lock and\nunlock. Figure I.4 shows the sequence of bus events for a processor to traverse the\nbarrier, assuming that the first process to grab the bus does not have the lock. There\nis a slight difference for the last process to reach the barrier, as described in the\ncaption.\nFor the ith process, the number of bus transactions is 3i+4. The last process to\nreach the barrier requires one less. Thus, for n processes, the number of bus trans-\nactions is\nX\nn\ni\u00bc1\n3i + 4\n\u00f0\n\u00de\n \n!\n\u00031 \u00bc 3n2 + 11n\n2\n\u00031\nFor 10 processes, this is 204 bus cycles or 20,400 clock cycles! Our barrier\noperation takes almost twice as long as the 10-processor lock-unlock sequence.\nlocal_sense =! local_sense; /* toggle local_sense */\nlock (counterlock);/* ensure update atomic */\ncount=count+1;/* count arrivals */\nif (count==total) {/* all arrived */\ncount=0;/* reset counter */\nrelease=local_sense;/* release processes */\n}\nunlock (counterlock);/* unlock */\nspin (release==local_sense);/* wait for signal */\n}\nFigure I.3 Code for a sense-reversing barrier. The key to making the barrier reusable is\nthe use of an alternating pattern of values for the flag release, which controls the exit\nfrom the barrier. If a process races ahead to the next instance of this barrier while some\nother processes are still in the barrier, the fast process cannot trap the other processes,\nsince it does not reset the value of release as it did in Figure I.2.\nI.4\nSynchronization: Scaling Up\n\u25a0\nI-15"
    },
    {
        "page": 1163,
        "text": "As we can see from these examples, synchronization performance can be a\nreal bottleneck when there is substantial contention among multiple processes.\nWhen there is little contention and synchronization operations are infrequent,\nwe are primarily concerned about the latency of a synchronization primitive\u2014\nthat is, how long it takes an individual process to complete a synchronization\noperation. Our basic spin lock operation can do this in two bus cycles: one to\ninitially read the lock and one to write it. We could improve this to a single bus\ncycle by a variety of methods. For example, we could simply spin on the swap\noperation. If the lock were almost always free, this could be better, but if the\nlock were not free, it would lead to lots of bus traffic, since each attempt to\nlock the variable would lead to a bus cycle. In practice, the latency of our spin\nlock is not quite as bad as we have seen in this example, since the write miss\nfor a data item present in the cache is treated as an upgrade and will be cheaper\nthan a true read miss.\nThe more serious problem in these examples is the serialization of each pro-\ncess\u2019s attempt to complete the synchronization. This serialization is a problem\nwhen there is contention because it greatly increases the time to complete the\nsynchronization operation. For example, if the time to complete all 10 lock\nand unlock operations depended only on the latency in the uncontended case,\nthen it would take 1000 rather than 15,000 cycles to complete the synchroniza-\ntion operations. The barrier situation is as bad, and in some ways worse, since it\nis highly likely to incur contention. The use of a bus interconnect exacerbates\nthese problems, but serialization could be just as serious in a directory-based\nmultiprocessor, where the latency would be large. The next subsection presents\nsome solutions that are useful when either the contention is high or the processor\ncount is large.\nEvent\nNumber of\ntimes for\nprocess i\nCorresponding source line\nComment\nLL counterlock\ni\nlock (counterlock);\nAll processes try for lock.\nStore conditional\ni\nlock (counterlock);\nAll processes try for lock.\nLD count\n1\ncount = count + 1;\nSuccessful process.\nLoad linked\ni\u00031\nlock (counterlock);\nUnsuccessful process; try again.\nSD count\n1\ncount = count + 1;\nMiss to get exclusive access.\nSD counterlock\n1\nunlock(counterlock);\nMiss to get the lock.\nLD release\n2\nspin (release==local_sense);/\nRead release: misses initially and\nwhen finally written.\nFigure I.4 Here are the actions, which require a bus transaction, taken when the ith process reaches the barrier.\nThe last process to reach the barrier requires one less bus transaction, since its read of release for the spin will hit in\nthe cache!\nI-16\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1164,
        "text": "Synchronization Mechanisms for Larger-Scale Multiprocessors\nWhat we would like are synchronization mechanisms that have low latency in\nuncontended cases and that minimize serialization in the case where contention\nis significant. We begin by showing how software implementations can improve\nthe performance of locks and barriers when contention is high; we then explore two\nbasic hardware primitives that reduce serialization while keeping latency low.\nSoftware Implementations\nThe major difficulty with our spin lock implementation is the delay due to conten-\ntion when many processes are spinning on the lock. One solution is to artificially\ndelay processes when they fail to acquire the lock. The best performance is\nobtained by increasing the delay exponentially whenever the attempt to acquire\nthe lock fails. Figure I.5 shows how a spin lock with exponential back-off is imple-\nmented. Exponential back-off is a common technique for reducing contention in\nshared resources, including access to shared networks and buses (see Sections\nF.4 to F.8). This implementation still attempts to preserve low latency when con-\ntention is small by not delaying the initial spin loop. The result is that if many pro-\ncesses are waiting, the back-off does not affect the processes on their first attempt\nto acquire the lock. We could also delay that process, but the result would be poorer\nDADDUI\nR3,R0,#1\n;R3 = initial delay\nlockit:\nLL\nR2,0(R1)\n;load linked\nBNEZ\nR2,lockit\n;not available-spin\nDADDUI\nR2,R2,#1\n;get locked value\nSC\nR2,0(R1)\n;store conditional\nBNEZ\nR2,gotit\n;branch if store succeeds\nDSLL\nR3,R3,#1\n;increase delay by factor of 2\nPAUSE\nR3\n;delays by value in R3\nJ\nlockit\ngotit:\nuse data protected by lock\nFigure I.5 A spin lock with exponential back-off. When the store conditional fails, the\nprocess delays itself by the value in R3. The delay can be implemented by decrementing\na copy of the value in R3 until it reaches 0. The exact timing of the delay is multiproces-\nsor dependent, although it should start with a value that is approximately the time to\nperform the critical section and release the lock. The statement pause R3 should cause\na delay of R3 of these time units. The value in R3 is increased by a factor of 2 every time\nthe store conditional fails, which causes the process to wait twice as long before trying\nto acquire the lock again. The small variations in the rate at which competing processors\nexecute instructions are usually sufficient to ensure that processes will not continually\ncollide. If the natural perturbation in execution time was insufficient, R3 could be initial-\nized with a small random value, increasing the variance in the successive delays and\nreducing the probability of successive collisions.\nI.4\nSynchronization: Scaling Up\n\u25a0\nI-17"
    },
    {
        "page": 1165,
        "text": "performance when the lock was in use by only two processes and the first one hap-\npened to find it locked.\nAnother technique for implementing locks is to use queuing locks. Queuing\nlocks work by constructing a queue of waiting processors; whenever a processor\nfrees up the lock, it causes the next processor in the queue to attempt access. This\neliminates contention for a lock when it is freed. We show how queuing locks oper-\nate in the next section using a hardware implementation, but software implementa-\ntions using arrays can achieve most of the same benefits. Before we look at\nhardware primitives, let\u2019s look at a better mechanism for barriers.\nOur barrier implementation suffers from contention both during the gather\nstage, when we must atomically update the count, and at the release stage, when\nall the processes must read the release flag. The former is more serious because it\nrequires exclusive access to the synchronization variable and thus creates much\nmore serialization; in comparison, the latter generates only read contention. We\ncan reduce the contention by using a combining tree, a structure where multiple\nrequests are locally combined in tree fashion. The same combining tree can be used\nto implement the release process, reducing the contention there.\nOur combining tree barrier uses a predetermined n-ary tree structure. We use\nthe variable k to stand for the fan-in; in practice, k\u00bc4 seems to work well. When\nthe kth process arrives at a node in the tree, we signal the next level in the tree.\nWhen a process arrives at the root, we release all waiting processes. As in our ear-\nlier example, we use a sense-reversing technique. A tree-based barrier, as shown in\nFigure I.6, uses a tree to combine the processes and a single signal to release the\nbarrier. Some MPPs (e.g., the T3D and CM-5) have also included hardware sup-\nport for barriers, but more recent machines have relied on software libraries for this\nsupport.\nHardware Primitives\nIn this subsection, we look at two hardware synchronization primitives. The first\nprimitive deals with locks, while the second is useful for barriers and a number of\nother user-level operations that require counting or supplying distinct indices. In\nboth cases, we can create a hardware primitive where latency is essentially iden-\ntical to our earlier version, but with much less serialization, leading to better scaling\nwhen there is contention.\nThe major problem with our original lock implementation is that it introduces a\nlarge amount of unneeded contention. For example, when the lock is released all\nprocessors generate both a read and a write miss, although at most one processor\ncan successfully get the lock in the unlocked state. This sequence happens on each\nof the 10 lock/unlock sequences, as we saw in the example on page I-12.\nWe can improve this situation by explicitly handing the lock from one waiting\nprocessor to the next. Rather than simply allowing all processors to compete every\ntime the lock is released, we keep a list of the waiting processors and hand the lock\nto one explicitly, when its turn comes. This sort of mechanism has been called a\nqueuing lock. Queuing locks can be implemented either in hardware, which we\nI-18\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1166,
        "text": "describe here, or in software using an array to keep track of the waiting processes.\nThe basic concepts are the same in either case. Our hardware implementation\nassumes a directory-based multiprocessor where the individual processor caches\nare addressable. In a bus-based multiprocessor, a software implementation would\nbe more appropriate and would have each processor using a different address for\nthe lock, permitting the explicit transfer of the lock from one process to another.\nHow does a queuing lock work? On the first miss to the lock variable, the\nmiss is sent to a synchronization controller, which may be integrated with the\nmemory controller (in a bus-based system) or with the directory controller. If\nthe lock is free, it is simply returned to the processor. If the lock is unavailable,\nstruct node{/* a node in the combining tree */\nint counterlock; /* lock for this node */\nint count; /* counter for this node */\nint parent; /* parent in the tree = 0..P-1 except for root */\n};\nstruct node tree [0..P\u20131]; /* the tree of nodes */\nint local_sense; /* private per processor */\nint release; /* global release flag */\n/* function to implement barrier */\nbarrier (int mynode, int local_sense) {\nlock (tree[mynode].counterlock); /* protect count */\ntree[mynode].count=tree[mynode].count+1;\n/* increment count */\nif (tree[mynode].count==k) {/* all arrived at mynode */\nif (tree[mynode].parent >=0) {\nbarrier(tree[mynode].parent);\n} else{\nrelease = local_sense;\n};\ntree[mynode].count = 0; /* reset for the next time */\nunlock (tree[mynode].counterlock); /* unlock */\nspin (release==local_sense); /* wait */\n};\n/* code executed by a processor to join barrier */\nlocal_sense =! local_sense;\nbarrier (mynode);\nFigure I.6 An implementation of a tree-based barrier reduces contention consider-\nably. The tree is assumed to be prebuilt statically using the nodes in the array tree. Each\nnode in the tree combines k processes and provides a separate counter and lock, so that\nat most k processes contend at each node. When the kth process reaches a node in the\ntree, it goes up to the parent, incrementing the count at the parent. When the count in\nthe parent node reaches k, the release flag is set. The count in each node is reset by the\nlast process to arrive. Sense-reversing is used to avoid races as in the simple barrier. The\nvalue of tree[root].parent should be set to \u00031 when the tree is initially built.\nI.4\nSynchronization: Scaling Up\n\u25a0\nI-19"
    },
    {
        "page": 1167,
        "text": "the controller creates a record of the node\u2019s request (such as a bit in a vector) and\nsends the processor back a locked value for the variable, which the processor then\nspins on. When the lock is freed, the controller selects a processor to go ahead\nfrom the list of waiting processors. It can then either update the lock variable\nin the selected processor\u2019s cache or invalidate the copy, causing the processor\nto miss and fetch an available copy of the lock.\nExample\nHow many bus transactions and how long does it take to have 10 processors lock\nand unlock the variable using a queuing lock that updates the lock on a miss? Make\nthe other assumptions about the system the same as those in the earlier example on\npage I-12.\nAnswer\nFor n processors, each will initially attempt a lock access, generating a bus trans-\naction; one will succeed and free up the lock, for a total of n+1 transactions for the\nfirst processor. Each subsequent processor requires two bus transactions, one to\nreceive the lock and one to free it up. Thus, the total number of bus transactions\nis (n+1)+2(n\u00031)\u00bc3n\u00031. Note that the number of bus transactions is now linear\nin the number of processors contending for the lock, rather than quadratic, as it was\nwith the spin lock we examined earlier. For 10 processors, this requires 29 bus\ncycles or 2900 clock cycles.\nThere are a couple of key insights in implementing such a queuing lock capability.\nFirst, we need to be able to distinguish the initial access to the lock, so we can per-\nform the queuing operation, and also the lock release, so we can provide the lock to\nanother processor. The queue of waiting processes can be implemented by a variety\nof mechanisms. In a directory-based multiprocessor, this queue is akin to the shar-\ning set, and similar hardware can be used to implement the directory and queuing\nlock operations. One complication is that the hardware must be prepared to reclaim\nsuch locks, since the process that requested the lock may have been context-\nswitched and may not even be scheduled again on the same processor.\nQueuing locks can be used to improve the performance of our barrier operation.\nAlternatively, we can introduce a primitive that reduces the amount of time needed\nto increment the barrier count, thus reducing the serialization at this bottleneck,\nwhich should yield comparable performance to using queuing locks. One primitive\nthat has been introduced for this and for building other synchronization operations\nis fetch-and-increment, which atomically fetches a variable and increments its\nvalue. The returned value can be either the incremented value or the fetched value.\nUsing fetch-and-increment we can dramatically improve our barrier implementa-\ntion, compared to the simple code-sensing barrier.\nExample\nWrite the code for the barrier using fetch-and-increment. Making the same assump-\ntions as in our earlier example and also assuming that a fetch-and-increment oper-\nation, which returns the incremented value, takes 100 clock cycles, determine the\ntime for 10 processors to traverse the barrier. How many bus cycles are required?\nI-20\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1168,
        "text": "Answer\nFigure I.7 shows the code for the barrier. For n processors, this implementation\nrequires n fetch-and-increment operations, n cache misses to access the count,\nand n cache misses for the release operation, for a total of 3n bus transactions.\nFor 10 processors, this is 30 bus transactions or 3000 clock cycles. Like the queue-\ning lock, the time is linear in the number of processors. Of course, fetch-and-\nincrement can also be used in implementing the combining tree barrier, reducing\nthe serialization at each node in the tree.\nAs we have seen, synchronization problems can become quite acute in largerscale\nmultiprocessors. When the challenges posed by synchronization are combined\nwith the challenges posed by long memory latency and potential load imbalance\nin computations, we can see why getting efficient usage of large-scale parallel pro-\ncessors is very challenging.\nI.5\nPerformance of Scientific Applications on\nShared-Memory Multiprocessors\nThis section covers the performance of the scientific applications from\nSection I.3 on both symmetric shared-memory and distributed shared-memory\nmultiprocessors.\nPerformance of a Scientific Workload on a Symmetric\nShared-Memory Multiprocessor\nWe evaluate the performance of our four scientific applications on a symmetric\nshared-memory multiprocessor using the following problem sizes:\n\u25a0\nBarnes-Hut\u201416 K bodies run for six time steps (the accuracy control is set to\n1.0, a typical, realistic value)\nlocal_sense =! local_sense; /* toggle local_sense */\nfetch_and_increment(count);/* atomic update */\nif (count==total) {/* all arrived */\ncount=0;/* reset counter */\nrelease=local_sense;/* release processes */\n}\nelse {/* more to come */\nspin (release==local_sense);/* wait for signal */\n}\nFigure I.7 Code for a sense-reversing barrier using fetch-and-increment to do the\ncounting.\nI.5\nPerformance of Scientific Applications on Shared-Memory Multiprocessors\n\u25a0\nI-21"
    },
    {
        "page": 1169,
        "text": "\u25a0\nFFT\u20141 million complex data points\n\u25a0\nLU\u2014A 512512 matrix is used with 1616 blocks\n\u25a0\nOcean\u2014A 130130 grid with a typical error tolerance\nIn looking at the miss rates as we vary processor count, cache size, and block\nsize, we decompose the total miss rate into coherence misses and normal uni-\nprocessor misses. The normal uniprocessor misses consist of capacity, conflict,\nand compulsory misses. We label these misses as capacity misses because that\nis the dominant cause for these benchmarks. For these measurements, we\ninclude as a coherence miss any write misses needed to upgrade a block from\nshared to exclusive, even though no one is sharing the cache block. This mea-\nsurement reflects a protocol that does not distinguish between a private and\nshared cache block.\nFigure I.8 shows the data miss rates for our four applications, as we increase the\nnumber of processors from 1 to 16, while keeping the problem size constant. As we\nincrease the number of processors, the total amount of cache increases, usually\ncausing the capacity misses to drop. In contrast, increasing the processor count\nusually causes the amount of communication to increase, in turn causing the coher-\nence misses to rise. The magnitude of these two effects differs by application.\nIn FFT, the capacity miss rate drops (from nearly 7% to just over 5%) but the\ncoherence miss rate increases (from about 1% to about 2.7%), leading to a constant\noverall miss rate. Ocean shows a combination of effects, including some that relate\nto the partitioning of the grid and how grid boundaries map to cache blocks. For a\ntypical 2D grid code the communication-generated misses are proportional to the\nboundary of each partition of the grid, while the capacity misses are proportional to\nthe area of the grid. Therefore, increasing the total amount of cache while keeping\nthe total problem size fixed will have a more significant effect on the capacity miss\nrate, at least until each subgrid fits within an individual processor\u2019s cache. The sig-\nnificant jump in miss rate between one and two processors occurs because of con-\nflicts that arise from the way in which the multiple grids are mapped to the caches.\nThis conflict is present for direct-mapped and two-way set associative caches, but\nfades at higher associativities. Such conflicts are not unusual in array-based appli-\ncations, especially when there are multiple grids in use at once. In Barnes and LU,\nthe increase in processor count has little effect on the miss rate, sometimes causing\na slight increase and sometimes causing a slight decrease.\nIncreasing the cache size usually has a beneficial effect on performance, since it\nreduces the frequency of costly cache misses. Figure I.9 illustrates the change in\nmiss rate as cache size is increased for 16 processors, showing the portion of the\nmiss rate due to coherence misses and to uniprocessor capacity misses. Two effects\ncan lead to a miss rate that does not decrease\u2014at least not as quickly as we might\nexpect\u2014as cache size increases: inherent communication and plateaus in the miss\nrate. Inherent communication leads to a certain frequency of coherence misses that\nare not significantly affected by increasing cache size. Thus, if the cache size is\nincreased while maintaining a fixed problem size, the coherence miss rate\nI-22\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1170,
        "text": "Miss rate\n0%\n3%\n2%\n1%\n1\n2\n4\nProcessor count\nFFT\n8\n16\n8%\n4%\n7%\n6%\n5%\nMiss rate\n0%\n6%\n4%\n2%\n1\n2\n4\nProcessor count\nOcean\n8\n16\n16%\n18%\n20%\n8%\n14%\n12%\n10%\nMiss rate\n0%\n1%\n1\n2\n4\nProcessor count\nLU\n8\n16\n2%\nMiss rate\n0%\n1\n2\n4\nProcessor count\nBarnes\n8\n16\n1%\nCoherence miss rate\nCapacity miss rate\nFigure I.8 Data miss rates can vary in nonobvious ways as the processor count is\nincreased from 1 to 16. The miss rates include both coherence and capacity miss rates.\nThe compulsory misses in these benchmarks are all very small and are included in the\ncapacity misses. Most of the misses in these applications are generated by accesses to\ndata that are potentially shared, although in the applications with larger miss rates (FFT\nand Ocean), it is the capacity misses rather than the coherence misses that comprise the\nmajority of the miss rate. Data are potentially shared if they are allocated in a portion of\nthe address space used for shared data. In all except Ocean, the potentially shared data\nare heavily shared, while in Ocean only the boundaries of the subgrids are actually\nshared, although the entire grid is treated as a potentially shared data object. Of course,\nsince the boundaries change as we increase the processor count (for a fixed-size prob-\nlem), different amounts of the grid become shared. The anomalous increase in capacity\nmiss rate for Ocean in moving from 1 to 2 processors arises because of conflict misses in\naccessing the subgrids. In all cases except Ocean, the fraction of the cache misses\ncaused by coherence transactions rises when a fixed-size problem is run on an increas-\ning number of processors. In Ocean, the coherence misses initially fall as we add pro-\ncessors due to a large number of misses that are write ownership misses to data that\nare potentially, but not actually, shared. As the subgrids begin to fit in the aggregate\ncache (around 16 processors), this effect lessens. The single-processor numbers include\nwrite upgrade misses, which occur in this protocol even if the data are not actually\nshared, since they are in the shared state. For all these runs, the cache size is 64 KB,\ntwo-way set associative, with 32-byte blocks. Notice that the scale on the y-axis for each\nbenchmark is different, so that the behavior of the individual benchmarks can be seen\nclearly.\nI.5\nPerformance of Scientific Applications on Shared-Memory Multiprocessors\n\u25a0\nI-23"
    },
    {
        "page": 1171,
        "text": "eventually limits the decrease in cache miss rate. This effect is most obvious in\nBarnes, where the coherence miss rate essentially becomes the entire miss rate.\nA less important effect is a temporary plateau in the capacity miss rate that\narises when the application has some fraction of its data present in cache but some\nsignificant portion of the dataset does not fit in the cache or in caches that are\nslightly bigger. In LU, a very small cache (about 4 KB) can capture the pair of\n1616 blocks used in the inner loop; beyond that, the next big improvement in\ncapacity miss rate occurs when both matrices fit in the caches, which occurs when\nthe total cache size is between 4 MB and 8 MB. This effect, sometimes called a\nworking set effect, is partly at work between 32 KB and 128 KB for FFT, where\nthe capacity miss rate drops only 0.3%. Beyond that cache size, a faster decrease in\nthe capacity miss rate is seen, as a major data structure begins to reside in the cache.\nThese plateaus are common in programs that deal with large arrays in a structured\nfashion.\nIncreasing the block size is another way to change the miss rate in a cache. In\nuniprocessors, larger block sizes are often optimal with larger caches. In\nMiss rate\n0%\n4%\n2%\n32\n64\n128\nCache size (KB)\nFFT\n256\n10%\n6%\n8%\nMiss rate\n0%\n1.5%\n1.0%\n32\n64\n128\nCache size (KB)\nLU\n256\n2.5%\n2.0%\nMiss rate\n0%\n6%\n2%\n4%\n32\n64\n128\nCache size (KB)\nOcean\n256\n14%\n10%\n8%\n12%\nMiss rate\n0%\n1.0%\n32\n64\n128\nCache size (KB)\nBarnes\n256\n2.0%\n1.5%\nCoherence miss rate\nCapacity miss rate\nFigure I.9 The miss rate usually drops as the cache size is increased, although coher-\nence misses dampen the effect. The block size is 32 bytes and the cache is two-way set\nassociative. The processor count is fixed at 16 processors. Observe that the scale for each\ngraph is different.\nI-24\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1172,
        "text": "multiprocessors, two new effects come into play: a reduction in spatial locality\nfor shared data and a potential increase in miss rate due to false sharing. Several\nstudies have shown that shared data have lower spatial locality than unshared\ndata. Poorer locality means that, for shared data, fetching larger blocks is less\neffective than in a uniprocessor because the probability is higher that the block\nwill be replaced before all its contents are referenced. Likewise, increasing the\nbasic size also increases the potential frequency of false sharing, increasing\nthe miss rate.\nFigure I.10 shows the miss rates as the cache block size is increased for a 16-\nprocessor run with a 64 KB cache. The most interesting behavior is in Barnes,\nwhere the miss rate initially declines and then rises due to an increase in the number\nof coherence misses, which probably occurs because of false sharing. In the other\nbenchmarks, increasing the block size decreases the overall miss rate. In Ocean and\nLU, the block size increase affects both the coherence and capacity miss rates about\nequally. In FFT, the coherence miss rate is actually decreased at a faster rate than\nthe capacity miss rate. This reduction occurs because the communication in FFT is\nstructured to be very efficient. In less optimized programs, we would expect more\nMiss rate\n0%\n6%\n4%\n2%\n16\n32\n64\nBlock size (bytes)\nFFT\n128\n14%\n10%\n8%\n12%\nMiss rate\n0%\n2%\n1%\n16\n32\n64\nBlock size (bytes)\nLU\n128\n4%\n3%\nMiss rate\n0%\n6%\n2%\n4%\n16\n32\n64\nBlock size (bytes)\nOcean\n128\n14%\n10%\n8%\n12%\nMiss rate\n0%\n16\n32\n64\nBlock size (bytes)\nBarnes\n128\n1%\nCoherence miss rate\nCapacity miss rate\nFigure I.10 The data miss rate drops as the cache block size is increased. All these\nresults are for a 16-processor run with a 64 KB cache and two-way set associativity. Once\nagain we use different scales for each benchmark.\nI.5\nPerformance of Scientific Applications on Shared-Memory Multiprocessors\n\u25a0\nI-25"
    },
    {
        "page": 1173,
        "text": "false sharing and less spatial locality for shared data, resulting in more behavior\nlike that of Barnes.\nAlthough the drop in miss rates with longer blocks may lead you to believe\nthat choosing a longer block size is the best decision, the bottleneck in bus-based\nmultiprocessors is often the limited memory and bus bandwidth. Larger blocks\nmean more bytes on the bus per miss. Figure I.11 shows the growth in bus traffic\nas the block size is increased. This growth is most serious in the programs that\nhave a high miss rate, especially Ocean. The growth in traffic can actually lead to\nperformance slowdowns due both to longer miss penalties and to increased bus\ncontention.\nPerformance of a Scientific Workload on a Distributed-Memory\nMultiprocessor\nThe performance of a directory-based multiprocessor depends on many of the same\nfactors that influence the performance of bus-based multiprocessors (e.g., cache\nsize, processor count, and block size), as well as the distribution of misses to var-\nious locations in the memory hierarchy. The location of a requested data item\ndepends on both the initial allocation and the sharing patterns. We start by exam-\nining the basic cache performance of our scientific/technical workload and then\nlook at the effect of different types of misses.\n7.0\n4.0\n5.0\n6.0\n3.0\n2.0\n1.0\nBytes per data reference\n0.0\nBlock size (bytes)\n16\n32\n64\n128\nFFT\nLU\nBarnes\nOcean\nFigure I.11 Bus traffic for data misses climbs steadily as the block size in the data\ncache is increased. The factor of 3 increase in traffic for Ocean is the best argument\nagainst larger block sizes. Remember that our protocol treats ownership or upgrade\nmisses the same as other misses, slightly increasing the penalty for large cache blocks;\nin both Ocean and FFT, this simplification accounts for less than 10% of the traffic.\nI-26\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1174,
        "text": "Because the multiprocessor is larger and has longer latencies than our\nsnooping-based multiprocessor, we begin with a slightly larger cache (128 KB)\nand a larger block size of 64 bytes.\nIn distributed-memory architectures, the distribution of memory requests\nbetween local and remote is key to performance because it affects both the con-\nsumption of global bandwidth and the latency seen by requests. Therefore, for\nthe figures in this section, we separate the cache misses into local and remote\nrequests. In looking at the figures, keep in mind that, for these applications, most\nof the remote misses that arise are coherence misses, although some capacity mis-\nses can also be remote, and in some applications with poor data distribution such\nmisses can be significant.\nAs Figure I.12 shows, the miss rates with these cache sizes are not affected\nmuch by changes in processor count, with the exception of Ocean, where the miss\nrate rises at 64 processors. This rise results from two factors: an increase in map-\nping conflicts in the cache that occur when the grid becomes small, which leads to a\nrise in local misses, and an increase in the number of the coherence misses, which\nare all remote.\nFigure I.13 shows how the miss rates change as the cache size is increased,\nassuming a 64-processor execution and 64-byte blocks. These miss rates decrease\nat rates that we might expect, although the dampening effect caused by little or no\nreduction in coherence misses leads to a slower decrease in the remote misses than\nin the local misses. By the time we reach the largest cache size shown, 512 KB, the\nremote miss rate is equal to or greater than the local miss rate. Larger caches would\namplify this trend.\nWe examine the effect of changing the block size in Figure I.14. Because these\napplications have good spatial locality, increases in block size reduce the miss rate,\neven for large blocks, although the performance benefits for going to the largest\nblocks are small. Furthermore, most of the improvement in miss rate comes from\na reduction in the local misses.\nRather than plot the memory traffic, Figure I.15 plots the number of bytes\nrequired per data reference versus block size, breaking the requirement into local\nand global bandwidth. In the case of a bus, we can simply aggregate the demands of\neach processor to find the total demand for bus and memory bandwidth. For a scal-\nable interconnect, we can use the data in Figure I.15 to compute the required per-\nnode global bandwidth and the estimated bisection bandwidth, as the next example\nshows.\nExample\nAssume a 64-processor multiprocessor with 1 GHz processors that sustain one\nmemory reference per processor clock. For a 64-byte block size, the remote miss\nrate is 0.7%. Find the per-node and estimated bisection bandwidth for FFT.\nAssume that the processor does not stall for remote memory requests; this might\nbe true if, for example, all remote data were prefetched. How do these bandwidth\nrequirements compare to various interconnection technologies?\nI.5\nPerformance of Scientific Applications on Shared-Memory Multiprocessors\n\u25a0\nI-27"
    },
    {
        "page": 1175,
        "text": "FFT performs all-to-all communication, so the bisection bandwidth is equal to\nthe number of processors times the per-node bandwidth, or about 64448 MB/\nsec\u00bc28.7 GB/sec. The SGI Origin 3000 with 64 processors has a bisection band-\nwidth of about 50 GB/sec. No standard networking technology comes close.\nAnswer\nThe per-node bandwidth is simply the number of data bytes per reference times the\nreference rate: 0.7%1 GB/sec64\u00bc448 MB/sec. This rate is somewhat higher\nthan the hardware sustainable transfer rate for the CrayT3E (using a block prefetch)\nMiss rate\n0%\n3%\n2%\n1%\n8\n16\n32\nProcessor count\nFFT\n64\n6%\n4%\n5%\nMiss rate\n0.0%\n0.5%\n8\n16\n32\nProcessor count\nLU\n64\n1.0%\nMiss rate\n0%\n4%\n2%\n8\n16\n32\nProcessor count\nOcean\n64\n8%\n6%\nMiss rate\n0.0%\n8\n16\n32\nProcessor count\nBarnes\n64\n0.5%\nLocal misses\nRemote misses\nFigure I.12 The data miss rate is often steady as processors are added for these\nbenchmarks. Because of its grid structure, Ocean has an initially decreasing miss rate,\nwhich rises when there are 64 processors. For Ocean, the local miss rate drops from 5%\nat 8 processors to 2% at 32, before rising to 4% at 64. The remote miss rate in Ocean,\ndriven primarily by communication, rises monotonically from 1% to 2.5%. Note that, to\nshow the detailed behavior of each benchmark, different scales are used on the y-axis.\nThe cache for all these runs is 128 KB, two-way set associative, with 64-byte blocks.\nRemote misses include any misses that require communication with another node,\nwhether to fetch the data or to deliver an invalidate. In particular, in this figure and other\ndata in this section, the measurement of remote misses includes write upgrade misses\nwhere the data are up to date in the local memory but cached elsewhere and, therefore,\nrequire invalidations to be sent. Such invalidations do indeed generate remote traffic,\nbut may or may not delay the write, depending on the consistency model.\nI-28\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1176,
        "text": "and lower than that for an SGI Origin 3000 (1.6 GB/processor pair). The FFT\nper-node bandwidth demand exceeds the bandwidth sustainable from the fastest\nstandard networks by more than a factor of 5.\nThe previous example looked at the bandwidth demands. The other key issue for a\nparallel program is remote memory access time, or latency. To get insight into this,\nwe use a simple example of a directory-based multiprocessor. Figure I.16 shows\nthe parameters we assume for our simple multiprocessor model. It assumes that the\ntime to first word for a local memory access is 85 processor cycles and that the path\nto local memory is 16 bytes wide, while the network interconnect is 4 bytes wide.\nThis model ignores the effects of contention, which are probably not too serious in\nthe parallel benchmarks we examine, with the possible exception of FFT, which\nuses all-to-all communication. Contention could have a serious performance\nimpact in other workloads.\nMiss rate\n0%\n4%\n2%\n32\n64\n128\nCache size (KB)\nFFT\n256 512\n10%\n6%\n8%\nMiss rate\n0.0%\n1.0%\n0.5%\n32\n64\n128\nCache size (KB)\nLU\nOcean\n256 512\n2.5%\n1.5%\n2.0%\nMiss rate\n0.0%\n0.5%\n32\n64\n128\nCache size (KB)\nBarnes\n256 512\n1.5%\n1.0%\nMiss rate\n0%\n10%\n5%\n32\n64\n128\nCache size (KB)\n256 512\n20%\n15%\nLocal misses\nRemote misses\nFigure I.13 Miss rates decrease as cache sizes grow. Steady decreases are seen in the\nlocal miss rate, while the remote miss rate declines to varying degrees, depending on\nwhether the remote miss rate had a large capacity component or was driven primarily\nby communication misses. In all cases, the decrease in the local miss rate is larger than\nthe decrease in the remote miss rate. The plateau in the miss rate of FFT, which we men-\ntioned in the last section, ends once the cache exceeds 128 KB. These runs were done\nwith 64 processors and 64-byte cache blocks.\nI.5\nPerformance of Scientific Applications on Shared-Memory Multiprocessors\n\u25a0\nI-29"
    },
    {
        "page": 1177,
        "text": "Figure I.17 shows the cost in cycles for the average memory reference,\nassuming the parameters in Figure I.16. Only the latencies for each reference\ntype are counted. Each bar indicates the contribution from cache hits, local\nmisses, remote misses, and three-hop remote misses. The cost is influenced\nby the total frequency of cache misses and upgrades, as well as by the distri-\nbution of the location where the miss is satisfied. The cost for a remote mem-\nory reference is fairly steady as the processor count is increased, except for\nOcean. The increasing miss rate in Ocean for 64 processors is clear in\nFigure I.12. As the miss rate increases, we should expect the time spent on\nmemory references to increase also.\nAlthough Figure I.17 shows the memory access cost, which is the dominant\nmultiprocessor cost in these benchmarks, a complete performance model would\nneed to consider the effect of contention in the memory system, as well as the\nlosses arising from synchronization delays.\nMiss rate\n0%\n4%\n6%\n2%\n16\n32\n64\nBlock size (bytes)\nFFT\n128\n12%\n8%\n10%\nMiss rate\n0%\n2%\n1%\n16\n32\n64\nBlock size (bytes)\nLU\n128\n4%\n3%\nMiss rate\n0%\n5%\n10%\n16\n32\n64\nBlock size (bytes)\nOcean\n128\n15%\nMiss rate\n0.0%\n0.1%\n16\n32\n64\nBlock size (bytes)\nBarnes\n128\n0.3%\n0.2%\nLocal misses\nRemote misses\nFigure I.14 Data miss rate versus block size assuming a 128 KB cache and 64 pro-\ncessors in total. Although difficult to see, the coherence miss rate in Barnes actually\nrises for the largest block size, just as in the last section.\nI-30\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1178,
        "text": "Bytes per data reference\nBytes per data reference\nBytes per data reference\nBytes per data reference\n0.0\n2.0\n3.0\n1.0\n16\n32\n64\nBlock size (bytes)\nFFT\n128\n6.0\n4.0\n5.0\n0.0\n0.2\n0.3\n0.1\n16\n32\n64\nBlock size (bytes)\nLU\n128\n0.6\n0.4\n0.5\n0.0\n2.0\n4.0\n6.0\n5.0\n3.0\n1.0\n16\n32\n64\nBlock size (bytes)\nOcean\n128\n7.0\n0.0\n0.1\n16\n32\n64\nBlock size (bytes)\nBarnes\n128\n0.4\n0.3\n0.2\nLocal\nGlobal\nFigure I.15 The number of bytes per data reference climbs steadily as block size is\nincreased. These data can be used to determine the bandwidth required per node both\ninternally and globally. The data assume a 128 KB cache for each of 64 processors.\nCharacteristic\nProcessor clock cycles\n\u226416 processors\nProcessor clock cycles\n17\u201364 processors\nCache hit\n1\n1\nCache miss to local memory\n85\n85\nCache miss to remote home\ndirectory\n125\n150\nCache miss to remotely cached\ndata (three-hop miss)\n140\n170\nFigure I.16 Characteristics of the example directory-based multiprocessor. Misses\ncan be serviced locally (including from the local directory), at a remote home node,\nor using the services of both the home node and another remote node that is caching\nan exclusive copy. This last case is called a three-hop miss and has a higher cost because\nit requires interrogating both the home directory and a remote cache. Note that this\nsimple model does not account for invalidation time but does include some factor\nfor increasing interconnect time. These remote access latencies are based on those\nin an SGI Origin 3000, the fastest scalable interconnect system in 2001, and assume a\n500 MHz processor.\nI.5\nPerformance of Scientific Applications on Shared-Memory Multiprocessors\n\u25a0\nI-31"
    },
    {
        "page": 1179,
        "text": "Average cycles per reference\n0.0\n8\n16\n32\nProcessor count\nFFT\n64\n5.5\n5.0\n4.5\n4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\nAverage cycles per reference\n0.0\n8\n16\n32\nProcessor count\nLU\n64\n5.5\n5.0\n4.5\n4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\nAverage cycles per memory\nreference\n0.0\n8\n16\n32\nProcessor count\nBarnes\n64\n5.5\n5.0\n4.5\n4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\nAverage cycles per reference\n0.0\n8\n16\n32\nProcessor count\nOcean\n64\n5.5\n5.0\n4.5\n4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\nCache hit\nLocal miss\nRemote miss\nThree-hop miss to remote cache\nFigure I.17 The effective latency of memory references in a DSM multiprocessor\ndepends both on the relative frequency of cache misses and on the location of\nthe memory where the accesses are served. These plots show the memory access cost\n(a metric called average memory access time in Chapter 2) for each of the benchmarks\nfor 8, 16, 32, and 64 processors, assuming a 512 KB data cache that is two-way set asso-\nciative with 64-byte blocks. The average memory access cost is composed of four dif-\nferent types of accesses, with the cost of each type given in Figure I.16. For the Barnes\nand LU benchmarks, the low miss rates lead to low overall access times. In FFT, the\nhigher access cost is determined by a higher local miss rate (1\u20134%) and a significant\nthree-hop miss rate (1%). The improvement in FFT comes from the reduction in local\nmiss rate from 4% to 1%, as the aggregate cache increases. Ocean shows the biggest\nchange in the cost of memory accesses, and the highest overall cost at 64 processors.\nThe high cost is driven primarily by a high local miss rate (average 1.6%). The memory\naccess cost drops from 8 to 16 processors as the grids more easily fit in the individual\ncaches. At 64 processors, the dataset size is too small to map properly and both local\nmisses and coherence misses rise, as we saw in Figure I.12.\nI-32\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1180,
        "text": "I.6\nPerformance Measurement of Parallel Processors\nwith Scientific Applications\nOne of the most controversial issues in parallel processing has been how to mea-\nsure the performance of parallel processors. Of course, the straightforward answer\nis to measure a benchmark as supplied and to examine wall-clock time. Measuring\nwall-clock time obviously makes sense; in a parallel processor, measuring CPU\ntime can be misleading because the processors may be idle but unavailable for\nother uses.\nUsers and designers are often interested in knowing not just how well a mul-\ntiprocessor performs with a certain fixed number of processors, but also how the\nperformance scales as more processors are added. In many cases, it makes sense to\nscale the application or benchmark, since if the benchmark is unscaled, effects aris-\ning from limited parallelism and increases in communication can lead to results\nthat are pessimistic when the expectation is that more processors will be used to\nsolve larger problems. Thus, it is often useful to measure the speedup as processors\nare added both for a fixed-size problem and for a scaled version of the problem,\nproviding an unscaled and a scaled version of the speedup curves. The choice\nof how to measure the uniprocessor algorithm is also important to avoid anomalous\nresults, since using the parallel version of the benchmark may understate the\nuniprocessor performance and thus overstate the speedup.\nOnce we have decided to measure scaled speedup, the question is how to scale\nthe application. Let\u2019s assume that we have determined that running a benchmark of\nsize n on p processors makes sense. The question is how to scale the benchmark to\nrun on mp processors. There are two obvious ways to scale the problem:\n(1) keeping the amount of memory used per processor constant, and (2) keeping\nthe total execution time, assuming perfect speedup, constant. The first method,\ncalled memory-constrained scaling, specifies running a problem of size mn\non mp processors. The second method, called time-constrained scaling, requires\nthat we know the relationship between the running time and the problem size, since\nthe former is kept constant. For example, suppose the running time of the appli-\ncation with data size n on p processors is proportional to n2/p. Then, with time-\nconstrained scaling, the problem to run is the problem whose ideal running time\non mp processors is still n2/p. The problem with this ideal running time has size\n\ufb03\ufb03\ufb03\ufb03m\np\nn.\nExample\nSuppose we have a problem whose execution time for a problem of size n is pro-\nportional to n3. Suppose the actual running time on a 10-processor multiprocessor\nis 1 hour. Under the time-constrained and memory-constrained scaling models,\nfind the size of the problem to run and the effective running time for a\n100-processor multiprocessor.\nAnswer\nFor the time-constrained problem, the ideal running time is the same, 1 hour,\nso the problem size is\n\ufb03\ufb03\ufb03\ufb03\ufb03\n10\n3p\nn or 2.15 times larger than the original. For\nI.6\nPerformance Measurement of Parallel Processors with Scientific Applications\n\u25a0\nI-33"
    },
    {
        "page": 1181,
        "text": "memory-constrained scaling, the size of the problem is 10n and the ideal execution\ntime is 103/10, or 100 hours! Since most users will be reluctant to run a problem on\nan order of magnitude more processors for 100 times longer, this size problem is\nprobably unrealistic.\nIn addition to the scaling methodology, there are questions as to how the pro-\ngram should be scaled when increasing the problem size affects the quality of the\nresult. Often, we must change other application parameters to deal with this effect.\nAs a simple example, consider the effect of time to convergence for solving a dif-\nferential equation. This time typically increases as the problem size increases,\nsince, for example, we often require more iterations for the larger problem. Thus,\nwhen we increase the problem size, the total running time may scale faster than the\nbasic algorithmic scaling would indicate.\nFor example, suppose that the number of iterations grows as the log of the prob-\nlem size. Then, for a problem whose algorithmic running time is linear in the size of\nthe problem, the effective running time actually grows proportional to n log n. If we\nscaled from a problem of size m on 10 processors, purely algorithmic scaling would\nallow us to run a problem of size 10 m on 100 processors. Accounting for the\nincrease in iterations means that a problem of size km, where k log k\u00bc10, will\nhave the same running time on 100 processors. This problem size yields a scaling\nof 5.72 m, rather than 10 m.\nIn practice, scaling to deal with error requires a good understanding of the\napplication and may involve other factors, such as error tolerances (for example,\nit affects the cell-opening criteria in Barnes-Hut). In turn, such effects often signif-\nicantly affect the communication or parallelism properties of the application as\nwell as the choice of problem size.\nScaled speedup is not the same as unscaled (or true) speedup; confusing the two\nhas led to erroneous claims (e.g., see the discussion in Section I.6). Scaled speedup\nhas an important role, but only when the scaling methodology is sound and the\nresults are clearly reported as using a scaled version of the application. Singh,\nHennessy, and Gupta [1993] described these issues in detail.\nI.7\nImplementing Cache Coherence\nIn this section, we explore the challenge of implementing cache coherence,\nstarting first by dealing with the challenges in a snooping coherence protocol,\nwhich we simply alluded to in Chapter 5. Implementing a directory protocol\nadds some additional complexity to a snooping protocol, primarily arising from\nthe absence of broadcast, which forces the use of a different mechanism to\nresolve races. Furthermore, the larger processor count of a directory-based\nmultiprocessor means that we cannot retain assumptions of unlimited buffering\nand must find new ways to avoid deadlock, Let\u2019s start with the snooping\nprotocols.\nI-34\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1182,
        "text": "As we mentioned in Chapter 5, the challenge of implementing misses in a\nsnooping coherence protocol without a bus lies in finding a way to make the multi-\nstep miss process appear atomic. Both an upgrade miss and a write miss require the\nsame basic processing and generate the same implementation challenges; for sim-\nplicity, we focus on upgrade misses. Here are the steps in handling an upgrade\nmiss:\n1. Detect the miss and compose an invalidate message for transmission to other\ncaches.\n2. When access to the broadcast communication link is available, transmit the\nmessage.\n3. When the invalidates have been processed, the processor updates the state of the\ncache block and then proceeds with the write that caused the upgrade miss.\nThere are two related difficulties that can arise. First, how will two processors, P1\nand P2, that attempt to upgrade the same cache block at the same time resolve the\nrace? Second, when at step 3, how does a processor know when all invalidates have\nbeen processed so that it can complete the step?\nThe solution to finding a winner in the race lies in the ordering imposed by the\nbroadcast communication medium. The communication medium must broadcast\nany cache miss to all the nodes. If P1 and P2 attempt to broadcast at the same time,\nwe must ensure that either P1\u2019s message will reach P2 first or P2\u2019s will reach P1 first.\nThis property will be true if there is a single channel through which all ingoing and\noutgoing requests from a node must pass through and if the communication network\ndoes not accept a message unless it can guarantee delivery (i.e., it is effectively circuit\nswitched, see Appendix F). If both P1 and P2 initiate their attempts to broadcast an\ninvalidate simultaneously, then the network can accept only one of these operations\nand delay the other. This ordering ensures that either P1 or P2 will complete its com-\nmunication in step 2 first. The network can explicitly signal when it accepts a mes-\nsage and can guarantee it will be the next transmission; alternatively, a processor can\nsimply watch the network for its own request, knowing that once the request is seen,\nit will be fully transmitted to all processors before any subsequent messages.\nNow, suppose P1 wins the race to transmit its invalidate; once it knows it has\nwon the race, it can continue with step 3 and complete the miss handling. There is a\npotential problem, however, for P2. When P2 undertook step 1, it believed that the\nblock was in the shared state, but for P1 to advance at step 3, it must know that P2\nhas processed the invalidate, which must change the state of the block at P2 to inva-\nlid! One simple solution is for P2 to notice that it has lost the race, by observing that\nP1\u2019s invalidate is broadcast before its own invalidate. P2 can then invalidate the\nblock and generate a write miss to get the data. P1 will see its invalidate before\nP2\u2019s, so it will change the block to modified and update the data, which guarantees\nforward progress and avoids deadlock. When P1 sees the subsequent invalidate to a\nblock in the Modified state (a possibility that cannot arise in our basic protocol\ndiscussed in Chapter 5), it knows that it was the winner of a race. It can simply\nI.7\nImplementing Cache Coherence\n\u25a0\nI-35"
    },
    {
        "page": 1183,
        "text": "ignore the invalidate, knowing that it will be followed by a write miss, or it can\nwrite the block back to memory and make its state invalid.\nAnother solution is to give precedence to incoming requests over outgoing\nrequests,sothatbeforeP2cantransmititsinvalidateitmusthandleanypendinginval-\nidates or write misses. If any of those misses are for blocks with the same address as a\npending outgoing message, the processor must be prepared to restart the write oper-\nation, since the incoming request may cause the state of the block to change. Notice\nthatP1knowsthattheinvalidateswillbeprocessedonceithassuccessfullycompleted\nthe broadcast, since precedence is given to invalidate messages over outgoing\nrequests.(Becauseitdoesnotemploybroadcast,aprocessorusingadirectoryprotocol\ncannot know when an invalidate is received; instead, explicit acknowledgments are\nrequired, as we discuss in the next section.Indeed, aswe will see,itcannot evenknow\nit has won the race to become the owner until its request is acknowledged.)\nReads will also require a multiple-step process, since we need to get the data\nback from memory or a remote cache (in a write-back cache system), but reads do\nnot introduce fundamentally new problems beyond what exists for writes.\nThere are, however, a few additional tricky edge cases that must be handled cor-\nrectly. For example, in a write-back cache, a processor can generate a read miss that\nrequires a write-back, which it could delay, while giving the read miss priority. If a\nsnooprequestappearsforthecacheblockthatistobewrittenback,theprocessormust\ndiscoverthisandsendthedataback.Failuretodosocancreateadeadlocksituation.A\nsimilar tricky situation exists when a processor generates a write miss, which will\nmake a block exclusive, but, before the processor receives the data and can update\ntheblock,otherprocessorsgeneratereadmissesforthatblock.Thereadmissescannot\nbe processed until the writing processor receives the data and updates the block.\nOne of the more difficult problems occurs in a write-back cache where the data\nfor a read or write miss can come either from memory or from one of the processor\ncaches, but the requesting processor will not know a priori where the data will\ncome from. In most bus-based systems, a single global signal is used to indicate\nwhether any processor has the exclusive (and hence the most up-to-date) copy; oth-\nerwise, the memory responds. These schemes can work with a pipelined intercon-\nnection by requiring that processors signal whether they have the exclusive copy\nwithin a fixed number of cycles after the miss is broadcast.\nIn a modern multiprocessor, however, it is essentially impossible to bound the\namount of time required for a snoop request to be processed. Instead, a mechanism\nis required to determine whether the memory has an up-to-date copy. One solution\nis to add coherence bits to the memory, indicating whether the data are exclusive in\na remote cache. This mechanism begins to move toward the directory approach,\nwhose implementation challenges we consider next.\nImplementing Cache Coherence in a DSM Multiprocessor\nImplementing a directory-based cache coherence protocol requires overcoming\nall the problems related to nonatomic actions for a snooping protocol without\nI-36\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1184,
        "text": "the use of broadcast (see Chapter 5), which forced a serialization on competing\nwrites and also ensured the serialization required for the memory consistency\nmodel. Avoiding the need to broadcast is a central goal for a directory-based\nsystem, so another method for ensuring serialization is necessary.\nThe serialization of requests for exclusive access to a memory block is eas-\nily enforced since those requests will be serialized when they reach the unique\ndirectory for the specified block. If the directory controller simply ensures that\none request is completely serviced before the next is begun, writes will be seri-\nalized. Because the requesters cannot know ahead of time who will win the\nrace and because the communication is not a broadcast, the directory must sig-\nnal to the winner when it completes the processing of the winner\u2019s request.\nThis is done by a message that supplies the data on a write miss or by an\nexplicit acknowledgment message that grants ownership in response to an\ninvalidation request.\nWhat about the loser in this race? The simplest solution is for the system to send\na negative acknowledge, or NAK, which requires that the requesting node regen-\nerate its request. (This is the equivalent of a collision in the broadcast network in a\nsnooping scheme, which requires that one of the transmitting nodes retry its com-\nmunication.) We will see in the next section why the NAK approach, as opposed to\nbuffering the request, is attractive.\nAlthough the acknowledgment that a requesting node has ownership is com-\npleted when the write miss or ownership acknowledgment message is transmit-\nted, we still do not know that the invalidates have been received and processed by\nthe nodes that were in the sharing set. All memory consistency models eventually\nrequire (either before the next cache miss or at a synchronization point, for exam-\nple) that a processor knows that all the invalidates for a write have been pro-\ncessed. In a snooping scheme, the nature of the broadcast network provides\nthis assurance.\nHow can we know when the invalidates are complete in a directory scheme?\nThe only way to know that the invalidates have been completed is to have the des-\ntination nodes of the invalidate messages (the members of the sharing set) explic-\nitly acknowledge the invalidation messages sent from the directory. Who should\nthey be acknowledged to? There are two possibilities. In the first the acknowledg-\nments can be sent to the directory, which can count them, and when all acknowl-\nedgments have been received, confirm this with a single message to the original\nrequester. Alternatively, when granting ownership, the directory can tell the reg-\nister how many acknowledgments to expect. The destinations of the invalidate\nmessages can then send an acknowledgment directly to the requester, whose iden-\ntity is provided by the directory. Most existing implementations use the latter\nscheme, since it reduces the possibility of creating a bottleneck at a directory.\nAlthough the requirement for acknowledgments is an additional complexity in\ndirectory protocols, this requirement arises from the avoidance of a serialization\nmechanism, such as the snooping broadcast operation, which in itself is the limit\nto scalability.\nI.7\nImplementing Cache Coherence\n\u25a0\nI-37"
    },
    {
        "page": 1185,
        "text": "Avoiding Deadlock from Limited Buffering\nA new complication in the implementation is introduced by the potential scale of a\ndirectory-based multiprocessor. In Chapter 5, we assumed that the network could\nalways accept a coherence message and that the request would be acted upon at\nsome point. In a much larger multiprocessor, this assumption of unlimited buffer-\ning may be unreasonable. What happens when the network does not have unlimited\nbuffering? The major implication of this limit is that a cache or directory controller\nmay be unable to complete a message send. This could lead to deadlock.\nThe potential deadlock arises from three properties, which characterize many\ndeadlock situations:\n1. More than one resource is needed to complete a transaction: Message buffers\nare needed to generate requests, create replies and acknowledgments, and\naccept replies.\n2. Resources are held until a nonatomic transaction completes: The buffer used to\ncreate the reply cannot be freed until the reply is accepted, for reasons we will\nsee shortly.\n3. There is no global partial order on the acquisition of resources: Nodes can gen-\nerate requests and replies at will.\nThese characteristics lead to deadlock, and avoiding deadlock requires breaking\none of these properties. Freeing up resources without completing a transaction\nis difficult, since the transaction must be completely backed out and cannot be left\nhalf-finished. Hence, our approach will be to try to resolve the need for multiple\nresources. We cannot simply eliminate this need, but we can try to ensure that the\nresources will always be available.\nOne way to ensure that a transaction can always complete is to guarantee that\nthere are always buffers to accept messages. Although this is possible for a small\nmultiprocessor with processors that block on a cache miss or have a small number\nof outstanding misses, it may not be very practical in a directory protocol, since a\nsingle write could generate many invalidate messages. In addition, features such as\nprefetch and multiple outstanding misses increase the amount of buffering\nrequired. There is an alternative strategy, which most systems use and which\nensures that a transaction will not actually be initiated until we can guarantee that\nit has the resources to complete. The strategy has four parts:\n1. A separate network (physical or virtual) is used for requests and replies, where a\nreply is any message that a controller waits for in transitioning between states.\nThis ensures that new requests cannot block replies that will free up buffers.\n2. Every request that expects a reply allocates space to accept the reply when the\nrequest is generated. If no space is available, the request waits. This ensures that\na node can always accept a reply message, which will allow the replying node to\nfree its buffer.\nI-38\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1186,
        "text": "3. Any controller can reject with a NAK any request, but it can never NAK a reply.\nThis prevents a transaction from starting if the controller cannot guarantee that it\nhas buffer space for the reply.\n4. Any request that receives a NAK in response is simply retried.\nTo see that there are no deadlocks with the four properties above, we must\nensure that all replies can be accepted and that every request is eventually ser-\nviced. Since a cache controller or directory controller always allocates a buffer to\nhandle the reply before issuing a request, it can always accept the reply when it\nreturns. To see that every request is eventually serviced, we need only show that\nany request could be completed. Since every request starts with a read or write\nmiss at a cache, it is sufficient to show that any read or write miss is eventually\nserviced. Since the write miss case includes the actions for a read miss as a sub-\nset, we focus on showing the write misses are serviced. The simplest situation is\nwhen the block is uncached; since that case is subsumed by the case when the\nblock is shared, we focus on the shared and exclusive cases. Let\u2019s consider the\ncase where the block is shared:\n\u25a0\nThe CPU attempts to do a write and generates a write miss that is sent to the\ndirectory. For simplicity, we can assume that the processor is stalled. Although\nit may issue further requests, it should not issue a request for the same cache\nblock until the first one is completed. Requests for independent blocks can be\nhandled separately.\n\u25a0\nThe write miss is sent to the directory controller for this memory block. Note\nthat although one cache controller handles all the requests for a given cache\nblock, regardless of its memory contents, the directory controller handles\nrequests for different blocks as independent events (assuming sufficient buff-\nering, which is allocated before the directory issues any further messages on\nbehalf of the request). The only conflict at the directory controller is when\ntwo requests arrive for the same block. The controller must wait for the first\noperation to be completed. It can simply NAK the second request or buffer\nit, but it should not service the second request for a given memory block until\nthe first is completed.\n\u25a0\nNow consider what happens at the directory controller: Suppose the write\nmiss is the next thing to arrive at the directory controller. The controller\nsends out the invalidates, which can always be accepted after a limited\ndelay by the cache controller. Note that one possibility is that the cache\ncontroller has an outstanding miss for the same block. This is the dual case\nto the snooping scheme, and we must once again break the tie by forcing\nthe cache controller to accept and act on the directory request. Depending\non the exact timing, this cache controller will either get the cache line\nlater from the directory or will receive a NAK and have to restart the\nprocess.\nI.7\nImplementing Cache Coherence\n\u25a0\nI-39"
    },
    {
        "page": 1187,
        "text": "The case where the block is exclusive is somewhat trickier. Our analysis begins\nwhen the write miss arrives at the directory controller for processing. There are two\ncases to consider:\n\u25a0\nThe directory controller sends a fetch/invalidate message to the processor\nwhere it arrives to find the block in the exclusive state. The cache controller\nsends a data write-back to the home directory and makes its state invalid. This\nreply arrives at the home directory controller, which can always accept the\nreply, since it preallocated the buffer. The directory controller sends back\nthe data to the requesting processor, which can always accept the reply; after\nthe cache is updated, the requesting cache controller notifies the processor.\n\u25a0\nThe directory controller sends a fetch/invalidate message to the node indicated\nas owner. When the message arrives at the owner node, it finds that this cache\ncontroller has taken a read or write miss that caused the block to be replaced. In\nthis case, the cache controller has already sent the block to the home directory\nwith a data write-back and made the data unavailable. Since this is exactly the\neffect of the fetch/invalidate message, the protocol operates correctly in this\ncase as well.\nWe have shown that our coherence mechanism operates correctly when the\ncache and directory controller can accept requests for operation on cache blocks\nfor which they have no outstanding operations in progress, when replies are\nalways accepted, and when requests can be NAKed and forced to retry. Like\nthe case of the snooping protocol, the cache controller must be able to break ties,\nand it always does so by favoring the instructions from the directory. The ability to\nNAK requests is what allows an implementation with finite buffering to avoid\ndeadlock.\nImplementing the Directory Controller\nTo implement a cache coherence scheme, the cache controller must have the same\nabilities it needed in the snooping case, namely, the capability of handling requests\nfor independent blocks while awaiting a response to a request from the local pro-\ncessor. The incoming requests are still processed in order, and each one is com-\npleted before beginning the next. Should a cache controller receive too many\nrequests in a short period of time, it can NAK them, knowing that the directory\nwill subsequently regenerate the request.\nThe directory must also be multithreaded and able to handle requests for mul-\ntiple blocks independently. This situation is somewhat different than having the\ncache controller handle incoming requests for independent blocks, since the direc-\ntory controller will need to begin processing one request while an earlier one is still\nunderway. The directory controller cannot wait for one to complete before servic-\ning the next request, since this could lead to deadlock. Instead, the directory con-\ntroller must be reentrant; that is, it must be capable of suspending its execution\nI-40\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1188,
        "text": "while waiting for a reply and accepting another transaction. The only place this\nmust occur is in response to read or write misses, while waiting for a response from\nthe owner. This leads to three important observations:\n1. The state of the controller need only be saved and restored while either a fetch\noperation from a remote location or a fetch/invalidate is outstanding.\n2. The implementation can bound the number of outstanding transactions being\nhandled in the directory by simply NAKing read or write miss requests that\ncould cause the number of outstanding requests to be exceeded.\n3. If instead of returning the data through the directory, the owner node forwards\nthe data directly to the requester (as well as returning it to the directory), we can\neliminate the need for the directory to handle more than one outstanding\nrequest. This motivation, in addition to the reduction of latency, is the reason\nfor using the forwarding style of protocol. There are other complexities from\nforwarding protocols that arise when requests arrive closely spaced in time.\nThe major remaining implementation difficulty is to handle NAKs. One alter-\nnative is for each processor to keep track of its outstanding transactions so it\nknows, when the NAK is received, what the requested transaction was. The alter-\nnative is to bundle the original request into the NAK, so that the controller receiv-\ning the NAK can determine what the original request was. Because every request\nallocates a slot to receive a reply and a NAK is a reply, NAKs can always be\nreceived. In fact, the buffer holding the return slot for the request can also hold\ninformation about the request, allowing the processor to reissue the request if it\nis NAKed.\nIn practice, great care is required to implement these protocols correctly and to\navoid deadlock. The key ideas we have seen in this section\u2014dealing with nona-\ntomicity and finite buffering\u2014are critical to ensuring a correct implementation.\nDesigners have found that both formal and informal verification techniques are\nhelpful for ensuring that implementations are correct.\nI.8\nThe Custom Cluster Approach: Blue Gene/L\nBlue Gene/L (BG/L) is a scalable message-passing supercomputer whose design\noffers unprecedented computing density as measured by compute power per watt.\nBy focusing on power efficiency, BG/L also achieves unmatched throughput per\ncubic foot. High computing density, combined with cost-effective nodes and exten-\nsive support for RAS, allows BG/L to efficiently scale to very large processor counts.\nBG/L is a distributed-memory, message-passing computer but one that is quite\ndifferent from the cluster-based, often throughput-oriented computers that rely on\ncommodity technology in the processors, interconnect, and, sometimes, the pack-\naging and system-level organization. BG/L uses a special customized processing\nnode that contains two processors (derived from low-power, lower-clock-rate\nPowerPC 440 chips used in the embedded market), caches, and interconnect logic.\nI.8\nThe Custom Cluster Approach: Blue Gene/L\n\u25a0\nI-41"
    },
    {
        "page": 1189,
        "text": "A complete computing node is formed by adding SDRAM chips, which are the\nonly commodity semiconductor parts in the BG/L design.\nBG/L consists of up to 64 K nodes organized into 32 racks each containing 1 K\nnodes in about 50 cubic feet. Each rack contains two double-sided boards with 512\nnodes each. Due to the high density within a board and rack, 85% of the intercon-\nnect is within a single rack, greatly reducing the complexity and latency associated\nwith connections between racks. Furthermore, the compact size of a rack, which is\nenabled by the low power and high density of each node, greatly improves effi-\nciency, since the interconnection network for connections within a single rack\nare integrated into the single compute chip that comprises each node.\nAppendix F discusses the main BL/G interconnect network, which is a three-\ndimensional torus. There are four other networks: Gigabit Ethernet, connected at\ndesignated I/O nodes; a JTAG network used for test; a barrier network; and a global\ncollective network. The barrier network contains four independent channels and\ncan be used for performing a global or or a global and across all the processors\nwith latency of less than 1.5 microseconds. The global collective network connects\nall the processors in a tree and is used for global operations. It supports a variety of\ninteger reductions directly, avoiding the need to involve the processor, and leading\nto times for large-scale reductions that are 10 to 100 times faster than in typical\nsupercomputers. The collective network can also be used to broadcast a single\nvalue efficiently. Support for the collective network as well as the torus is included\nin the chip that forms of the heart of each processing node.\nThe Blue Gene/L Computing Node\nEach BG/L node consists of a single processing chip and several SDRAM chips.\nThe BG/L processing chip, shown in Figure I.18, contains the following:\n1. Two PowerPC 440 CPUs, each a two-issue superscalar with a seven-stage\npipeline and speculative out-order issue capability, clocked at a modest\n(and power-saving) 700 MHz. Each CPU has separate 32 KB I and D caches\nthat are nonbblocking with up to four outstanding misses. Cache coherence\nmust be enforced in software. Each CPU also contains a pair of floating-point\ncoprocessors, each with its own FP register set and each capable of issuing a\nmultiply-add each clock cycle, supporting a special SIMD instruction set\ncapability that includes complex arithmetic using a pair of registers and\n128-bit operands.\n2. Separate fully associative L2 caches, each with 2 KB of data and a 128-byte\nblock size, that act essentially like prefetch buffers. The L2 cache controllers\nrecognize streamed data access and also handle prefetch from L3 or main mem-\nory. They have low latency (11 cycles) and provide high bandwidth (5 bytes per\nclock). The L2 prefetch buffer can supply 5.5 GB/sec to the L1 caches.\n3. A 4 MB L3 cache implemented with embedded DRAM. Each L2 buffer is con-\nnected by a bus supplying 11 GB/sec of bandwidth from the L3 cache.\nI-42\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1190,
        "text": "4. A memory bus supporting 256 to 512 MB of DDR DRAMS and providing\n5.5 GB/sec of memory bandwidth to the L3 cache. This amount of memory\nmight seem rather modest for each node, given that the node contains two pro-\ncessors, each with two FP units. Indeed Amdahl\u2019s rule of thumb (1 MB per 1\nMIPS) and an assumption of 25% of peak performance would favor about 2.7\ntimes the memory per node. For floating-point-intensive applications where the\ncomputational need usually grows faster than linear in the memory size, the\nupper limit of 512 MB/node is probably reasonable.\n5. Support logic for the five interconnection networks.\nByplacingallthelogicotherthanDRAMsintoasinglechip,BG/Lachieveshigher\ndensity, lower power, and lower cost, making it possible to pack the processing nodes\nextremelydensely.Thedensityintermsallowstheinterconnectionnetworkstobelow\nlatency,highbandwidth,andquitecosteffective.Thecombinationyieldsasupercom-\nputer that scales very cost-effectively, yielding an order-of-magnitude improvement\n32K/32K L1\n256\n11 GB/sec\n256\n5.5 GB/sec\n1024\n22 GB/sec\n144 ECC\nL2 prefetch buffer\n128\nPPC 440\nCPU\nShared L3\ndirectory for\nembedded\nDRAM\nIncludes\nerror\ncorrection\ncontrol\n(ECC)\nDouble-issue\nFPU\n32K/32K L1\nEthernet\nGbit\nGigabit\nEthernet\nSnoop\nL2 prefetch buffer\n128\nPPC 440\nCPU\nDouble-issue\nFPU\nJTAG\naccess\nIEEE\n1149.1\n(JTAG)\nTorus\n6 out and\n6 in, each at\n1.4 GB/sec\nlink\n5.5 GB/sec\nCollective\n3 out and\n3 in, each at\n2.8 GB/sec\nlink\nGlobal\ninterrupt/\nlockbox\n4 global\nbarriers or\ninterrupts\nDDR\ncontrol\nwith ECC\n144-bit-wide\nDDR\n256/ 512 MB\n256\n4 MB\nembedded\nDRAM\nL3 cache\nor\nmemory\n256\n11 GB/sec\n128\nFigure I.18 The BG/L processing node. The unfilled boxes are the PowerPC processors\nwith added floating-point units. The solid gray boxes are network interfaces, and the\nshaded lighter gray boxes are part of the memory system, which is supplemented by\nDDR RAMS.\nI.8\nThe Custom Cluster Approach: Blue Gene/L\n\u25a0\nI-43"
    },
    {
        "page": 1191,
        "text": "in GFLOPs/watt over other approaches as well as significant improvements in\nGFLOPS/$ for very large-scale multiprocessors.\nFor example, BG/L with 64 K nodes has a peak performance of 360 TF and\nuses about 1.4 megawatts. To achieve 360 TF peak using the Power5+, which\nis the most power-efficient, high-end FP processor, would require about 23,500\nprocessors (the dual processor can execute up to 8 FLOPs/clock at 1.9 GHz).\nThe power requirement for just the processors, without external cache, DRAM,\nor interconnect, would be about 2.9 megawatts, or about double the power of\nthe entire BG/L system. Likewise, the smaller die size of the BG/L node and its\nneed for DRAMs as the only external chip produce significant cost savings versus\na node built using a high-end multiprocessor. Figure I.19 shows a photo of the 64K\nnode BG/L. The total size occupied by this 128K-processor multiprocessor is com-\nparable to that occupied by earlier multiprocessors with 16K processors.\nI.9\nConcluding Remarks\nThe landscape of large-scale multiprocessors has changed dramatically over the\npast five to ten years. While some form of clustering is now used for all the\nlargest-scale multiprocessors, calling them all \u201cclusters\u201d ignores significant differ-\nences in architecture, implementation style, cost, and performance. Bell and Gray\nFigure I.19 The 64 K-processor Blue Gene/L system.\nI-44\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1192,
        "text": "[2002] discussed this trend, arguing that clusters will dominate. While Dongarra\net al. [2005] agreed that some form of clustering is almost inevitable in the largest\nmultiprocessors, they developed a more nuanced classification that attempts to dis-\ntinguish among a variety of different approaches.\nIn Figure I.20 we summarize the range of terminology that has been used for\nlarge-scale multiprocessors and focus on defining the terms from an architectural\nand implementation perspective. Figure I.21 shows the hierarchical relationship of\nthese different architecture approaches. Although there has been some conver-\ngence in architectural approaches over the past 15 years, the TOP500 list, which\nreports the 500 fastest computers in the world as measured by the Linpack bench-\nmark, includes commodity clusters, customized clusters, Symmetric Multiproces-\nsors (SMPs), DSMs, and constellations, as well as processors that are both scalar\nand vector.\nTerminology\nCharacteristics\nExamples\nMPP\nOriginally referred to a class of architectures characterized by large\nnumbers of small, typically custom processors and usually using\nan SIMD style architecture.\nConnection Machines CM-2\nSMP (symmetric\nmultiprocessor)\nShared-memory multiprocessors with a symmetric relationship to\nmemory; also called UMA (uniform memory access). Scalable\nversions of these architectures used multistage interconnection\nnetworks, typically configured with at most 64 to 128 processors.\nSUN Sunfire, NEC Earth\nSimulator\nDSM (distributed\nshared memory)\nA class of architectures that support scalable shared memory in a\ndistributed fashion. These architectures are available both with and\nwithout cache coherence and typically can support hundreds to\nthousands of processors.\nSGI Origin and Altix, Cray\nT3E, Cray X1, IBM p5 590/5\nCluster\nA class of multiprocessors using message passing. The individual\nnodes are either commodities or customized, likewise the\ninterconnect.\nSee commodity and custom\nclusters\nCommodity\ncluster\nA class of clusters where the nodes are truly commodities,\ntypically headless workstations, motherboards, or blade servers,\nconnected with a SAN or LAN usually accessible via an I/O bus.\n\u201cBeowulf\u201d and other\n\u201chomemade\u201d clusters\nCustom cluster\nA cluster architecture where the nodes and the interconnect are\ncustomized and more tightly integrated than in a commodity\ncluster. Also called distributed memory or message passing\nmultiprocessors.\nIBM Blue Gene, Cray XT3\nConstellation\nLarge-scale multiprocessors that use clustering of smaller-scale\nmultiprocessors, typically with a DSM or SMP architecture and 32\nor more processors.\nLarger SGI Origin/Altix,\nASC Purple\nFigure I.20 A classification of large-scale multiprocessors. The term MPP, which had the original meaning\ndescribed above, has been used more recently, and less precisely, to refer to all large-scale multiprocessors. None\nof the commercial shipping multiprocessors is a true MPP in the original sense of the word, but such an approach\nmay make sense in the future. Both the SMP and DSM class includes multiprocessors with vector support. The term\nconstellation has been used in different ways; the above usage seems both intuitive and precise [Dongarra et al. 2005].\nI.9\nConcluding Remarks\n\u25a0\nI-45"
    },
    {
        "page": 1193,
        "text": "Nonetheless, there are some clearly emerging trends, which we can see by\nlooking at the distribution of types of multiprocessors in the TOP500 list:\n1. Clusters represent a majority of the systems. The lower development effort for\nclusters has clearly been a driving force in making them more popular. The\nhigh-end multiprocessor market has not grown sufficiently large to support\nfull-scale, highly customized designs as the dominant choice.\n2. The majority of the clusters are commodity clusters, often put together by users,\nrather than a system vendor designing a standard product.\n3. Although commodity clusters dominate in their representation, the top 25\nentries on the list are much more varied and include 9 custom clusters (primarily\ninstances of Blue Gene or Cray XT3 systems), 2 constellations, 8 commodity\nclusters, 2 SMPs (one of which is the NEC Earth Simulator, which has nodes\nwith vector processors), and 4 DSM multiprocessors.\n4. Vector processors, which once dominated the list, have almost disappeared.\n5. The IBM Blue Gene dominates the top 10 systems, showing the advantage of an\napproach the uses some commodity processor cores, but customizes many other\nfunctions and balances performance, power, and packaging density.\n6. Architectural convergence has been driven more by market effects (lack of\ngrowth, limited suppliers, etc.) than by a clear-cut consensus on the best archi-\ntectural approaches.\nLarger\nmultiprocessors\nShared address\nspace\nSymmetric shared \nmemory (SMP)\nExamples: IBM eServer, \nSUN Sunfire\nDistributed shared\nmemory (DSM)\nCommodity clusters:\nBeowulf and others\nCustom\ncluster\nUniform cluster:\nIBM Blue Gene\nCache coherent: \nccNUMA:\nSGI Origin/Altix\nConstellation cluster of \nDSMs or SMPs\nSGI Altix, ASC Purple\nNoncache coherent: \nCray T3E, X1\nDistributed\naddress space\nFigure I.21 The space of large-scale multiprocessors and the relation of different classes.\nI-46\n\u25a0\nAppendix I Large-Scale Multiprocessors and Scientific Applications"
    },
    {
        "page": 1194,
        "text": "Software, both applications and programming languages and environments,\nremains the big challenge for parallel computing, just as it was 30 years ago, when\nmultiprocessors such as the Illiac IV were being designed. The combination of ease\nof programming with high parallel performance remains elusive. Until better pro-\ngress is made on this front, convergence toward a single programming model and\nunderlying architectural approach (remembering that for uniprocessors we essen-\ntially have one programming model and one architectural approach!) will be slow\nor will be driven by factors other than proven architectural superiority.\nI.9\nConcluding Remarks\n\u25a0\nI-47"
    },
    {
        "page": 1195,
        "text": "J.1\nIntroduction\nJ-2\nJ.2\nBasic Techniques of Integer Arithmetic\nJ-2\nJ.3\nFloating Point\nJ-13\nJ.4\nFloating-Point Multiplication\nJ-17\nJ.5\nFloating-Point Addition\nJ-21\nJ.6\nDivision and Remainder\nJ-27\nJ.7\nMore on Floating-Point Arithmetic\nJ-32\nJ.8\nSpeeding Up Integer Addition\nJ-37\nJ.9\nSpeeding Up Integer Multiplication and Division\nJ-44\nJ.10\nPutting It All Together\nJ-57\nJ.11\nFallacies and Pitfalls\nJ-62\nJ.12\nHistorical Perspective and References\nJ-63\nExercises\nJ-67"
    },
    {
        "page": 1196,
        "text": "J\nComputer Arithmetic\nby David Goldberg\nXerox Palo Alto Research Center\nThe Fast drives out the Slow even if the Fast is wrong.\nW. Kahan"
    },
    {
        "page": 1197,
        "text": "J.1\nIntroduction\nAlthough computer arithmetic is sometimes viewed as a specialized part of CPU\ndesign, it is a very important part. This was brought home for Intel in 1994 when\ntheir Pentium chip was discovered to have a bug in the divide algorithm. This\nfloating-point flaw resulted in a flurry of bad publicity for Intel and also cost them\na lot of money. Intel took a $300 million write-off to cover the cost of replacing\nthe buggy chips.\nIn this appendix, we will study some basic floating-point algorithms, includ-\ning the division algorithm used on the Pentium. Although a tremendous variety\nof algorithms have been proposed for use in floating-point accelerators, actual\nimplementations are usually based on refinements and variations of the few basic\nalgorithms presented here. In addition to choosing algorithms for addition, sub-\ntraction, multiplication, and division, the computer architect must make other\nchoices. What precisions should be implemented? How should exceptions be\nhandled? This appendix will give you the background for making these and other\ndecisions.\nOur discussion of floating point will focus almost exclusively on the IEEE\nfloating-point standard (IEEE 754) because of its rapidly increasing acceptance.\nAlthough floating-point arithmetic involves manipulating exponents and shifting\nfractions, the bulk of the time in floating-point operations is spent operating on\nfractions using integer algorithms (but not necessarily sharing the hardware that\nimplements integer instructions). Thus, after our discussion of floating point,\nwe will take a more detailed look at integer algorithms.\nSome good references on computer arithmetic, in order from least to most\ndetailed, are Chapter 3 of Patterson and Hennessy [2009]; Chapter 7 of Hamacher,\nVranesic, and Zaky [1984]; Gosling [1980]; and Scott [1985].\nJ.2\nBasic Techniques of Integer Arithmetic\nReaders who have studied computer arithmetic before will find most of this section\nto be review.\nRipple-Carry Addition\nAdders are usually implemented by combining multiple copies of simple com-\nponents. The natural components for addition are half adders and full adders.\nThe half adder takes two bits a and b as input and produces a sum bit s and a\ncarry bit cout as output. Mathematically, s\u00bc(a+b) mod 2, and cout\u00bcb(a+b)/2c,\nwhere b c is the floor function. As logic equations, s \u00bc ab + ab and cout\u00bcab,\nwhere ab means a ^ b and a+b means a _ b. The half adder is also called\na (2,2) adder, since it takes two inputs and produces two outputs. The full adder\nJ-2\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1198,
        "text": "is a (3,2) adder and is defined by s\u00bc(a+b+c) mod 2, cout\u00bcb(a+b+c)/2c, or\nthe logic equations\ns \u00bc abc + abc + abc + abc\nJ:2:1\ncout \u00bc ab + ac + bc\nJ:2:2\nThe principal problem in constructing an adder for n-bit numbers out of smaller\npieces is propagating the carries from one piece to the next. The most obvious\nway to solve this is with a ripple-carry adder, consisting of n full adders, as\nillustrated in Figure J.1. (In the figures in this appendix, the least-significant\nbit is always on the right.) The inputs to the adder are an1an2\u22efa0\nand\nbn1bn2\u22efb0,\nwhere\nan1an2\u22efa0\nrepresents\nthe\nnumber\nan12n1 + an22n2 + \u22ef+ a0. The ci+1 output of the ith adder is fed into the ci + 1\ninput of the next adder (the (i+1)-th adder) with the lower-order carry-in c0\nset to 0. Since the low-order carry-in is wired to 0, the low-order adder could be a half\nadder. Later, however, we will see that setting the low-order carry-in bit to 1 is useful\nfor performing subtraction.\nIn general, the time a circuit takes to produce an output is proportional to the\nmaximum number of logic levels through which a signal travels. However, deter-\nmining the exact relationship between logic levels and timings is highly technology\ndependent. Therefore, when comparing adders we will simply compare the number\nof logic levels in each one. How many levels are there for a ripple-carry adder? It\ntakes two levels to compute c1 from a0 and b0. Then it takes two more levels to com-\npute c2 from c1, a1, b1, and so on, up to cn. So, there are a total of 2n levels. Typical\nvalues of n are 32 for integer arithmetic and 53 for double-precision floating point.\nThe ripple-carry adder is the slowest adder, but also the cheapest. It can be built with\nonly n simple cells, connected in a simple, regular way.\nBecause the ripple-carry adder is relatively slow compared with the designs\ndiscussed in Section J.8, you might wonder why it is used at all. In technologies\nlike CMOS, even though ripple adders take time O(n), the constant factor is very\nsmall. In such cases short ripple adders are often used as building blocks in larger\nadders.\nbn\u20131\nan\u20131\nsn\u20131\nFull\nadder\ncn\u20131\nsn\u20132\ncn\nan\u20132 bn\u20132\nFull\nadder\nb1\na1\ns1\nFull\nadder\ns0\na0\nb0\nFull\nadder\nc2\nc1\n0\nFigure J.1 Ripple-carry adder, consisting of n full adders. The carry-out of one full\nadder is connected to the carry-in of the adder for the next most-significant bit. The\ncarries ripple from the least-significant bit (on the right) to the most-significant bit\n(on the left).\nJ.2\nBasic Techniques of Integer Arithmetic\n\u25a0\nJ-3"
    },
    {
        "page": 1199,
        "text": "Radix-2 Multiplication and Division\nThe simplest multiplier computes the product of two unsigned numbers, one bit at a\ntime, as illustrated in Figure J.2(a). The numbers to be multiplied are an1an2\u22efa0\nand bn1bn2\u22efb0, and they are placed in registers A and B, respectively. Register\nP is initially 0. Each multiply step has two parts:\nMultiply Step\n(i) If the least-significant bit of A is 1, then register B, containing bn1bn2\u22efb0, is\nadded to P; otherwise, 00\u22ef00 is added to P. The sum is placed back into P.\n(ii) Registers P and A are shifted right, with the carry-out of the sum being moved\ninto the high-order bit of P, the low-order bit of P being moved into register A,\nand the rightmost bit of A, which is not used in the rest of the algorithm, being\nshifted out.\nCarry-out\nA\nP\nn\nn\nn\nShift\nP\nB\n0\nA\nn + 1\nn\n1\nn\nShift\n(a)\n(b)\n1\nB\nFigure J.2 Block diagram of (a) multiplier and (b) divider for n-bit unsigned integers.\nEach multiplication step consists of adding the contents of P to either B or 0 (depending\non the low-order bit of A), replacing P with the sum, and then shifting both P and A one\nbit right. Each division step involves first shifting P and A one bit left, subtracting B from\nP, and, if the difference is nonnegative, putting it into P. If the difference is nonnegative,\nthe low-order bit of A is set to 1.\nJ-4\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1200,
        "text": "After n steps, the product appears in registers P and A, with A holding the\nlower-order bits.\nThe simplest divider also operates on unsigned numbers and produces the\nquotient bits one at a time. A hardware divider is shown in Figure J.2(b). To\ncompute a/b, put a in the A register, b in the B register, and 0 in the P register\nand then perform n divide steps. Each divide step consists of four parts:\nDivide Step\n(i) Shift the register pair (P,A) one bit left.\n(ii) Subtract the content of register B (which is bn1bn2\u22efb0) from register P,\nputting the result back into P.\n(iii) If the result of step 2 is negative, set the low-order bit of A to 0, otherwise to 1.\n(iv) If the result of step 2 is negative, restore the old value of P by adding the\ncontents of register B back into P.\nAfter repeating this process n times, the A register will contain the quotient, and\nthe P register will contain the remainder. This algorithm is the binary version of the\npaper-and-pencil method; a numerical example is illustrated in Figure J.3(a).\nNotice that the two block diagrams in Figure J.2 are very similar. The main\ndifference is that the register pair (P,A) shifts right when multiplying and left when\ndividing. By allowing these registers to shift bidirectionally, the same hardware\ncan be shared between multiplication and division.\nThe division algorithm illustrated in Figure J.3(a) is called restoring, because\nif subtraction by b yields a negative result, the P register is restored by adding b\nback in. The restoring algorithm has a variant that skips the restoring step and\ninstead works with the resulting negative numbers. Each step of this nonrestoring\nalgorithm has three parts:\nNonrestoring\nIf P is negative,\nDivide Step\n(i-a) Shift the register pair (P,A) one bit left.\n(ii-a) Add the contents of register B to P.\nElse,\n(i-b) Shift the register pair (P,A) one bit left.\n(ii-b) Subtract the contents of register B from P.\n(iii) If P is negative, set the low-order bit of A to 0, otherwise set it to 1.\nAfter repeating this n times, the quotient is in A. If P is nonnegative, it is the\nremainder. Otherwise, it needs to be restored (i.e., add b), and then it will be the\nremainder. A numerical example is given in Figure J.3(b). Since steps (i-a) and (i-\nb) are the same, you might be tempted to perform this common step first, and then\ntest the sign of P. That doesn\u2019t work, since the sign bit can be lost when shifting.\nJ.2\nBasic Techniques of Integer Arithmetic\n\u25a0\nJ-5"
    },
    {
        "page": 1201,
        "text": "P\nA\n00000\n1110\nDivide 14\u00bc11102 by 3\u00bc112. B always contains 00112.\n00001\n110\nstep 1(i): shift.\n00011\nstep 1(ii): subtract.\n00010\n1100\nstep 1(iii): result is negative, set quotient bit to 0.\n00001\n1100\nstep 1(iv): restore.\n00011\n100\nstep 2(i): shift.\n00011\nstep 2(ii): subtract.\n00000\n1001\nstep 2(iii): result is nonnegative, set quotient bit to 1.\n00001\n001\nstep 3(i): shift.\n00011\nstep 3(ii): subtract.\n00010\n0010\nstep 3(iii): result is negative, set quotient bit to 0.\n00001\n0010\nstep 3(iv): restore.\n00010\n010\nstep 4(i): shift.\n00011\nstep 4(ii): subtract.\n00001\n0100\nstep 4(iii): result is negative, set quotient bit to 0.\n00010\n0100\nstep 4(iv): restore. The quotient is 01002 and the remainder is 000102.\n(a)\n00000\n1110\nDivide 14\u00bc11102 by 3\u00bc112. B always contains 00112.\n00001\n110\nstep 1(i-b): shift.\n+11101\nstep 1(ii-b): subtract b (add two\u2019s complement).\n11110\n1100\nstep 1(iii): P is negative, so set quotient bit to 0.\n11101\n100\nstep 2(i-a): shift.\n+00011\nstep 2(ii-a): add b.\n00000\n1001\nstep 2(iii): P is nonnegative, so set quotient bit to 1.\n00001\n001\nstep 3(i-b): shift.\n+11101\nstep 3(ii-b): subtract b.\n11110\n0010\nstep 3(iii): P is negative, so set quotient bit to 0.\n11100\n010\nstep 4(i-a): shift.\n+00011\nstep 4(ii-a): add b.\n11111\n0100\nstep 4(iii): P is negative, so set quotient bit to 0.\n+00011\nRemainder is negative, so do final restore step.\n00010\nThe quotient is 01002 and the remainder is 000102.\n(b)\nFigure J.3 Numerical example of (a) restoring division and (b) nonrestoring division.\nJ-6\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1202,
        "text": "The explanation for why the nonrestoring algorithm works is this. Let rk be the\ncontents of the (P,A) register pair at step k, ignoring the quotient bits (which are sim-\nply sharing the unused bits of register A). In Figure J.3(a), initially A contains 14, so\nr0\u00bc14. At the end of the first step, r1\u00bc28, and so on. In the restoring algorithm, part\n(i) computes 2rk and then part (ii) 2rk2nb (2nb since b is subtracted from the left\nhalf). If 2rk2nb\u00030, both algorithms end the step with identical values in (P,A). If\n2rk2nb<0, then the restoring algorithm restores this to 2rk, and the next step\nbegins by computing rres\u00bc2(2rk)2nb. In the non-restoring algorithm, 2rk2nb\nis kept as a negative number, and in the next step rnonres\u00bc2(2rk2nb)+\n2nb\u00bc4rk2nb\u00bcrres. Thus (P,A) has the same bits in both algorithms.\nIf a and b are unsigned n-bit numbers, hence in the range 0\u0004a,b\u00042n1, then\nthe multiplier in Figure J.2 will work if register P is n bits long. However, for\ndivision, P must be extended to n+1 bits in order to detect the sign of P. Thus\nthe adder must also have n+1 bits.\nWhy would anyone implement restoring division, which uses the same hard-\nware as nonrestoring division (the control is slightly different) but involves an extra\naddition? In fact, the usual implementation for restoring division doesn\u2019t actually\nperform an add in step (iv). Rather, the sign resulting from the subtraction is tested\nat the output of the adder, and only if the sum is nonnegative is it loaded back into\nthe P register.\nAs a final point, before beginning to divide, the hardware must check to see\nwhether the divisor is 0.\nSigned Numbers\nThere are four methods commonly used to represent signed n-bit numbers: sign\nmagnitude, two\u2019s complement, one\u2019s complement, and biased. In the sign magni-\ntude system, the high-order bit is the sign bit, and the low-order n1 bits are the\nmagnitude of the number. In the two\u2019s complement system, a number and its\nnegative add up to 2n. In one\u2019s complement, the negative of a number is obtained\nby complementing each bit (or, alternatively, the number and its negative add up to\n2n1). In each of these three systems, nonnegative numbers are represented in the\nusual way. In a biased system, nonnegative numbers do not have their usual rep-\nresentation. Instead, all numbers are represented by first adding them to the bias\nand then encoding this sum as an ordinary unsigned number. Thus, a negative num-\nber k can be encoded as long as k+bias\u00030. A typical value for the bias is 2n1.\nExample\nUsing 4-bit numbers (n\u00bc4), if k\u00bc3 (or in binary, k\u00bc00112), how isk expressed\nin each of these formats?\nAnswer\nIn signed magnitude, the leftmost bit in k\u00bc00112 is the sign bit, so flip it to 1: k is\nrepresented by 10112. In two\u2019s complement, k+11012\u00bc2n\u00bc16. Sok is repre-\nsented by 11012. In one\u2019s complement, the bits of k\u00bc00112 are flipped, sok\nis represented by 11002. For a biased system, assuming a bias of 2n1\u00bc8, k is\nrepresented by k+bias\u00bc10112, andk byk+bias\u00bc01012.\nJ.2\nBasic Techniques of Integer Arithmetic\n\u25a0\nJ-7"
    },
    {
        "page": 1203,
        "text": "The most widely used system for representing integers, two\u2019s complement, is the\nsystem we will use here. One reason for the popularity of two\u2019s complement is that\nit makes signed addition easy: Simply discard the carry-out from the highorder bit.\nTo add 5+2, for example, add 01012 and 11102 to obtain 00112, resulting in the\ncorrect value of 3. A useful formula for the value of a two\u2019s complement number\nan1an2\u22efa1a0 is\nan12n1 + an22n2 + \u22ef+ a121 + a0\nJ:2:3\nAs an illustration of this formula, the value of 11012 as a 4-bit two\u2019s complement\nnumber is 1\u000523+1\u000522+0\u000521+1\u000520\u00bc8+4+1\u00bc3, confirming the result of\nthe example above.\nOverflow occurs when the result of the operation does not fit in the represen-\ntation being used. For example, if unsigned numbers are being represented using 4\nbits, then 6\u00bc01102 and 11\u00bc10112. Their sum (17) overflows because its binary\nequivalent (100012) doesn\u2019t fit into 4 bits. For unsigned numbers, detecting over-\nflow is easy; it occurs exactly when there is a carry-out of the most-significant bit.\nFor two\u2019s complement, things are trickier: Overflow occurs exactly when the carry\ninto the high-order bit is different from the (to be discarded) carry-out of the high-\norder bit. In the example of 5+2 above, a 1 is carried both into and out of the\nleftmost bit, avoiding overflow.\nNegating a two\u2019s complement number involves complementing each bit and\nthen adding 1. For instance, to negate 00112, complement it to get 11002 and then\nadd 1 to get 11012. Thus, to implement ab using an adder, simply feed a and b\n(where b is the number obtained by complementing each bit of b) into the adder and\nset the low-order, carry-in bit to 1. This explains why the rightmost adder in\nFigure J.1 is a full adder.\nMultiplying two\u2019s complement numbers is not quite as simple as adding them.\nThe obvious approach is to convert both operands to be nonnegative, do an\nunsigned multiplication, and then (if the original operands were of opposite signs)\nnegate the result. Although this is conceptually simple, it requires extra time and\nhardware. Here is a better approach: Suppose that we are multiplying a times b\nusing the hardware shown in Figure J.2(a). Register A is loaded with the number\na; B is loaded with b. Since the content of register B is always b, we will use B and\nb interchangeably. If B is potentially negative but A is nonnegative, the only\nchange needed to convert the unsigned multiplication algorithm into a two\u2019s com-\nplement one is to ensure that when P is shifted, it is shifted arithmetically; that is,\nthe bit shifted into the high-order bit of P should be the sign bit of P (rather than the\ncarry-out from the addition). Note that our n-bit-wide adder will now be adding\nn-bit two\u2019s complement numbers between 2n1 and 2n11.\nNext, suppose a is negative. The method for handling this case is called Booth\nrecoding. Booth recoding is a very basic technique in computer arithmetic and\nwill play a key role in Section J.9. The algorithm on page J-4 computes a\u0006b by\nexamining the bits of a from least significant to most significant. For example, if\na\u00bc7\u00bc01112, then step (i) will successively add B, add B, add B, and add 0. Booth\nrecoding \u201crecodes\u201d the number 7 as 81 \u00bc 10002 00012 \u00bc 1001, where 1\nJ-8\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1204,
        "text": "represents 1. This gives an alternative way to compute a\u0006b, namely, successively\nsubtract B, add 0, add 0, and add B. This is more complicated than the unsigned algo-\nrithm on page J-4, since it uses both addition and subtraction. The advantage shows\nup for negative values of a. With the proper recoding, we can treat a as though it were\nunsigned. For example, take a\u00bc4\u00bc11002. Think of 11002 as the unsigned num-\nber 12, and recodeitas 12 \u00bc 164 \u00bc 100002 01002 \u00bc 10100.Ifthe multiplication\nalgorithm is only iterated n times (n\u00bc4 in this case), the high-order digit is ignored,\nand we end up subtracting 01002\u00bc4 times the multiplier\u2014exactly the right answer.\nThis suggests that multiplying using a recoded form of a will work equally well for\nboth positive and negative numbers. And, indeed, to deal with negative values of a,\nall that is required is to sometimes subtract b from P, instead of adding either b or 0 to\nP. Here are the precise rules: If the initial content of A is an1\u22efa0, then at the ith\nmultiply step the low-order bit of register A is ai, and step (i) in the multiplication\nalgorithm becomes:\nI. If ai\u00bc0 and ai1\u00bc0, then add 0 to P.\nII. If ai\u00bc0 and ai1\u00bc1, then add B to P.\nIII. If ai\u00bc1 and ai1\u00bc0, then subtract B from P.\nIV. If ai\u00bc1 and ai1\u00bc1, then add 0 to P.\nFor the first step, when i\u00bc0, take ai1 to be 0.\nExample\nWhen multiplying 6 times 5, what is the sequence of values in the (P,A)\nregister pair?\nAnswer\nSee Figure J.4.\nP\nA\n0000\n1010\nPut 6\u00bc10102 into A, 5\u00bc10112 into B.\n0000\n1010\nstep 1(i): a0\u00bca1\u00bc0, so from rule I add 0.\n0000\n0101\nstep 1(ii): shift.\n+0101\nstep 2(i): a1\u00bc1, a0\u00bc0.Rule III says subtract b (or\naddb\u00bc10112\u00bc01012).\n0101\n0101\n0010\n1010\nstep 2(ii): shift.\n+ 1011\nstep 3(i): a2\u00bc0, a1\u00bc1. Rule II says add b (1011).\n1101\n1010\n1110\n1101\nstep 3(ii): shift. (Arithmetic shift\u2014load 1 into leftmost bit.)\n+ 0101\nstep 4(i): a3\u00bc1, a2\u00bc0. Rule III says subtract b.\n0011\n1101\n0001\n1110\nstep 4(ii): shift. Final result is 000111102\u00bc30.\nFigure J.4 Numerical example of Booth recoding. Multiplication of a\u00bc6 by b\u00bc5\nto get 30.\nJ.2\nBasic Techniques of Integer Arithmetic\n\u25a0\nJ-9"
    },
    {
        "page": 1205,
        "text": "The four prior cases can be restated as saying that in the ith step you should add\n(ai1ai)B to P. With this observation, it is easy to verify that these rules work,\nbecause the result of all the additions is\nX\nn1\ni\u00bc0\nb ai1 ai\n\u00f0\n\u00de2i \u00bc b an12n1 + an22n2 + \u2026 + a12 + a0\n\n\u0003\n+ ba1\nUsing Equation J.2.3 (page J-8) together with a1\u00bc0, the right-hand side is seen to\nbe the value of b\u0006a as a two\u2019s complement number.\nThe simplest way to implement the rules for Booth recoding is to extend the A\nregister one bit to the right so that this new bit will contain ai1. Unlike the naive\nmethod of inverting any negative operands, this technique doesn\u2019t require extra\nsteps or any special casing for negative operands. It has only slightly more control\nlogic. If the multiplier is being shared with a divider, there will already be the capa-\nbility for subtracting b, rather than adding it. To summarize, a simple method for\nhandling two\u2019s complement multiplication is to pay attention to the sign of P when\nshifting it right, and to save the most recently shifted-out bit of A to use in deciding\nwhether to add or subtract b from P.\nBooth recoding is usually the best method for designing multiplication hardware\nthat operates on signed numbers. For hardware that doesn\u2019t directly implement it,\nhowever, performing Booth recoding in software or microcode is usually too slow\nbecause of the conditional tests and branches. If the hardware supports arithmetic\nshifts (so that negative b is handled correctly), then the following method can be\nused. Treat the multiplier a as if it were an unsigned number, and perform the first\nn1 multiply steps using the algorithm on page J-4. If a<0 (in which case there will\nbe a 1 in the low-order bit of the A register at this point), then subtract b from P;\notherwise (a\u00030), neither add nor subtract. In either case, do a final shift (for a total\nof\nn\nshifts).\nThis\nworks\nbecause\nit\namounts\nto\nmultiplying\nb\nby\nan12n1 + \u22ef+ a12 + a0, which is the value of an1\u22efa0 as a two\u2019s complement\nnumber by Equation J.2.3. If the hardware doesn\u2019t support arithmetic shift, then\nconverting the operands to be nonnegative is probably the best approach.\nTwo final remarks: A good way to test a signed-multiply routine is to\ntry 2n1\u00062n1, since this is the only case that produces a 2n1 bit result.\nUnlike multiplication, division is usually performed in hardware by converting\nthe operands to be nonnegative and then doing an unsigned divide. Because divi-\nsion is substantially slower (and less frequent) than multiplication, the extra time\nused to manipulate the signs has less impact than it does on multiplication.\nSystems Issues\nWhen designing an instruction set, a number of issues related to integer arithmetic\nneed to be resolved. Several of them are discussed here.\nFirst, what should be done about integer overflow? This situation is compli-\ncated by the fact that detecting overflow differs depending on whether the operands\nare signed or unsigned integers. Consider signed arithmetic first. There are three\nJ-10\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1206,
        "text": "approaches: Set a bit on overflow, trap on overflow, or do nothing on overflow. In\nthe last case, software has to check whether or not an overflow occurred. The most\nconvenient solution for the programmer is to have an enable bit. If this bit is turned\non, then overflow causes a trap. If it is turned off, then overflow sets a bit (or, alter-\nnatively, have two different add instructions). The advantage of this approach is\nthat both trapping and nontrapping operations require only one instruction. Fur-\nthermore, as we will see in Section J.7, this is analogous to how the IEEE\nfloating-point standard handles floating-point overflow. Figure J.5 shows how\nsome common machines treat overflow.\nWhat about unsigned addition? Notice that none of the architectures in\nFigure J.5 traps on unsigned overflow. The reason for this is that the primary\nuse of unsigned arithmetic is in manipulating addresses. It is convenient to be able\nto subtract from an unsigned address by adding. For example, when n\u00bc4, we can\nsubtract 2 from the unsigned address 10\u00bc10102 by adding 14\u00bc11102. This\ngenerates an overflow, but we would not want a trap to be generated.\nA second issue concerns multiplication. Should the result of multiplying two\nn-bit numbers be a 2n-bit result, or should multiplication just return the low-order n\nbits, signaling overflow if the result doesn\u2019t fit in n bits? An argument in favor of an\nn-bit result is that in virtually all high-level languages, multiplication is an oper-\nation in which arguments are integer variables and the result is an integer variable\nof the same type. Therefore, compilers won\u2019t generate code that utilizes a double-\nprecision result. An argument in favor of a 2n-bit result is that it can be used by an\nassembly language routine to substantially speed up multiplication of multiple-\nprecision integers (by about a factor of 3).\nA third issue concerns machines that want to execute one instruction every cycle.\nIt is rarely practical to perform a multiplication or division in the same amount of time\nthatanadditionorregister-register movetakes.Thereare threepossibleapproachesto\nthis problem. The first is to have a single-cycle multiply-step instruction. This might\ndo one step of the Booth algorithm. The second approach is to do integer multipli-\ncationin the floating-point unit and haveit bepartof the floating-point instruction set.\nMachine\nTrap on signed overflow?\nTrap on unsigned\noverflow?\nSet bit on signed\noverflow?\nSet bit on unsigned\noverflow?\nVAX\nIf enable is on\nNo\nYes. Add sets V\nbit.\nYes. Add sets C bit.\nIBM 370\nIf enable is on\nNo\nYes. Add sets\ncond code.\nYes. Logical add\nsets cond code.\nIntel\n8086\nNo\nNo\nYes. Add sets V\nbit.\nYes. Add sets C bit.\nMIPS\nR3000\nTwo add instructions; one always\ntraps, the other never does.\nNo\nNo. Software must deduce it from sign of\noperands and result.\nSPARC\nNo\nNo\nAddcc sets V bit.\nAdd does not.\nAddcc sets C bit.\nAdd does not.\nFigure J.5 Summary of how various machines handle integer overflow. Both the 8086 and SPARC have an instruc-\ntion that traps if the V bit is set, so the cost of trapping on overflow is one extra instruction.\nJ.2\nBasic Techniques of Integer Arithmetic\n\u25a0\nJ-11"
    },
    {
        "page": 1207,
        "text": "(This is what DLX does.) The third approach is to have an autonomous unit in the\nCPU do the multiplication. In this case, the result either can be guaranteed to be deliv-\nered in a fixed number of cycles\u2014and the compiler charged with waiting the proper\namount of time\u2014or there can be an interlock. The same comments apply to division\nas well. As examples, the original SPARC had a multiply-step instruction but no\ndivide-stepinstruction,while the MIPSR3000hasanautonomousunitthat doesmul-\ntiplication and division (newer versions of the SPARC architecture added an integer\nmultiply instruction). The designers of the HP Precision Architecture did an espe-\ncially thorough job of analyzing the frequency of the operands for multiplication\nand division, and they based their multiply and divide steps accordingly. (See\nMagenheimer et al. [1988] for details.)\nThe final issue involves the computation of integer division and remainder for\nnegative numbers. For example, what is 5 DIV 3 and 5 MOD 3? When computing\nx DIV y and x MOD y, negative values of x occur frequently enough to be worth some\ncareful consideration. (On the other hand, negative values of y are quite rare.) If\nthere are built-in hardware instructions for these operations, they should corre-\nspond to what high-level languages specify. Unfortunately, there is no agreement\namong existing programming languages. See Figure J.6.\nOne definition for these expressions stands out as clearly superior, namely,\nx DIV y\u00bcbx/yc, so that 5 DIV 3\u00bc1 and 5 DIV 3\u00bc2. And MOD should satisfy\nx\u00bc(x DIV y)\u0006y+x MOD y, so that x MOD y\u00030. Thus, 5 MOD 3\u00bc2, and 5 MOD\n3\u00bc1. Some of the many advantages of this definition are as follows:\n1. A calculation to compute an index into a hash table of size N can use MOD N and\nbe guaranteed to produce a valid index in the range from 0 to N1.\n2. In graphics, when converting from one coordinate system to another, there is no\n\u201cglitch\u201d near 0. For example, to convert from a value x expressed in a system\nthat uses 100 dots per inch to a value y on a bitmapped display with 70 dots per\ninch, the formula y\u00bc(70\u0006x) DIV 100 maps one or two x coordinates into each\ny coordinate. But if DIV were defined as in Pascal to be x/y rounded to 0, then\n0 would have three different points (1, 0, 1) mapped into it.\n3. x MOD 2k is the same as performing a bitwise AND with a mask of k bits, and x DIV\n2k is the same as doing a k-bit arithmetic right shift.\nLanguage\nDivision\nRemainder\nFORTRAN\n5/3\u00bc1\nMOD(5, 3)\u00bc2\nPascal\n5 DIV 3\u00bc1\n5 MOD 3\u00bc1\nAda\n5/3\u00bc1\n5 MOD 3\u00bc1\n5 REM 3\u00bc2\nC\n5/3 undefined\n5% 3 undefined\nModula-3\n5 DIV 3\u00bc2\n5 MOD 3\u00bc1\nFigure J.6 Examples of integer division and integer remainder in various program-\nming languages.\nJ-12\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1208,
        "text": "Finally, a potential pitfall worth mentioning concerns multiple-precision addi-\ntion. Many instruction sets offer a variant of the add instruction that adds three\noperands: two n-bit numbers together with a third single-bit number. This third\nnumber is the carry from the previous addition. Since the multiple-precision num-\nber will typically be stored in an array, it is important to be able to increment the\narray pointer without destroying the carry bit.\nJ.3\nFloating Point\nMany applications require numbers that aren\u2019t integers. There are a number of\nways that nonintegers can be represented. One is to use fixed point; that is, use inte-\nger arithmetic and simply imagine the binary point somewhere other than just to\nthe right of the least-significant digit. Adding two such numbers can be done with\nan integer add, whereas multiplication requires some extra shifting. Other repre-\nsentations that have been proposed involve storing the logarithm of a number and\ndoing multiplication by adding the logarithms, or using a pair of integers (a,b) to\nrepresent the fraction a/b. However, only one noninteger representation has gained\nwidespread use, and that is floating point. In this system, a computer word is\ndivided into two parts, an exponent and a significand. As an example, an exponent\nof 3 and a significand of 1.5 might represent the number 1.5\u000623\u00bc0.1875. The\nadvantages of standardizing a particular representation are obvious. Numerical\nanalysts can build up high-quality software libraries, computer designers can\ndevelop techniques for implementing high-performance hardware, and hardware\nvendors can build standard accelerators. Given the predominance of the\nfloating-point representation, it appears unlikely that any other representation will\ncome into widespread use.\nThe semantics of floating-point instructions are not as clear-cut as the seman-\ntics of the rest of the instruction set, and in the past the behavior of floating-point\noperations varied considerably from one computer family to the next. The varia-\ntions involved such things as the number of bits allocated to the exponent and\nsignificand, the range of exponents, how rounding was carried out, and the actions\ntaken on exceptional conditions like underflow and overflow. Computer architec-\nture books used to dispense advice on how to deal with all these details, but\nfortunately this is no longer necessary. That\u2019s because the computer industry is rap-\nidly converging on the format specified by IEEE standard 754-1985 (also an inter-\nnational standard, IEC 559). The advantages of using a standard variant of\nfloating point are similar to those for using floating point over other noninteger\nrepresentations.\nIEEE arithmetic differs from many previous arithmetics in the following major\nways:\n1. When rounding a \u201chalfway\u201d result to the nearest floating-point number, it picks\nthe one that is even.\n2. It includes the special values NaN, \u221e, and\u221e.\nJ.3\nFloating Point\n\u25a0\nJ-13"
    },
    {
        "page": 1209,
        "text": "3. It uses denormal numbers to represent the result of computations whose value is\nless than 1:0\u00062Emin.\n4. It rounds to nearest by default, but it also has three other rounding modes.\n5. It has sophisticated facilities for handling exceptions.\nTo elaborate on (1), note that when operating on two floating-point numbers,\nthe result is usually a number that cannot be exactly represented as another\nfloating-point number. For example, in a floating-point system using base 10\nand two significant digits, 6.1\u00060.5\u00bc3.05. This needs to be rounded to two digits.\nShould it be rounded to 3.0 or 3.1? In the IEEE standard, such halfway cases are\nrounded to the number whose low-order digit is even. That is, 3.05 rounds to 3.0,\nnot 3.1. The standard actually has four rounding modes. The default is round to\nnearest, which rounds ties to an even number as just explained. The other modes\nare round toward 0, round toward+\u221e, and round toward\u221e.\nWe will elaborate on the other differences in following sections. For further\nreading, see IEEE [1985], Cody et al. [1984], and Goldberg [1991].\nSpecial Values and Denormals\nProbably the most notable feature of the standard is that by default a computation\ncontinues in the face of exceptional conditions, such as dividing by 0 or taking the\nsquare root of a negative number. For example, the result of taking the square root\nof a negative number is a NaN (Not a Number), a bit pattern that does not represent\nan ordinary number. As an example of how NaNs might be useful, consider the\ncode for a zero finder that takes a function F as an argument and evaluates F at\nvarious points to determine a zero for it. If the zero finder accidentally probes out-\nside the valid values for F, then F may well cause an exception. Writing a zero\nfinder that deals with this case is highly language and operating-system dependent,\nbecause it relies on how the operating system reacts to exceptions and how this\nreaction is mapped back into the programming language. In IEEE arithmetic it\nis easy to write a zero finder that handles this situation and runs on many different\nsystems. After each evaluation of F, it simply checks to see whether F has returned\na NaN; if so, it knows it has probed outside the domain of F.\nIn IEEE arithmetic, if the input to an operation is a NaN, the output is NaN\n(e.g., 3+NaN\u00bcNaN). Because of this rule, writing floating-point subroutines that\ncan accept NaN as an argument rarely requires any special case checks. For exam-\nple, suppose that arccos is computed in terms of arctan, using the formula\narccosx \u00bc 2arctan\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\n1x\n\u00f0\n\u00de= 1 + x\n\u00f0\n\u00de\np\n\n\u0003\n. If arctan handles an argument of NaN\nproperly, arccos will automatically do so, too. That\u2019s because if x is a NaN,\n1 +x, 1x, (1+x)/(1x), and\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\n1x\n\u00f0\n\u00de= 1 + x\n\u00f0\n\u00de\np\nwill also be NaNs. No checking\nfor NaNs is required.\nWhile the result of\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\n1\np\nis a NaN, the result of 1/0 is not a NaN, but +\u221e, which\nis another special value. The standard defines arithmetic on infinities (there are\nJ-14\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1210,
        "text": "both +\u221e\nand \u221e)\nusing\nrules\nsuch\nas\n1/\u221e\u00bc0.\nThe\nformula\narccosx \u00bc 2arctan\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\n1x\n\u00f0\n\u00de= 1 + x\n\u00f0\n\u00de\np\n\n\u0003\nillustrates how infinity arithmetic can be\nused. Since arctan x asymptotically approaches \u03c0/2 as x approaches \u221e, it is natural\nto define arctan(\u221e)\u00bc\u03c0/2, in which case arccos(1) will automatically be com-\nputed correctly as 2 arctan(\u221e)\u00bc\u03c0.\nThe final kind of special values in the standard are denormal numbers. In many\nfloating-point systems, if Emin is the smallest exponent, a number less than 1:0\u0006\n2Emin cannot be represented, and a floating-point operation that results in a number\nless than this is simply flushed to 0. In the IEEE standard, on the other hand, num-\nbers less than 1:0\u00062Emin are represented using significands less than 1. This is\ncalled gradual underflow. Thus, as numbers decrease in magnitude below 2Emin,\nthey gradually lose their significance and are only represented by 0 when all their\nsignificance has been shifted out. For example, in base 10 with four significant\nfigures, let x \u00bc 1:234\u000610Emin. Then, x/10 will be rounded to 0:123\u000610Emin, having\nlost a digit of precision. Similarly x/100 rounds to 0:012\u000610Emin, and x/1000 to\n0:001\u000610Emin, while x/10000 is finally small enough to be rounded to 0. Denor-\nmals make dealing with small numbers more predictable by maintaining familiar\nproperties such as x\u00bcy , xy\u00bc0. For example, in a flush-to-zero system (again\nin base 10 with four significant digits), if x \u00bc 1:256\u000610Emin and y \u00bc 1:234\u000610Emin,\nthen xy \u00bc 0:022\u000610Emin, which flushes to zero. So even though x6\u00bcy, the\ncomputed value of xy\u00bc0. This never happens with gradual underflow. In this\nexample, xy \u00bc 0:022\u000610Emin is a denormal number, and so the computation of\nxy is exact.\nRepresentation of Floating-Point Numbers\nLet us consider how to represent single-precision numbers in IEEE arithmetic.\nSingle-precision numbers are stored in 32 bits: 1 for the sign, 8 for the exponent,\nand 23 for the fraction. The exponent is a signed number represented using the bias\nmethod (see the subsection \u201cSigned Numbers,\u201d page J-7) with a bias of 127. The\nterm biased exponent refers to the unsigned number contained in bits 1 through 8,\nand unbiased exponent (or just exponent) means the actual power to which 2 is to\nbe raised. The fraction represents a number less than 1, but the significand of the\nfloating-point number is 1 plus the fraction part. In other words, if e is the biased\nexponent (value of the exponent field) and f is the value of the fraction field, the\nnumber being represented is 1. f\u00062e127.\nExample\nWhat single-precision number does the following 32-bit word represent?\n1 10000001 01000000000000000000000\nAnswer\nConsidered as an unsigned number, the exponent field is 129, making the value of\nthe exponent 129127\u00bc2. The fraction part is .012\u00bc.25, making the significand\n1.25. Thus, this bit pattern represents the number 1.25\u000622\u00bc5.\nJ.3\nFloating Point\n\u25a0\nJ-15"
    },
    {
        "page": 1211,
        "text": "The fractional part of a floating-point number (.25 in the example above) must\nnot be confused with the significand, which is 1 plus the fractional part. The lead-\ning 1 in the significand 1.f does not appear in the representation; that is, the leading\nbit is implicit. When performing arithmetic on IEEE format numbers, the fraction\npart is usually unpacked, which is to say the implicit 1 is made explicit.\nFigure J.7 summarizes the parameters for single (and other) precisions.\nIt shows the exponents for single precision to range from 126 to 127; accord-\ningly, the biased exponents range from 1 to 254. The biased exponents of 0 and\n255 are used to represent special values. This is summarized in Figure J.8. When\nthe biased exponent is 255, a zero fraction field represents infinity, and a nonzero\nfraction field represents a NaN. Thus, there is an entire family of NaNs. When the\nbiased exponent and the fraction field are 0, then the number represented is 0.\nBecause of the implicit leading 1, ordinary numbers always have a significand\ngreater than or equal to 1. Thus, a special convention such as this is required to\nrepresent 0. Denormalized numbers are implemented by having a word with a zero\nexponent field represent the number 0:f \u00062Emin.\nThe primary reason why the IEEE standard, like most other floating-point for-\nmats, uses biased exponents is that it means nonnegative numbers are ordered in\nthe same way as integers. That is, the magnitude of floating-point numbers can be\ncompared using an integer comparator. Another (related) advantage is that 0 is repre-\nsented by a word of all 0s. The downside of biased exponents is that adding them is\nslightly awkward, because it requires that the bias be subtracted from their sum.\nSingle\nSingle extended\nDouble\nDouble extended\np (bits of precision)\n24\n\u000332\n53\n\u000364\nEmax\n127\n\u00031023\n1023\n\u000316383\nEmin\n126\n\u00041022\n1022\n\u000416382\nExponent bias\n127\n1023\nFigure J.7 Format parameters for the IEEE 754 floating-point standard. The first row\ngives the number of bits in the significand. The blanks are unspecified parameters.\nExponent\nFraction\nRepresents\ne\u00bcEmin1\nf\u00bc0\n\u00070\ne\u00bcEmin1\nf6\u00bc0\n0:f \u00062Emin\nEmin\u0004e\u0004Emax\n\u2014\n1.f\u00062e\ne\u00bcEmax+1\nf\u00bc0\n\u0007\u221e\ne\u00bcEmax+1\nf6\u00bc0\nNaN\nFigure J.8 Representation of special values. When the exponent of a number falls out-\nside the range Emin\u0004e\u0004Emax, then that number has a special interpretation as indicated\nin the table.\nJ-16\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1212,
        "text": "J.4\nFloating-Point Multiplication\nThe simplest floating-point operation is multiplication, so we discuss it first. A\nbinary floating-point number x is represented as a significand and an exponent,\nx\u00bcs\u00062e. The formula\ns1 \u00062e1\n\n\u0003\n\b s2 \u00062e2\n\n\u0003\n\u00bc s1 \bs2\n\u00f0\n\u00de\u00062e1 + e2\nshows that a floating-point multiply algorithm has several parts. The first part mul-\ntiplies the significands using ordinary integer multiplication. Because floating-\npoint numbers are stored in sign magnitude form, the multiplier need only deal\nwith unsigned numbers (although we have seen that Booth recoding handles\nsigned two\u2019s complement numbers painlessly). The second part rounds the result.\nIf the significands are unsigned p-bit numbers (e.g., p\u00bc24 for single precision),\nthen the product can have as many as 2p bits and must be rounded to a p-bit num-\nber. The third part computes the new exponent. Because exponents are stored with\na bias, this involves subtracting the bias from the sum of the biased exponents.\nExample\nHow does the multiplication of the single-precision numbers\n1 1000001 0000\u2026 \u00bc 1\u000623\n0 1000001 1000\u2026 \u00bc\n1\u000624\nproceed in binary?\nAnswer\nWhen unpacked, the significands are both 1.0, their product is 1.0, and so the result\nis of the form:\n1 ???????? 000\u2026\nTo compute the exponent, use the formula:\nbiased exp e1 + e2\n\u00f0\n\u00de \u00bc biased exp e1\n\u00f0\n\u00de + biased exp e2\n\u00f0\n\u00debias\nFrom Figure J.7, the bias is 127\u00bc011111112, so in two\u2019s complement 127 is\n100000012. Thus, the biased exponent of the product is\n10000010\n10000011\n+ 10000001\n10000110\nSince this is 134 decimal, it represents an exponent of 134bias\u00bc134127, as\nexpected.\nThe interesting part of floating-point multiplication is rounding. Some of the\ndifferent cases that can occur are illustrated in Figure J.9. Since the cases are similar\nin all bases, the figure uses human-friendly base 10, rather than base 2.\nJ.4\nFloating-Point Multiplication\n\u25a0\nJ-17"
    },
    {
        "page": 1213,
        "text": "In the figure, p\u00bc3, so the final result must be rounded to three significant\ndigits. The three most-significant digits are in boldface. The fourth most-\nsignificant digit (marked with an arrow) is the round digit, denoted by r.\nIf the round digit is less than 5, then the bold digits represent the rounded result. If\nthe round digit is greater than 5 (as in part (a)), then 1 must be added to the least-\nsignificant bold digit. If the round digit is exactly 5 (as in part (b)), then additional\ndigits must be examined to decide between truncation or incrementing by 1. It is only\nnecessary to know if any digits past 5 are nonzero. In the algorithm below, this will be\nrecorded in a sticky bit. Comparing parts (a) and (b) in the figure shows that there are\ntwo possible positions for the round digit (relative to the least-significant digit of the\nproduct). Case (c) illustrates that, when adding 1 to the least-significant bold digit,\nthere may be a carry-out. When this happens, the final significand must be 10.0.\nThere is a straightforward method of handling rounding using the multiplier of\nFigure J.2 (page J-4) together with an extra sticky bit. If p is the number of bits in\nthe significand, then the A, B, and P registers should be p bits wide. Multiply the\ntwo significands to obtain a 2p-bit product in the (P,A) registers (see Figure J.10).\nDuring the multiplication, the first p2 times a bit is shifted into the A register, OR\nit into the sticky bit. This will be used in halfway cases. Let s represent the sticky\nbit, g (for guard) the most-significant bit of A, and r (for round) the second most-\nsignificant bit of A. There are two cases:\n1. The high-order bit of P is 0. Shift P left 1 bit, shifting in the g bit from A. Shift-\ning the rest of A is not necessary.\n2. The high-order bit of P is 1. Set s :\u00bc s _ r and r :\u00bc g, and add 1 to the exponent.\nNow if r\u00bc0, P is the correctly rounded product. If r\u00bc1 and s\u00bc1, then P+1 is\nthe product (where by P+1 we mean adding 1 to the least-significant bit of P).\n(a)\n1.23\n\u0006 6.78\nr\u00bc9>5 so round up\nrounds to 8.34\n8.3394\n\"\n(b)\n2.83\n\u0006 4.47\nr\u00bc5 and a following digit6\u00bc0 so round up\nrounds to 1.27\u0006101\n12.6501\n\"\n(c)\n1.28\n\u0006 7.81\nr\u00bc6>5 so round up\nrounds to 1.00\u0006101\n09.9968\n\"\nFigure J.9 Examples of rounding a multiplication. Using base 10 and p\u00bc3, parts (a)\nand (b) illustrate that the result of a multiplication can have either 2p1 or 2p digits;\nhence, the position where a 1 is added when rounding up (just left of the arrow) can\nvary. Part (c) shows that rounding up can cause a carry-out.\nJ-18\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1214,
        "text": "If r\u00bc1 and s\u00bc0, we are in a halfway case and round up according to the least-\nsignificant bit of P. As an example, apply the decimal version of these rules to\nFigure J.9(b). After the multiplication, P\u00bc126 and A\u00bc501, with g\u00bc5, r\u00bc0\nand s\u00bc1. Since the high-order digit of P is nonzero, case (2) applies and\nr :\u00bc g, so that r\u00bc5, as the arrow indicates in Figure J.9. Since r\u00bc5, we could\nbe in a halfway case, but s\u00bc1 indicates that the result is in fact slightly over\n1/2, so add 1 to P to obtain the correctly rounded product.\nThe precise rules for rounding depend on the rounding mode and are given in\nFigure J.11. Note that P is nonnegative, that is, it contains the magnitude of the\nresult. A good discussion of more efficient ways to implement rounding is in\nSantoro, Bewick, and Horowitz [1989].\nExample\nIn binary with p\u00bc4, show how the multiplication algorithm computes the product\n5\u000610 in each of the four rounding modes.\nAnswer\nIn binary, 5 is 1.0102\u000622 and 10\u00bc1.0102\u000623. Applying the integer multipli-\ncation algorithm to the significands gives 011001002, so P\u00bc01102, A\u00bc01002,\ng\u00bc0, r\u00bc1, and s\u00bc0. The high-order bit of P is 0, so case (1) applies. Thus, P\nbecomes 11002, and since the result is negative, Figure J.11 gives:\nround to\u221e\n11012\nadd 1 since r _ s\u00bc1 / 0\u00bcTRUE\nround to+\u221e\n11002\nround to 0\n11002\nround to nearest\n11002\nno add since\nr ^ p0\u00bc1 ^ 0\u00bcFALSE and\nr ^ s\u00bc1 ^ 0\u00bcFALSE\nThe exponent is 2+3\u00bc5, so the result is 1.1002\u000625\u00bc48, except when round-\ning to\u221e, in which case it is 1.1012\u000625\u00bc52.\nProduct\nCase (1): x0 = 0\nShift needed\nCase (2): x0 = 1\nIncrement exponent\nAdjust binary point,\nadd 1 to exponent to compensate\nrnd\nsticky\nrnd\nsticky\n x2  x3  x4  x5\nx0 . x1\nx1 .  x2  x3  x4  x5\ng\nx0  x1 .  x2  x3  x4  x5\ng\nr\ns\ns\ns\ns\nP\nA\nFigure J.10 The two cases of the floating-point multiply algorithm. The top line\nshows the contents of the P and A registers after multiplying the significands, with\np\u00bc6. In case (1), the leading bit is 0, and so the P register must be shifted. In case\n(2), the leading bit is 1, no shift is required, but both the exponent and the round\nand sticky bits must be adjusted. The sticky bit is the logical OR of the bits marked s.\nJ.4\nFloating-Point Multiplication\n\u25a0\nJ-19"
    },
    {
        "page": 1215,
        "text": "Overflow occurs when the rounded result is too large to be represented. In sin-\ngle precision, this occurs when the result has an exponent of 128 or higher. If e1 and\ne2 are the two biased exponents, then 1\u0004ei\u0004254, and the exponent calculation\ne1+e2127 gives numbers between 1+1127 and 254+254127, or between\n125 and 381. This range of numbers can be represented using 9 bits. So one way\nto detect overflow is to perform the exponent calculations in a 9-bit adder (see\nExercise J.12). Remember that you must check for overflow after rounding\u2014\nthe example in Figure J.9(c) shows that this can make a difference.\nDenormals\nChecking for underflow is somewhat more complex because of denormals. In sin-\ngle precision, if the result has an exponent less than 126, that does not necessarily\nindicate underflow, because the result might be a denormal number. For example,\nthe product of (1\u0006264) with (1\u0006265) is 1\u00062129, and 129 is below the legal\nexponent limit. But this result is a valid denormal number, namely, 0.125\u00062126.\nIn general, when the unbiased exponent of a product dips below 126, the result-\ning product must be shifted right and the exponent incremented until the exponent\nreaches 126. If this process causes the entire significand to be shifted out, then\nunderflow has occurred. The precise definition of underflow is somewhat subtle\u2014\nsee Section J.7 for details.\nWhen one of the operands of a multiplication is denormal, its significand will\nhave leading zeros, and so the product of the significands will also have leading\nzeros. If the exponent of the product is less than 126, then the result is denormal,\nso right-shift and increment the exponent as before. If the exponent is greater than\n126, the result may be a normalized number. In this case, left-shift the product\n(while decrementing the exponent) until either it becomes normalized or the\nexponent drops to 126.\nDenormal numbers present a major stumbling block to implementing\nfloating-point multiplication, because they require performing a variable\nshift in the multiplier, which wouldn\u2019t otherwise be needed. Thus, high-\nperformance, floating-point multipliers often do not handle denormalized\nRounding mode\nSign of result\u22650\nSign of result<0\n\u221e\n+1 if r _ s\n+\u221e\n+1 if r _ s\n0\nNearest\n+1 if r ^ p0 or r ^ s\n+1 if r ^ p0 or r ^ s\nFigure J.11 Rules for implementing the IEEE rounding modes. Let S be the magnitude\nof the preliminary result. Blanks mean that the p most-significant bits of S are the actual\nresult bits. If the condition listed is true, add 1 to the pth most-significant bit of S. The\nsymbols r and s represent the round and sticky bits, while p0 is the pth most-significant\nbit of S.\nJ-20\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1216,
        "text": "numbers, but instead trap, letting software handle them. A few practical codes\nfrequently underflow, even when working properly, and these programs will\nrun quite a bit slower on systems that require denormals to be processed by\na trap handler.\nSo far we haven\u2019t mentioned how to deal with operands of zero. This can be\nhandled by either testing both operands before beginning the multiplication or test-\ning the product afterward. If you test afterward, be sure to handle the case 0\u0006\u221e\nproperly: This results in NaN, not 0. Once you detect that the result is 0, set the\nbiased exponent to 0. Don\u2019t forget about the sign. The sign of a product is the\nXOR of the signs of the operands, even when the result is 0.\nPrecision of Multiplication\nIn the discussion of integer multiplication, we mentioned that designers must\ndecide whether to deliver the low-order word of the product or the entire prod-\nuct. A similar issue arises in floating-point multiplication, where the exact\nproduct can be rounded to the precision of the operands or to the next higher\nprecision. In the case of integer multiplication, none of the standard high-level\nlanguages contains a construct that would generate a \u201csingle times single gets\ndouble\u201d instruction. The situation is different for floating point. Many lan-\nguages allow assigning the product of two single-precision variables to a\ndouble-precision one, and the construction can also be exploited by numerical\nalgorithms. The best-known case is using iterative refinement to solve linear\nsystems of equations.\nJ.5\nFloating-Point Addition\nTypically, a floating-point operation takes two inputs with p bits of precision and\nreturns a p-bit result. The ideal algorithm would compute this by first performing\nthe operation exactly, and then rounding the result to p bits (using the current\nrounding mode). The multiplication algorithm presented in the previous section\nfollows this strategy. Even though hardware implementing IEEE arithmetic must\nreturn the same result as the ideal algorithm, it doesn\u2019t need to actually perform the\nideal algorithm. For addition, in fact, there are better ways to proceed. To see this,\nconsider some examples.\nFirst, the sum of the binary 6-bit numbers 1.100112 and 1.100012\u000625: When\nthe summands are shifted so they have the same exponent, this is\n1:10011\n+ :0000110001\nUsing a 6-bit adder (and discarding the low-order bits of the second addend) gives\n1:10011\n+\n:00001\n+ 1:10100\nJ.5\nFloating-Point Addition\n\u25a0\nJ-21"
    },
    {
        "page": 1217,
        "text": "The first discarded bit is 1. This isn\u2019t enough to decide whether to round up. The\nrest of the discarded bits, 0001, need to be examined. Or, actually, we just need to\nrecord whether any of these bits are nonzero, storing this fact in a sticky bit just as\nin the multiplication algorithm. So, for adding two p-bit numbers, a p-bit adder is\nsufficient, as long as the first discarded bit (round) and the OR of the rest of the\nbits (sticky) are kept. Then Figure J.11 can be used to determine if a roundup is\nnecessary, just as with multiplication. In the example above, sticky is 1, so a\nroundup is necessary. The final sum is 1.101012.\nHere\u2019s another example:\n1:11011\n+\n:0101001\nA 6-bit adder gives:\n1:11011\n+\n:01010\n+ 10:00101\nBecause of the carry-out on the left, the round bit isn\u2019t the first discarded bit; rather,\nit is the low-order bit of the sum (1). The discarded bits, 01, are OR\u2019ed together to\nmake sticky. Because round and sticky are both 1, the high-order 6 bits of the sum,\n10.00102, must be rounded up for the final answer of 10.00112.\nNext, consider subtraction and the following example:\n1:00000\n\n:00000101111\nThe simplest way of computing this is to convert.000001011112 to its two\u2019s\ncomplement form, so the difference becomes a sum:\n1:00000\n+ 1:11111010001\nComputing this sum in a 6-bit adder gives:\n1:00000\n+ 1:11111\n0:11111\nBecause the top bits canceled, the first discarded bit (the guard bit) is needed to fill in\nthe least-significant bit of the sum, which becomes 0.1111102, and the second dis-\ncarded bit becomes the round bit. This is analogous to case (1) in the multiplication\nalgorithm (see page J-19). The round bit of 1 isn\u2019t enough to decide whether to round\nup. Instead, we need to OR all the remaining bits (0001) into a sticky bit. In this case,\nstickyis1,sothefinalresultmustberoundedupto0.111111.Thisexampleshowsthat\nif subtraction causes the most-significant bit to cancel, then one guard bit is needed. It\nis natural to ask whether two guard bits are needed for the case when the two most-\nsignificant bits cancel. The answer is no, because if x and y are so close that the top\ntwo bits of xy cancel, then xy will be exact, so guard bits aren\u2019t needed at all.\nJ-22\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1218,
        "text": "To summarize, addition is more complex than multiplication because, depend-\ning on the signs of the operands, it may actually be a subtraction. If it is an addition,\nthere can be carry-out on the left, as in the second example. If it is subtraction, there\ncan be cancellation, as in the third example. In each case, the position of the round\nbit is different. However, we don\u2019t need to compute the exact sum and then round.\nWe can infer it from the sum of the high-order p bits together with the round and\nsticky bits.\nThe rest of this section is devoted to a detailed discussion of the floatingpoint\naddition algorithm. Let a1 and a2 be the two numbers to be added. The notations ei\nand si are used for the exponent and significand of the addends ai. This means that\nthe floating-point inputs have been unpacked and that si has an explicit leading bit.\nTo add a1 and a2, perform these eight steps:\n1. If e1<e2, swap the operands. This ensures that the difference of the exponents\nsatisfies d\u00bce1e2\u00030. Tentatively set the exponent of the result to e1.\n2. If the signs of a1 and a2 differ, replace s2 by its two\u2019s complement.\n3. Place s2 in a p-bit register and shift it d\u00bce1e2 places to the right (shifting in\n1\u2019s if s2 was complemented in the previous step). From the bits shifted out, set g\nto the most-significant bit, set r to the next most-significant bit, and set sticky to\nthe OR of the rest.\n4. Compute a preliminary significand S\u00bcs1+s2 by adding s1 to the p-bit register\ncontaining s2. If the signs of a1 and a2 are different, the most-significant bit of S\nis 1, and there was no carry-out, then S is negative. Replace S with its two\u2019s\ncomplement. This can only happen when d\u00bc0.\n5. Shift S as follows. If the signs of a1 and a2 are the same and there was a carryout\nin step 4, shift S right by one, filling in the high-order position with 1 (the carry-\nout). Otherwise, shift it left until it is normalized. When left-shifting, on the first\nshift fill in the low-order position with the g bit. After that, shift in zeros. Adjust\nthe exponent of the result accordingly.\n6. Adjust r and s. If S was shifted right in step 5, set r :\u00bc low-order bit of S before\nshifting and s :\u00bc g OR r OR s. If there was no shift, set r :\u00bc g, s :\u00bc r OR s. If\nthere was a single left shift, don\u2019t change r and s. If there were two or more left\nshifts, r :\u00bc 0, s :\u00bc 0. (In the last case, two or more shifts can only happen when\na1 and a2 have opposite signs and the same exponent, in which case the com-\nputation s1+s2 in step 4 will be exact.)\n7. Round S using Figure J.11; namely, if a table entry is nonempty, add 1 to the\nlow-order bit of S. If rounding causes carry-out, shift S right and adjust the expo-\nnent. This is the significand of the result.\n8. Compute the sign of the result. If a1 and a2 have the same sign, this is the sign of\nthe result. If a1 and a2 have different signs, then the sign of the result depends on\nwhich of a1 or a2 is negative, whether there was a swap in step 1, and whether S\nwas replaced by its two\u2019s complement in step 4. See Figure J.12.\nJ.5\nFloating-Point Addition\n\u25a0\nJ-23"
    },
    {
        "page": 1219,
        "text": "Example\nUse the algorithm to compute the sum (1.0012\u000622)+(1.1112\u000620).\nAnswer\ns1\u00bc1.001, e1\u00bc2, s2\u00bc1.111, e2\u00bc0\n1. e1<e2, so swap. d\u00bc2. Tentative exp\u00bc0.\n2. Signs of both operands negative, don\u2019t negate s2.\n3. Shift s2 (1.001 after swap) right by 2, giving s2\u00bc.010, g\u00bc0, r\u00bc1, s\u00bc0.\n4.\n1:111\n+\n:010\n1\n\u00f0 \u00de0:001\nS \u00bc 0:001, with acarryout:\n5. Carry-out, so shift S right, S\u00bc1.000, exp\u00bcexp+1, so exp\u00bc1.\n6. r\u00bclow-order bit of sum\u00bc1, s\u00bcg _ r _ s\u00bc0 _ 1 _ 0\u00bc1.\n7. r AND s\u00bc TRUE, so Figure J.11 says round up, S\u00bcS+1 or S\u00bc1.001.\n8. Both\nsigns\nnegative,\nso\nsign\nof\nresult\nis\nnegative.\nFinal\nanswer:\nS\u00062exp\u00bc1.0012\u000621.\nExample\nUse the algorithm to compute the sum (1.0102)+1.1002.\nAnswer\ns1\u00bc1.010, e1\u00bc0, s2\u00bc1.100, e2\u00bc0\n1. No swap, d\u00bc0, tentative exp\u00bc0.\n2. Signs differ, replace s2 with 0.100.\n3. d\u00bc0, so no shift. r\u00bcg\u00bcs\u00bc0.\n4.\n1:010\n+ 0:100\n1:110\nSigns are different, most-significant bit is 1, nocarry-out, so\nmust two\u2019scomplement sum, giving S \u00bc 0:010:\nswap\ncompl\nsign(a1)\nsign(a2)\nsign(result)\nYes\n+\n\n\nYes\n\n+\n+\nNo\nNo\n+\n\n+\nNo\nNo\n\n+\n\nNo\nYes\n+\n\n\nNo\nYes\n\n+\n+\nFigure J.12 Rules for computing the sign of a sum when the addends have\ndifferent signs. The swap column refers to swapping the operands in step 1, while the\ncompl column refers to performing a two\u2019s complement in step 4. Blanks are \u201cdon\u2019t care.\u201d\nJ-24\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1220,
        "text": "5. Shift left twice, so S\u00bc1.000, exp\u00bcexp2, or exp\u00bc2.\n6. Two left shifts, so r\u00bcg\u00bcs\u00bc0.\n7. No addition required for rounding.\n8. Answer is sign\u0006S\u00062exp or sign\u00061.000\u000622. Get sign from Figure J.12.\nSince complement but no swap and sign(a1) is , the sign of the sum is +. Thus,\nthe answer\u00bc1.0002\u000622.\nSpeeding Up Addition\nLet\u2019s estimate how long it takes to perform the algorithm above. Step 2 may require\nan addition, step 4 requires one or two additions, and step 7 may require an addi-\ntion. If it takes T time units to perform a p-bit add (where p\u00bc24 for single preci-\nsion, 53 for double), then it appears the algorithm will take at least 4 T time units.\nBut that is too pessimistic. If step 4 requires two adds, then a1 and a2 have the same\nexponent and different signs, but in that case the difference is exact, so no roundup\nis required in step 7. Thus, only three additions will ever occur. Similarly,\nit appears that a variable shift may be required both in step 3 and step 5. But if\nje1e2j\u00041, then step 3 requires a right shift of at most one place, so only step\n5 needs a variable shift. And, if je1e2j>1, then step 3 needs a variable shift,\nbut step 5 will require a left shift of at most one place. So only a single variable\nshift will be performed. Still, the algorithm requires three sequential adds, which,\nin the case of a 53-bit double-precision significand, can be rather time consuming.\nA numberoftechniquescanspeed upaddition.One istouse pipelining. The\u201cPut-\nting It All Together\u201d section gives examples of how some commercial chips pipeline\naddition. Another method (used on the Intel 860 [Kohn and Fu 1989]) is to perform\ntwo additions in parallel. We now explain how this reduces the latency from 3T to T.\nThere are three cases to consider. First, suppose that both operands have the\nsame sign. We want to combine the addition operations from steps 4 and 7. The\nposition of the high-order bit of the sum is not known ahead of time, because\nthe addition in step 4 may or may not cause a carry-out. Both possibilities are\naccounted for by having two adders. The first adder assumes the add in step 4 will\nnot result in a carry-out. Thus, the values of r and s can be computed before the add\nis actually done. If r and s indicate that a roundup is necessary, the first adder will\ncompute S\u00bcs1+s2+1, where the notation +1 means adding 1 at the position of the\nleast-significant bit of s1. This can be done with a regular adder by setting the low-\norder carry-in bit to 1. If r and s indicate no roundup, the adder computes S\u00bcs1+s2\nas usual. One extra detail: When r\u00bc1, s\u00bc0, you will also need to know the low-\norder bit of the sum, which can also be computed in advance very quickly. The\nsecond adder covers the possibility that there will be carry-out. The values of r\nand s and the position where the roundup 1 is added are different from above,\nbut again they can be quickly computed in advance. It is not known whether there\nwill be a carry-out until after the add is actually done, but that doesn\u2019t matter. By\ndoing both adds in parallel, one adder is guaranteed to reduce the correct answer.\nJ.5\nFloating-Point Addition\n\u25a0\nJ-25"
    },
    {
        "page": 1221,
        "text": "The next case is when a1 and a2 have opposite signs but the same exponent.\nThe sum a1+a2 is exact in this case (no roundup is necessary) but the sign isn\u2019t\nknown until the add is completed. So don\u2019t compute the two\u2019s complement (which\nrequires an add) in step 2, but instead compute s1 + s2 + 1 and s1 + s2 + 1 in parallel.\nThe first sum has the result of simultaneously complementing s1 and computing the\nsum, resulting in s2s1. The second sum computes s1s2. One of these will be\nnonnegative and hence the correct final answer. Once again, all the additions\nare done in one step using two adders operating in parallel.\nThe last case, when a1 and a2 have opposite signs and different exponents, is\nmore complex. If je1e2j>1, the location of the leading bit of the difference is in\none of two locations, so there are two cases just as in addition. When je1e2j\u00bc1,\ncancellation is possible and the leading bit could be almost anywhere. However,\nonly if the leading bit of the difference is in the same position as the leading bit of s1\ncould a roundup be necessary. So one adder assumes a roundup, and the other\nassumes no roundup. Thus, the addition of step 4 and the rounding of step 7\ncan be combined. However, there is still the problem of the addition in step 2!\nTo eliminate this addition, consider the following diagram of step 4:\nj__ __ p __ __j\ns1\n1:xxxxxxx\ns2\n1yyzzzzz\nIf the bits marked z are all 0, then the high-order p bits of S\u00bcs1s2 can be com-\nputed as s1 + s2 + 1. If at least one of the z bits is 1, use s1 + s2. So s1s2 can be\ncomputed with one addition. However, we still don\u2019t know g and r for the two\u2019s\ncomplement of s2, which are needed for rounding in step 7.\nTo compute s1s2 and get the proper g and r bits, combine steps 2 and 4 as\nfollows. Don\u2019t complement s2 in step 2. Extend the adder used for computing S two\nbits to the right (call the extended sum S0). If the preliminary sticky bit (computed\nin step 3) is 1, compute S0 \u00bc s0\n1 + s0\n2, where s10 has two 0 bits tacked onto the right,\nand s20 has preliminary g and r appended. If the sticky bit is 0, compute s0\n1 + s0\n2 + 1.\nNow the two low-order bits of S0 have the correct values of g and r (the sticky\nbit was already computed properly in step 3). Finally, this modification can be\ncombined with the modification that combines the addition from steps 4 and 7\nto provide the final result in time T, the time for one addition.\nA few more details need to be considered, as discussed in Santoro, Bewick, and\nHorowitz [1989] and Exercise J.17. Although the Santoro paper is aimed at mul-\ntiplication, much of the discussion applies to addition as well. Also relevant is\nExercise J.19, which contains an alternative method for adding signed magnitude\nnumbers.\nDenormalized Numbers\nUnlike multiplication, for addition very little changes in the preceding description\nif one of the inputs is a denormal number. There must be a test to see if the exponent\nfield is 0. If it is, then when unpacking the significand there will not be a leading 1.\nJ-26\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1222,
        "text": "By setting the biased exponent to 1 when unpacking a denormal, the algorithm\nworks unchanged.\nTo deal with denormalized outputs, step 5 must be modified slightly. Shift S\nuntil it is normalized, or until the exponent becomes Emin (that is, the biased expo-\nnent becomes 1). If the exponent is Emin and, after rounding, the high-order bit of S\nis 1, then the result is a normalized number and should be packed in the usual way,\nby omitting the 1. If, on the other hand, the high-order bit is 0, the result is denor-\nmal. When the result is unpacked, the exponent field must be set to 0. Section J.7\ndiscusses the exact rules for detecting underflow.\nIncidentally, detecting overflow is very easy. It can only happen if step 5\ninvolves a shift right and the biased exponent at that point is bumped up to 255\nin single precision (or 2047 for double precision), or if this occurs after rounding.\nJ.6\nDivision and Remainder\nIn this section, we\u2019ll discuss floating-point division and remainder.\nIterative Division\nWe earlier discussed an algorithm for integer division. Converting it into a floating-\npoint division algorithm is similar to converting the integer multiplication algo-\nrithm into floating point. The formula\ns1 \u00062e1\n\u00f0\n\u00de= s2 \u00062e2\n\u00f0\n\u00de \u00bc s1=s2\n\u00f0\n\u00de\u00062e1e2\nshows that if the divider computes s1/s2, then the final answer will be this quotient\nmultiplied by 2e1e2. Referring to Figure J.2(b) (page J-4), the alignment of oper-\nands is slightly different from integer division. Load s2 into B and s1 into P. The A\nregister is not needed to hold the operands. Then the integer algorithm for divi-\nsion (with the one small change of skipping the very first left shift) can be used,\nand the result will be of the form q0 \u0005 q1\u22ef. To round, simply compute two addi-\ntional quotient bits (guard and round) and use the remainder as the sticky bit. The\nguard digit is necessary because the first quotient bit might be 0. However, since\nthe numerator and denominator are both normalized, it is not possible for the two\nmost-significant quotient bits to be 0. This algorithm produces one quotient bit in\neach step.\nA different approach to division converges to the quotient at a quadratic\nrather than a linear rate. An actual machine that uses this algorithm will be dis-\ncussed in Section J.10. First, we will describe the two main iterative algorithms,\nand then we will discuss the pros and cons of iteration when compared with the\ndirect algorithms. A general technique for constructing iterative algorithms,\ncalled Newton\u2019s iteration, is shown in Figure J.13. First, cast the problem in\nthe form of finding the zero of a function. Then, starting from a guess for the zero,\napproximate the function by its tangent at that guess and form a new guess based\nJ.6\nDivision and Remainder\n\u25a0\nJ-27"
    },
    {
        "page": 1223,
        "text": "on where the tangent has a zero. If xi is a guess at a zero, then the tangent line has\nthe equation:\nyf xi\n\u00f0 \u00de \u00bc f 0 xi\n\u00f0 \u00de xxi\n\u00f0\n\u00de\nThis equation has a zero at\nx \u00bc xi + 1 \u00bc xi  f xi\n\u00f0 \u00de\nf 0 xi\n\u00f0 \u00de\nJ:6:1\nTo recast division as finding the zero of a function, consider f(x)\u00bcx1b. Since the\nzero of this function is at 1/b, applying Newton\u2019s iteration to it will give an iterative\nmethod of computing 1/b from b. Using f 0(x)\u00bc1/x2, Equation J.6.1 becomes:\nxi + 1 \u00bc xi 1=xi b\n1=x2\ni\n\u00bc xi + xi x2\ni b \u00bc xi 2xib\n\u00f0\n\u00de\nJ:6:2\nThus, we could implement computation of a/b using the following method:\n1. Scale b to lie in the range 1\u0004b<2 and get an approximate value of 1/b (call it\nx0) using a table lookup.\n2. Iterate xi+1\u00bcxi(2xib) until reaching an xn that is accurate enough.\n3. Compute axn and reverse the scaling done in step 1.\nHere are some more details. How many times will step 2 have to be iterated? To\nsay that xi is accurate to p bits means that j(xi1/b)/(1/b)j\u00bc2p, and a simple alge-\nbraic manipulation shows that when this is so, then (xi+11/b)/(1/b)\u00bc22p. Thus,\nthe number of correct bits doubles at each step. Newton\u2019s iteration is self-correct-\ning in the sense that making an error in xi doesn\u2019t really matter. That is, it treats xi as\na guess at 1/b and returns xi+1 as an improvement on it (roughly doubling the\ndigits). One thing that would cause xi to be in error is rounding error. More\nx\nxi+1\nxi\nf(x)\nf(xi)\nFigure J.13 Newton\u2019s iteration for zero finding. If xi is an estimate for a zero of f, then\nxi +1 is a better estimate. To compute xi+1, find the intersection of the x-axis with the\ntangent line to f at f(xi).\nJ-28\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1224,
        "text": "importantly, however, in the early iterations we can take advantage of the fact that\nwe don\u2019t expect many correct bits by performing the multiplication in reduced pre-\ncision, thus gaining speed without sacrificing accuracy. Another application of\nNewton\u2019s iteration is discussed in Exercise J.20.\nThe second iterative division method is sometimes called Goldschmidt\u2019s algo-\nrithm. It is based on the idea that to compute a/b, you should multiply the numer-\nator and denominator by a number r with rb\t1. In more detail, let x0\u00bca\nand y0\u00fe\u00bcb. At each step compute xi+1\u00bcrixi and yi+1\u00bcriyi. Then the quotient\nxi+1/yi+1\u00bcxi/yi\u00bca/b is constant. If we pick ri so that yi!1, then xi!a/b, so the\nxi converge to the answer we want. This same idea can be used to compute other\nfunctions. For example, to compute the square root of a, let x0\u00bca and y0\u00bca, and at\neach step compute xi+1\u00bcri\n2xi, yi+1\u00bcriyi. Then xi+1/yi+1\n2 \u00bcxi/yi\n2\u00bc1/a, so if the ri are\nchosen to drive xi!1, then yi !\n\ufb03\ufb03\ufb03a\np . This technique is used to compute square\nroots on the TI 8847.\nReturning to Goldschmidt\u2019s division algorithm, set x0\u00bca and y0\u00bcb, and write\nb\u00bc1\u03b4, where j\u03b4j<1. If we pick r0\u00bc1+\u03b4, then y1\u00bcr0y0\u00bc1\u03b42. We next pick\nr1\u00bc1+\u03b42, so that y2\u00bcr1y1\u00bc1\u03b44, and so on. Since j\u03b4j<1, yi!1. With this\nchoice\nof\nri,\nthe\nxi\nwill\nbe\ncomputed\nas\nxi + 1 \u00bc rixi \u00bc 1 + \u03b42i\n\u0005\n\u0006\nxi \u00bc\n1 + 1b\n\u00f0\n\u00de2i\n\u0005\n\u0006\nxi, or\nxi + 1 \u00bc a 1 + 1b\n\u00f0\n\u00de\n\u00bd\n\n 1 + 1b\n\u00f0\n\u00de2\nh\ni\n1 + 1b\n\u00f0\n\u00de4\nh\ni\n\u22ef1 + 1b\n\u00f0\n\u00de2i\nh\ni\nJ:6:3\nThere appear to be two problems with this algorithm. First, convergence is slow\nwhen b is not near 1 (that is, \u03b4 is not near 0), and, second, the formula isn\u2019t self-\ncorrecting\u2014since the quotient is being computed as a product of independent\nterms, an error in one of them won\u2019t get corrected. To deal with slow convergence,\nif you want to compute a/b, look up an approximate inverse to b (call it b0), and run\nthe algorithm on ab0/bb0. This will converge rapidly since bb0 \t1.\nTo deal with the self-correction problem, the computation should be run with\na few bits of extra precision to compensate for rounding errors. However, Gold-\nschmidt\u2019s algorithm does have a weak form of self-correction, in that the precise\nvalue of the ri does not matter. Thus, in the first few iterations, when the full pre-\ncision of 1\u03b42i is not needed you can choose ri to be a truncation of 1 + \u03b42i, which\nmay make these iterations run faster without affecting the speed of convergence.\nIf ri is truncated, then yi is no longer exactly 1\u03b42i. Thus, Equation J.6.3 can no\nlonger be used, but it is easy to organize the computation so that it does not\ndepend on the precise value of ri. With these changes, Goldschmidt\u2019s algorithm\nis as follows (the notes in brackets show the connection with our earlier\nformulas).\n1. Scale a and b so that 1\u0004b<2.\n2. Look up an approximation to 1/b (call it b0) in a table.\n3. Set x0\u00bcab0 and y0\u00bcbb0.\nJ.6\nDivision and Remainder\n\u25a0\nJ-29"
    },
    {
        "page": 1225,
        "text": "4. Iterate until xi is close enough to a/b:\nLoop\nr \t 2y\nif yi \u00bc 1 + \u03b4i,then r \t 1\u03b4i\n\u00bd\n\ny \u00bc y\u0006r\nyi + 1 \u00bc yi \u0006r \t 1\u03b4i2\n\u0007\n\b\nxi +1 \u00bc xi \u0006r\nxi +1 \u00bc xi \u0006r\n\u00bd\n\nEnd loop\nThe two iteration methods are related. Suppose in Newton\u2019s method that we\nunroll the iteration and compute each term xi+1 directly in terms of b, instead of\nrecursively in terms of xi. By carrying out this calculation (see Exercise J.22),\nwe discover that\nxi + 1 \u00bc x0 2x0b\n\u00f0\n\u00de\n1 + x0b1\n\u00f0\n\u00de2\n\u0005\ni\n1 + x0b1\n\u00f0\n\u00de4\nh\ni\n\u22ef1 + x0b1\n\u00f0\n\u00de2i\nh\ni\nh\nThis formula is very similar to Equation J.6.3. In fact, they are identical if a and b in\nJ.6.3 are replaced with ax0, bx0, and a\u00bc1. Thus, if the iterations were done to infi-\nnite precision, the two methods would yield exactly the same sequence xi.\nThe advantage of iteration is that it doesn\u2019t require special divide hardware.\nInstead, it can use the multiplier (which, however, requires extra control). Further,\non each step, it delivers twice as many digits as in the previous step\u2014unlike ordi-\nnary division, which produces a fixed number of digits at every step.\nThere are two disadvantages with inverting by iteration. The first is that the\nIEEE standard requires division to be correctly rounded, but iteration only delivers\na result that is close to the correctly rounded answer. In the case of Newton\u2019s iter-\nation, which computes 1/b instead of a/b directly, there is an additional problem.\nEven if 1/b were correctly rounded, there is no guarantee that a/b will be. An exam-\nple in decimal with p\u00bc2 is a\u00bc13, b\u00bc51. Then a/b\u00bc.2549\u2026, which rounds to\n.25. But 1/b\u00bc.0196\u2026, which rounds to .020, and then a\u0006.020\u00bc.26, which is off\nby 1. The second disadvantage is that iteration does not give a remainder. This is\nespecially troublesome if the floating-point divide hardware is being used to\nperform integer division, since a remainder operation is present in almost every\nhigh-level language.\nTraditional folklore has held that the way to get a correctly rounded result from\niteration is to compute 1/b to slightly more than 2p bits, compute a/b to slightly\nmore than 2p bits, and then round to p bits. However, there is a faster way, which\napparently was first implemented on the TI 8847. In this method, a/b is computed\nto about 6 extra bits of precision, giving a preliminary quotient q. By comparing qb\nwith a (again with only 6 extra bits), it is possible to quickly decide whether q\nis correctly rounded or whether it needs to be bumped up or down by 1 in the\nleast-significant place. This algorithm is explored further in Exercise J.21.\nOne factor to take into account when deciding on division algorithms is the rel-\native speed of division and multiplication. Since division is more complex than mul-\ntiplication, it will run more slowly. A common rule of thumb is that division\nalgorithms should try to achieve a speed that is about one-third that of multiplication.\nJ-30\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1226,
        "text": "One argument in favor of this rule is that there are real programs (such as some ver-\nsions of spice) where the ratio of division to multiplication is 1:3. Another place\nwhere a factor of 3 arises is in the standard iterative method for computing square\nroot. This method involves one division per iteration, but it can be replaced by one\nusing three multiplications. This is discussed in Exercise J.20.\nFloating-Point Remainder\nFor nonnegative integers, integer division and remainder satisfy:\na \u00bc a DIV b\n\u00f0\n\u00deb + a REM b,\n0 \u0004 a REM b < b\nA floating-point remainder x REM y can be similarly defined as x\u00bc INT(x/y)y+x REM\ny. How should x/y be converted to an integer? The IEEE remainder function uses\nthe round-to-even rule. That is, pick n\u00bc INT (x/y) so that jx/ynj\u00041/2. If two dif-\nferent n satisfy this relation, pick the even one. Then REM is defined to be xyn.\nUnlike integers where 0\u0004a REM b<b, for floating-point numbers jx REM yj\u0004y/2.\nAlthough this defines REM precisely, it is not a practical operational definition,\nbecause n can be huge. In single precision, n could be as large as 2127/\n2126\u00bc2253\t1076.\nThere is a natural way to compute REM if a direct division algorithm is used.\nProceed as if you were computing x/y. If x \u00bc s12e1 and y \u00bc s22e2 and the divider\nis as in Figure J.2(b) (page J-4), then load s1 into P and s2 into B. After e1e2\ndivision steps, the P register will hold a number r of the form xyn satisfying\n0\u0004r<y. Since the IEEE remainder satisfies jREMj\u0004y/2, REM is equal to either r\nor ry. It is only necessary to keep track of the last quotient bit produced, which\nis needed to resolve halfway cases. Unfortunately, e1e2 can be a lot of steps, and\nfloating-point units typically have a maximum amount of time they are allowed to\nspend on one instruction. Thus, it is usually not possible to implement REM directly.\nNone of the chips discussed in Section J.10 implements REM, but they could by\nproviding a remainder-step instruction\u2014this is what is done on the Intel 8087 fam-\nily. A remainder step takes as arguments two numbers x and y, and performs divide\nsteps until either the remainder is in P or n steps have been performed, where n is a\nsmall number, such as the number of steps required for division in the highest-\nsupported precision. Then REM can be implemented as a software routine that calls\nthe REM step instruction b(e1e2)/nc times, initially using x as the numerator but\nthen replacing it with the remainder from the previous REM step.\nREM can be used for computing trigonometric functions. To simplify things,\nimagine that we are working in base 10 with five significant figures, and consider\ncomputing sin x. Suppose that x\u00bc7. Then we can reduce by \u03c0 \u00bc3.1416 and com-\npute sin(7)\u00bcsin(72\u00063.1416)\u00bcsin(0.7168) instead. But, suppose we want to\ncompute sin(2.0\u0006105). Then 2\u0006105/3.1416\u00bc63661.8, which in our five-place\nsystem comes out to be 63662. Since multiplying 3.1416 times 63662 gives\n200000.5392, which rounds to 2.0000\u0006105, argument reduction reduces\n2\u0006105 to 0, which is not even close to being correct. The problem is that our\nJ.6\nDivision and Remainder\n\u25a0\nJ-31"
    },
    {
        "page": 1227,
        "text": "five-place system does not have the precision to do correct argument reduction.\nSuppose we had the REM operator. Then we could compute 2\u0006105 REM 3.1416\nand get.53920. However, this is still not correct because we used 3.1416, which\nis an approximation for \u03c0. The value of 2\u0006105 REM \u03c0 is.071513.\nTraditionally, there have been two approaches to computing periodic functions\nwith large arguments. The first is to return an error for their value when x is large.\nThe second is to store \u03c0 to a very large number of places and do exact argument\nreduction. The REM operator is not much help in either of these situations. There is a\nthird approach that has been used in some math libraries, such as the Berkeley\nUNIX 4.3bsd release. In these libraries, \u03c0 is computed to the nearest floating-point\nnumber. Let\u2019s call this machine \u03c0, and denote it by \u03c00. Then, when computing sin x,\nreduce x using x REM \u03c00. As we saw in the above example, x REM \u03c00 is quite different\nfrom x REM \u03c0 when x is large, so that computing sin x as sin(x REM \u03c00) will not give\nthe exact value of sin x. However, computing trigonometric functions in this fash-\nion has the property that all familiar identities (such as sin2x+cos2x\u00bc1) are true to\nwithin a few rounding errors. Thus, using REM together with machine \u03c0 provides a\nsimple method of computing trigonometric functions that is accurate for small\narguments and still may be useful for large arguments.\nWhen REM is used for argument reduction, it is very handy if it also returns the\nlow-order bits of n (where x REM y\u00bcxny). This is because a practical implemen-\ntation of trigonometric functions will reduce by something smaller than 2\u03c0.\nFor example, it might use \u03c0/2, exploiting identities such as sin(x\u03c0/2)\u00bccos\nx, sin(x\u03c0)\u00bcsin x. Then the low bits of n are needed to choose the correct\nidentity.\nJ.7\nMore on Floating-Point Arithmetic\nBefore leaving the subject of floating-point arithmetic, we present a few additional\ntopics.\nFused Multiply-Add\nProbably the most common use of floating-point units is performing matrix\noperations, and the most frequent matrix operation is multiplying a matrix times\na matrix (or vector), which boils down to computing an inner product,\nx1\u0005y1+x2\u0005y2+\u2026+xn\u0005yn. Computing this requires a series of multiply-add\ncombinations.\nMotivated by this, the IBM RS/6000 introduced a single instruction that\ncomputes ab+c, the fused multiply-add. Although this requires being able to read\nthree operands in a single instruction, it has the potential for improving the perfor-\nmance of computing inner products.\nThe fused multiply-add computes ab+c exactly and then rounds. Although\nrounding only once increases the accuracy of inner products somewhat, that is\nnot its primary motivation. There are two main advantages of rounding once. First,\nJ-32\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1228,
        "text": "as we saw in the previous sections, rounding is expensive to implement because it\nmay require an addition. By rounding only once, an addition operation has been\neliminated. Second, the extra accuracy of fused multiply-add can be used to com-\npute correctly rounded division and square root when these are not available\ndirectly in hardware. Fused multiply-add can also be used to implement efficient\nfloating-point multiple-precision packages.\nThe implementation of correctly rounded division using fused multiply-add\nhas many details, but the main idea is simple. Consider again the example from\nSection J.6 (page J-30), which was computing a/b with a\u00bc13, b\u00bc51. Then 1/b\nrounds to b0 \u00bc.020, and ab0 rounds to q0 \u00bc.26, which is not the correctly rounded\nquotient. Applying fused multiply-add twice will correctly adjust the result, via the\nformulas\nr \u00bc abq0\nq00 \u00bc q0 + rb0\nComputing to two-digit accuracy, bq0 \u00bc51\u0006.26 rounds to 13, and so r\u00bcabq0\nwould be 0, giving no adjustment. But using fused multiply-add gives\nr\u00bcabq0 \u00bc13(51\u0006.26)\u00bc.26, and then q00 \u00bcq0 +rb0 \u00bc.26.0052\u00bc.2548,\nwhich rounds to the correct quotient, .25. More details can be found in the papers\nby Montoye, Hokenek, and Runyon [1990] and Markstein [1990].\nPrecisions\nThe standard specifies four precisions: single, single extended, double, and double\nextended. The properties of these precisions are summarized in Figure J.7 (page J-\n16). Implementations are not required to have all four precisions, but are encour-\naged to support either the combination of single and single extended or all of sin-\ngle, double, and double extended. Because of the widespread use of double\nprecision in scientific computing, double precision is almost always implemented.\nThus, the computer designer usually only has to decide whether to support double\nextended and, if so, how many bits it should have.\nThe Motorola 68882 and Intel 387 coprocessors implement extended precision\nusing the smallest allowable size of 80 bits (64 bits of significand). However, many\nof the more recently designed, high-performance floating-point chips do not imple-\nment 80-bit extended precision. One reason is that the 80-bit width of extended\nprecision is awkward for 64-bit buses and registers. Some new architectures, such\nas SPARC V8 and PA-RISC, specify a 128-bit extended (or quad) precision. They\nhave established a de facto convention for quad that has 15 bits of exponent and\n113 bits of significand.\nAlthough most high-level languages do not provide access to extended preci-\nsion, it is very useful to writers of mathematical software. As an example, consider\nwriting a library routine to compute the length of a vector (x,y) in the plane, namely,\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\nx2 + y2\np\n. If x is larger than 2Emax=2, then computing this in the obvious way will\noverflow. This means that either the allowable exponent range for this subroutine\nJ.7\nMore on Floating-Point Arithmetic\n\u25a0\nJ-33"
    },
    {
        "page": 1229,
        "text": "will be cut in half or a more complex algorithm using scaling will have to be\nemployed. But, if extended precision is available, then the simple algorithm will\nwork. Computing the length of a vector is a simple task, and it is not difficult to\ncome up with an algorithm that doesn\u2019t overflow. However, there are more com-\nplex problems for which extended precision means the difference between a\nsimple, fast algorithm and a much more complex one. One of the best examples\nof this is binary-to-decimal conversion. An efficient algorithm for binary-to-\ndecimal conversion that makes essential use of extended precision is very readably\npresented in Coonen [1984]. This algorithm is also briefly sketched in Goldberg\n[1991]. Computing accurate values for transcendental functions is another example\nof a problem that is made much easier if extended precision is present.\nOne very important fact about precision concerns double rounding. To illus-\ntrate in decimals, suppose that we want to compute 1.9\u00060.66 and that single\nprecision is two digits, while extended precision is three digits. The exact result\nof the product is 1.254. Rounded to extended precision, the result is 1.25. When\nfurther rounded to single precision, we get 1.2. However, the result of 1.9\u00060.66\ncorrectly rounded to single precision is 1.3. Thus, rounding twice may not pro-\nduce the same result as rounding once. Suppose you want to build hardware that\nonly does double-precision arithmetic. Can you simulate single precision by\ncomputing first in double precision and then rounding to single? The above\nexample suggests that you can\u2019t. However, double rounding is not always\ndangerous. In fact, the following rule is true (this is not easy to prove, but\nsee Exercise J.25).\nIf x and y have p-bit significands, and x+y is computed exactly and then rounded\nto q places, a second rounding to p places will not change the answer if q\u00032p+2.\nThis is true not only for addition, but also for multiplication, division, and square\nroot.\nIn our example above, q\u00bc3 and p\u00bc2, so q \u0160 2p+2 is not true. On the other\nhand, for IEEE arithmetic, double precision has q\u00bc53 and p\u00bc24, so q\u00bc53 \u0160\n2p +2\u00bc50. Thus, single precision can be implemented by computing in double\nprecision\u2014that is, computing the answer exactly and then rounding to double\u2014\nand then rounding to single precision.\nExceptions\nThe IEEE standard defines five exceptions: underflow, overflow, divide by zero,\ninexact, and invalid. By default, when these exceptions occur, they merely set a\nflag and the computation continues. The flags are sticky, meaning that once set they\nremain set until explicitly cleared. The standard strongly encourages implementa-\ntions to provide a trap-enable bit for each exception. When an exception with an\nenabled trap handler occurs, a user trap handler is called, and the value of the asso-\nciated exception flag is undefined. In Section J.3 we mentioned that\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\n3\np\nhas the\nvalue NaN and 1/0 is \u221e. These are examples of operations that raise an exception.\nJ-34\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1230,
        "text": "By default, computing\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\n3\np\nsets the invalid flag and returns the value NaN.\nSimilarly 1/0 sets the divide-by-zero flag and returns \u221e.\nThe underflow, overflow, and divide-by-zero exceptions are found in most\nother systems. The invalid exception is for the result of operations such as\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\n1\np\n, 0/0, or \u221e\u221e, which don\u2019t have any natural value as a floating-point num-\nber or as\u0007\u221e. The inexact exception is peculiar to IEEE arithmetic and occurs\neither when the result of an operation must be rounded or when it overflows. In\nfact, since 1/0 and an operation that overflows both deliver \u221e, the exception flags\nmust be consulted to distinguish between them. The inexact exception is an\nunusual \u201cexception,\u201d in that it is not really an exceptional condition because it\noccurs so frequently. Thus, enabling a trap handler for the inexact exception will\nmost likely have a severe impact on performance. Enabling a trap handler doesn\u2019t\naffect whether an operation is exceptional except in the case of underflow. This is\ndiscussed below.\nThe IEEE standard assumes that when a trap occurs, it is possible to identify the\noperation that trapped and its operands. On machines with pipelining or multiple\narithmetic units, when an exception occurs, it may not be enough to simply have\nthe trap handler examine the program counter. Hardware support may be necessary\nto identify exactly which operation trapped.\nAnother problem is illustrated by the following program fragment.\nr1 = r2/r3\nr2 = r4 + r5\nThese two instructions might well be executed in parallel. If the divide traps, its\nargument r2 could already have been overwritten by the addition, especially since\naddition is almost always faster than division. Computer systems that support trap-\nping in the IEEE standard must provide some way to save the value of r2, either in\nhardware or by having the compiler avoid such a situation in the first place. This\nkind of problem is not peculiar to floating point. In the sequence\nr1 = 0(r2)\nr2 = r3\nit would be efficient to execute r2 = r3 while waiting for memory. But, if acces-\nsing 0(r2) causes a page fault, r2 might no longer be available for restarting the\ninstruction r1 = 0(r2).\nOne approach to this problem, used in the MIPS R3010, is to identify instructions\nthat may cause an exception early in the instruction cycle. For example, an addition\ncan overflow only if one of the operands has an exponent of Emax, and so on. This\nearly check is conservative: It might flag an operation that doesn\u2019t actually cause an\nexception. However, if such false positives are rare, then this technique will have\nexcellent performance. When an instruction is tagged as being possibly exceptional,\nspecial code in a trap handler can compute it without destroying any state. Remember\nthat all these problems occur only when trap handlers are enabled. Otherwise, setting\nthe exception flags during normal processing is straightforward.\nJ.7\nMore on Floating-Point Arithmetic\n\u25a0\nJ-35"
    },
    {
        "page": 1231,
        "text": "Underflow\nWe have alluded several times to the fact that detection of underflow is more\ncomplex than for the other exceptions. The IEEE standard specifies that if an\nunderflow trap handler is enabled, the system must trap if the result is denormal.\nOn the other hand, if trap handlers are disabled, then the underflow flag is set\nonly if there is a loss of accuracy\u2014that is, if the result must be rounded. The\nrationale is, if no accuracy is lost on an underflow, there is no point in setting a\nwarning flag. But if a trap handler is enabled, the user might be trying to sim-\nulate flush-to-zero and should therefore be notified whenever a result dips\nbelow 1:0\u00062Emin.\nSo if there is no trap handler, the underflow exception is signaled only when\nthe result is denormal and inexact, but the definitions of denormal and inexact\nare both subject to multiple interpretations. Normally, inexact means there was\na result that couldn\u2019t be represented exactly and had to be rounded. Consider\nthe example (in a base 2 floating-point system with 3-bit significands) of\n1:112 \u000622\n\n\u0003\n\u0006 1:112 \u00062Emin\n\n\u0003\n\u00bc 0:1100012 \u00062Emin,withroundtonearest ineffect.\nThe delivered result is 0:112 \u00062Emin, which had to be rounded, causing inexact to\nbe signaled. But is it correct to also signal underflow? Gradual underflow loses\nsignificance because the exponent range is bounded. If the exponent range were\nunbounded, the delivered result would be 1:102 \u00062Emin1, exactly the same answer\nobtained with gradual underflow. The fact that denormalized numbers have fewer\nbits in their significand than normalized numbers therefore doesn\u2019t make any\ndifference in this case. The commentary to the standard [Cody et al. 1984] encour-\nages this as the criterion for setting the underflow flag. That is, it should be\nset whenever the delivered result is different from what would be delivered in a\nsystem with the same fraction size, but with a very large exponent range. However,\nowing to the difficulty of implementing this scheme, the standard allows setting\nthe underflow flag whenever the result is denormal and different from the infinitely\nprecise result.\nThere are two possible definitions of what it means for a result to be denormal.\nConsider the example of 1.102\u000621 multiplied by 1:102 \u00062Emin. The exact product\nis 0:1111\u00062Emin. The rounded result is the normal number 1:002 \u00062Emin. Should\nunderflow be signaled? Signaling underflow means that you are using the before\nrounding rule, because the result was denormal before rounding. Not signaling\nunderflow means that you are using the after rounding rule, because the result\nis normalized after rounding. The IEEE standard provides for choosing either rule;\nhowever, the one chosen must be used consistently for all operations.\nTo illustrate these rules, consider floating-point addition. When the result of an\naddition (or subtraction) is denormal, it is always exact. Thus, the underflow flag\nnever needs to be set for addition. That\u2019s because if traps are not enabled then no\nexception is raised. And if traps are enabled, the value of the underflow flag is\nundefined, so again it doesn\u2019t need to be set.\nOne final subtlety should be mentioned concerning underflow. When there is\nno underflow trap handler, the result of an operation on p-bit numbers that causes\nJ-36\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1232,
        "text": "an underflow is a denormal number with p1 or fewer bits of precision. When\ntraps are enabled, the trap handler is provided with the result of the operation\nrounded to p bits and with the exponent wrapped around. Now there is a potential\ndouble-rounding problem. If the trap handler wants to return the denormal result, it\ncan\u2019t just round its argument, because that might lead to a double-rounding error.\nThus, the trap handler must be passed at least one extra bit of information if it is to\nbe able to deliver the correctly rounded result.\nJ.8\nSpeeding Up Integer Addition\nThe previous section showed that many steps go into implementing floating-point\noperations; however, each floating-point operation eventually reduces to an integer\noperation. Thus, increasing the speed of integer operations will also lead to faster\nfloating point.\nInteger addition is the simplest operation and the most important. Even for\nprograms that don\u2019t do explicit arithmetic, addition must be performed to incre-\nment the program counter and to calculate addresses. Despite the simplicity of\naddition, there isn\u2019t a single best way to perform high-speed addition. We will dis-\ncuss three techniques that are in current use: carry-lookahead, carry-skip, and\ncarry-select.\nCarry-Lookahead\nAn n-bit adder is just a combinational circuit. It can therefore be written by a logic\nformula whose form is a sum of products and can be computed by a circuit with two\nlevels of logic. How do you figure out what this circuit looks like? From\nEquation J.2.1 (page J-3) the formula for the ith sum can be written as:\nsi \u00bc aibici + aibici + aibici + aibici\nJ:8:1\nwhere ci is both the carry-in to the ith adder and the carry-out from the (i1)-st\nadder.\nThe problem with this formula is that, although we know the values of ai\nand bi\u2014they are inputs to the circuit\u2014we don\u2019t know ci. So our goal is to write\nci in terms of ai and bi. To accomplish this, we first rewrite Equation J.2.2\n(page J-3) as:\nci \u00bc gi1 + pi1ci1, gi1 \u00bc ai1bi1, pi1 \u00bc ai1 + bi1\nJ:8:2\nHere is the reason for the symbols p and g: If gi1 is true, then ci is certainly\ntrue, so a carry is generated. Thus, g is for generate. If pi1 is true, then if ci1 is\ntrue, it is propagated to ci. Start with Equation J.8.1 and use Equation J.8.2 to\nreplace ci with gi1+pi1ci1. Then, use Equation J.8.2 with i1 in place of i\nto replace ci1 with ci2, and so on. This gives the result:\nci \u00bc gi1 + pi1gi2 + pi1pi2gi3 + \u22ef+ pi1pi2\u22efp1g0 + pi1pi2\u22efp1p0c0\nJ:8:3\nJ.8\nSpeeding Up Integer Addition\n\u25a0\nJ-37"
    },
    {
        "page": 1233,
        "text": "An adder that computes carries using Equation J.8.3 is called a carry-lookahead\nadder, or CLA. A CLA requires one logic level to form p and g, two levels to form\nthe carries, and two for the sum, for a grand total of five logic levels. This is a vast\nimprovement over the 2n levels required for the ripple-carry adder.\nUnfortunately, as is evident from Equation J.8.3 or from Figure J.14, a carry-\nlookahead adder on n bits requires a fan-in of n+1 at the OR gate as well as at the\nrightmost AND gate. Also, the pn1 signal must drive n AND gates. In addition, the\nrather irregular structure and many long wires of Figure J.14 make it impractical to\nbuild a full carry-lookahead adder when n is large.\nHowever, we can use the carry-lookahead idea to build an adder that has about\nlog2n logic levels (substantially fewer than the 2n required by a ripplecarry adder)\nand yet has a simple, regular structure. The idea is to build up the p\u2019s and g\u2019s in\nsteps. We have already seen that\nc1 \u00bc g0 + c0p0\nThis says there is a carry-out of the 0th position (c1) either if there is a carry gen-\nerated in the 0th position or if there is a carry into the 0th position and the carry\npropagates. Similarly,\nc2 \u00bc G01 + P01c0\nG01 means there is a carry generated out of the block consisting of the first two bits.\nP01 means that a carry propagates through this block. P and G have the following\nlogic equations:\nG01 \u00bc g1 + p1g0\nP01 \u00bc p1p0\ngn\u20131\npn\u20131\ncn\ngn\u20132\npn\u20132 gn\u20133\np1 g0\np0 c0\ncn= gn\u20131+ pn\u20131 gn\u20132 + . . . + pn\u20131 pn\u20132 . . . p1g0 + pn\u20131 pn\u20132 . . . p0c0\nFigure J.14 Pure carry-lookahead circuit for computing the carry-out cn of an\nn-bit adder.\nJ-38\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1234,
        "text": "More generally, for any j with i<j, j+1<k, we have the recursive relations:\nck + 1 \u00bc Gik + Pikci\nJ:8:4\nGik \u00bc Gj + 1,k + Pj + 1,kGij\nJ:8:5\nPik \u00bc PijPj + 1,k\nJ:8:6\nEquation J.8.5 says that a carry is generated out of the block consisting of bits i\nthrough k inclusive if it is generated in the high-order part of the block (j+1, k)\nor if it is generated in the low-order part of the block (i,j) and then propagated\nthrough the high part. These equations will also hold for i\u0004j<k if we set Gii\u00bcgi\nand Pii\u00bcpi.\nExample\nExpress P03 and G03 in terms of p\u2019s and g\u2019s.\nAnswer\nUsing Equation J.8.6, P03\u00bcP01P23\u00bcP00P11P22P33. Since Pii\u00bcpi, P03\u00bcp0p1p2p3.\nFor G03, Equation J.8.5 says G03\u00bcG23+P23G01\u00bc(G33+P33G22)+(P22P33)\n(G11+P11G00)\u00bcg3+p3g2+p3p2g1+p3p2p1g0.\nWith these preliminaries out of the way, we can now show the design of a\npractical CLA. The adder consists of two parts. The first part computes various\nvalues of P and G from pi and gi, using Equations J.8.5 and J.8.6; the second part\nuses these P and G values to compute all the carries via Equation J.8.4. The first\npart of the design is shown in Figure J.15. At the top of the diagram, input num-\nbers a7\u2026 a0 and b7\u2026 b0 are converted to p\u2019s and g\u2019s using cells of type 1. Then\nvarious P\u2019s and G\u2019s are generated by combining cells of type 2 in a binary tree\nstructure. The second part of the design is shown in Figure J.16. By feeding c0 in\nat the bottom of this tree, all the carry bits come out at the top. Each cell must\nknow a pair of (P,G) values in order to do the conversion, and the value it needs\nis written inside the cells. Now compare Figures J.15 and J.16. There is a one-to-\none correspondence between cells, and the value of (P,G) needed by the carry-\ngenerating cells is exactly the value known by the corresponding (P,G)-\ngenerating cells. The combined cell is shown in Figure J.17. The numbers to\nbe added flow into the top and downward through the tree, combining with c0\nat the bottom and flowing back up the tree to form the carries. Note that one thing\nis missing from Figure J.17: a small piece of extra logic to compute c8 for the\ncarry-out of the adder.\nThe bits in a CLA must pass through about log2 n logic levels, compared with\n2n for a ripple-carry adder. This is a substantial speed improvement, especially for\na large n. Whereas the ripple-carry adder had n cells, however, the CLA has 2n\ncells, although in our layout they will take n log n space. The point is that a small\ninvestment in size pays off in a dramatic improvement in speed.\nA number of technology-dependent modifications can improve CLAs. For\nexample, if each node of the tree has three inputs instead of two, then the height\nJ.8\nSpeeding Up Integer Addition\n\u25a0\nJ-39"
    },
    {
        "page": 1235,
        "text": "of the tree will decrease from log2 n to log3 n. Of course, the cells will be more\ncomplex and thus might operate more slowly, negating the advantage of the\ndecreased height. For technologies where rippling works well, a hybrid design\nmight be better. This is illustrated in Figure J.19. Carries ripple between adders\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n2\na7 b7\na6 b6\na5 b5\na4 b4\na3 b3\na2 b2\na1 b1\na0 b0\np0\ng0\np1\ng1\ng7\np7\nG6, 7\nP6, 7\nG4, 5\nP4, 5\nG2, 3\nP2, 3\nG0 ,1\nP0 ,1\nG4, 7\nP4, 7\nG0, 3\nP0, 3\nG0, 7\nP0, 7\ngi = aibi pi = ai + bi \nGi, k = Gj+1, k + Pj+1, k Gi, j\nPi, k = Pi, j Pj+1,k\nPi, j\nGi, j\nGj+1, k\nai bi\nPj+1, k\n2\n2\n2\nFigure J.15 First part of carry-lookahead tree. As signals flow from the top to the\nbottom, various values of P and G are computed.\nc7\nc6\nc5\nc4\nc3\nc2\nc1\nc0\np0\ng0\np2\ng2\nP 0, 1\nG 0, 1\np4\ng4\np6\ng6\nc6\nc4\nc2\nc0\nc0\nc4\nc0\ncj+1 = Gi j + Pi j ci\nci\nPi, j\nGi, j\nci\np4, 5\nG4, 5\nP0, 3\nG0, 3\nFigure J.16 Second part of carry-lookahead tree. Signals flow from the bottom to the\ntop, combining with P and G to form the carries.\nJ-40\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1236,
        "text": "at the top level, while the \u201cB\u201d boxes are the same as those in Figure J.17. This\ndesign will be faster if the time to ripple between four adders is faster than the time\nit takes to traverse a level of \u201cB\u201d boxes. (To make the pattern more clear,\nFigure J.19 shows a 16-bit adder, so the 8-bit adder of Figure J.17 corresponds\nto the right half of Figure J.19.)\nCarry-Skip Adders\nA carry-skip adder sits midway between a ripple-carry adder and a carry-\nlookahead adder, both in terms of speed and cost. (A carry-skip adder is not called\na CSA, as that name is reserved for carry-save adders.) The motivation for this\nadder comes from examining the equations for P and G. For example,\nP03 \u00bc p0p1p2p3\nG03 \u00bc g3 + p3g2 + p3p2g1 + p3p2p1g0\nComputing P is much simpler than computing G, and a carry-skip adder only\ncomputes the P\u2019s. Such an adder is illustrated in Figure J.18. Carries begin rippling\nA\nA\nA\nB\nB\ns7\na7 b7\nc7\nA\nA\nA\nA\nA\nA\nB\nB\nB\nB\n+\n+\nB\ns1\na1 b1\ns0\na0 b0\nc6\nc5\nc4\nc3\nc2\nc1\nc0\nc0\nc0\nP0, 3\nG0, 3\nc4\nc0\nsi ai bi\nsi = ai \npi = ai + bi\ngi = ai  bi\ngi pi ci\nGi, k Pi, k ci\nci\nPij\nGij\ncj +1\nPj +1,k\nGj +1,k\nbi \nci \nc2\nc4\nc6\nB\nFigure J.17 Complete carry-lookahead tree adder. This is the combination of\nFigures J.15 and J.16. The numbers to be added enter at the top, flow to the bottom\nto combine with c0, and then flow back up to compute the sum bits.\nJ.8\nSpeeding Up Integer Addition\n\u25a0\nJ-41"
    },
    {
        "page": 1237,
        "text": "simultaneously through each block. If any block generates a carry, then the carry-out\nofablockwillbetrue,eventhoughthecarry-intotheblockmaynotbecorrectyet.Ifat\nthe start of each add operation the carry-in to each block is 0, then no spurious carry-\noutswill begenerated.Thus, the carry-out of eachblock can bethought of as ifitwere\nthe G signal. Once the carry-out from the least-significant block is generated, it not\nonly feeds into the next block but is also fed through the AND gate with the\nP signal from that next block. If the carry-out and P signals are both true, then the\ncarry skips the second block and is ready to feed into the third block, and so on.\nThe carry-skip adder is only practical if the carry-in signals can be easily cleared\nat the start of each operation\u2014for example, by precharging in CMOS.\nTo analyze the speed of a carry-skip adder, let\u2019s assume that it takes 1 time\nunit for a signal to pass through two logic levels. Then it will take k time units for\na carry to ripple across a block of size k, and it will take 1 time unit for a carry to\nskip a block. The longest signal path in the carry-skip adder starts with a carry\nbeing generated at the 0th position. If the adder is n bits wide, then it takes k time\nunits to ripple through the first block, n/k2 time units to skip blocks, and k more\nto ripple through the last block. To be specific: if we have a 20-bit adder broken\ninto groups of 4 bits, it will take 4+(20/42)+4\u00bc11 time units to perform an\na3 b3 a2 b2 a1 b1 a0 b0\nc4\nc0\nP4, 7\nc8\nc12\nP12, 15\nP8, 11\na19\na18\nb19\nb18\nc16\nc20\nFigure J.18 Carry-skip adder. This is a 20-bit carry-skip adder (n\u00bc20) with each block 4 bits wide (k\u00bc4).\nc15\nc14\nc13\nc12\nP12, 15\nP8, 15\nc8\nc0\nP0, 7\nc8\nc4\nc0\nc0\nG0, 3\nP0, 3\nc1\nc2\nc3\nC\nB\nB\nC\nC\nC\nB\nFigure J.19 Combination of CLA and ripple-carry adder. In the top row, carries ripple\nwithin each group of four boxes.\nJ-42\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1238,
        "text": "add. Some experimentation reveals that there are more efficient ways to divide 20\nbits into blocks. For example, consider five blocks with the least-significant 2 bits\nin the first block, the next 5 bits in the second block, followed by blocks of size 6,\n5, and 2. Then the add time is reduced to 9 time units. This illustrates an important\ngeneral principle. For a carry-skip adder, making the interior blocks larger will\nspeed up the adder. In fact, the same idea of varying the block sizes can\nsometimes speed up other adder designs as well. Because of the large amount\nof rippling, a carry-skip adder is most appropriate for technologies where rippling\nis fast.\nCarry-Select Adder\nA carry-select adder works on the following principle: Two additions are\nperformed in parallel, one assuming the carry-in is 0 and the other assuming the\ncarry-in is 1. When the carry-in is finally known, the correct sum (which has been\nprecomputed) is simply selected. An example of such a design is shown in\nFigure J.20. An 8-bit adder is divided into two halves, and the carry-out from\nthe lower half is used to select the sum bits from the upper half. If each block\nis computing its sum using rippling (a linear time algorithm), then the design in\nFigure J.20 is twice as fast at 50% more cost. However, note that the c4 signal must\ndrive many muxes, which may be very slow in some technologies. Instead of divid-\ning the adder into halves, it could be divided into quarters for a still further speedup.\nThis is illustrated in Figure J.21. If it takes k time units for a block to add k-bit\nnumbers, and if it takes 1 time unit to compute the mux input from the two\ncarry-out signals, then for optimal operation each block should be 1 bit wider than\nthe next, as shown in Figure J.21. Therefore, as in the carry-skip adder, the best\ndesign involves variable-size blocks.\nAs a summary of this section, the asymptotic time and space requirements\nfor the different adders are given in Figure J.22. (The times for carry-skip and\nc0\ns0\ns1\ns2\ns3\nc4\ns4\na4 b4\ns5\ns6\ns7\n1\n0\na3   b3   a2 b2   a1 b1   a0      \n    b0\na7     b7\na4    b4\nFigure J.20 Simple carry-select adder. At the same time that the sum of the low-order\n4 bits is being computed, the high-order bits are being computed twice in parallel: once\nassuming that c4\u00bc0 and once assuming c4\u00bc1.\nJ.8\nSpeeding Up Integer Addition\n\u25a0\nJ-43"
    },
    {
        "page": 1239,
        "text": "carry-select come from a careful choice of block size. See Exercise J.26 for the\ncarry-skip adder.) These different adders shouldn\u2019t be thought of as disjoint\nchoices, but rather as building blocks to be used in constructing an adder. The util-\nity of these different building blocks is highly dependent on the technology used.\nFor example, the carry-select adder works well when a signal can drive many\nmuxes, and the carry-skip adder is attractive in technologies where signals can\nbe cleared at the start of each operation. Knowing the asymptotic behavior of\nadders is useful in understanding them, but relying too much on that behavior is\na pitfall. The reason is that asymptotic behavior is only important as n grows very\nlarge. But n for an adder is the bits of precision, and double precision today is the\nsame as it was 20 years ago\u2014about 53 bits. Although it is true that as computers\nget faster, computations get longer\u2014and thus have more rounding error, which in\nturn requires more precision\u2014this effect grows very slowly with time.\nJ.9\nSpeeding Up Integer Multiplication and Division\nThe multiplication and division algorithms presented in Section J.2 are fairly slow,\nproducing 1 bit per cycle (although that cycle might be a fraction of the CPU\ninstruction cycle time). In this section, we discuss various techniques for\nhigher-performance multiplication and division, including the division algorithm\nused in the Pentium chip.\nc13\nc8\nc4\nc0\ns0\ns1\ns2\ns3\ns4\ns5\ns6\ns7\ns8\ns9\n1\n0\n0\n1\ns10\ns11\ns12\ns13\nc13\nc8\n1\ns14\ns15\ns16\ns17\ns18\n0\nFigure J.21 Carry-select adder. As soon as the carry-out of the rightmost block is\nknown, it is used to select the other sum bits.\nAdder\nTime\nSpace\nRipple\n0(n)\n0(n)\nCLA\n0(log n)\n0(n log n)\nCarry-skip\n0\n\ufb03\ufb03\ufb03n\np\n\u00f0\n\u00de\n0(n)\nCarry-select\n0\n\ufb03\ufb03\ufb03n\np\n\u00f0\n\u00de\n0(n)\nFigure J.22 Asymptotic time and space requirements for four different types of\nadders.\nJ-44\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1240,
        "text": "Shifting over Zeros\nAlthough the technique of shifting over zeros is not currently used much, it is\ninstructive to consider. It is distinguished by the fact that its execution time is oper-\nand dependent. Its lack of use is primarily attributable to its failure to offer enough\nspeedup over bit-at-a-time algorithms. In addition, pipelining, synchronization\nwith the CPU, and good compiler optimization are difficult with algorithms that\nrun in variable time. In multiplication, the idea behind shifting over zeros is to\nadd logic that detects when the low-order bit of the A register is 0 (see\nFigure J.2(a) on page J-4) and, if so, skips the addition step and proceeds directly\nto the shift step\u2014hence the term shifting over zeros.\nWhat about shifting for division? In nonrestoring division, an ALU oper-\nation (either an addition or subtraction) is performed at every step. There\nappears to be no opportunity for skipping an operation. But think about\ndivision this way: To compute a/b, subtract multiples of b from a, and then\nreport how many subtractions were done. At each stage of the subtraction pro-\ncess the remainder must fit into the P register of Figure J.2(b) (page J-4). In the\ncase when the remainder is a small positive number, you normally subtract b;\nbut suppose instead you only shifted the remainder and subtracted b the next\ntime. As long as the remainder was sufficiently small (its high-order bit 0),\nafter shifting it still would fit into the P register, and no information would\nbe lost. However, this method does require changing the way we keep track\nof the number of times b has been subtracted from a. This idea usually goes\nunder the name of SRT division, for Sweeney, Robertson, and Tocher, who\nindependently proposed algorithms of this nature. The main extra complica-\ntion of SRT division is that the quotient bits cannot be determined immediately\nfrom the sign of P at each step, as they can be in ordinary nonrestoring\ndivision.\nMore precisely, to divide a by b where a and b are n-bit numbers, load a and b\ninto the A and B registers, respectively, of Figure J.2 (page J-4).\nSRT Division\n1. If B has k leading zeros when expressed using n bits, shift all the registers left\nk bits.\n2. For i\u00bc0, n1,\na) If the top three bits of P are equal, set qi\u00bc0 and shift (P,A) one bit left.\nb) If the top three bits of P are not all equal and P is negative, set qi\u00bc1 (also\nwritten as 1), shift (P,A) one bit left, and add B.\nc) Otherwise set qi\u00bc1, shift (P,A) one bit left, and subtract B.\nEnd loop\n3. If the final remainder is negative, correct the remainder by adding B, and correct\nthe quotient by subtracting 1 from q0. Finally, the remainder must be shifted k\nbits right, where k is the initial shift.\nJ.9\nSpeeding Up Integer Multiplication and Division\n\u25a0\nJ-45"
    },
    {
        "page": 1241,
        "text": "A numerical example is given in Figure J.23. Although we are discussing integer\ndivision, it helps in explaining the algorithm to imagine the binary point just left of\nthe most-significant bit. This changes Figure J.23 from 010002/00112 to 0.10002/\n.00112. Since the binary point is changed in both the numerator and denominator,\nthe quotient is not affected. The (P,A) register pair holds the remainder and is a two\u2019s\ncomplement number. For example, if P contains 111102 and A\u00bc0, then the remain-\nder is 1.11102\u00bc1/8. If r is the value of the remainder, then 1\u0004r<1.\nGiven these preliminaries, we can now analyze the SRT division algorithm. The\nfirst step of the algorithm shifts b so that b\u00031/2. The rule for which ALU operation\nto perform is this: If 1/4\u0004r<1/4 (true whenever the top three bits of P are equal),\nthen compute 2r by shifting (P,A) left one bit; if r<0 (and hence r<1/4, since\notherwise it would have been eliminated by the first condition), then compute 2r\n+b by shifting and then adding; if r\u00031/4 and subtract b from 2r. Using b\u00031/2,\nit is easy to check that these rules keep 1/2\u0004r<1/2. For nonrestoring division,\nwe only have jrj\u0004b, and we need P to be n+1 bits wide. But, for SRT division,\nthe bound on r is tighter, namely, 1/2\u0004r<1/2. Thus, we can save a bit by elim-\ninating the high-order bit of P (and b and the adder). In particular, the test for equality\nof the top three bits of P becomes a test on just two bits.\nThe algorithm might change slightly in an implementation of SRT division.\nAfter each ALU operation, the P register can be shifted as many places as necessary\nto make either r\u00031/4 or r<1/4. By shifting k places, k quotient bits are set equal\nto zero all at once. For this reason SRT division is sometimes described as one that\nkeeps the remainder normalized to jrj\u00031/4.\nP\nA\n00000\n1000\nDivide 8\u00bc1000 by 3\u00bc0011. B contains 0011.\n00010\n0000\nStep 1: B had two leading 0 s, so shift left by 2. B now contains 1100.\nStep 2.1: Top three bits are equal. This is case (a), so\n00100\n0000\nset q0\u00bc0 and shift.\nStep 2.2: Top three bits not equal and P\u00030 is case (c), so\n01000\n0001\nset q1\u00bc1 and shift.\n+ 10100\nSubtract B.\n11100\n0001\nStep 2.3: Top bits equal is case (a), so\n11000\n0010\nset q2\u00bc0 and shift.\nStep 2.4: Top three bits unequal is case (b), so\n10000\n0101\nset q3\u00bc1 and shift.\n+ 01100\nAdd B.\n11100\nStep 3. remainder is negative so restore it and subtract 1 from q.\n+ 01100\n01000\nMust undo the shift in step 1, so right-shift by 2 to get true remainder.\nRemainder\u00bc10, quotient \u00bc 0101 1 \u00bc 0010.\nFigure J.23 SRT division of 10002/00112. The quotient bits are shown in bold, using\nthe notation 1 for 1.\nJ-46\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1242,
        "text": "Notice that the value of the quotient bit computed in a given step is based on\nwhich operation is performed in that step (which in turn depends on the result of the\noperation from the previous step). This is in contrast to nonrestoring division,\nwhere the quotient bit computed in the ith step depends on the result of the oper-\nation in the same step. This difference is reflected in the fact that when the final\nremainder is negative, the last quotient bit must be adjusted in SRT division,\nbut not in nonrestoring division. However, the key fact about the quotient bits\nin SRT division is that they can include 1. Although Figure J.23 shows the quotient\nbits being stored in the low-order bits of A, an actual implementation can\u2019t do this\nbecause you can\u2019t fit the three values 1, 0, 1 into one bit. Furthermore, the quo-\ntient must be converted to ordinary two\u2019s complement in a full adder. A common\nway to do this is to accumulate the positive quotient bits in one register and the\nnegative quotient bits in another, and then subtract the two registers after all the\nbits are known. Because there is more than one way to write a number in terms\nof the digits 1, 0, 1, SRT division is said to use a redundant quotient\nrepresentation.\nThe differences between SRT division and ordinary nonrestoring division can\nbe summarized as follows:\n1. ALU decision rule\u2014In nonrestoring division, it is determined by the sign of P;\nin SRT, it is determined by the two most-significant bits of P.\n2. Final quotient\u2014In nonrestoring division, it is immediate from the successive\nsigns of P; in SRT, there are three quotient digits (1, 0, 1), and the final quotient\nmust be computed in a full n-bit adder.\n3. Speed\u2014SRT division will be faster on operands that produce zero quotient bits.\nThe simple version of the SRT division algorithm given above does not offer\nenough of a speedup to be practical in most cases. However, later on in this section\nwe will study variants of SRT division that are quite practical.\nSpeeding Up Multiplication with a Single Adder\nAs mentioned before, shifting-over-zero techniques are not used much in current\nhardware. We now discuss some methods that are in widespread use. Methods that\nincrease the speed of multiplication can be divided into two classes: those that use a\nsingle adder and those that use multiple adders. Let\u2019s first discuss techniques that\nuse a single adder.\nIn the discussion of addition we noted that, because of carry propagation, it is\nnot practical to perform addition with two levels of logic. Using the cells of\nFigure J.17, adding two 64-bit numbers will require a trip through seven cells\nto compute the P\u2019s and G\u2019s and seven more to compute the carry bits, which will\nrequire at least 28 logic levels. In the simple multiplier of Figure J.2 on page J-4,\neach multiplication step passes through this adder. The amount of computation in\neach step can be dramatically reduced by using carry-save adders (CSAs). A carry-\nsave adder is simply a collection of n independent full adders. A multiplier using\nJ.9\nSpeeding Up Integer Multiplication and Division\n\u25a0\nJ-47"
    },
    {
        "page": 1243,
        "text": "such an adder is illustrated in Figure J.24. Each circle marked \u201c+\u201d is a single-bit full\nadder, and each box represents one bit of a register. Each addition operation results\nin a pair of bits, stored in the sum and carry parts of P. Since each add is indepen-\ndent, only two logic levels are involved in the add\u2014a vast improvement over 28.\nTo operate the multiplier in Figure J.24, load the sum and carry bits of P with\nzero and perform the first ALU operation. (If Booth recoding is used, it might be a\nsubtraction rather than an addition.) Then shift the low-order sum bit of P into A, as\nwell as shifting A itself. The n1 high-order bits of P don\u2019t need to be shifted\nbecause on the next cycle the sum bits are fed into the next lower-order adder. Each\naddition step is substantially increased in speed, since each add cell is working\nindependently of the others, and no carry is propagated.\nThere are two drawbacks to carry-save adders. First, they require more\nhardware because there must be a copy of register P to hold the carry outputs\nof the adder. Second, after the last step, the high-order word of the result must\nbe fed into an ordinary adder to combine the sum and carry parts. One way to\naccomplish this is by feeding the output of P into the adder used to perform\nthe addition operation. Multiplying with a carry-save adder is sometimes called\nredundant multiplication because P is represented using two registers. Since there\nare many ways to represent P as the sum of two registers, this representation is\nredundant. The term carry-propagate adder (CPA) is used to denote an adder that\nis not a CSA. A propagate adder may propagate its carries using ripples, carry-\nlookahead, or some other method.\nAnother way to speed up multiplication without using extra adders is to examine\nk low-order bits of A at each step, rather than just one bit. This is often called higher-\nradix multiplication. As an example, suppose that k\u00bc2. If the pair of bits is 00, add\n0 to P; if it is 01, add B. If it is 10, simply shift b one bit left before adding it to P.\nUnfortunately, if the pair is 11, it appears we would have to compute b+2b. But this\ncan be avoided by using a higher-radix version of Booth recoding. Imagine A as a\nbase 4 number: When the digit 3 appears, change it to 1 and add 1 to the next higher\ndigit to compensate. An extra benefit of using this scheme is that just like ordinary\nBooth recoding, it works for negative as well as positive integers (Section J.2).\nB\nA\nP\nSum bits\nCarry bits\nciai\nci+1 si\nbi\nShift\n+\n+\n+\n+\n+\n+\n+\nFigure J.24 Carry-save multiplier. Each circle represents a (3,2) adder working indepen-\ndently. At each step, the only bit of P that needs to be shifted is the low-order sum bit.\nJ-48\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1244,
        "text": "The precise rules for radix-4 Booth recoding are given in Figure J.25. At the ith\nmultiply step, the two low-order bits of the A register contain a2i and a2i+1. These\ntwo bits, together with the bit just shifted out (a2i1), are used to select the multiple\nof b that must be added to the P register. A numerical example is given in\nFigure J.26. Another name for this multiplication technique is overlapping triplets,\nsince it looks at 3 bits to determine what multiple of b to use, whereas ordinary\nBooth recoding looks at 2 bits.\nBesides having more complex control logic, overlapping triplets also requires\nthat the P register be 1 bit wider to accommodate the possibility of 2b or 2b being\nadded to it. It is possible to use a radix-8 (or even higher) version of Booth recod-\ning. In that case, however, it would be necessary to use the multiple 3B as a poten-\ntial summand. Radix-8 multipliers normally compute 3B once and for all at the\nbeginning of a multiplication operation.\nLow-order bits of A\nLast bit shifted out\n2i+1\n2i\n2i1\nMultiple\n0\n0\n0\n0\n0\n0\n1\n+b\n0\n1\n0\n+b\n0\n1\n1\n+2b\n1\n0\n0\n2b\n1\n0\n1\nb\n1\n1\n0\nb\n1\n1\n1\n0\nFigure J.25 Multiples of b to use for radix-4 Booth recoding. For example, if the two\nlow-order bits of the A register are both 1, and the last bit to be shifted out of the A\nregister is 0, then the correct multiple isb, obtained from the second-to-last row of\nthe table.\nP\nA\nL\n00000\n1001\nMultiply 7\u00bc1001 times 5\u00bc1011. B contains 1011.\n+ 11011\nLow-order bits of A are 0, 1; L\u00bc0, so add B.\n11011\n1001\n11110\n1110\n0\nShift right by two bits, shifting in 1 s on the left.\n+ 01010\nLow-order bits of A are 1, 0; L\u00bc0, so add 2b.\n01000\n1110\n0\n00010\n0011\n1\nShift right by two bits.\nProduct is 35\u00bc0100011.\nFigure J.26 Multiplication of 27 times 25 using radix-4 Booth recoding. The column\nlabeled L contains the last bit shifted out the right end of A.\nJ.9\nSpeeding Up Integer Multiplication and Division\n\u25a0\nJ-49"
    },
    {
        "page": 1245,
        "text": "Faster Multiplication with Many Adders\nIf the space for many adders is available, then multiplication speed can be\nimproved. Figure J.27 shows a simple array multiplier for multiplying two 5-bit\nnumbers, using three CSAs and one propagate adder. Part (a) is a block diagram\nof the kind we will use throughout this section. Parts (b) and (c) show the adder in\nmore detail. All the inputs to the adder are shown in (b); the actual adders with their\ninterconnections are shown in (c). Each row of adders in (c) corresponds to a box in\n(b)\n(c)\nb0 a1\nb0 a0\nb0 A\nb1 A\nb2 A\nb3 A\nb4 A\nb4 a1\nb4 a0\nb0 a4\nb0 A\nb1 A\nb2 A\nb1 a4\np9\np8\np7\np6\np5\np4\np3\np2\np1 p0\n(a)\nb4A\nb3A\nb2A\nb1A\nb0A\nCSA\nCSA\nCSA\nPropagate adder\nFigure J.27 An array multiplier. The 5-bit number in A is multiplied by b4b3b2b1b0. Part\n(a) shows the block diagram, (b) shows the inputs to the array, and (c) expands the array\nto show all the adders.\nJ-50\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1246,
        "text": "(a). The picture is \u201ctwisted\u201d so that bits of the same significance are in the same\ncolumn. In an actual implementation, the array would most likely be laid out as\na square instead.\nThe array multiplier in Figure J.27 performs the same number of additions as\nthe design in Figure J.24, so its latency is not dramatically different from that of a\nsingle carry-save adder. However, with the hardware in Figure J.27, multiplication\ncan be pipelined, increasing the total throughput. On the other hand, although this\nlevel of pipelining is sometimes used in array processors, it is not used in any of the\nsingle-chip, floating-point accelerators discussed in Section J.10. Pipelining is dis-\ncussed in general in Appendix C and by Kogge [1981] in the context of multipliers.\nSometimes the space budgeted on a chip for arithmetic may not hold an array\nlarge enough to multiply two double-precision numbers. In this case, a popular\ndesign is to use a two-pass arrangement such as the one shown in Figure J.28.\nThe first pass through the array \u201cretires\u201d 5 bits of B. Then the result of this first\npass is fed back into the top to be combined with the next three summands. The\nresult of this second pass is then fed into a CPA. This design, however, loses\nthe ability to be pipelined.\nIf arrays require as many addition steps as the much cheaper arrangements in\nFigures J.2 and J.24, why are they so popular? First of all, using an array has a\nsmaller latency than using a single adder\u2014because the array is a combinational\ncircuit, the signals flow through it directly without being clocked. Although the\ntwo-pass adder of Figure J.28 would normally still use a clock, the cycle time\nfor passing through k arrays can be less than k times the clock that would be needed\nfor designs like the ones in Figures J.2 or J.24. Second, the array is amenable to\nvarious schemes for further speedup. One of them is shown in Figure J.29. The\nidea of this design is that two adds proceed in parallel or, to put it another way,\neach stream passes through only half the adders. Thus, it runs at almost twice\nCSA\nCPA\nb5A\nb2A\nb6A\nb3A\nb7A\nb4A\nb1A\nb0A\nCSA\nCSA\nFigure J.28 Multipass array multiplier. Multiplies two 8-bit numbers with about half\nthe hardware that would be used in a one-pass design like that of Figure J.27. At the\nend of the second pass, the bits flow into the CPA. The inputs used in the first pass\nare marked in bold.\nJ.9\nSpeeding Up Integer Multiplication and Division\n\u25a0\nJ-51"
    },
    {
        "page": 1247,
        "text": "the speed of the multiplier in Figure J.27. This even/odd multiplier is popular in\nVLSI because of its regular structure. Arrays can also be speeded up using asyn-\nchronous logic. One of the reasons why the multiplier of Figure J.2 (page J-4)\nneeds a clock is to keep the output of the adder from feeding back into the input\nof the adder before the output has fully stabilized. Thus, if the array in Figure J.28 is\nlong enough so that no signal can propagate from the top through the bottom in the\ntime it takes for the first adder to stabilize, it may be possible to avoid clocks alto-\ngether. Williams et al. [1987] discussed a design using this idea, although it is for\ndividers instead of multipliers.\nThe techniques of the previous paragraph still have a multiply time of 0(n), but\nthe time can be reduced to log n using a tree. The simplest tree would combine pairs\nof summands b0A\u22efbn1A, cutting the number of summands from n to n/2. Then\nthese n/2 numbers would be added in pairs again, reducing to n/4, and so on, and\nresulting in a single sum after log n steps. However, this simple binary tree idea\ndoesn\u2019t map into full (3,2) adders, which reduce three inputs to two rather than\nreducing two inputs to one. A tree that does use full adders, known as a Wallace\ntree, is shown in Figure J.30. When computer arithmetic units were built out of\nb2A\nb4A\nb3A\nb5A\nb1A\nb0A\nCSA\nCSA\nb6A\nb7A\nCSA\nCSA\nCSA\nCPA\nCSA\nFigure J.29 Even/odd array. The first two adders work in parallel. Their results are fed\ninto the third and fourth adders, which also work in parallel, and so on.\nJ-52\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1248,
        "text": "MSI parts, a Wallace tree was the design of choice for high-speed multipliers.\nThere is, however, a problem with implementing it in VLSI. If you try to fill in\nall the adders and paths for the Wallace tree of Figure J.30, you will discover that\nit does not have the nice, regular structure of Figure J.27. This is why VLSI\ndesigners have often chosen to use other log n designs such as the binary tree mul-\ntiplier, which is discussed next.\nThe problem with adding summands in a binary tree is coming up with a (2,1)\nadder that combines two digits and produces a single-sum digit. Because of\ncarries, this isn\u2019t possible using binary notation, but it can be done with some\nother representation. We will use the signed-digit representation 1, 1, and 0,\nwhich we used previously to understand Booth\u2019s algorithm. This representation\nhas two costs. First, it takes 2 bits to represent each signed digit. Second, the algo-\nrithm for adding two signed-digit numbers ai and bi is complex and requires\nexamining aiai1ai2 and bibi1bi2. Although this means you must look 2 bits\nback, in binary addition you might have to look an arbitrary number of bits back\nbecause of carries.\nWe can describe the algorithm for adding two signed-digit numbers as follows.\nFirst, compute sum and carry bits si and ci+1 using Figure J.31. Then compute the\nfinal sum as si+ci. The tables are set up so that this final sum does not generate a\ncarry.\nExample\nWhat is the sum of the signed-digit numbers 1102 and 0012?\nAnswer\nThe two low-order bits sum to 0 + 1 \u00bc 11, the next pair sums to 1 + 0 \u00bc 01, and the\nhigh-order pair sums to 1+0\u00bc01, so the sum is 11 + 010 + 0100 \u00bc 1012.\nCSA\nCSA\nCSA\nCSA\nb7A\nb6A\nb5A\nb4A\nb3A\nb2 A\nb1 A\nb0 A\nCSA\nCSA\nPropagate adder\nFigure J.30 Wallace tree multiplier. An example of a multiply tree that computes a\nproduct in 0(log n) steps.\nJ.9\nSpeeding Up Integer Multiplication and Division\n\u25a0\nJ-53"
    },
    {
        "page": 1249,
        "text": "This, then, defines a (2,1) adder. With this in hand, we can use a straightforward\nbinary tree to perform multiplication. In the first step it adds b0A+b1A in parallel\nwith b2A+b3A, \u2026, bn2A+bn1A. The next step adds the results of these sums in\npairs, and so on. Although the final sum must be run through a carry-propagate\nadder to convert it from signed-digit form to two\u2019s complement, this final add step\nis necessary in any multiplier using CSAs.\nTo summarize, both Wallace trees and signed-digit trees are log n multipliers.\nThe Wallace tree uses fewer gates but is harder to lay out. The signed-digit tree has\na more regular structure, but requires 2 bits to represent each digit and has more\ncomplicated add logic. As with adders, it is possible to combine different multiply\ntechniques. For example, Booth recoding and arrays can be combined. In\nFigure J.27 instead of having each input be biA, we could have it be bibi1A.\nTo avoid having to compute the multiple 3b, we can use Booth recoding.\nFaster Division with One Adder\nThe two techniques we discussed for speeding up multiplication with a single\nadder were carry-save adders and higher-radix multiplication. However, there is\na difficulty when trying to utilize these approaches to speed up nonrestoring divi-\nsion. If the adder in Figure J.2(b) on page J-4 is replaced with a carry-save adder,\nthen P will be replaced with two registers, one for the sum bits and one for the carry\nbits (compare with the multiplier in Figure J.24). At the end of each cycle, the sign\nof P is uncertain (since P is the unevaluated sum of the two registers), yet it is the\nsign of P that is used to compute the quotient digit and decide the next ALU oper-\nation. When a higher radix is used, the problem is deciding what value to subtract\nfrom P. In the paper-and-pencil method, you have to guess the quotient digit. In\nbinary division, there are only two possibilities. We were able to finesse the prob-\nlem by initially guessing one and then adjusting the guess based on the sign of P.\nThis doesn\u2019t work in higher radices because there are more than two possible quo-\ntient digits, rendering quotient selection potentially quite complicated: You would\nhave to compute all the multiples of b and compare them to P.\nBoth the carry-save technique and higher-radix division can be made to work\nif we use a redundant quotient representation. Recall from our discussion of SRT\ndivision (page J-45) that by allowing the quotient digits to be 1, 0, or 1, there is\noften a choice of which one to pick. The idea in the previous algorithm was to\nchoose 0 whenever possible, because that meant an ALU operation could be\n1\n1\n+ 1\n0 0\n1\n+ 1\n1 0\n0\n1 x\n+ 0 y\n1 1\n0 1\n1 x\n+ 0 y\n1 1\n1 1\n+ 1\n+ 0\n1 0\n0 0\nif x\u00030 and\ny\u00030 otherwise\nif x\u00030 and\ny\u00030 otherwise\nFigure J.31 Signed-digit addition table. The leftmost sum shows that when comput-\ning 1+1, the sum bit is 0 and the carry bit is 1.\nJ-54\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1250,
        "text": "skipped. In carry-save division, the idea is that, because the remainder (which is the\nvalue of the (P,A) register pair) is not known exactly (being stored in carry-save\nform), the exact quotient digit is also not known. But, thanks to the redundant rep-\nresentation, the remainder doesn\u2019t have to be known precisely in order to pick a\nquotient digit. This is illustrated in Figure J.32, where the x-axis represents ri,\nthe remainder after i steps. The line labeled qi\u00bc1 shows the value that ri+1 would\nbe if we chose qi\u00bc1, and similarly for the lines qi\u00bc0 and qi\u00bc1. We can choose\nany value for qi, as long as ri+1\u00bc2riqib satisfies jri+1j\u0004b. The allowable ranges\nare shown in the right half of Figure J.32. This shows that you don\u2019t need to know\nthe precise value of ri in order to choose a quotient digit qi. You only need to know\nthat r lies in an interval small enough to fit entirely within one of the overlapping\nbars shown in the right half of Figure J.32.\nThis is the basis for using carry-save adders. Look at the high-order bits of the\ncarry-save adder and sum them in a propagate adder. Then use this approximation\nof r (together with the divisor, b) to compute qi, usually by means of a lookup table.\nThe same technique works for higher-radix division (whether or not a carry-save\nadder is used). The high-order bits P can be used to index a table that gives one of\nthe allowable quotient digits.\nThe design challenge when building a high-speed SRT divider is figuring out\nhow many bits of P and B need to be examined. For example, suppose that we take\na radix of 4, use quotient digits of 2, 1, 0, 1, 2, but have a propagate adder. How\nmany bits of P and B need to be examined? Deciding this involves two steps. For\nordinary radix-2 nonrestoring division, because at each stage jrj\u0004b, the P buffer\nwon\u2019t overflow. But, for radix 4, ri+1\u00bc4riqib is computed at each stage, and if ri\nis near b, then 4ri will be near 4b, and even the largest quotient digit will not bring r\nback to the range jri+1j\u0004b. In other words, the remainder might grow without\nbound. However, restricting jrij\u00042b/3 makes it easy to check that ri will stay\nbounded.\nAfter figuring out the bound that ri must satisfy, we can draw the diagram in\nFigure J.33, which is analogous to Figure J.32. For example, the diagram shows\nb\n\u2013b\n\u2013b\nb\n\u2013b\n0\nqi = \u20131\nqi = 0\nqi = 1\nqi = \u20131\nqi = 0\nqi = 1\nri\nri\nri +1 = 2ri \u2013 qib\nFigure J.32 Quotient selection for radix-2 division. The x-axis represents the ith\nremainder, which is the quantity in the (P,A) register pair. The y-axis shows the value\nof the remainder after one additional divide step. Each bar on the right-hand\ngraph gives the range of ri values for which it is permissible to select the associated\nvalue of qi.\nJ.9\nSpeeding Up Integer Multiplication and Division\n\u25a0\nJ-55"
    },
    {
        "page": 1251,
        "text": "that if ri is between (1/12)b and (5/12)b, we can pick q\u00bc1, and so on. Or, to put it\nanother way, if r/b is between 1/12 and 5/12, we can pick q\u00bc1. Suppose the\ndivider examines 5 bits of P (including the sign bit) and 4 bits of b (ignoring\nthe sign, since it is always nonnegative). The interesting case is when the high bits\nof P are 00011xxx\u22ef, while the high bits of b are 1001xxx\u22ef. Imagine the binary\npoint at the left end of each register. Since we truncated, r (the value of P\nconcatenated with A) could have a value from 0.00112 to 0.01002, and b could\nhave a value from .10012 to .10102. Thus, r/b could be as small as 0.00112/\n.10102 or as large as 0.01002/.10012, but 0.00112/.10102\u00bc3/10<1/3 would\nrequire a quotient bit of 1, while 0.01002/.10012\u00bc4/9>5/12 would require a quo-\ntient bit of 2. In other words, 5 bits of P and 4 bits of b aren\u2019t enough to pick a\nquotient bit. It turns out that 6 bits of P and 4 bits of b are enough. This can be\nverified by writing a simple program that checks all the cases. The output of such\na program is shown in Figure J.34.\nExample\nUsing 8-bit registers, compute 149/5 using radix-4 SRT division.\nAnswer\nFollow the SRT algorithm on page J-45, but replace the quotient selection rule in\nstep 2 with one that uses Figure J.34. See Figure J.35.\nThe Pentium uses a radix-4 SRT division algorithm like the one just presented,\nexcept that it uses a carry-save adder. Exercises J.34(c) and J.35 explore this in\ndetail. Although these are simple cases, all SRT analyses proceed in the same\nway. First compute the range of ri, then plot ri against ri+1 to find the quotient\nranges, and finally write a program to compute how many bits are necessary.\n(It is sometimes also possible to compute the required number of bits analytically.)\nVarious details need to be considered in building a practical SRT divider.\n2b\n3\n\u20132b\n3\n2b\n3\n5b\n12\nb\n3\nb\n6\nb\n12\n0\nqi = \u20132\nqi = \u20131\nqi = 1\nqi = 0\nqi = 2\nri\nri +1 = 4ri \u2013 qib\nqi = 2\nqi = 1\nri\nqi = 0\nqi = \u20132\nqi = \u20131\n\u20132b\n3\nri +1\nFigure J.33 Quotient selection for radix-4 division with quotient digits 22, 21, 0, 1, 2.\nJ-56\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1252,
        "text": "For example, the quotient lookup table has a fairly regular structure, which means it\nis usually cheaper to encode it as a PLA rather than in ROM. For more details about\nSRT division, see Burgess and Williams [1995].\nJ.10\nPutting It All Together\nIn this section, we will compare the Weitek 3364, the MIPS R3010, and the Texas\nInstruments 8847 (see Figures J.36 and J.37). In many ways, these are ideal chips\nto compare. They each implement the IEEE standard for addition, subtraction,\nb\nRange of P\nq\nb\nRange of P\nq\n8\n12\n7\n2\n12\n18\n10\n2\n8\n6\n3\n1\n12\n10\n4\n1\n8\n2\n1\n0\n12\n4\n3\n0\n8\n2\n5\n1\n12\n3\n9\n1\n8\n6\n11\n2\n12\n9\n17\n2\n9\n14\n8\n2\n13\n19\n11\n2\n9\n7\n3\n1\n13\n10\n4\n1\n9\n3\n2\n0\n13\n4\n3\n0\n9\n2\n6\n1\n13\n3\n9\n1\n9\n7\n13\n2\n13\n10\n18\n2\n10\n15\n9\n2\n14\n20\n11\n2\n10\n8\n3\n1\n14\n11\n4\n1\n10\n3\n2\n0\n14\n4\n3\n0\n10\n2\n7\n1\n14\n3\n10\n1\n10\n8\n14\n2\n14\n10\n19\n2\n11\n16\n9\n2\n15\n22\n12\n2\n11\n9\n3\n1\n15\n12\n4\n1\n11\n3\n2\n0\n15\n5\n4\n0\n11\n2\n8\n1\n15\n3\n11\n1\n11\n8\n15\n2\n15\n11\n21\n2\nFigure J.34 Quotient digits for radix-4 SRT division with a propagate adder. The top\nrow says that if the high-order 4 bits of b are 10002\u00bc8, and if the top 6 bits of P are\nbetween 1101002\u00bc12 and 1110012\u00bc7, then 2 is a valid quotient digit.\nJ.10\nPutting It All Together\n\u25a0\nJ-57"
    },
    {
        "page": 1253,
        "text": "P\nA\n000000000\n10010101\nDivide 149 by 5. B contains 00000101.\n000010010\n10100000\nStep 1:\nB had 5 leading 0s, so shift left by 5. B now\ncontains 10100000, so use b\u00bc10 section of\ntable.\nStep 2.1:\nTop 6 bits of P are 2, so shift left by 2. From\ntable, can pick q to be 0 or 1. Choose q0\u00bc0.\n001001010\n1000000\nStep 2.2:\nTop 6 bits of P are 9, so shift left 2. q1\u00bc2.\n100101010\n000002\n+ 011000000\nSubtract 2b.\n111101010\n000002\nStep 2.3:\nTop bits\u00bc3, so shift left 2. Can pick 0 or 1\nfor q, pick q2\u00bc0.\n110101000\n00020\nStep 2.4:\nTop bits\u00bc11, so shift left 2. q3\u00bc2.\n010100000\n0202\n+ 101000000\nAdd 2b.\n111100000\nStep 3:\nRemainder is negative, so restore by adding b\nand subtract 1 from q.\n+ 010100000\n010000000\nAnswer:\nq \u00bc 0202 1 \u00bc 29\nTo get remainder, undo shift in step 1 so\nremainder\u00bc010000000 >>5\u00bc4.\nFigure J.35 Example of radix-4 SRT division. Division of 149 by 5.\nFeatures\nMIPS R3010\nWeitek 3364\nTI 8847\nClock cycle time (ns)\n40\n50\n30\nSize (mil2)\n114,857\n147,600\n156,180\nTransistors\n75,000\n165,000\n180,000\nPins\n84\n168\n207\nPower (watts)\n3.5\n1.5\n1.5\nCycles/add\n2\n2\n2\nCycles/mult\n5\n2\n3\nCycles/divide\n19\n17\n11\nCycles/square root\n\n30\n14\nFigure J.36 Summary of the three floating-point chips discussed in this section. The\ncycle times are for production parts available in June 1989. The cycle counts are for\ndouble-precision operations.\nJ-58\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1254,
        "text": "Figure J.37 Chip layout for the TI 8847, MIPS R3010, and Weitek 3364. In the left-hand columns are the photo-\nmicrographs; the right-hand columns show the corresponding floor plans.\n(Continued)\nJ.10\nPutting It All Together\n\u25a0\nJ-59"
    },
    {
        "page": 1255,
        "text": "multiplication, and division on a single chip. All were introduced in 1988 and run\nwith a cycle time of about 40 nanoseconds. However, as we will see, they use quite\ndifferent algorithms. The Weitek chip is well described in Birman et al. [1990], the\nMIPS chip is described in less detail in Rowen, Johnson, and Ries [1988], and\ndetails of the TI chip can be found in Darley et al. [1989].\nThese three chips have a number of things in common. They perform addition\nand multiplication in parallel, and they implement neither extended precision nor a\nremainder step operation. (Recall from Section J.6 that it is easy to implement the\nIEEE remainder function in software if a remainder step instruction is available.)\nThe designers of these chips probably decided not to provide extended precision\nbecause the most influential users are those who run portable codes, which can\u2019t\nrely on extended precision. However, as we have seen, extended precision can\nmake for faster and simpler math libraries.\nIn the summary of the three chips given in Figure J.36, note that a higher tran-\nsistor count generally leads to smaller cycle counts. Comparing the cycles/op num-\nbers needs to be done carefully, because the figures for the MIPS chip are those for\na complete system (R3000/3010 pair), while the Weitek and TI numbers are for\nstand-alone chips and are usually larger when used in a complete system.\nThe MIPS chip has the fewest transistors of the three. This is reflected in the\nfact that it is the only chip of the three that does not have any pipelining or hardware\nFigure J.37\n(Continued)\nJ-60\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1256,
        "text": "square root. Further, the multiplication and addition operations are not completely\nindependent because they share the carry-propagate adder that performs the final\nrounding (as well as the rounding logic).\nAddition on the R3010 uses a mixture of ripple, CLA, and carry-select. A\ncarry-select adder is used in the fashion of Figure J.20 (page J-43). Within each\nhalf, carries are propagated using a hybrid ripple-CLA scheme of the type indicated\nin Figure J.19 (page J-42). However, this is further tuned by varying the size of\neach block, rather than having each fixed at 4 bits (as they are in Figure J.19).\nThe multiplier is midway between the designs of Figures J.2 (page J-4) and\nJ.27 (page J-50). It has an array just large enough so that output can be fed back\ninto the input without having to be clocked. Also, it uses radix-4 Booth recoding\nand the even/odd technique of Figure J.29 (page J-52). The R3010 can do a divide\nand multiply in parallel (like the Weitek chip but unlike the TI chip). The divider is\na radix-4 SRT method with quotient digits 2, 1, 0, 1, and 2, and is similar to that\ndescribed in Taylor [1985]. Double-precision division is about four times slower\nthan multiplication. The R3010 shows that for chips using an 0(n) multiplier, an\nSRT divider can operate fast enough to keep a reasonable ratio between multiply\nand divide.\nThe Weitek 3364 has independent add, multiply, and divide units. It also uses\nradix-4 SRT division. However, the add and multiply operations on the Weitek\nchip are pipelined. The three addition stages are (1) exponent compare, (2) add\nfollowed by shift (or vice versa), and (3) final rounding. Stages (1) and (3) take\nonly a half-cycle, allowing the whole operation to be done in two cycles, even\nthough there are three pipeline stages. The multiplier uses an array of the style\nof Figure J.28 but uses radix-8 Booth recoding, which means it must compute 3\ntimes the multiplier. The three multiplier pipeline stages are (1) compute 3b,\n(2) pass through array, and (3) final carry-propagation add and round. Single pre-\ncision passes through the array once, double precision twice. Like addition, the\nlatency is two cycles.\nThe Weitek chip uses an interesting addition algorithm. It is a variant on the\ncarry-skip adder pictured in Figure J.18 (page J-42). However, Pij, which is the log-\nical\nAND of many terms, is computed by rippling, performing one\nAND per\nripple. Thus, while the carries propagate left within a block, the value of Pij is prop-\nagating right within the next block, and the block sizes are chosen so that both waves\ncomplete at the same time. Unlike the MIPS chip, the 3364 has hardware square root,\nwhich shares the divide hardware. The ratio of double-precision multiply to divide is\n2:17. The large disparity between multiply and divide is due to the fact that multi-\nplication uses radix-8 Booth recoding, while division uses a radix-4 method. In the\nMIPS R3010, multiplication and division use the same radix.\nThe notable feature of the TI 8847 is that it does division by iteration (using the\nGoldschmidt algorithm discussed in Section J.6). This improves the speed of divi-\nsion (the ratio of multiply to divide is 3:11), but means that multiplication and divi-\nsion cannot be done in parallel as on the other two chips. Addition has a two-stage\npipeline. Exponent compare, fraction shift, and fraction addition are done in the\nfirst stage, normalization and rounding in the second stage. Multiplication uses\nJ.10\nPutting It All Together\n\u25a0\nJ-61"
    },
    {
        "page": 1257,
        "text": "a binary tree of signed-digit adders and has a three-stage pipeline. The first stage\npasses through the array, retiring half the bits; the second stage passes through the\narray a second time; and the third stage converts from signed-digit form to two\u2019s\ncomplement. Since there is only one array, a new multiply operation can only be\ninitiated in every other cycle. However, by slowing down the clock, two passes\nthrough the array can be made in a single cycle. In this case, a new multiplication\ncan be initiated in each cycle. The 8847 adder uses a carry-select algorithm rather\nthan carry-lookahead. As mentioned in Section J.6, the TI carries 60 bits of pre-\ncision in order to do correctly rounded division.\nThese three chips illustrate the different trade-offs made by designers with sim-\nilar constraints. One of the most interesting things about these chips is the diversity\nof their algorithms. Each uses a different add algorithm, as well as a different mul-\ntiply algorithm. In fact, Booth recoding is the only technique that is universally\nused by all the chips.\nJ.11\nFallacies and Pitfalls\nFallacy\nUnderflows rarely occur in actual floating-point application code\nAlthough most codes rarely underflow, there are actual codes that underflow fre-\nquently. SDRWAVE [Kahaner 1988], which solves a one-dimensional wave equa-\ntion, is one such example. This program underflows quite frequently, even when\nfunctioning properly. Measurements on one machine show that adding hardware\nsupport for gradual underflow would cause SDRWAVE to run about 50% faster.\nFallacy\nConversions between integer and floating point are rare\nIn fact, in spice they are as frequent as divides. The assumption that conversions\nare rare leads to a mistake in the SPARC version 8 instruction set, which does not\nprovide an instruction to move from integer registers to floating-point registers.\nPitfall\nDon\u2019t increase the speed of a floating-point unit without increasing its memory\nbandwidth\nA typical use of a floating-point unit is to add two vectors to produce a third vector.\nIf these vectors consist of double-precision numbers, then each floating-point\nadd will use three operands of 64 bits each, or 24 bytes of memory. The memory\nbandwidth requirements are even greater if the floating-point unit can perform\naddition and multiplication in parallel (as most do).\nPitfall\nx is not the same as 0x\nThis is a fine point in the IEEE standard that has tripped up some designers.\nBecause floating-point numbers use the sign magnitude system, there are two\nzeros, +0 and 0. The standard says that 00\u00bc+0, whereas(0)\u00bc0. Thus,\nx is not the same as 0x when x\u00bc0.\nJ-62\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1258,
        "text": "J.12\nHistorical Perspective and References\nThe earliest computers used fixed point rather than floating point. In \u201cPreliminary\nDiscussion of the Logical Design of an Electronic Computing Instrument,\u201d Burks,\nGoldstine, and von Neumann [1946] put it like this:\nThere appear to be two major purposes in a \u201cfloating\u201d decimal point system both of\nwhich arise from the fact that the number of digits in a word is a constant fixed by\ndesign considerations for each particular machine. The first of these purposes is to\nretain in a sum or product as many significant digits as possible and the second of\nthese is to free the human operator from the burden of estimating and inserting into\na problem \u201cscale factors\u201d\u2014multiplicative constants which serve to keep numbers\nwithin the limits of the machine.\nThere is, of course, no denying the fact that human time is consumed in arrang-\ning for the introduction of suitable scale factors. We only argue that the time so\nconsumed is a very small percentage of the total time we will spend in preparing\nan interesting problem for our machine. The first advantage of the floating point\nis, we feel, somewhat illusory. In order to have such a floating point, one must\nwaste memory capacity that could otherwise be used for carrying more digits\nper word. It would therefore seem to us not at all clear whether the modest advan-\ntages of a floating binary point offset the loss of memory capacity and the\nincreased complexity of the arithmetic and control circuits.\nThis enables us to see things from the perspective of early computer designers,\nwho believed that saving computer time and memory were more important than\nsaving programmer time.\nThe original papers introducing the Wallace tree, Booth recoding, SRT divi-\nsion, overlapped triplets, and so on are reprinted in Swartzlander [1990]. A good\nexplanation of an early machine (the IBM 360/91) that used a pipelined Wallace\ntree, Booth recoding, and iterative division is in Anderson et al. [1967]. A discus-\nsion of the average time for single-bit SRT division is in Freiman [1961]; this is one\nof the few interesting historical papers that does not appear in Swartzlander.\nThe standard book of Mead and Conway [1980] discouraged the use of CLAs\nas not being cost effective in VLSI. The important paper by Brent and Kung [1982]\nhelped combat that view. An example of a detailed layout for CLAs can be found in\nNgai and Irwin [1985] or in Weste and Eshraghian [1993], and a more theoretical\ntreatment is given by Leighton [1992]. Takagi, Yasuura, and Yajima [1985] pro-\nvide a detailed description of a signed-digit tree multiplier.\nBefore the ascendancy of IEEE arithmetic, many different floating-point for-\nmats were in use. Three important ones were used by the IBM 370, the DEC VAX,\nand the Cray. Here is a brief summary of these older formats. The VAX format is\nclosest to the IEEE standard. Its single-precision format (F format) is like IEEE\nsingle precision in that it has a hidden bit, 8 bits of exponent, and 23 bits of fraction.\nHowever, it does not have a sticky bit, which causes it to round halfway cases up\ninstead of to even. The VAX has a slightly different exponent range from IEEE\nJ.12\nHistorical Perspective and References\n\u25a0\nJ-63"
    },
    {
        "page": 1259,
        "text": "single: Emin is 128 rather than 126 as in IEEE, and Emax is 126 instead of 127.\nThe main differences between VAX and IEEE are the lack of special values and\ngradual underflow. The VAX has a reserved operand, but it works like a signaling\nNaN: It traps whenever it is referenced. Originally, the VAX\u2019s double precision\n(D format) also had 8 bits of exponent. However, as this is too small for many\napplications, a G format was added; like the IEEE standard, this format has 11 bits\nof exponent. The VAX also has an H format, which is 128 bits long.\nThe IBM 370 floating-point format uses base 16 rather than base 2. This means it\ncannot use a hidden bit. In single precision, it has 7 bits of exponent and 24 bits (6 hex\ndigits) of fraction. Thus, the largest representable number is 1627\u00bc24\u000627\u00bc229,\ncompared with 228 for IEEE. However, a number that is normalized in the hexadec-\nimal sense only needs to havea nonzero leading digit. When interpreted in binary, the\nthree most-significantbitscould bezero.Thus, there are potentiallyfewer than24bits\nof significance. The reason for using the higher base was to minimize the amount of\nshifting required when adding floating-point numbers. However, this is less signifi-\ncant in current machines, where the floating-point add time is usually fixed indepen-\ndently of the operands. Another difference between 370 arithmetic and IEEE\narithmetic is that the 370 has neither a round digit nor a sticky digit, which effectively\nmeans that it truncates rather thanrounds. Thus, in manycomputations, the result will\nsystematically be too small. Unlike the VAX and IEEE arithmetic, every bit pattern is\na valid number. Thus,library routinesmustestablish conventionsfor whattoreturnin\ncase of errors. In the IBM FORTRAN library, for example,\n\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\n4\np\nreturns 2!\nArithmetic on Cray computers is interesting because it is driven by a motiva-\ntion for the highest possible floating-point performance. It has a 15-bit exponent\nfield and a 48-bit fraction field. Addition on Cray computers does not have a guard\ndigit, and multiplication is even less accurate than addition. Thinking of multipli-\ncation as a sum of p numbers, each 2p bits long, Cray computers drop the low-order\nbits of each summand. Thus, analyzing the exact error characteristics of the mul-\ntiply operation is not easy. Reciprocals are computed using iteration, and division\nof a by b is done by multiplying a times 1/b. The errors in multiplication and recip-\nrocation combine to make the last three bits of a divide operation unreliable. At\nleast Cray computers serve to keep numerical analysts on their toes!\nThe IEEE standardization process began in 1977, inspired mainly by W. Kahan\nand based partly on Kahan\u2019s work with the IBM 7094 at the University of Toronto\n[Kahan 1968]. The standardization process was a lengthy affair, with gradual\nunderflow causing the most controversy. (According to Cleve Moler, visitors to\nthe United States were advised that the sights not to be missed were Las Vegas,\nthe Grand Canyon, and the IEEE standards committee meeting.) The standard\nwas finally approved in 1985. The Intel 8087 was the first major commercial IEEE\nimplementation and appeared in 1981, before the standard was finalized. It con-\ntains features that were eliminated in the final standard, such as projective bits.\nAccording to Kahan, the length of double-extended precision was based on what\ncould be implemented in the 8087. Although the IEEE standard was not based on\nany existing floating-point system, most of its features were present in some other\nsystem. For example, the CDC 6600 reserved special bit patterns for INDEFINITE\nJ-64\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1260,
        "text": "and INFINITY, while the idea of denormal numbers appears in Goldberg [1967] as\nwell as in Kahan [1968]. Kahan was awarded the 1989 Turing prize in recognition\nof his work on floating point.\nAlthough floating point rarely attracts the interest of the general press, news-\npapers were filled with stories about floating-point division in November 1994. A\nbug in the division algorithm used on all of Intel\u2019s Pentium chips had just come to\nlight. It was discovered by Thomas Nicely, a math professor at Lynchburg College\nin Virginia. Nicely found the bug when doing calculations involving reciprocals of\nprime numbers. News of Nicely\u2019s discovery first appeared in the press on the front\npage of the November 7 issue of Electronic Engineering Times. Intel\u2019s immediate\nresponse was to stonewall, asserting that the bug would only affect theoretical\nmathematicians. Intel told the press, \u201cThis doesn\u2019t even qualify as an errata \u2026 even\nif you\u2019re an engineer, you\u2019re not going to see this.\u201d\nUnder more pressure, Intel issued a white paper, dated November 30, explain-\ning why they didn\u2019t think the bug was significant. One of their arguments was\nbased on the fact that if you pick two floating-point numbers at random and divide\none into the other, the chance that the resulting quotient will be in error is about 1 in\n9 billion. However, Intel neglected to explain why they thought that the typical\ncustomer accessed floating-point numbers randomly.\nPressure continued to mount on Intel. One sore point was that Intel had known\nabout the bug before Nicely discovered it, but had decided not to make it public.\nFinally, on December 20, Intel announced that they would unconditionally replace\nany Pentium chip that used the faulty algorithm and that they would take an unspe-\ncified charge against earnings, which turned out to be $300 million.\nThe Pentium uses a simple version of SRT division as discussed in Section J.9.\nThe bug was introduced when they converted the quotient lookup table to a PLA.\nEvidently there were a few elements of the table containing the quotient digit 2 that\nIntel thought would never be accessed, and they optimized the PLA design using\nthis assumption. The resulting PLA returned 0 rather than 2 in these situations.\nHowever, those entries were really accessed, and this caused the division bug.\nEven though the effect of the faulty PLA was to cause 5 out of 2048 table entries\nto be wrong, the Pentium only computes an incorrect quotient 1 out of 9 billion\ntimes on random inputs. This is explored in Exercise J.34.\nReferences\nAnderson, S.F., Earle, J.G., Goldschmidt, R.E., Powers, D.M., 1967. The IBM System/360 Model 91:\nFloating-point execution unit. IBM J. Research and Development 11, 34\u201353. Reprinted in\nSwartzlander [1990]. Good description of an early high-performance floating-point unit that used\na pipelined Wallace tree multiplier and iterative division.\nBell, C.G., Newell, A., 1971. Computer Structures: Readings and Examples. McGraw-Hill, New York.\nBirman, M., Samuels, A., Chu, G., Chuk, T., Hu, L., McLeod, J., Barnes, J., 1990. Developing the\nWRL3170/3171 SPARC floating-point coprocessors. IEEE Micro 10 (1), 55\u201364. These chips have\nthe same floating-point core as the Weitek 3364, and this paper has a fairly detailed description of\nthat floating-point design.\nJ.12\nHistorical Perspective and References\n\u25a0\nJ-65"
    },
    {
        "page": 1261,
        "text": "Brent, R.P., Kung, H.T., 1982. A regular layout for parallel adders. IEEE Trans. on Computers\nC-31, 260\u2013264. This is the paper that popularized CLAs in VLSI.\nBurgess, N., Williams, T., 1995. Choices of operand truncation in the SRT division algorithm. IEEE\nTrans. on Computers 44, 7. Analyzes how many bits of divisor and remainder need to be examined\nin SRT division.\nBurks, A.W., Goldstine, H.H., von Neumann, J., 1946. Preliminary discussion of the logical design of\nan electronic computing instrument. In: Aspray, W., Burks, A. (Eds.), Papers of John von Neumann.\nReport to the U.S. Army Ordnance Department, p. 1; also appears. MIT Press, Cambridge, Mass,\npp. 97\u2013146. Tomash Publishers, Los Angeles, 1987.\nCody, W.J., Coonen, J.T., Gay, D.M., Hanson, K., Hough, D., Kahan, W., Karpinski, R., Palmer, J.,\nRis, F.N., Stevenson, D., 1984. A proposed radix- and word-length-independent standard for\nfloating-point arithmetic. IEEE Micro 4 (4), 86\u2013100. Contains a draft of the 854 standard, which\nis more general than 754. The significance of this article is that it contains commentary on the stan-\ndard, most of which is equally relevant to 754. However, be aware that there are some differences\nbetween this draft and the final standard.\nCoonen, J., 1984. Contributions to a proposed standard for binary floating point arithmetic. Ph.D. thesis.\nUniversity of California\u2013Berkeley. The only detailed discussion of how rounding modes can be used\nto implement efficient binary decimal conversion.\nDarley, H.M., et al., 1989. Floating point/integer processor with divide and square root functions. U.S.\nPatent. 4 (878,190) October 31, 1989. Pretty readable as patents go. Gives a high-level view of the TI\n8847 chip, but doesn\u2019t have all the details of the division algorithm.\nDemmel, J.W., Li, X., 1994. Faster numerical algorithms via exception handling. IEEE Trans. on\nComputers 43 (8), 983\u2013992. A good discussion of how the features unique to IEEE floating point\ncan improve the performance of an important software library.\nFreiman, C.V., 1961. Statistical analysis of certain binary division algorithms. Proc. IRE 49 (1), 91\u2013103.\nContains an analysis of the performance of shifting-over-zeros SRT division algorithm.\nGoldberg, D., 1991. What every computer scientist should know about floating-point arithmetic.\nComputing Surveys 23 (1), 5\u201348. Contains an in-depth tutorial on the IEEE standard from the soft-\nware point of view.\nGoldberg, I.B., 1967. 27 bits are not enough for 8-digit accuracy. Comm. ACM 10 (2), 105\u2013106. This\npaper proposes using hidden bits and gradual underflow.\nGosling, J.B., 1980. Design of Arithmetic Units for Digital Computers. Springer-Verlag, New York.\nA concise, well-written book, although it focuses on MSI designs.\nHamacher, V.C., Vranesic, Z.G., Zaky, S.G., 1984. Computer Organization, 2nd ed. McGraw-Hill, New\nYork. Introductory computer architecture book with a good chapter on computer arithmetic.\nHwang, K., 1979. Computer Arithmetic: Principles, Architecture, and Design. Wiley, New York. This\nbook contains the widest range of topics of the computer arithmetic books.\nIEEE, 1985. IEEE standard for binary floating-point arithmetic. SIGPLAN Notices 22 (2), 9\u201325. IEEE\n754 is reprinted here.\nKahan, W., 1968. 7094-II system support for numerical analysis. SHARE Secretarial Distribution. SSD-\n159. This system had many features that were incorporated into the IEEE floating-point standard.\nKahaner, D.K., 1988. Benchmarks for \u2018real\u2019 programs. SIAM News.(November).. The benchmark pre-\nsented in this article turns out to cause many underflows.\nKnuth, D., 1981. 2nd ed. The Art of Computer Programming.Vol. II. Addison-Wesley, Reading, Mass.\nHas a section on the distribution of floating-point numbers.\nKogge, P., 1981. The Architecture of Pipelined Computers. McGraw-Hill, New York. Has a brief dis-\ncussion of pipelined multipliers.\nKohn, L., Fu, S.-W., 1989. A 1,000,000 transistor microprocessor. In: IEEE Int\u2019l. Solid-State Circuits\nConf. Digest of Technical Papers, pp. 54\u201355. There are several articles about the i860, but this one\ncontains the most details about its floating-point algorithms.\nKoren, I., 1989. Computer Arithmetic Algorithms. Prentice Hall, Englewood Cliffs, N.J..\nLeighton, F.T., 1992. Introduction to Parallel Algorithms and Architectures: Arrays. Trees, Hypercubes,\nMorgan Kaufmann, San Francisco. This is an excellent book, with emphasis on the complexity anal-\nysis of algorithms. Section 1.2.1 has a nice discussion of carry-lookahead addition on a tree.\nJ-66\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1262,
        "text": "Magenheimer, D.J., Peters, L., Pettis, K.W., Zuras, D., 1988. Integer multiplication and division on the\nHP Precision architecture. IEEE Trans. on Computers 37 (8), 980\u2013990. Gives rationale for the\ninteger- and divide-step instructions in the Precision architecture.\nMarkstein, P.W., 1990. Computation of elementary functions on the IBM RISC System/6000 processor.\nIBM J. of Research and Development 34 (1), 111\u2013119. Explains how to use fused muliply-add to\ncompute correctly rounded division and square root.\nMead, C., Conway, L., 1980. Introduction to VLSI Systems. Addison-Wesley, Reading, Mass.\nMontoye, R.K., Hokenek, E., Runyon, S.L., 1990. Design of the IBM RISC System/6000 floating-point\nexecution. IBM J. of Research and Development 34 (1), 59\u201370. Describes one implementation of\nfused multiply-add.\nNgai, T.-F., Irwin, M.J., 1985. Regular, area-time efficient carry-lookahead adders. In: Proc. Seventh\nIEEE Symposium on Computer Arithmetic, pp. 9\u201315. Describes a CLA like that of Figure J.17,\nwhere the bits flow up and then come back down.\nPatterson, D.A., Hennessy, J.L., 2009. Computer Organization and Design: The Hardware/Software\nInterface, 4th Edition Morgan Kaufmann, San Francisco. Chapter 3 is a gentler introduction to\nthe first third of this appendix.\nPeng, V., Samudrala, S., Gavrielov, M., 1987. On the implementation of shifters, multipliers, and\ndividers in VLSI floating point units. In: Proc. Eighth IEEE Symposium on Computer Arithmetic,\npp. 95\u2013102. Highly recommended survey of different techniques actually used in VLSI designs.\nRowen, C., Johnson, M., Ries, P., 1988. The MIPS R3010 floating-point coprocessor. IEEE Micro\n53\u201362 (June).\nSantoro, M.R., Bewick, G., Horowitz, M.A., 1989. Rounding algorithms for IEEE multipliers. In: Proc.\nNinth IEEE Symposium on Computer Arithmetic, pp. 176\u2013183. A very readable discussion of how to\nefficiently implement rounding for floating-point multiplication.\nScott, N.R., 1985. Computer Number Systems and Arithmetic. Prentice Hall, Englewood Cliffs, N.J.\nSwartzlander, E. (Ed.), 1990. Computer Arithmetic. IEEE Computer Society Press, Los Alamitos, Calif.\nA collection of historical papers in two volumes.\nTakagi, N., Yasuura, H., Yajima, S., 1985. High-speed VLSI multiplication algorithm with a redundant\nbinary addition tree. IEEE Trans. on Computers C-34 (9), 789\u2013796. A discussion of the binary tree\nsigned multiplier that was the basis for the design used in the TI 8847.\nTaylor, G.S., 1981. Compatible hardware for division and square root. In: Proc. Fifth IEEE Symposium\non Computer Arithmetic, May 18\u201319, 1981. Ann Arbor, Mich, pp. 127\u2013134. Good discussion of a\nradix-4 SRT division algorithm.\nTaylor, G.S., 1985. Radix 16 SRT dividers with overlapped quotient selection stages. In: Proc. Seventh\nIEEE Symposium on Computer Arithmetic, June 4\u20136, 1985, pp. 64\u201371 Urbana, Ill.. Describes a very\nsophisticated high-radix division algorithm.\nWeste, N., Eshraghian, K., 1993. Principles of CMOS VLSI Design: A Systems Perspective, 2nd ed.\nAddison-Wesley, Reading, Mass. This textbook has a section on the layouts of various kinds of\nadders.\nWilliams, T.E., Horowitz, M., Alverson, R.L., Yang, T.S., 1987. A self-timed chip for division.\nIn: Advanced Research in VLSI, Proc. 1987 Stanford Conf. MIT Press, Cambridge, Mass. Describes\na divider that tries to get the speed of a combinational design without using the area that would be\nrequired by one.\nExercises\nJ.1\n[12]<J.2>Using n bits, what is the largest and smallest integer that can be repre-\nsented in the two\u2019s complement system?\nJ.2\n[20/25]<J.2>In the subsection \u201cSigned Numbers\u201d (page J-7), it was stated that\ntwo\u2019s complement overflows when the carry into the high-order bit position is dif-\nferent from the carry-out from that position.\nExercises\n\u25a0\nJ-67"
    },
    {
        "page": 1263,
        "text": "a. [20]<J.2>Give examples of pairs of integers for all four combinations of\ncarry-in and carry-out. Verify the rule stated above.\nb. [25]<J.2>Explain why the rule is always true.\nJ.3\n[12]<J.2>Using 4-bit binary numbers, multiply 8\u00068 using Booth recoding.\nJ.4\n[15]<J.2>Equations J.2.1 and J.2.2 are for adding two n-bit numbers.\nDerive similar equations for subtraction, where there will be a borrow instead\nof a carry.\nJ.5\n[25]<J.2>On a machine that doesn\u2019t detect integer overflow in hardware, show\nhow you would detect overflow on a signed addition operation in software.\nJ.6\n[15/15/20]<J.3>Represent the following numbers as single-precision and\ndouble-precision IEEE floating-point numbers:\na. [15]<J.3>10.\nb. [15]<J.3>10.5.\nc. [20]<J.3>0.1.\nJ.7\n[12/12/12/12/12]<J.3>Below is a list of floating-point numbers. In single preci-\nsion, write down each number in binary, in decimal, and give its representation in\nIEEE arithmetic.\na. [12]<J.3>The largest number less than 1.\nb. [12]<J.3>The largest number.\nc. [12]<J.3>The smallest positive normalized number.\nd. [12]<J.3>The largest denormal number.\ne. [12]<J.3>The smallest positive number.\nJ.8\n[15]<J.3>Is the ordering of nonnegative floating-point numbers the same as inte-\ngers when denormalized numbers are also considered?\nJ.9\n[20]<J.3>Write a program that prints out the bit patterns used to represent\nfloating-point numbers on your favorite computer. What bit pattern is used\nfor NaN?\nJ.10\n[15]<J.4>Using p\u00bc4, show how the binary floating-point multiply algorithm\ncomputes the product of 1.875\u00061.875.\nJ.11\n[12/10]<J.4>Concerning the addition of exponents in floating-point multiply:\na. [12]<J.4>What would the hardware that implements the addition of expo-\nnents look like?\nb. [10]<J.4>If the bias in single precision were 129 instead of 127, would addi-\ntion be harder or easier to implement?\nJ.12\n[15/12]<J.4>In the discussion of overflow detection for floating-point multipli-\ncation, it was stated that (for single precision) you can detect an overflowed expo-\nnent by performing exponent addition in a 9-bit adder.\nJ-68\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1264,
        "text": "a. [15]<J.4>Give the exact rule for detecting overflow.\nb. [12]<J.4>Would overflow detection be any easier if you used a 10-bit adder\ninstead?\nJ.13\n[15/10]<J.4>Floating-point multiplication:\na. [15]<J.4>Construct two single-precision floating-point numbers whose prod-\nuct doesn\u2019t overflow until the final rounding step.\nb. [10]<J.4>Is there any rounding mode where this phenomenon cannot occur?\nJ.14\n[15]<J.4>Give an example of a product with a denormal operand but a normal-\nized output. How large was the final shifting step? What is the maximum possible\nshift that can occur when the inputs are double-precision numbers?\nJ.15\n[15]<J.5>Use the floating-point addition algorithm on page J-23 to compute\n1.0102.10012 (in 4-bit precision).\nJ.16\n[10/15/20/20/20]<J.5>In certain situations, you can be sure that a+b is exactly\nrepresentable as a floating-point number, that is, no rounding is necessary.\na. [10]<J.5>If a, b have the same exponent and different signs, explain why a\n+b is exact. This was used in the subsection \u201cSpeeding Up Addition\u201d on page\nJ-25.\nb. [15]<J.5>Give an example where the exponents differ by 1, a and b have dif-\nferent signs, and a+b is not exact.\nc. [20]<J.5>If a\u0003b\u00030, and the top two bits of a cancel when computing ab,\nexplain why the result is exact (this fact is mentioned on page J-22).\nd. [20]<J.5>If a\u0003b\u00030, and the exponents differ by 1, show that ab is exact\nunless the high order bit of ab is in the same position as that of a (mentioned\nin \u201cSpeeding Up Addition,\u201d page J-25).\ne. [20]<J.5>If the result of ab or a+b is denormal, show that the result is\nexact (mentioned in the subsection \u201cUnderflow,\u201d on page J-36).\nJ.17\n[15/20]<J.5>Fast floating-point addition (using parallel adders) for p\u00bc5.\na. [15]<J.5>Step through the fast addition algorithm for a+b, where\na\u00bc1.01112 and b\u00bc.110112.\nb. [20]<J.5>Suppose the rounding mode is toward+\u221e. What complication\narises in the above example for the adder that assumes a carry-out? Suggest\na solution.\nJ.18\n[12]<J.4, J.5>How would you use two parallel adders to avoid the final round-up\naddition in floating-point multiplication?\nJ.19\n[30/10]<J.5>This problem presents a way to reduce the number of addition steps\nin floating-point addition from three to two using only a single adder.\na. [30]<J.5>Let A and B be integers of opposite signs, with a and b their mag-\nnitudes. Show that the following rules for manipulating the unsigned numbers a\nand b gives A+B.\nExercises\n\u25a0\nJ-69"
    },
    {
        "page": 1265,
        "text": "1. Complement one of the operands.\n2. Use end-around carry to add the complemented operand and the other\n(uncomplemented) one.\n3. If there was a carry-out, the sign of the result is the sign associated with the\nuncomplemented operand.\n4. Otherwise, if there was no carry-out, complement the result, and give it the\nsign of the complemented operand.\nb. [10]<J.5>Usetheabovetoshowhow steps 2and4inthefloating-pointaddi-\ntion algorithm on page J-23 can be performed using only a single addition.\nJ.20\n[20/15/20/15/20/15]<J.6>Iterative square root.\na. [20]<J.6>Use Newton\u2019s method to derive an iterative algorithm for square\nroot. The formula will involve a division.\nb. [15]<J.6>What is the fastest way you can think of to divide a floating-point\nnumber by 2?\nc. [20]<J.6>If division is slow, then the iterative square root routine will also be\nslow. Use Newton\u2019s method on f(x)\u00bc1/x2a to derive a method that doesn\u2019t\nuse any divisions.\nd. [15]<J.6>Assume that the ratio division by 2 : floating-point add : floating-\npoint multiply is 1:2:4. What ratios of multiplication time to divide time makes\neach iteration step in the method of part (c) faster than each iteration in the\nmethod of part (a)?\ne. [20]<J.6>When using the method of part (a), how many bits need to be in the\ninitial guess in order to get double-precision accuracy after three iterations?\n(You may ignore rounding error.)\nf. [15]<J.6>Suppose that when spice runs on the TI 8847, it spends 16.7% of its\ntime in the square root routine (this percentage has been measured on other\nmachines). Using the values in Figure J.36 and assuming three iterations,\nhow much slower would spice run if square root were implemented in software\nusing the method of part(a)?\nJ.21\n[10/20/15/15/15]<J.6>Correctly rounded iterative division. Let a and b be\nfloating-point numbers with p-bit significands (p\u00bc53 in double precision). Let\nq be the exact quotient q\u00bca/b, 1\u0004q<2. Suppose that q is the result of an iteration\nprocess, that q has a few extra bits of precision, and that 0 < qq < 2p. For the\nfollowing, it is important that q < q, even when q can be exactly represented as a\nfloating-point number.\na. [10]<J.6>If x is a floating-point number, and 1\u0004x<2, what is the next rep-\nresentable number after x?\nb. [20]<J.6>Show how to compute q0 from q, where q0 has p+1 bits of precision\nand jqq0j<2p.\nc. [15]<J.6>Assuming round to nearest, show that the correctly rounded quo-\ntient is either q0, q0 2p, or q0 +2p.\nJ-70\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1266,
        "text": "d. [15]<J.6>Give rules for computing the correctly rounded quotient from q0\nbased on the low-order bit of q0 and the sign of abq0.\ne. [15]<J.6>Solve part (c) for the other three rounding modes.\nJ.22\n[15]<J.6>Verify the formula on page J-30. (Hint: If xn \u00bc x0 2x0b\n\u00f0\n\u00de\u0006\n\u03a0i\u00bc1,n 1 + 1x0b\n\u00f0\n\u00de2i\nh\ni\n,\nthen\n2xnb \u00bc 2x0b 2x0b\n\u00f0\n\u00de\u03a0 1 + 1x0b\n\u00f0\n\u00de2i\nh\ni\n\u00bc\n2 1 1x0b\n\u00f0\n\u00de2\nh\ni\n\u03a0 1 + 1x0b\n\u00f0\n\u00de2i\nh\ni\n.)\nJ.23\n[15]<J.7>Our example that showed that double rounding can give a different\nanswer from rounding once used the round-to-even rule. If halfway cases are\nalways rounded up, is double rounding still dangerous?\nJ.24\n[10/10/20/20]<J.7>Some of the cases of the italicized statement in the \u201cPreci-\nsions\u201d subsection (page J-33) aren\u2019t hard to demonstrate.\na. [10]<J.7>What form must a binary number have if rounding to q bits followed\nby rounding to p bits gives a different answer than rounding directly to p bits?\nb. [10]<J.7>Show that for multiplication of p-bit numbers, rounding to q bits\nfollowed by rounding to p bits is the same as rounding immediately to p bits\nif q\u00032p.\nc. [20]<J.7>If a and b are p-bit numbers with the same sign, show that rounding\na+b to q bits followed by rounding to p bits is the same as rounding immedi-\nately to p bits if q\u00032p+1.\nd. [20]<J.7>Do part (c) when a and b have opposite signs.\nJ.25\n[Discussion]<J.7>In the MIPS approach to exception handling, you need a test\nfor determining whether two floating-point operands could cause an exception.\nThis should be fast and also not have too many false positives. Can you come\nup with a practical test? The performance cost of your design will depend on\nthe distribution of floating-point numbers. This is discussed in Knuth [1981]\nand the Hamming paper in Swartzlander [1990].\nJ.26\n[12/12/10]<J.8>Carry-skip adders.\na. [12]<J.8>Assuming that time is proportional to logic levels, how long does it\ntake an n-bit adder divided into (fixed) blocks of length k bits to perform an\naddition?\nb. [12]<J.8>What value of k gives the fastest adder?\nc. [10]<J.8>Explain why the carry-skip adder takes time 0\n\ufb03\ufb03\ufb03n\np\n\u00f0\n\u00de.\nJ.27\n[10/15/20]<J.8>Complete the details of the block diagrams for the following\nadders.\na. [10]<J.8>In Figure J.15, show how to implement the \u201c1\u201d and \u201c2\u201d boxes in\nterms of AND and OR gates.\nb. [15]<J.8>In Figure J.19, what signals need to flow from the adder cells in the\ntop row into the \u201cC\u201d cells? Write the logic equations for the \u201cC\u201d box.\nc. [20]<J.8>Show how to extend the block diagram in J.17 so it will produce the\ncarry-out bit c8.\nExercises\n\u25a0\nJ-71"
    },
    {
        "page": 1267,
        "text": "J.28\n[15]<J.9>For ordinary Booth recoding, the multiple of b used in the ith step\nis simply ai1ai. Can you find a similar formula for radix-4 Booth recoding\n(overlapped triplets)?\nJ.29\n[20]<J.9>Expand Figure J.29 in the fashion of J.27, showing the individual\nadders.\nJ.30\n[25]<J.9>Write out the analog of Figure J.25 for radix-8 Booth recoding.\nJ.31\n[18]<J.9>Suppose that an1 \u2026 a1a0 and bn1 \u2026 b1b0 are being added in a\nsigned-digit adder as illustrated in the example on page J-53. Write a formula\nfor the ith bit of the sum, si, in terms of ai, ai1, ai2, bi, bi1, and bi2.\nJ.32\n[15]<J.9>The text discussed radix-4 SRT division with quotient digits of 2,\n1, 0, 1, 2. Suppose that 3 and 3 are also allowed as quotient digits. What relation\nreplaces jrij\u00042b/3?\nJ.33\n[25/20/30]<J.9>Concerning the SRT division table, Figure J.34:\na. [25]<J.9>Write a program to generate the results of Figure J.34.\nb. [20]<J.9>Note that Figure J.34 has a certain symmetry with respect to pos-\nitive and negative values of P. Can you find a way to exploit the symmetry and\nonly store the values for positive P?\nc. [30]<J.9>Suppose a carry-save adder is used instead of a propagate adder.\nThe input to the quotient lookup table will be k bits of divisor and l bits of\nremainder, where the remainder bits are computed by summing the top l bits\nof the sum and carry registers. What are k and l? Write a program to generate\nthe analog of Figure J.34.\nJ.34\n[12/12/12]<J.9, J.12>The first several million Pentium chips produced had a\nflaw that caused division to sometimes return the wrong result. The Pentium uses\na radix-4 SRT algorithm similar to the one illustrated in the example on page J-56\n(but with the remainder stored in carry-save format; see Exercise J.33(c)). Accord-\ning to Intel, the bug was due to five incorrect entries in the quotient lookup table.\na. [12]<J.9, J.12>The bad entries should have had a quotient of plus or minus 2,\nbut instead had a quotient of 0. Because of redundancy, it\u2019s conceivable that the\nalgorithm could \u201crecover\u201d from a bad quotient digit on later iterations. Show\nthat this is not possible for the Pentium flaw.\nb. [12]<J.9, J.12>Since the operation is a floating-point divide rather than an\ninteger divide, the SRT division algorithm on page J-45 must be modified in\ntwo ways. First, step 1 is no longer needed, since the divisor is already normal-\nized. Second, the very first remainder may not satisfy the proper bound\n(jrj\u00042b/3 for Pentium; see page J-55). Show that skipping the very first left\nshift in step 2(a) of the SRT algorithm will solve this problem.\nc. [12]<J.9, J.12>If the faulty table entries were indexed by a remainder that\ncould occur at the very first divide step (when the remainder is the divisor), ran-\ndom testing would quickly reveal the bug. This didn\u2019t happen. What does that\ntell you about the remainder values that index the faulty entries?\nJ-72\n\u25a0\nAppendix J Computer Arithmetic"
    },
    {
        "page": 1268,
        "text": "J.35\n[12]<J.6, J.9>The discussion of the remainder-step instruction assumed that\ndivision was done using a bit-at-a-time algorithm. What would have to change\nif division were implemented using a higher-radix method?\nJ.36\n[25]<J.9>In the array of Figure J.28, the fact that an array can be pipelined is not\nexploited. Can you come up with a design that feeds the output of the bottom CSA\ninto the bottom CSAs instead of the top one, and that will run faster than the\narrangement of Figure J.28?\nExercises\n\u25a0\nJ-73"
    },
    {
        "page": 1269,
        "text": "K.1\nIntroduction\nK-2\nK.2\nA Survey of RISC Architectures for Desktop, Server, and Embedded\nComputers\nK-3\nK.3\nThe Intel 80x86\nK-30\nK.4\nThe VAX Architecture\nK-50\nK.5\nThe IBM 360/370 Architecture for Mainframe Computers\nK-69\nK.6\nHistorical Perspective and References\nK-75"
    },
    {
        "page": 1270,
        "text": "K\nSurvey of Instruction\nSet Architectures\nRISC: any computer announced after 1985.\nSteven Przybylski\nA Designer of the Stanford MIPS"
    },
    {
        "page": 1271,
        "text": "K.1\nIntroduction\nThis appendix covers 10 instruction set architectures, some of which remain a vital\npart of the IT industry and some of which have retired to greener pastures. We keep\nthem all in part to show the changes in fashion of instruction set architecture\nover time.\nWe start with eight RISC architectures, using RISC V as our basis for compar-\nison. There are billions of dollars of computers shipped each year for ARM (includ-\ning Thumb-2), MIPS (including microMIPS), Power, and SPARC. ARM\ndominates in both the PMD (including both smart phones and tablets) and the\nembedded markets.\nThe 80x86 remains the highest dollar-volume ISA, dominating the desktop and\nthe much of the server market. The 80x86 did not get traction in either the embed-\nded or PMD markets, and has started to lose ground in the server market. It has\nbeen extended more than any other ISA in this book, and there are no plans to stop\nit soon. Now that it has made the transition to 64-bit addressing, we expect this\narchitecture to be around, although it may play a smaller role in the future then\nit did in the past 30 years.\nThe VAX typifies an ISA where the emphasis was on code size and offering a\nhigher level machine language in the hopes of being a better match to programming\nlanguages. The architects clearly expected it to be implemented with large amounts\nof microcode, which made single chip and pipelined implementations more chal-\nlenging. Its successor was the Alpha, a RISC architecture similar to MIPS and\nRISC V, but which had a short life.\nThe vulnerable IBM 360/370 remains a classic that set the standard for many\ninstruction sets to follow. Among the decisions the architects made in the early\n1960s were:\n\u25a0\n8-bit byte\n\u25a0\nByte addressing\n\u25a0\n32-bit words\n\u25a0\n32-bit single precision floating-point format + 64-bit double precision floating-\npoint format\n\u25a0\n32-bit general-purpose registers, separate 64-bit floating-point registers\n\u25a0\nBinary compatibility across a family of computers with different cost-\nperformance\n\u25a0\nSeparation of architecture from implementation\nAs mentioned in Chapter 2, the IBM 370 was extended to be virtualizable, so it\nhad the lowest overhead for a virtual machine of any ISA. The IBM 360/370\nremains the foundation of the IBM mainframe business in a version that has\nextended to 64 bits.\nK-2\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1272,
        "text": "K.2\nA Survey of RISC Architectures for Desktop, Server,\nand Embedded Computers\nIntroduction\nWe cover two groups of Reduced Instruction Set Computer (RISC) architectures in\nthis section. The first group is the desktop, server RISCs, and PMD processors:\n\u25a0\nAdvanced RISC Machines ARMv8, AArch64, the 64-bit ISA,\n\u25a0\nMIPS64, version 6, the most recent the 64-bit ISA,\n\u25a0\nPower version 3.0, which merges the earlier IBM Power architecture and the\nPowerPC architecture.\n\u25a0\nRISC-V, specifically RV64G, the 64-bit extension of RISC-V.\n\u25a0\nSPARCv9, the 64-bit ISA.\nAs Figure K.1 shows these architectures are remarkably similar.\nThere are two other important historical RISC processors that are almost iden-\ntical to those in the list above: the DEC Alpha processor, which was made by Dig-\nital Equipment Corporation from 1992 to 2004 and is almost identical to MIPS64.\nHewlett-Packard\u2019s PA-RISC was produced by HP from about 1986 to 2005, when\nit was replaced by Itanium. PA-RISC is most closely related to the Power ISA,\nwhich emerged from the IBM Power design, itself a descendant of IBM 801.\nThe second group is the embedded RISCs designed for lower-end applications:\n\u25a0\nAdvanced RISC Machines, Thumb-2: an 32-bit instruction set with 16-bit and\n32-bit instructions. The architecture includes features from both ARMv7\nand ARMv8.\n\u25a0\nmicroMIPS64:\na\nversion\nof\nthe\nMIPS64\ninstruction\nset\nwith\n16-it\ninstructions, and\n\u25a0\nRISC-V Compressed extension (RV64GC), a set of 16-bit instructions added\nto RV64G\nBoth RV64GC and microMIPS64 have corresponding 32-bit versions: RV32GC\nand microMIPS32.\nSince the comparison of the base 32-bit or 64-bit desktop and server architec-\nture will examine the differences among those ISAs, our discussion of the embed-\nded architectures focuses on the 16-bit instructions. Figure K.2 shows that these\nembedded architectures are also similar. In all three, the 16-bit instructions are ver-\nsions of 32-bit instructions, typically with a restricted set of registers.The idea is to\nreduce the code size by replacing common 32-bit instructions with 16-bit versions.\nFor RV32GC or Thumb-2, including the 16-bit instructions yields a reduction in\ncode size to about 0.73 of the code size using only the 32-bit ISA (either RV32G\nor ARMv7).\nK.2\nA Survey of RISC Architectures for Desktop, Server, and Embedded Computers\n\u25a0\nK-3"
    },
    {
        "page": 1273,
        "text": "Figure K.1 Summary of the most recent version of five architectures for desktop, server, and PMD use (all had\nearlier versions). Except for the number of data address modes and some instruction set details, the integer instruc-\ntion sets of these architectures are very similar. Contrast this with Figure K.29. In ARMv8, register 31 is a 0 (like register\n0 in the other architectures), but when it is used in a load or store, it is the current stack pointer, a special purpose\nregister. We can either think of SP-based addressing as a different mode (which is how the assembly mnemonics\noperate) or as simply a register + offset addressing mode (which is how the instruction is encoded).\nFigure K.2 Summary of three recent architectures for embedded applications. All three use 16-bit extensions of a\nbase instruction set. Except for number of data address modes and a number of instruction set details, the integer\ninstruction sets of these architectures are similar. Contrast this with Figure K.29. An earlier 16-bit version of the MIPS\ninstruction set, called MIPS16, was created in 1995 and was replaced by microMIPS32 and microMIPS64. The first\nThumb architecture had only 16-bit instructions and was created in 1996. Thumb-2 is built primarily on ARMv7,\nthe 32-bit ARM instruction set; it offers 16 registers. RISC-V also defines RV32E, which has only 16 registers, includes\nthe 16-bit instructions, and cannot have floating point. It appears that most implementations for embedded appli-\ncations opt for RV32C or RV64GC."
    },
    {
        "page": 1274,
        "text": "A key difference among these three architectures is the structure of the base\n32-bit ISA. In the case of RV64GC, the 32-bit instructions are exactly those of\nRV64G. This is possible because RISC V planned for the 16-it option from the\nbeginning, and branch addresses and jump addresses are specified to 16-it\nboundaries. In the case of microMIPS64, the base ISA is MIPS64, with one\nchange: branch and jump offsets are interpreted as 16-bit rather than 32-bit\naligned. (microMIPS also uses the encoding space that was reserved in MIPS64\nfor user-defined instruction set extensions; such extensions are not part of the\nbase ISA.)\nThumb-2 uses a slightly different approach. The 32-bit instructions in Thumb-\n2 are mostly a subset of those in ARMv7; certain features that were dropped in\nARMv8 are not included (e.g., conditional execution of most instructions and\nthe ability to write the PC as a GPR). Thumb-2 also includes a few dozen instruc-\ntions introduced in ARMv8, specifically bit field manipulation, additional system\ninstructions, and synchronization support. Thus, the 32-bit instructions in Thumb-\n2 constitute a unique ISA.\nEarlier versions of the 16-bit instruction sets for MIPS (MIPS16) and ARM\n(Thumb), took the approach of creating a separate mode, invoked by a procedure\ncall, to transfer control to a code segment that employed only 16-bit instructions.\nThe 16-bit instruction set was not complete and was only intended for user pro-\ngrams that were code-size critical.\nOne complication of this description is that some of the older RISCs have been\nextended over the years. We decided to describe the most recent versions of the\narchitectures: ARMv8 (the 64-bit architecture AArch64), MIPS64 R6, Power\nv3.0, RV64G, and SPARC v9 for the desktop/server/PMD, and the 16-bit subset\nof the ISAs for microMIPS64, RV64GC, and Thumb-2.\nThe remaining sections proceed as follows. After discussing the addressing\nmodes and instruction formats of our RISC architectures, we present the survey\nof the instructions in five steps:\n\u25a0\nInstructions found in the RV64G core, described in Appendix A.\n\u25a0\nInstructions not found in the RV64G or RV64GC but found in two or more of\nthe other architectures. We describe and organize these by functionality, e.g.\ninstructions that support extended integer arithmetic.\n\u25a0\nInstruction groups unique to ARM, MIPS, Power, or SPARC, organized by\nfunction.\n\u25a0\nMultimedia extensions of the desktop/server/PMD RISCs\n\u25a0\nDigital signal-processing extensions of the embedded RISCs\nAlthough the majority of the instructions in these architectures are included, we\nhave not included every single instruction; this is especially true for the Power\nand ARM ISAs, which have many instructions.\nK.2\nA Survey of RISC Architectures for Desktop, Server, and Embedded Computers\n\u25a0\nK-5"
    },
    {
        "page": 1275,
        "text": "Addressing Modes and Instruction Formats\nFigure K.3 shows the data addressing modes supported by the desktop/server/\nPMD architectures. Since all, but ARM, have one register that always has the value\n0 when used in address modes, the absolute address mode with limited range can be\nsynthesized using register 0 as the base in displacement addressing. (This register\ncan be changed by arithmetic-logical unit (ALU) operations in PowerPC, but is\nalways zero when it is used in an address calculation.) Similarly, register indirect\naddressing is synthesized by using displacement addressing with an offset of 0.\nSimplified addressing modes is one distinguishing feature of RISC architectures.\nAs Figure K.4 shows, the embedded architectures restrict the registers that\ncan be accessed with the 16-bit instructions, typically to only 8 registers, for most\ninstructions, and a few special instructions that refer to other registers. Figure K.5\nshows the data addressing modes supported by the embedded architectures in their\n16-bit instruction mode. These versions of load/store instructions restrict the reg-\nisters that can be used in address calculations, as well as significantly shorten the\nimmediate fields, used for displacements.\nReferences to code are normally PC-relative, although jump register indirect is\nsupported for returning from procedures, for case statements, and for pointer func-\ntion calls. One variation is that PC-relative branch addresses are often shifted left 2\nbits before being added to the PC for the desktop RISCs, thereby increasing the\nbranch distance. This works because the length of all instructions for the desktop\nFigure K.3 Summary of data addressing modes supported by the desktop architectures, where B, H, W, D indi-\ncate what datatypes can use the addressing mode. Note that ARM includes two different types of address modes\nwith updates, one of which is included in Power.\nK-6\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1276,
        "text": "RISCs is 32 bits and instructions must be aligned on 32-bit words in memory.\nEmbedded architectures and RISC V (when extended) have 16-bit-long instruc-\ntions and usually shift the PC-relative address by 1 for similar reasons.\nFigure K.6 shows the most important instruction formats of the desktop/server/\nPMD RISC instructions. Each instruction set architecture uses four primary\ninstruction formats, which typically include 90\u201398% of the instructions. The\nregister-register format is used for register-register ALU instructions, while the\nALU immediate format is used for ALU instructions with an immediate operand\nand also for loads and stores. The branch format is used for conditional branches,\nand the jump/call format for unconditional branches (jumps) and procedures calls.\nThere are a number of less frequently used instruction formats that Figure K.6\nleaves out. Figure K.7 summarizes these for the desktop/server/PMD architectures.\nUnlike, their 32-bit base architectures, the 16-bit extensions (microMIPS64,\nRV64GC, and Thumb-2) are focused on minimizing code. As a result, there are\na larger number of instruction formats, even though there are far fewer instructions.\nFigure K.4 Register encodings for the 16-bit subsets of microMIPS64, RV64GC, and Thumb-2, including the core\ngeneral purpose registers, and special-purpose registers accessible by some instructions.\nFigure K.5 Summary of data addressing modes supported by the embedded architectures. microMIPS64, RV64c,\nand Thumb-2 show only the modes supported in 16-bit instruction formats. The stack pointer in RV64GC and micro-\nMIPS64 is a designed GPR; it is another version of r31 is Thumb-2. In microMIPS64, the global pointer is register 30 and\nis used by the linkage convention to point to the global variable data pool. Notice that typically only 8 registers are\naccessible as base registers (and as we will see as ALU sources and destinations).\nK.2\nA Survey of RISC Architectures for Desktop, Server, and Embedded Computers\n\u25a0\nK-7"
    },
    {
        "page": 1277,
        "text": "Opcode\nRegister\nConstant\nARM\nMIPS\nPower\nRISC-V\nSPARC\nARM\nMIPS\nPower\nRISC-V\nSPARC\nArm\nMIPS\nPower\nRISC-V \nSPARC\nARM\nMIPS\nPower\nRISC-V\nSPARC\nRegister-register\nRegister-immediate\nBranch\nJump/call\nOp6\n31\n25\n20\n15\n10\n4\n0\n0\n5\n1\n0\n2\n5\n2\n1\n3\n31\n25\n31\n25\n20\n0\n5\n1\n0\n2\n0\nOpx5\nRs15\nRd5\nRs15\nConst5\nConst21\nRs15\nOpx5/Rs25\nConst16\nOpx6\nOpx3\nOpx11\nRs15\nConst14\nOpx22\nOpx2\nRs25\nRs15\nConst19\nConst26\nConst24\nConst21\nConst30\nConst16\nConst16\nRs15\nRd5\nRd5\nRs15\nRs15\nRd5\nRd5\nOpx6\nRs15\n1\nConst13\nRs15\nRs25\nRd5\nRd5\nRs15\nRs15\nRs25\nOpx11\nOpx6\nOpx3\nRs25\nRs15\nOpx8\n0\nRd5\nOpx6\nRs25\nOp6\nOp6\nOp6\nOp6\nOp6\nOp2\nOp2\nOp2\nOp2\nOp6\nOp6\nOp6\nOp6\nOp6\nOp6\n31\n29\n24\n18\n13 12\n4\n0\n0\n2\n1\n3\n1\n8\n1\n4\n2\n9\n2\n1\n3\n0\n1\n2\n1\n8\n1\n9\n2\n1\n3\n0\n1\n2\n1\n5\n1\n0\n2\n9\n2\n1\n3\nRd5\nConst26\nRs25\nConst5\nOp7\nOpx2/Const7\nRd5\nConst12\nOpx4\nOp7\nOpx3\nConst12\nRs15\nConst7\nOp7\nConst5\nRd5\nOp7\nFigure K.6 Instruction formats for desktop/server RISC architectures. These four formats are found in all five archi-\ntectures. (The superscript notation in this figure means the width of a field in bits.) Although the register fields are\nlocated in similar pieces of the instruction, be aware that the destination and two source fields are sometimes scram-\nbled. Op \u00bc the main opcode, Opx \u00bc an opcode extension, Rd \u00bc the destination register, Rs1 \u00bc source register 1, Rs2\n\u00bc source register 2, and Const \u00bc a constant (used as an immediate, address, mask, or sift amount). Although the labels\non the instruction formats tell where various instructions are encoded, there are variations. For example, loads and\nstores, both use the ALU immediate form in MIPS. In RISC-V, loads use the ALU immediate format, while stores use the\nbranch format.\nK-8\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1278,
        "text": "microMIPs64 and RV64GC have eight and seven major formats, respectively, and\nThumb-2 has 15. As Figure K.8 shows, these involve varying number of register\noperands (0 to 3), different immediate sizes, and even different size register spec-\nifiers, with a small number of registers accessible my most instructions, and fewer\ninstructions able to access all 32 registers.\nInstructions\nThe similarities of each architecture allow simultaneous descriptions, starting with\nthe operations equivalent to the RISC-V 64-bit ISA.\nFigure K.7 Other instruction formats beyond the four major formats of the previous figure. In some cases, there\nare formats very similar to one of the four core formats, but where a register field is used for other purposes. The\nPower architecture also includes a number of formats for vector operations.\nK.2\nA Survey of RISC Architectures for Desktop, Server, and Embedded Computers\n\u25a0\nK-9"
    },
    {
        "page": 1279,
        "text": "Architecture\nOpcode \nmain: \nextended\nRegister \nspeci\ufb01ers x \nlength\nImmediate \n\ufb01eld \nlength\nTypical instructions\nmicroMIPS64\n6\nnone\n10\nJumps\n6\n1x5\n5\nRegister-register operation (32 registers) and Load using SP \nas base register; any destination\n6\n1x3\n7\nBranches equal/not equal zero. Loads using GP. as base.\n,n\noitare\np\no\nretsig\ner-retsig\ne\nR\n3\nx\n2\n4:6\n rd/rs1, and rs2; 8 registers\n6:1\n2x3\n3\nRegister-register immediate, rd/rs1, and rs2; 8 registers\n6\n2x3\n4\nLoads and stores; 8 registers\nitare\np\no\nretsig\ner-retsig\ne\nR\n3\nx\n2\n4:6\non, rd, and rs1; 8 registers\nretsig\ner-retsig\ne\nR\n5\nx\n2\n6\noperation; 32 registers.\nRV64GC\n2:3\n11\nJumps\n2:3\n1x3\n7\nBranch\n2:3\n1x3\n8\nImmediate one source register.\n2:3\n1x5\n6\nStore using SP as base.\n2:3\n1x5\n6\nALU immediate and load using SP as base.\nn\noitare\np\no\nretsig\ner-retsig\ne\nR\n5\nx\n2\n4:2\n2:3\n2x3\n5\nLoads and stores using 8 registers. \nThumb-2\n3:2\n2x3\n5\nShift, move, load/store word/byte\n3:2\n1x3\n8\nimmediates: add, subtract, move, and compare\n4:1\n1x3\n8\nLoad/store with stack pointer as base, Add to SP or PC, \nLoad/store multiple\nd\ne\nx\ne\nd\nni\nretsig\ner\nd\na\no\nL\n3\nx\n3\n3:4\n4:4\n8\nConditional branch, system instruction\ntn\nereffid\n2\n2\n:s\nu\no\ne\nn\nalle\nc\nsi\nM\n2\n1:4\ninstructions with 12 formats \n(includes compare and branch on zero, pop/push registers, \nadjust stack pointer, reverse bytes, IF-THEN instruction). \n5\n1x3\n8\nLoad relative to PC\nh\nc\nn\nar\nb\nla\nn\noitid\nn\no\nc\nn\nU\n1\n1\n5\ntc\nartb\nu\ns/d\nd\nA\n3\nx\n3\n1:6\n6:3\n1x4, 1x3\nSpecial data processing\ng\nniss\ne\nc\no\nr\np\nata\nd\nla\ncig\no\nL\n3\nx\n2\n4:6\nts\nni\ne\ng\nn\na\nh\nc\nd\nn\na\nh\nc\nn\nar\nB\n4\nx\n1\n6:6\nruction set (ARM vs. Thumb)\nFigure K.8 Instruction formats for the 16-bit instructions of microMIPS64, RV64GC, and Thumb-2. For instructions\nwith a destination and two sources, but only two register fields, the instruction uses one of the registers as both\nsource and destination. Note that the extended opcode field (or function field) and immediate field sometimes over-\nlap or are identical. For RV64GC and microMIPS64, all the formats are shown; for Thumb-2, the Miscellaneous format\nincludes 22 instructions with 12 slightly different formats; we use the extended opcode field, but a few of these\ninstructions have immediate or register fields.\nK-10\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1280,
        "text": "RV64G Core Instructions\nAlmost every instruction found in the RV64G is found in the other architectures, as\nFigures K.9 through K.19 show. (For reference, definitions of the RISC-V instruc-\ntions are found in Section A.9.) Instructions are listed under four categories: data\ntransfer (Figure K.9); arithmetic, logical (Figure K.10); control (Figure K.11 and\nFigure K.12); and floating point (Figure K.13).\nIf a RV64G core instruction requires a short sequence of instructions in other\narchitectures, these instructions are separated by semicolons in Figure K.9 through\nFigure K.13. (To avoid confusion, the destination register will always be the left-\nmost operand in this appendix, independent of the notation normally used with\neach architecture.).\nCompare and Conditional Branch\nEvery architecture must have a scheme for compare and conditional branch, but\ndespite all the similarities, each of these architectures has found a different way\nto perform the operation! Figure K.11 summarizes the control instructions, while\nFigure K.12 shows details of how conditional branches are handled. SPARC uses\nthe traditional four condition code bits stored in the program status word: negative,\nzero, carry, and overflow. They can be set on any arithmetic or logical instruction;\nunlike earlier architectures, this setting is optional on each instruction. An explicit\noption leads to fewer problems in pipelined implementation. Although condition\ncodes can be set as a side effect of an operation, explicit compares are synthesized\nwith a subtract using r0 as the destination. SPARC conditional branches test con-\ndition codes to determine all possible unsigned and signed relations. Floating point\nuses separate condition codes to encode the EEE 754 conditions, requiring a\nfloating-point compare instruction. Version 9 expanded SPARC branches in four\nways: a separate set of condition codes for 64-bit operations; a branch that tests the\ncontents of a register and branches if the value is =, not=, <, <=, >=, or\n<= 0; three more sets of floating-point condition codes; and branch instructions\nthat encode static branch prediction.\nPower also uses four condition codes: less than, greater than, equal, and sum-\nmary overflow, but it has eight copies of them. This redundancy allows the Power\ninstructions to use different condition codes without conflict, essentially giving\nPower eight extra 4-bit registers. Any of these eight condition codes can be the target\nof a compare instruction, and any can be the source of a conditional branch. The\ninteger instructions have an option bit that behaves as if the integer is followed\nby a compare to zero that sets the first condition \u201cregister.\u201d Power also lets the second\n\u201cregister\u201d be optionally set by floating-point instructions. PowerPC provides logical\noperations among these eight 4-bit condition code registers (CRAND, CROR,\nCRXOR, CRNAND, CRNOR, CREQV), allowing more complex conditions to be\ntested by a single branch. Finally, Power includes a set of branch count registers,\nthat are automatically decremented when tested, and can be used in a branch con-\ndition. There are also special instructions for moving from/to the condition register.\nK.2\nA Survey of RISC Architectures for Desktop, Server, and Embedded Computers\n\u25a0\nK-11"
    },
    {
        "page": 1281,
        "text": "Figure K.9 Desktop RISC data transfer instructions equivalent to RV64G core. A sequence of instructions to syn-\nthesize a RV64G instruction is shown separated by semicolons. The MIPS and Power instructions for atomic opera-\ntions load and conditionally store a pair of registers and can be used to implement the RV64G atomic operations with\nat most one intervening ALU instruction. The SPARC instructions: compare-and-swap, swap, LDSTUB provide atomic\nupdates to a memory location and can be used to build the RV64G instructions. The Power3 instructions provide all\nthe functionality, as the RV64G instructions, depending on a function field.\nK-12\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1282,
        "text": "Figure K.10 Desktop RISC arithmetic/logical instructions equivalent to RISC-V integer ISA. MIPS also provides\ninstructions that trap on arithmetic overflow, which are synthesized in other architectures with multiple instructions.\nNote that in the \u201cArithmetic/logical\u201d category all machines but SPARC use separate instruction mnemonics to indicate\nan immediate operand; SPARC offers immediate versions of these instructions but uses a single mnemonic. (Of\ncourse, these are separate opcodes!)\nK.2\nA Survey of RISC Architectures for Desktop, Server, and Embedded Computers\n\u25a0\nK-13"
    },
    {
        "page": 1283,
        "text": "RISC-V and MIPS are most similar. RISC-V uses a compare and branch with a\nfull set of arithmetic comparisons. MIPS also uses compare and branch, but the\ncomparisons are limited to equality and tests against zero. This limited set of con-\nditions simplifies the branch determination (since an ALU operation is not required\nto test the condition), at the cost of sometimes requiring the use of a set-on-less-than\ninstruction (SLT, SLTI, SLTU, SLTIU), which compares two operands and then\nset the destination register to 1 if less and to 0 otherwise. Figure K.12 provides\nInstruction name\nARMv8\nMIPS64\nPowerPC\nRISC-V\nSPARC v.9\nBranch on integer  \ncompare\nB.cond, \nCBZ, CBNZ\nBEQ, BNE, \nB_Z (<, \n>, \n<=, >=) \nOR\nS***; BEZ\nBC\nBEQ, BNE, \nBLT, BGE, \nBLTU, BGEU \nBR_Z, BPcc \n(<, >,\n<=, >=, =, \nnot=)\nBranch on floating-point \ncompare\nB.cond\nBC1T, \nBC1F\nBC\nBEZ, BNZ\nFBPfcc (<, >, \n<=,\n>=, =,...)\nJump, jump register\nB, BR\nJ, JR\nB, BCLR, \nBCCTR\nJAL, JALR \n(with x0)\nBA, JMPL \nr0,...\nCall, call register\nBL, BLR\nJAL,\nJALR\nBL, BLA, \nBCLRL, \nBCCTRL\nJAL, JALR\nCALL, JMPL\nTrap\nSVC, HVC, \nSMC\nBREAK\nTW, TWI\nECALL\nTicc, SIR\nReturn from interrupt\nERET\nJR; ERET\nRFI\nEBREAK\nDONE, RETRY,\nRETURN \nFigure K.11 Desktop RISC control instructions equivalent to RV64G.\nARMv8\nMIPS64\nPowerPC\nRISC-V\nSPARC v.9\nNumber of condition code bits  \n(integer and FP)\n16 (8 + the \ninverse)\nnone\n8  4 both\nnone\n2  4 \ninteger,\n4  2 FP\nBasic compare instructions  \n(integer and FP)\n1 integer; 1 \nFP\n1 integer, 1 FP\n4 integer, 2 FP\n2 integer; 3 FP\n1 FP\nBasic branch instructions  \n(integer and FP)\n1\n2 integer, 1 FP\n1 both\n4 integer (used \nfor FP as well)\n3 integer,\n1 FP\nCompare register with register/  \nconstant and branch\n\u2014\n=, not=\n\u2014\n=, not =, >=, <\n\u2014\nCompare register to zero and  \nbranch\n\u2014\n=, not=, <, <=, \n>, >=\n\u2014\n=, not=, <, <=, \n>, >=\n=, not=, <, \n<=, >, >=\nFigure K.12 Summary of five desktop RISC approaches to conditional branches. Integer compare on SPARC is\nsynthesized with an arithmetic instruction that sets the condition codes using r0 as the destination.\nK-14\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1284,
        "text": "additional details on conditional branch. RISC-V floating point comparisons sets an\ninteger register to 0 or 1, and then use conditional branches on that content.MIPS also\nuses separate floating-point compare, which sets a floating point register to 0 or 1,\nwhich is then tested by a floating-point conditional branch.\nFloating point  (instruc-\nMultiply add; Negative \nmultiply add: single, \ndouble\nMultiply subtract single, \ndouble, Negative multiply \nsubtract: single, double\nCopy sign or negative sign \ndouble or single to another \nFP register\nReplace sign bit with XOR \nof sign bits single double\nMaximum or minimum \nsingle, double\nClassify floating point \nvalue single double\nConvert between FP single \nor double and FP single or \ndouble, OR integer single \nor double, signed and \nunsigned with rounding\ntion formats)\nR-R\nR-R\nR-R\nR-R\nR-R\nInstruction name\nARMv8\nMIPS64\nPowerPC\nRISC-V\nSPARC v.9\nAdd single, double\nFADD\nADD.*\nFADD*\nFADD.*\nFADD*\nSubtract single, double\nFSUB\nSUB.*\nFSUB*\nFSUB.*\nFSUB*\nMultiply single, double\nFMUL\nMUL.*\nFMUL*\nFMUL.*\nFMUL*\nDivide single, double\nFDIV\nDIV.*\nFDIV*\nFDIV.*\nFDIV*\nSquare root single, double\nFSQRT\nSQRT.*\nFSQRT*\nFSQRT.*\nFSQRT*\nFMADD, \nFNMADD\nMADD.*\nNMAD.*\nFMADD*, \nFNMADD*\nFMADD.*\nFNMADD.*\nFMSUB, \nFNMSUB\nMSUB.*, \nNMSUB.*\nFMSUB*, \nFNMSUB*\nFMSUB.*,\nFNMSUB.*\nFMOV, \nFNEG\nFMOV.*, FNEG.*\nFMOV*, \nFNEG*\nFSGNJ.*, \nFSGNJN.*\nFMOV*, \nFNEG*\nFABS\nFABS.*\nFABS*\nFSGNJX.*\nFABS*\nFMAX, \nFMIN\nMAX.*, MIN.*\nFMAX.*, FMIN.*\nFCLASS.*\nCLASS.*\nCompare\nFCMP\nCMP.*\nFCMP*\nFCMP.*\nFCMP*\nFCVT\nCVT, CEIL, \nFLOO R\nFCVT\nF*TO*\nFigure K.13 Desktop RISC floating-point instructions equivalent to RV64G ISA with an empty entry meaning that\nthe instruction is unavailable. ARMv8 uses the same assembly mnemonic for single and double precision; the reg-\nister designator indicates the precision. \u201c*\u201d is used as an abbreviation for S or D. For floating point compares all con-\nditions: equal, not equal, less than, and less-then or equal are provided. Moves operate in both directions from/to\ninteger registers. Classify sets a register based on whether the floating point quantity is plus or minus infinity, denorm,\n+/  0, etc.). The sign-injection instructions take two operands, but are primarily used to form floating point move,\nnegate, and absolute value, which are separate instructions in the other ISAs.\nK.2\nA Survey of RISC Architectures for Desktop, Server, and Embedded Computers\n\u25a0\nK-15"
    },
    {
        "page": 1285,
        "text": "ARM is similar to SPARC, in that it provides four traditional condition codes\nthat are optionally set. CMP subtracts one operand from the other and the difference\nsets the condition codes. Compare negative (CMN) adds one operand to the other,\nand the sum sets the condition codes. TST performs logical AND on the two oper-\nands to set all condition codes but overflow, while TEQ uses exclusive OR to set\nthe first three condition codes. Like SPARC, the conditional version of the ARM\nbranch instruction tests condition codes to determine all possible unsigned and\nsigned relations. ARMv8 added both bit-test instructions and also compare and\nbranch against zero. Floating point compares on ARM, set the integer condition\ncodes, which are used by the B.cond instruction.\nAs Figure K.13 shows the floating point support is similar on all five\narchitectures.\nRV64GC Core 16-bit Instructions\nFigures K.14 through K.17 summarize the data transfer, ALU, and control instruc-\ntions for our three embedded processors: microMIPS64, RV64GC, and Thumb-2.\nSince these architectures are all based on 32-bit or 64-bit versions of the full archi-\ntecture, we focus our attention on the functionality implemented by the 16-bit\ninstructions. Since floating point is optional, we do not include it. I\nInstruction name\nmicroMIPS64\nrs1;rs2/dst; o\ufb00set\nRV64GC\nrs1;rs2/dst; o\ufb00set\nThumb-2\nrs1;rs2/dst; o\ufb00set\nLoad word\n8;8;4\n8;8;5\n8;8;5\nLoad double word\n8;8;5\nLoad word with stack pointer as base register\n1;32;5\n1;32;6\n1;3;8\nLoad double word with stack pointer as base register\n1;32;6\nStore word\n8;8;4\n8;8;5\n8;8;5\nStore double word\n8;8;5\nStore word with stack pointer as base register\n1;32;5\n1;32;6\n1;3;8\nStore double with stack pointer as base register\n1;32;6\nFigure K.14 Embedded RISC data transfer instructions equivalent to RV64GC 16-bit ISA; a blank indicates that the\ninstruction is not a 16-bit instruction. Rather than show the instruction name, where appropriate, we show the num-\nber of registers that can the base register for the address calculation, followed by the number of registers that can be\nthe destination for a load or the source for a store, and finally, the size of the immediate used for address calculation.\nFor example: 8; 8; 5 for a load means that there are 8 possible base registers, 8 possible destination registers for the\nload, and a 5-bit offset for the address calculation. For a store, 8; 8; 5, specifies that the source of the value to store\ncomes from one of 8 registers. Remember that Thumb-2 also has 32-bit instructions (although not the full ARMv8 set)\nand that RV64GC and microMIPS64 have the full set of 32-bit instructions in RV64I or MIPS64.\nK-16\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1286,
        "text": "microMIPS64\nRV64GC\nThumb-2\nsff\no\ntib\n-\n0\n1\nh\nc\nn\nar\nb\nla\nn\noitid\nn\no\nc\nn\nU\net\n11-bit offset\n11-bit offset\nUnconditional branch and link\n11-bit offset\n11-bit offset\nUnconditional branch to register w/wo link\nany of 32 registers\nany of 32 registers\nCompare register to zero (=/!=) and branch\n8 registers; 7-bit offset 8 registers; 8-bit \noffset\nno: but see caption\nFigure K.16 Summary of three embedded RISC approaches to conditional branches. A blank indicates that\nthe instruction does not exist. Thumb-2 uses 4 condition code bits; it provides a conditional branch that tests the\n4-bit condition code and has a branch offset of 8 bits.\nInstruction Name/Function\nLoad immediate\netaid\ne\nm\nm\ni\nre\np\np\nu\nd\na\no\nL\nadd immediate\nadd immediate word (32 bits) & sign \nextend\nadd immediate to stack pointer\nadd immediate to stack pointer store \nin reg.\nshift left/right logical \ncite\nm\nhtira\nth\ngir\ntfih\ns\nAND immediate\ne\nv\no\nm\nd\nd\na\nAND, OR, XOR\nThumb-2\n8;8\n8;8;3\n1;7\n8;8;5 (shift amt.)\n8;8;5 (shift amt.)\n8;8\n6\n1;6\n1\n8;8;8\n16;16\n8;8\n8;8;8\nmicroMIPS64\n8;7\n32;4\n1;9\n1;8;6\n8;8;3 (shift amt.)\n8;8;4\n2\n3;2\n3\n8;8;8\n8;8\n8;8;8\ntc\nartb\nu\ns\nadd word, subtract word (32 bits)\n& sign extend \nRV64GC\n32;6\n6;2\n3\n32;6\n32;6\n1;6 \n(adds 16x imm.)\n1;8;6\n(adds 4x imm.)\n8;6(shift amt.)\nfih\ns(\n6;8\nt amt.)\n8;6\n2\n3;2\n3\n2\n3;2\n3\n8;8\n8;8\n8;8\nFigure K.15 ALU instructions provided in RV64GC and the equivalents, if any, in the 16-bit instructions of micro-\nMIPS64 or Thumb-2. An entry shows the number of register sources/destinations, followed by the size of the imme-\ndiate field, if it exists for that instruction. The add to stack pointer with scaled immediate instructions are used for\nadjusting the stack pointer and creating a pointer to a location on the stack. In Thumb, the add has two forms one\nwith three operands from the 8-register subset (Lo) and one with two operands but any of 16-registers.\nK.2\nA Survey of RISC Architectures for Desktop, Server, and Embedded Computers\n\u25a0\nK-17"
    },
    {
        "page": 1287,
        "text": "Instructions: Common Extensions beyond RV64G\nFigures K.15 through K.18 list instructions not found in Figures K.9 through K.13\nin the same four categories (data transfer, ALU, and control. The only significant\nfloating point extension is the reciprocal instruction, which both MIPS64 and\nPower support. Instructions are put in these lists if they appear in more than\none of the standard architectures. Recall that Figure K.3 on page 6 showed the\naddress modes supported by the various instruction sets. All three processors pro-\nvide more address modes than provided by RV64G. The loads and stores using\nthese additional address modes are not shown in Figure K.17, but are effectively\nadditional data transfer instructions. This means that ARM has 64 additional load\nand store instructions, while Power3 has 12, and MIPS64 and SPARVv9 each\nhave 4.\nTo accelerate branches, modern processors use dynamic branch prediction (see\nSection 3.3). Many of these architectures in earlier versions supported delayed\nbranches, although they have been dropped or largely eliminated in later versions\nFunction\nDe\ufb01nition\nARMv8\nMIPS64\nPowerPC\nSPARC v.9\nLoad/store \nmultiple registers \nLoads or stores 2 or \nmore registers \nLoad pair, \nstore pair\nLoad store \nmultiple (<=31 \nregisters), \nCache \nmanipulation and \nprefetch\nModifies status of a \ncache line or does a \nprefetch\nPrefetch\nCACHE, \nPREFETCH\nPrefetch\nPrefetch\nFigure K.17 Data transfer instructions not found in RISC-V core but found in two or more of the five desktop\narchitectures. SPARC requires memory accesses to be aligned, while the other architectures support unaligned\naccess, albeit, often with major performance penalties. The other architectures do not require alignment, but may\nuse slow mechanisms to handle unaligned accesses.MIPS provides a set of instructions to handle misaligned\naccesses: LDL and LDR (load double left and load double right instructions) work as a pair to load a misaligned word;\nthe corresponding store instructions perform the inverse. The Prefetch instruction causes a cache prefetch, while\nCACHE provides limited user control over the cache state.\nName\nDe\ufb01nition\nARMv8\nMIPS64\nPowerPC\nSPARC v.9\nDelayed branches\nDelayed branches \nwith/without cancellation\nBEQ, BNE, BGTZ, \nBLEZ, BCxEQZ, \nBCxNEZ\nBPcc, A,\nFPBcc, A\nConditional trap\nTraps if a condition is true\nTEQ, TNE, TGE, \nTLT, TGEU, TLTU\nTW, TD, \nTWI, TDI\nTcc\nFigure K.18 Control instructions not found in RV64G core but found in two or more of the other architectures.\nMIPS64 Release 6 has nondelayed and normal delayed branches, while SPARC v.9 has delayed branches with can-\ncellation based on the static prediction.\nK-18\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1288,
        "text": "of the architecture, typically by offering a nondelayed version, as the preferred con-\nditional branch. The SPARC \u201cannulling\u201d branch is an optimized form of delayed\nbranch that executes the instruction in the delay slot only if the branch is taken;\notherwise, the instruction is annulled. This means the instruction at the target of\nthe branch can safely be copied into the delay slot since it will only be executed\nif the branch is taken. The restrictions are that the target is not another branch and\nthat the target is known at compile time. (SPARC also offers a nondelayed jump\nbecause an unconditional branch with the annul bit set does not execute the follow-\ning instruction.).\nIn contrast to the differences among the full ISAs, the 16-bit subsets of the three\nembedded ISAs have essentially no significant differences other than those\ndescribed in the earlier figures (e.g. size of immediate fields, uses of SP or other\nregisters, etc.).\nNow that we have covered the similarities, we will focus on the unique features\nof each architecture. We first cover the desktop/server RISCs, ordering them by\nlength of description of the unique features from shortest to longest, and then\nthe embedded RISCs.\nInstructions Unique to MIPS64 R6\nMIPS has gone through six generations of instruction sets. Generations 1\u20134 mostly\nadded instructions. Release 6 eliminated many older instructions but also provided\nsupport for nondelayed branches and misaligned data access. Figure K.19 summa-\nrizes the unique instructions in MIPS64 R6.\nInstruction \nclass\nInstruction name(s)\nFunction\nALU\nByte align\nTake a pair of registers and extract a word or double word of bytes. \nUsed to implement unaligned byte copies. \nAlign Immediate to PC\nAdds the upper 16 bits of the PC to an immediate shifted left 16 bits \nand puts the result in a register; Used to get a PC-relative address. \nBit swap\nReverses the bits in each byte of a register.\nNo-op and link\nPuts the value of PC+8 into a register\nLogical NOR\nComputes the NOR of 2 registers\nControl transfer\nBranch and Link conditional Compares a register to 0 and does a branch if condition is true; places \nthe return address in the link register.\nJump indexed, Jump and \nlink indexed\nAdds an offset and register to get new PC, w/wo link address\nFigure K.19 Additional instructions provided MIPS64 R6. In addition, there are several instructions for supporting\nvirtual machines, most are privileged.\nK.2\nA Survey of RISC Architectures for Desktop, Server, and Embedded Computers\n\u25a0\nK-19"
    },
    {
        "page": 1289,
        "text": "Instructions Unique to SPARC v.9\nSeveral features are unique to SPARC. We review the major figures and then sum-\nmarize those and small differences in a figure.\nRegister Windows\nThe primary unique feature of SPARC is register windows, an optimization for\nreducing register traffic on procedure calls. Several banks of registers are used, with\na new one allocated on each procedure call. Although this could limit the depth of\nprocedure calls, the limitation is avoided by operating the banks as a circular buffer.\nThe knee of the cost-performance curve seems to be six to eight banks; programs\nwith deeper call stacks, would need to save and restore the registers to memory.\nSPARC can have between 2 and 32 windows, typically using 8 registers each\nfor the globals, locals, incoming parameters, and outgoing parameters. (Given that\neach window has 16 unique registers, an implementation of SPARC can have as\nfew as 40 physical registers and as many as 520, although most have 128 to 136, so\nfar.) Rather than tie window changes with call and return instructions, SPARC has\nthe separate instructions SAVE and RESTORE. SAVE is used to \u201csave\u201d the caller\u2019s\nwindow by pointing to the next window of registers in addition to performing an\nadd instruction. The trick is that the source registers are from the caller\u2019s window of\nthe addition operation, while the destination register is in the callee\u2019s window.\nSPARC compilers typically use this instruction for changing the stack pointer\nto allocate local variables in a new stack frame. RESTORE is the inverse of SAVE,\nbringing back the caller\u2019s window while acting as an add instruction, with the\nsource registers from the callee\u2019s window and the destination register in the caller\u2019s\nwindow. This automatically deallocates the stack frame. Compilers can also make\nuse of it for generating the callee\u2019s final return value.\nThe danger of register windows is that the larger number of registers could\nslow down the clock rate. This was not the case for early implementations. The\nSPARC architecture (with register windows) and the MIPS R2000 architecture\n(without) have been built in several technologies since 1987. For several genera-\ntions the SPARC clock rate has not been slower than the MIPS clock rate for imple-\nmentations in similar technologies, probably because cache access times dominate\nregister access times in these implementations. With the advent of multiple issue,\nwhich requires many more register ports, as will as register renaming or reorder\nbuffers, register windows posed a larger penalty.Register windows were a feature\nof the original Berkeley RISC designs, and their inclusion in SPARC was inspired\nby those designs. Tensilica is the only other major architecture in use today\nemploys them, and they were not included in the RISC-V ISA.\nFast Traps\nSPARCv9 includes support to make traps fast. It expands the single level of traps to at\nleastfourlevels,allowingthewindowoverflowandunderflowtraphandlerstobeinter-\nrupted. The extra levels mean the handler does not need to check for page faults or\nK-20\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1290,
        "text": "misalignedstackpointersexplicitlyinthecode,therebymakingthehandlerfaster.Two\nnew instructions were added to return from this multilevel handler: RETRY (which\nretries the interrupted instruction) and DONE (which does not). To support user-level\ntraps, the instruction RETURN will return from the trap in nonprivileged mode.\nSupport for LISP and Smalltalk\nThe primary remaining arithmetic feature is tagged addition and subtraction. The\ndesigners of SPARC spent some time thinking about languages like LISP and\nSmalltalk, and this influenced some of the features of SPARC already discussed:\nregister windows, conditional trap instructions, calls with 32-bit instruction\naddresses, and multi-word arithmetic (see Taylor et al. [1986] and Ungar et al.\n[1984]). A small amount of support is offered for tagged data types with operations\nfor addition, subtraction, and hence comparison. The two least-significant bits indi-\ncate whether the operand is an integer (coded as 00), so TADDcc and TSUBcc set\nthe overflow bit if either operand is not tagged as an integer or if the result is too\nlarge. A subsequent conditional branch or trap instruction can decide what to do.\n(If the operands are not integers, software recovers the operands, checks the types\nof the operands, and invokes the correct operation based on those types.) It turns\nout that the misaligned memory access trap can also be put to use for tagged data,\nsince loading from a pointer with the wrong tag can be an invalid access.\nFigure K.20 shows both types of tag support.\n(a) Add, sub, or\ncompare integers\n(coded as 00)\n(b) Loading via\nvalid pointer\n(coded as 11)\n00\n(R5)\n00\n(R6)\n00\n(R7)\n11\n3\n(R4)\n00\n(Word\naddress)\nTADDcc r7, r5, r6\nLD rD, r4, \u2013 3\n+\u2013\n\u2013\nFigure K.20 SPARC uses the two least-significant bits to encode different data types\nfor the tagged arithmetic instructions. (a) Integer arithmetic, which takes a single cycle\nas long as the operands and the result are integers. (b) The misaligned trap can be used\nto catch invalid memory accesses, such as trying to use an integer as a pointer. For lan-\nguages with paired data like LISP, an offset of \u20133 can be used to access the even word of\na pair (CAR) and +1 can be used for the odd word of a pair (CDR).\nK.2\nA Survey of RISC Architectures for Desktop, Server, and Embedded Computers\n\u25a0\nK-21"
    },
    {
        "page": 1291,
        "text": "Figure K.21 summarizes the additional instructions mentioned above as well as\nseveral others.\nInstructions Unique to ARM\nEarlier versions of the ARM architecture (ARM v6 and v7) had a number of\nunusual features including conditional execution of all instructions, and making\nthe PC a general purpose register. These features were eliminated with the arrival\nof ARMv8 (in both the 32-bit and 64-bit ISA). What remains, however, is much of\nthe complexity, at least in terms of the size of the instruction set. As Figure K.3 on\npage 6 shows, ARM has the most addressing modes, including all those listed in\nthe table; remember that these addressing modes add dozens of load/store instruc-\ntions compared to RVG, even though they are not listed in the table that follows. As\nFigure K.6 on page 8 shows, ARMv8 also has by far the largest number of different\ninstruction formats, which reflects a variety of instructions, as well as the different\naddressing modes, some of which are applicable to some loads and stores but not\nothers.\nMost ARMv8 ALU instructions allow the second operand to be shifted before\nthe operation is completed. This extends the range of immediates, but operand\nshifting is not limited to immediates. The shift options are shift left logical, shift\nright logical, shift right arithmetic, and rotate right. In addition, as in Power3, most\nALU instructions can optionally set the condition flags. Figure K.22 includes the\nadditional instructions, but does not enumerate all the varieties (such as optional\nsetting of the condition flags); see the caption for more detail. While conditional\nexecution of all instructions was eliminated, ARMv8 provides a number of condi-\ntional instructions beyond the conditional move and conditional set, mentioned\nearlier.\nInstruction \nclass\nInstruction name(s)\nFunction\nData transfer\nSAVE, RESTORE\nSave or restore a register window\nNonfaulting load\nVersion of load instructions that do not generate faults on address \nexceptions; allows speculation for loads.\nALU\nTagged add, Tagged subtract, \nwith and without trap \nPerform a tagged add/subtract, set condition codes, optionally \ntrap.\nControl transfer\nRetry, Return, and Done\nTo provide handling for traps.\nFloating Point \nInstructions\nFMOVcc\nConditional move between FP registers based on integer or FP \ncondition codes.\nFigure K.21 Additional instructions provided in SPARCv9. Although register windows are by far the most signif-\nicant distinction, they do not require many instructions!\nK-22\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1292,
        "text": "Instructions Unique to Power3\nPower3 is the result of several generations of IBM commercial RISC machines\u2014\nIBM RT/PC, IBM Power1, and IBM Power2, and the PowerPC development,\nundertaken primarily by IBM and Motorola. First, we describe branch registers\nand the support for loop branches. Figure K.23 then lists the other instructions pro-\nvided only in Power3.\nInstruction \nclass\nInstruction name(s)\nFunction\nData transfer\nLoad/Store Non-temporal pair\nLoads/stores a pair of registers with an indication not to cache \nthe data. Base + scaled offset addressing mode only. \nALU\nAdd Extended word/double word\nAdd 2 registers after left shifting the second register operand \nand extending it.\nAdd with shift; add immediate with \nshift\nAdds with shift of the second operand.\nAddress of page\nComputes the address of a page based on PC (similar to \nADDUIPC, which is the same as ADR in ARMv8)\nAND, OR, XOR, XOR NOT shifted \nregister\nLogical operation on a register and a shifted register.\nBit field clear shifted\nShift operand, invert and AND with another operand\nConditional compare, immediate, \nnegative, negative immediate\nIf condition true, then set condition flags to compare result, \notherwise leave condition flags untouched. \nConditional increment, invert, \nnegate\nIf condition then set destination to increment/invert/negate of \nsource register\nelb\nu\no\nd\n,d\nr\no\nw\nfla\nh\n,d\nr\no\nw\n,ety\nb\n:\nm\nu\ns\nk\nc\ne\nh\nc\nC\nR\nC\na\ns\netu\np\nm\no\nC\nC\nR\nC\nMultiply add, subtract\nInteger multiply-add or multiply-subtract\nMultiply negate\nNegate the product of two integers; word & double word\nMove immediate or inverse\nReplace 16-bits in a register with immediate, possibly shifted\nReverse bit order\nReverses the order of bits in a register\nSigned bit field move\nMove a signed bit field; sign extend to left; zero extend to right\nUnsigned divide, multiple, multiply \nnegate, multiply-add, multiply-sub\nUnsigned versions of the basic instructions\nControl transfer CBNZ, CBZ\nCompare branch =/!= 0, indicating this is not a call or return.\nTBNZ, TBZ\nTests bit in a register =/!= 0, and branch.\nFigure K.22 Additional instructions provided in ARMv8, the AArch64 instruction set. Unless noted the instruction\nis available in a word and double word format, if there is a difference. Most of the ALU instructions can optionally set\nthe condition codes; these are not included as separate instructions here or in earlier tables.\nK.2\nA Survey of RISC Architectures for Desktop, Server, and Embedded Computers\n\u25a0\nK-23"
    },
    {
        "page": 1293,
        "text": "Instruction \nclass\nInstruction name(s)\nFunction\nData transfer\nLHBRX, LWBRX, LDBRX\nLoads a halfword/word/double word but reverses the byte order.\nSHBRX, SWBRX, SDBRX\nStores a halfword/word/double word but reverses the byte order \nLDQ, STQ\nLoad/store quadword to a register pair.\nretsig\ner\na\nni\nre\nb\nm\nu\nn\nm\no\nd\nn\nar\na\netare\nn\ne\nG\nN\nA\nR\nD\nU\nL\nA\nCMPB\nCMPRB\nCompares the individual bytes in a register and sets another \nregister byte by byte. \nCompares a byte (x) against two other bytes (y and z) and sets a \ncondition to indicate if the value of y<=x<=z. \nCRAND, CRNAND, CROR, \nCRNOR, CRXOR, CREQV, \nCORC, CRANDC\nLogical operations on the condition register.\nZCMPEQB\nCompares a byte (x) against the eight bytes in another register and \nsets a condition to indicate if x = any of the 8 bytes\nEXTSWSL\nSign extend word and shift left\nPOPCNTB, POPCNTW\nPOPCNTD\nCount number of 1s in each byte and place total in another byte.\nCount number of 1s in each word and place total in another word.\nCount number of 1s in a double word.\nPRTYD, PRTYW\nCompute byte parity of the bytes in a word or double word.\nBPERMD\nPermutes the bits in a double word, producing a permuted byte.\nCDTBCD, CDCBCD, \nADDGCS\nInstructions to convert from/to binary coded decimal (BCD) or \noperate on two BCD values\nControl transfer\nBA, BCA\nBranches to an absolute address, conditionally & unconditionally \nBCCTR, BCCTRL\nConditional branch to address in the count register, w/wo linking \nBCTSAR, BCTARL\nConditional branch to address in the Branch Target Address \nregister, w/wo linking \nCLRBHRB, MFBHRBE\nManipulate the branch history rolling buffer. \nFloating Point \nInstructions\nFRSQRTE\nComputes an estimate of reciprocal of the square root,\nFTDIV, FTSQRT\nTests for divide by zero or square of negative number\nd\nn\na\no\nre\nz\nts\nnia\ng\na\nretsig\ner\nts\ne\nT\nL\nE\nS\nF\nselect one of two operands to move\nDecimal floating point \noperations\nA series of 48 instructions to support decimal floating point.\nFigure K.23 Additional instructions provided in Power3. Rotate instructions have two forms: one that sets a con-\ndition register and one that does not. There are a set of string instructions that load up to 32 bytes from an arbitrary\naddress to a set of registers. These instructions will be phased out in future implementations, and hence we just\nmention them here.\nK-24\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1294,
        "text": "Branch Registers: Link and Counter\nRather than dedicate one of the 32 general-purpose registers to save the return\naddress on procedure call, Power3 puts the address into a special register called\nthe link register. Since many procedures will return without calling another pro-\ncedure, link doesn\u2019t always have to be saved away. Making the return address a\nspecial register makes the return jump faster since the hardware need not go\nthrough the register read pipeline stage for return jumps.\nIn a similar vein, Power3 has a count register to be used in for loops where the\nprogram iterates for a fixed number of times. By using a special register the branch\nhardware can determine quickly whether a branch based on the count register is\nlikely to branch, since the value of the register is known early in the execution\ncycle. Tests of the value of the count register in a branch instruction will automat-\nically decrement the count register.\nGiven that the count register and link register are already located with the hard-\nware that controls branches, and that one of the problems in branch prediction is\ngetting the target address early in the pipeline (see Appendix C), the Power archi-\ntects decided to make a second use of these registers. Either register can hold a\ntarget address of a conditional branch. Thus, PowerPC supplements its basic con-\nditional branch with two instructions that get the target address from these registers\n(BCLR, BCCTR). Figure K.23 shows the several dozen instructions that have been\nadded; note that there is an extensive facility for decimal floating point, as well.\nInstructions: Multimedia Extensions of the\nDesktop/Server RISCs\nSupport for multimedia and graphics operations developed in several phases,\nbeginning in 1996 with Intel MMX, MIPS MDMX, and SPARC VIS. As described\nin Section 4.3, which we assume the reader has read, these extensions allowed a\nregister to be treated as multiple independent small integers (8 or 16 bits long) with\narithmetic and logical operations done in parallel on all the items in a register.\nThese initial SIMD extensions, sometimes called packed SIMD, were further\ndeveloped after 2000 by widening the registers, partially or totally separating them\nfrom the general purpose or floating pointer registers, and by adding support for\nparallel floating point operations. RISC-V has reserved an extension for such\npacked SIMD instructions, but the designers have opted to focus on a true vector\nextension for the present. The vector extension RV64V is a vector architecture,\nand, as Section 4.3 points out, a true vector instruction set is considerably more\ngeneral, and can typically perform the operations handled by the SIMD extensions\nusing vector operations.\nFigure K.24 shows the basic structure of the SIMD extensions in ARM, MIPS,\nPower, and SPARC. Note the difference in how the SIMD \u201cvector registers\u201d are\nstructured: repurposing the floating point, extending the floating point, or adding\nadditional registers. Other key differences include support for FP as well as integers,\nK.2\nA Survey of RISC Architectures for Desktop, Server, and Embedded Computers\n\u25a0\nK-25"
    },
    {
        "page": 1295,
        "text": "support for 128-bit integers, and provisions for immediate fields as operands in inte-\nger and logical operations. Standard load and store instructions are used for moving\ndata from the SIMD registers to memory with special extensions to handle moving\nless than a full SIMD register. SPARC VIS, which was one of the earliest ISA exten-\nsions for graphics, is much more limited: only add, subtract, and multiply are\nincluded, there is no FP support, and only limited instructions for bit element oper-\nations; we include it in Figure K.24 but will not be going into more detail.\nFigure K.25 shows the arithmetic instructions included in these SIMD exten-\nsions; only those appearing in at least two extensions are included. MIPS SIMD\nincludes many other instructions, as does the Power 3 Vector-Scalar extension,\nwhich we do not cover. One frequent feature not generally found in general-\npurpose microprocessors is saturating operations. Saturation means that when a\ncalculation overflows the result is set to the largest positive number or most neg-\native number, rather than a modulo calculation as in two\u2019s complement arithmetic.\nCommonly found in digital signal processors (see the next subsection), these sat-\nurating operations are helpful in routines for filtering. Another common extension\nare instructions for accumulating values within a single register; the dot product\ninstruction an the maximum/minimum instructions are typical examples.\nIn addition to the arithmetic instructions, the most common additions are log-\nical and bitwise operations and instructions for doing version of permutations and\npacking elements into the SIMD registers. These additions are summarized in\nFigure K.26, Lastly, all three extensions support SIMD FP operations, as summa-\nrized in Figure K.27.\nARMv8\nMIPS64 R6\nPower v3.0\nSPARCv9\nName of ISA extension\nAdvanced SIMD\nMIPS64 SIMD \nArchitecture\nVector Facility\nVIS\nDate of Current Version\n2011\n2012\n2015\n1995\nVector registers: # x size\n32 x 128 bits\n32 x 128 bits\n32 x 128 bits\n32 x 64 bits\nUse GP/FP registers or \nindependent set\nextend FP registers \ndoubling width\nextend FP registers \ndoubling width\nIndependent \nSame as FP registers\nInteger data sizes\n8, 16, 32, 64\n8, 16, 32, 64\n8, 16, 32, 64, 128\n8,16, 32\nFP data sizes\n32, 64\n32, 64\n32\nImmediates for integer and \nlogical operations\n5 bits arithmetic\n8 bits logical\nFigure K.24 Structure of the SIMD extensions intended for multimedia support. In addition to the vector facility,\nThe last row states whether the SIMD instruction set supports immediates (e.g, add vector immediate or AND vector\nimmediate); the entry states the size of immediates for those ISAs that support them. Note that the fact that an imme-\ndiate is present is encoded in the opcode space, and could alternatively be added to the next table as additional\ninstructions. Power 3 has an optional Vector-Scalar Extension. The Vector-Scalar Extension defines a set of vector\nregisters that overlap the FP and normal vector registers, eliminating the need to move data back and forth to\nthe vector registers. It also supports double precision floating point operations.\nK-26\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1296,
        "text": "Instruction category\nARM Advanced SIMD\nMIPS SIMD\nPower Vector Facility\nQ\n,\nD\n2\n,\nW\n4\n,\nH\n8\n,\nB\n6\n1\nD\n2\n;\nW\n4\n;\nH\n8\n,\nB\n6\n1\nD\n2\n;\nW\n4\n,\nH\n8\n,\nB\n6\n1\ntc\nartb\nu\ns/d\nd\nA\nSaturating add/sub\n16B, 8H, 4W; 2 D\n16B, 8H; 4W; 2 D\n16B, 8H, 4W, 2 D, Q\nAbsolute value of difference\n16B, 8H, 4W; 2 D\n16B, 8H, 4W; 2 D\n16B, 8H, 4W; 2 D; Q\nAdjacent add & subtract (pairwise) \n16B, 8H, 4W\n16B, 8H, 4W\n16B, 8H, 4W; 2 D\nQ\n;\nD\n2\n;\nW\n4\n,\nH\n8\n,\nB\n6\n1\nD\n2\n;\nW\n4\n,\nH\n8\n,\nB\n6\n1\ne\ng\nare\nv\nA\nDot product add, dot product subtract\n16B, 8H, 4W\n16B, 8H, 4W\n16B, 8H, 4W; 2 D\nDivide: signed, unsigned\n16B, 8H, 4W\n16B, 8H, 4W; 2 D\n16B, 8H, 4W; 2 D; Q\nMultiply: signed, unsigned\n16B, 8H, 4W\n16B, 8H, 4W\n16B, 8H, 4W; 2 D\nMultiply add, multiply subtract\n16B, 8H, 4W\n16B, 8H, 4W\n16B, 8H, 4W; 2 D\nMaximum, signed & unsigned\n16B, 8H, 4W; 2 D\n16B, 8H, 4W; 2 D\n16B, 8H, 4W; 2 D; Q\nMinimum, signed & unsigned\n16B, 8H, 4W; 2 D\n16B, 8H, 4W; 2 D\n16B, 8H, 4W; 2 D; Q\nQ\n;\nD\n2\n;\nW\n4\n,\nH\n8\n,\nB\n6\n1\nD\n2\n;\nW\n4\n,\nH\n8\n,\nB\n6\n1\nd\ne\nn\ngis\nn\nu\n&\nd\ne\nn\ngis\n,olu\nd\no\nM\nQ\n;\nD\n2\n;\nW\n4\n,\nH\n8\n,\nB\n6\n1\nD\n2\n;\nW\n4\n,\nH\n8\n,\nB\n6\n1\nD\n2\n;\nW\n4\n,\nH\n8\n,\nB\n6\n1\nla\nu\nq\ne\nera\np\nm\no\nC\nCompare <, <=, signed, unsigned\n16B, 8H, 4W; 2 D\n16B, 8H, 4W; 2 D\n16B, 8H, 4W; 2 D; Q\nFigure K.25 Summary of arithmetic SIMD instructions. B stands for byte (8 bits), H for half word (16 bits), and W for\nword (32 bits), D for double word (64 bits), and Q for quad word (128 bits). Thus, 8B means an operation on 8 bytes in a\nsingle instruction. Note that some instructions\u2013such as adjacent add/subtract, or multiply\u2013produce results that are\ntwice the width of the inputs (e.g. multiply on 16 bytes produces 8 halfword results). Dot product is a multiply and\naccumulate. The SPARC VIS instructions are aimed primarily at graphics and are structured accordingly.\nInstruction category\nARM Advanced SIMD\nMIPS SIMD\nPower Vector Facility\nShift right/left, logical, arithmetic\n16B, 8H, 4W; 2 D\n16B, 8H, 4W; 2 D; Q\n16B, 8H, 4W; 2 D; Q\nCount leading or trailing zeros\n16B, 8H, 4W; 2 D\n16B, 8H, 4W; 2 D\n16B, 8H, 4W; 2 D; Q\nQ\nQ\nQ\nr\no\nx/r\no/d\nn\na\nBit insert & extract\n16B, 8H, 4W; 2 D\n16B, 8H, 4W; 2 D\n16B, 8H, 4W; 2 D; Q\n;\nW\n4\n,\nH\n8\n,\nB\n6\n1\ntn\nu\no\nc\nn\noitalu\np\no\nP\n2 D\n16B, 8H, 4W; 2 D; Q\nD\n2\n;\nW\n4\n,\nH\n8\n,\nB\n6\nD\n2\n;\nW\n4\n,\nH\n8\n,\nB\n6\n1\nth\ngir/tfel,d\nd\no/n\ne\nv\ne\ne\nv\na\nelretn\nI\nD\n2\n;\nW\n4\n,\nH\n8\n,\nB\n6\nD\n2\n;\nW\n4\n,\nH\n8\n,\nB\n6\n1\nd\nd\no/n\ne\nv\ne\nk\nc\na\nP\nD\n2\n;\nW\n4\n,\nH\n8\n,\nB\n6\n1\nD\n2\n;\nW\n4\n,\nH\n8\n,\nB\n6\n1\nelff\nu\nh\nS\nD\n2\n;\nW\n4\n,\nH\n8\n,\nB\n6\n1\nD\n2\n;\nW\n4\n,\nH\n8\n,\nB\n6\n1\nT\nA\nL\nP\nS\nFigure K.26 Summary of logical, bitwise, permute, and pack/unpack instructions, using the same format as the\nprevious figure. When there is a single operand the instruction applies to the entire register; for logical operations\nthere is no difference.Interleave puts together the elements (all even, odd, leftmost or rightmost) from two different\nregisters to create one value; it can be used for unpacking. Pack moves the even or odd elements from two different\nregisters to the leftmost and rightmost halves of the result. Shuffle creates a from two registers based on a mask that\nselects which source for each item. SPLAT copies a value into each item in a register."
    },
    {
        "page": 1297,
        "text": "Instructions: Digital Signal-Processing Extensions\nof the Embedded RISCs\nBoth Thumb2 and microMIPS32 provide instructions for DSP (Digital Signal Pro-\ncessing) and multimedia operations. In Thumb2, these are part of the core instruc-\ntion set; in microMIPS32, they are part of the DSP extension. These extensions,\nwhich are encoded as 32-bit instructions, are less extensive than the multimedia\nand graphics support provided in the SIMD/Vector extensions of MIPS64 or\nARMv8 (AArch64). Like those more comprehensive extensions, the ones in\nThumb2 and microMIPS32 also rely on packed SIMD, but they use the existing\ninteger registers, with a small extension to allow a wide accumulator, and only\noperate on integer data. RISC-V has specified that the \u201cP\u201d extension will support\npacked integer SIMD using the floating point registers, but at the time of publica-\ntion, the specification was not completed.\nDSP operations often include linear algebra functions and operations such\nas convolutions; these operations produce intermediate results that will be larger\nthan the inputs. In Thumb2, this is handled by a set of operations that produce\n64-bit results using a pair of integer registers. In microMIPS32 DSP, there are 4\n64-bit accumulator registers, including the Hi-Lo register, which is already\nexists for doing integer multiply and divide. Both architectures provide parallel\narithmetic using bytes, halfwords, and words, as in the multimedia extensions in\nARMv8 and MIPS64. In addition, the MIPS DSP extension handles fractional\ndata, such data is heavily used in DSP operations. Fractional data items have a\nsign bit and the remaining bits are used to represent the fraction, providing a\nrange of values from -1.0 to 0.9999 (in decimal). MIPS DSP supports two\nfractional data sizes Q15 and Q31 each with one sign bit and 15 or 31 bits\nof fraction.\nFigure K.28 shows the common operations using the same notation as was\nused in Figure K.25. Remember that the basic 32-bit instruction set provides\nadditional\nfunctionality,\nincluding\nbasic\narithmetic,\nlogical,\nand\nbit\nmanipulation.\nFigure K.27 Summary of floating point, using the same format as the previous figure.\nK-28\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1298,
        "text": "Concluding Remarks\nThis survey covers the addressing modes, instruction formats, and almost all the\ninstructions found in 8 RISC architectures. Although the later sections concentrate\non the differences, it would not be possible to cover 8 architectures in these few\npages if there were not so many similarities. In fact, we would guess that more than\n90% of the instructions executed for any of these architectures would be found in\nFigures K.9 through K.13. To contrast this homogeneity, Figure K.29 gives a sum-\nmary for four architectures from the 1970s in a format similar to that shown in\nFigure K.1. (Since it would be impossible to write a single section in this style\nfor those architectures, the next three sections cover the 80x86, VAX, and IBM\n360/370.) In the history of computing, there has never been such widespread agree-\nment on computer architecture as there has been since the RISC ideas emerged in\nthe 1980s.\nP\nS\nD\n2\n3\nS\nP\nI\nM\no\nr\nci\nm\n2\n-\nb\nm\nu\nh\nT\nn\no\nit\nc\nn\nu\nF\n5\n1\nQ\n2\n,\nB\n4\nH\n2\n,\nB\n4\ntc\nartb\nu\nS\n/d\nd\nA\n1\n3\nQ\n,5\n1\nQ\n2\n,\nB\n4\nH\n2\n,\nB\n4\nn\noitar\nuta\ns\nhti\nw\ntc\nartb\nu\nS\n/\nd\nd\nA\nAdd/Subtract with Exchange (exchanges halfwords in rt, then adds first \nhalfword and subtracts second) with optional saturation\n2H\nB\n4\n)s\ne\nula\nv\ne\nht\nm\nu\ns(\nd\nd\na\ny\nb\ne\nc\nu\nd\ne\nR\n1\n3\nQ\n,5\n1\nQ\n2\ne\nula\nv\netulo\ns\nb\nA\nPrecision reduce/increase (reduces or increases the precision of a value)\n2B, Q15, 2Q15, Q31\nH\n2\n,\nB\n4\nn\noitar\nuta\ns\nla\nn\noitp\no\nhti\nw\n,cite\nm\nhtira\n&\nla\ncig\nol,th\ngir\n,tfel\n:stfih\nS\n5\n1\nQ\n2\n,\nH\n2\n,\nB\n2\nH\n2\nylpitlu\nM\nMultiply add/subtract (to GPR or accumulator register in MIPS)\n2H\n2Q15\nComplex multiplication step (2 multiplies and addition/subtraction)\n2H\n2Q15\nMultiply and accumulate (by addition or subtraction)\n2H\nQ15, Q31\nH\n,\nB\nstib\neta\ncilp\ne\nR\nCompare: =, <, <=, se\nH\n2\n,\nB\n4\ndleif\nn\noitid\nn\no\nc\nst\nPick (use condition bits to choose bytes or halfwords from two operands)\n4B, 2H\nH\nd\nn\nare\np\no\nh\nc\na\ne\nm\no\nrf\nd\nr\no\nw\nfla\nh\na\ng\nnis\no\no\nh\nc\nk\nc\na\nP\nExtract \nQ63\nMove from/to \nW\nD\nr\notalu\nm\nu\nc\nc\na\nFigure K.28 Summary of two embedded RISC DSP operations, showing the data types for each operation. A blank\nindicates that the operation is not supported as a single instruction. Byte quantities are usually unsigned. Complex\nmultiplication step implements multiplication of complex numbers where each component is a Q15 value. ARM uses\nits standard condition register, while MIPS adds a set of condition bits as part of the state in the DSP extension.\nK.2\nA Survey of RISC Architectures for Desktop, Server, and Embedded Computers\n\u25a0\nK-29"
    },
    {
        "page": 1299,
        "text": "K.3\nThe Intel 80x86\nIntroduction\nMIPS was the vision of a single architect. The pieces of this architecture fit nicely\ntogether and the whole architecture can be described succinctly. Such is not the case\nof the 80x86: It is the product of several independent groups who evolved the archi-\ntecture over 20 years, adding new features to the original instruction set as you might\nadd clothing to a packed bag. Here are important 80x86 milestones:\n\u25a0\n1978\u2014The Intel 8086 architecture was announced as an assembly language\u2013\ncompatible extension of the then-successful Intel 8080, an 8-bit microproces-\nsor. The 8086 is a 16-bit architecture, with all internal registers 16 bits wide.\nWhereas the 8080 was a straightforward accumulator machine, the 8086\nextended the architecture with additional registers. Because nearly every reg-\nister has a dedicated use, the 8086 falls somewhere between an accumulator\nmachine and a general-purpose register machine, and can fairly be called an\nextended accumulator machine.\n\u25a0\n1980\u2014The Intel 8087 floating-point coprocessor is announced. This architec-\nture extends the 8086 with about 60 floating-point instructions. Its architects\nrejected extended accumulators to go with a hybrid of stacks and registers,\nIBM 360/370\nIntel 8086\nMotorola 68000\nDEC VAX\nDate announced\n1964/1970\n1978\n1980\n1977\nInstruction size(s) (bits)\n16, 32, 48\n8, 16, 24, 32, 40, 48\n16, 32, 48, 64, 80\n8, 16, 24, 32, ... , \n432\nAddressing (size, model)\n24 bits, flat/  \n31 bits, flat\n4 + 16 bits, \nsegmented\n24 bits, flat\n32 bits, flat\nData aligned?\nYes 360/No 370\nNo\n16-bit aligned\nNo\nData addressing modes\n2/3\n5\n9\n=14\nProtection\nPage\nNone\nOptional\nPage\nPage size\n2 KB & 4 KB\n\u2014\n0.25 to 32 KB\n0.5 KB\nd\ne\np\np\na\nm\ny\nr\no\nm\ne\nM\nd\ne\np\np\na\nm\ny\nr\no\nm\ne\nM\ne\nd\no\nc\np\nO\ne\nd\no\nc\np\nO\nO\n/I\nInteger registers (size, model, \nnumber)\n16 GPR \u00d7 32 bits\n8 dedicated  \ndata \u00d7 16 bits\n8 data and 8 address  \n\u00d7 32 bits\n15 GPR \u00d7 32 bits\nSeparate floating-point \nregisters\n4 \u00d7 64 bits\nOptional: 8 \u00d7 80 bits Optional: 8 \u00d7 80 bits\n0\nFloating-point format\nIBM (floating  \nhexadecimal)\nIEEE 754 single,  \ndouble, extended\nIEEE 754 single,  \ndouble, extended\nDEC \nFigure K.29 Summary of four 1970s architectures. Unlike the architectures in Figure K.1, there is little agreement\nbetween these architectures in any category. (See Section K.3 for more details on the 80x86 and Section K.4 for a\ndescription of the VAX.)\nK-30\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1300,
        "text": "essentially an extended stack architecture: A complete stack instruction set is\nsupplemented by a limited set of register-memory instructions.\n\u25a0\n1982\u2014The 80286 extended the 8086 architecture by increasing the address\nspace to 24 bits, by creating an elaborate memory mapping and protection\nmodel, and by adding a few instructions to round out the instruction set and\nto manipulate the protection model. Because it was important to run 8086 pro-\ngrams without change, the 80286 offered a real addressing mode to make the\nmachine look just like an 8086.\n\u25a0\n1985\u2014The 80386 extended the 80286 architecture to 32 bits. In addition to a\n32-bit architecture with 32-bit registers and a 32-bit address space, the 80386\nadded new addressing modes and additional operations. The added instructions\nmake the 80386 nearly a general-purpose register machine. The 80386 also\nadded paging support in addition to segmented addressing (see Chapter 2). Like\nthe 80286, the 80386 has a mode to execute 8086 programs without change.\nThis history illustrates the impact of the \u201cgolden handcuffs\u201d of compatibility on\nthe 80x86, as the existing software base at each step was too important to jeopar-\ndize with significant architectural changes. Fortunately, the subsequent 80486 in\n1989, Pentium in 1992, and P6 in 1995 were aimed at higher performance, with\nonly four instructions added to the user-visible instruction set: three to help with\nmultiprocessing plus a conditional move instruction.\nSince 1997 Intel has added hundreds of instructions to support multimedia by\noperating on many narrower data types within a single clock (see Appendix A).\nThese SIMD or vector instructions are primarily used in hand-coded libraries or\ndrivers and rarely generated by compilers. The first extension, called MMX,\nappeared in 1997. It consists of 57 instructions that pack and unpack multiple\nbytes, 16-bit words, or 32-bit double words into 64-bit registers and performs shift,\nlogical, and integer arithmetic on the narrow data items in parallel. It supports both\nsaturating and nonsaturating arithmetic. MMX uses the registers comprising the\nfloating-point stack and hence there is no new state for operating systems to save.\nIn 1999 Intel added another 70 instructions, labeled SSE, as part of Pentium III.\nThe primary changes were to add eight separate registers, double their width to 128\nbits, and add a single-precision floating-point data type. Hence, four 32-bit\nfloating-point operations can be performed in parallel. To improve memory perfor-\nmance, SSE included cache prefetch instructions plus streaming store instructions\nthat bypass the caches and write directly to memory.\nIn 2001, Intel added yet another 144 instructions, this time labeled SSE2. The\nnew data type is double-precision arithmetic, which allows pairs of 64-bit floating-\npoint operations in parallel. Almost all of these 144 instructions are versions of\nexisting MMX and SSE instructions that operate on 64 bits of data in parallel.\nNot only does this change enable multimedia operations, but it also gives the com-\npiler a different target for floating-point operations than the unique stack architec-\nture. Compilers can choose to use the eight SSE registers as floating-point registers\nas found in the RISC machines. This change has boosted performance on the\nPentium 4, the first microprocessor to include SSE2 instructions. At the time of\nK.3\nThe Intel 80x86\n\u25a0\nK-31"
    },
    {
        "page": 1301,
        "text": "announcement, a 1.5 GHz Pentium 4 was 1.24 times faster than a 1 GHz Pentium\nIII for SPECint2000(base), but it was 1.88 times faster for SPECfp2000(base).\nIn 2003 a company other than Intel enhanced the IA-32 architecture this time.\nAMD announced a set of architectural extensions to increase the address space for\n32 to 64 bits. Similar to the transition from 16- to 32-bit address space in 1985 with\nthe 80386, AMD64 widens all registers to 64 bits. It also increases the number of\nregisters to sixteen and has 16 128-bit registers to support XMM, AMD\u2019s answer to\nSSE2. Rather than expand the instruction set, the primary change is adding a new\nmode called long mode that redefines the execution of all IA-32 instructions with\n64-bit addresses. To address the larger number of registers, it adds a new prefix to\ninstructions. AMD64 still has a 32-bit mode that is backwards compatible to the\nstandard Intel instruction set, allowing a more graceful transition to 64-bit addres-\nsing than the HP/Intel Itanium. Intel later followed AMD\u2019s lead, making almost\nidentical changes so that most software can run on either 64-bit address version\nof the 80x86 without change.\nWhatever the artistic failures of the 80x86, keep in mind that there are more\ninstances of this architectural family than of any other server or desktop processor\nin the world. Nevertheless, its checkered ancestry has led to an architecture that is\ndifficult to explain and impossible to love.\nWe start our explanation with the registers and addressing modes, move on to\nthe integer operations, then cover the floating-point operations, and conclude with\nan examination of instruction encoding.\n80x86 Registers and Data Addressing Modes\nThe evolution of the instruction set can be seen in the registers of the 80x86\n(Figure K.30). Original registers are shown in black type, with the extensions of\nthe 80386 shown in a lighter shade, a coloring scheme followed in subsequent fig-\nures. The 80386 basically extended all 16-bit registers (except the segment regis-\nters) to 32 bits, prefixing an \u201cE\u201d to their name to indicate the 32-bit version. The\narithmetic, logical, and data transfer instructions are two-operand instructions that\nallow the combinations shown in Figure K.31.\nTo explain the addressing modes, we need to keep in mind whether we are talk-\ning about the 16-bit mode used by both the 8086 and 80286 or the 32-bit mode\navailable on the 80386 and its successors. The seven data memory addressing\nmodes supported are\n\u25a0\nAbsolute\n\u25a0\nRegister indirect\n\u25a0\nBased\n\u25a0\nIndexed\n\u25a0\nBased indexed with displacement\n\u25a0\nBased with scaled indexed\n\u25a0\nBased with scaled indexed and displacement\nK-32\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1302,
        "text": "FPR 0\nFPR 1\nFPR 2\nFPR 3\nFPR 4\nFPR 5\nFPR 6\nFPR 7\n0\n79\n0\n15\n0\n15\n8\n7\n31\nGPR 0\nAccumulator\nEAX\nAX\nAH\nAL\nGPR 3\nBase addr. reg\nEBX\nBX\nBH\nBL\nGPR 1\nCount reg: string, loop\nECX\nCX\nCH\nCL\nGPR 2\nData reg: multiply, divide\nEDX\nDX\nDH\nDL\nGPR 6\nESI\nIndex reg, string source ptr.\nSI\nCode segment ptr.\nCS\nStack  segment ptr. (top of stack)\nSS\nData segment ptr.\nDS\nExtra data segment ptr. \nES\nData segment ptr. 2\nFS\nData segment ptr. 3\nGS\nGPR 7\nEDI\nIndex reg, string dest. ptr.\nDI\nGPR 5\nEBP\nBase ptr. (for base of stack seg.)\nBP\nPC\nGPR 4\nESP\nStack ptr.\nSP\nEIP\nInstruction ptr. (PC)\nIP\nEFLAGS\nCondition codes\nFLAGS\n Top of FP stack,\n FP condition codes\nStatus\n80x86, 80x286\n80x386, 80x486, Pentium\nFigure K.30 The 80x86 has evolved over time, and so has its register set. The original set is shown in black and the\nextended set in gray. The 8086 divided the first four registers in half so that they could be used either as one 16-bit\nregister or as two 8-bit registers. Starting with the 80386, the top eight registers were extended to 32 bits and could\nalso be used as general-purpose registers. The floating-point registers on the bottom are 80 bits wide, and although\nthey look like regular registers they are not. They implement a stack, with the top of stack pointed to by the status\nregister. One operand must be the top of stack, and the other can be any of the other seven registers below the top\nof stack.\nK.3\nThe Intel 80x86\n\u25a0\nK-33"
    },
    {
        "page": 1303,
        "text": "Displacements can be 8 or 32 bits in 32-bit mode, and 8 or 16 bits in 16-bit mode.\nIf we count the size of the address as a separate addressing mode, the total is 11\naddressing modes.\nAlthough a memory operand can use any addressing mode, there are restric-\ntions on what registers can be used in a mode. The section \u201c80x86 Instruction\nEncoding\u201d on page K-11 gives the full set of restrictions on registers, but the fol-\nlowing description of addressing modes gives the basic register options:\n\u25a0\nAbsolute\u2014With 16-bit or 32-bit displacement, depending on the mode.\n\u25a0\nRegister indirect\u2014BX, SI, DI in 16-bit mode and EAX, ECX, EDX, EBX,\nESI, and EDI in 32-bit mode.\n\u25a0\nBased mode with 8-bit or 16-bit/32-bit displacement\u2014BP, BX, SI, and DI\nin 16-bit mode and EAX, ECX, EDX, EBX, ESI, and EDI in 32-bit mode.\nThe displacement is either 8 bits or the size of the address mode: 16 or 32 bits.\n(Intel gives two different names to this single addressing mode, based and\nindexed, but they are essentially identical and we combine them. This book\nuses indexed addressing to mean something different, explained next.)\n\u25a0\nIndexed\u2014The address is the sum of two registers. The allowable combinations\nare BX+SI, BX+DI, BP+SI, and BP+DI. This mode is called based\nindexed on the 8086. (The 32-bit mode uses a different addressing mode to\nget the same effect.)\n\u25a0\nBased indexed with 8- or 16-bit displacement\u2014The address is the sum of dis-\nplacement and contents of two registers. The same restrictions on registers\napply as in indexed mode.\n\u25a0\nBase plus scaled indexed\u2014This addressing mode and the next were added in\nthe 80386 and are only available in 32-bit mode. The address calculation is\nBase register + 2Scale \u0003Index\u0003register\nSource/destination operand type\nSecond source operand\nretsig\ne\nR\netaid\ne\nm\nm\nI\ny\nr\no\nm\ne\nM\nretsig\ne\nR\netaid\ne\nm\nm\nI\nretsig\ne\nR\nretsig\ne\nR\nretsig\ne\nR\ny\nr\no\nm\ne\nM\ny\nr\no\nm\ne\nM\nFigure K.31 Instruction types for the arithmetic, logical, and data transfer instruc-\ntions. The 80x86 allows the combinations shown. The only restriction is the absence\nof a memory-memory mode. Immediates may be 8, 16, or 32 bits in length; a register\nis any one of the 14 major registers in Figure K.30 (not IP or FLAGS).\nK-34\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1304,
        "text": "where Scale has the value 0, 1, 2, or 3; Index register can be any of the eight\n32-bit general registers except ESP; and Base register can be any of the eight\n32-bit general registers.\n\u25a0\nBase plus scaled index with 8- or 32-bit displacement\u2014The address is the sum\nof the displacement and the address calculated by the scaled mode immediately\nabove. The same restrictions on registers apply.\nThe 80x86 uses Little Endian addressing.\nIdeally, we would refer discussion of 80x86 logical and physical addresses to\nChapter 2, but the segmented address space prevents us from hiding that infor-\nmation. Figure K.32 shows the memory mapping options on the generations of\n80x86 machines; Chapter 2 describes the segmented protection scheme in greater\ndetail.\nThe assembly language programmer clearly must specify which segment reg-\nister should be used with an address, no matter which address mode is used. To\nsave space in the instructions, segment registers are selected automatically depend-\ning on which address register is used. The rules are simple: References to instruc-\ntions (IP) use the code segment register (CS), references to the stack (BP or SP)\nuse the stack segment register (SS), and the default segment register for the other\nregisters is the data segment register (DS). The next section explains how they can\nbe overridden.\n80x86 Integer Operations\nThe 8086 provides support for both 8-bit (byte) and 16-bit (called word) data\ntypes. The data type distinctions apply to register operations as well as memory\naccesses. The 80386 adds 32-bit addresses and data, called double words. Almost\nevery operation works on both 8-bit data and one longer data size. That size is\ndetermined by the mode and is either 16 or 32 bits.\nClearly some programs want to operate on data of all three sizes, so the 80x86\narchitects provide a convenient way to specify each version without expanding\ncode size significantly. They decided that most programs would be dominated\nby either 16- or 32-bit data, and so it made sense to be able to set a default large\nsize. This default size is set by a bit in the code segment register. To override the\ndefault size, an 8-bit prefix is attached to the instruction to tell the machine to use\nthe other large size for this instruction.\nThe prefix solution was borrowed from the 8086, which allows multiple prefixes\nto modify instruction behavior. The three original prefixes override the default seg-\nment register, lock the bus so as to perform a semaphore (see Chapter 5), or repeat the\nfollowing instruction until CX counts down to zero. This last prefix was intended to\nbe paired with a byte move instruction to move a variable number of bytes. The\n80386 also added a prefix to override the default address size.\nK.3\nThe Intel 80x86\n\u25a0\nK-35"
    },
    {
        "page": 1305,
        "text": "The 80x86 integer operations can be divided into four major classes:\n1. Data movement instructions, including move, push, and pop\n2. Arithmetic and logic instructions, including logical operations, test, shifts, and\ninteger and decimal arithmetic operations\n3. Control flow, including conditional branches and unconditional jumps, calls,\nand returns\n4. String instructions, including string move and string compare\nOffset\nSegment\n16\n32\n32\n32\n32\n20\n20\n20\n10\n10\n12\nPhysical address\nPhysical address\nLinear address\nLogical address\nPaging\nSegmentation\nOffset\nSegment\n16\n16\n24\n24\nLogical address\nOffset\nSegment\n16\nPhysical address\n12\n4\n16\n20\nLogical address\nSegmentation\ne\nd\no\nm\n \nd\ne\nt\nc\ne\nt\no\nr\nP\ne\nd\no\nm\n l\na\ne\nR\n)\n6\n8\n2\n0\n8\n(\n)\n6\n8\n0\n8\n(\n(80386, 80486, Pentium)\nFigure K.32 The original segmented scheme of the 8086 is shown on the left. All 80x86 processors support this style\nof addressing, called real mode. It simply takes the contents of a segment register, shifts it left 4 bits, and adds it to the\n16-bit offset, forming a 20-bit physical address. The 80286 (center) used the contents of the segment register to select\na segment descriptor, which includes a 24-bit base address among other items. It is added to the 16-bit offset to form\nthe 24-bit physical address. The 80386 and successors (right) expand this base address in the segment descriptor to\n32 bits and also add an optional paging layer below segmentation. A 32-bit linear address is first formed from the\nsegment and offset, and then this address is divided into two 10-bit fields and a 12-bit page offset. The first 10-bit field\nselects the entry in the first-level page table, and then this entry is used in combination with the second 10-bit field to\naccess the second-level page table to select the upper 20 bits of the physical address. Prepending this 20-bit address\nto the final 12-bit field gives the 32-bit physical address. Paging can be turned off, redefining the 32-bit linear address\nas the physical address. Note that a \u201cflat\u201d 80x86 address space comes simply by loading the same value in all the\nsegment registers; that is, it doesn\u2019t matter which segment register is selected.\nK-36\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1306,
        "text": "Figure K.33 shows some typical 80x86 instructions and their functions.\nThe data transfer, arithmetic, and logic instructions are unremarkable, except\nthat the arithmetic and logic instruction operations allow the destination to be either\na register or a memory location.\nControl flow instructions must be able to address destinations in another seg-\nment. This is handled by having two types of control flow instructions: \u201cnear\u201d for\nintrasegment (within a segment) and \u201cfar\u201d for intersegment (between segments)\ntransfers. In far jumps, which must be unconditional, two 16-bit quantities follow\nthe opcode in 16-bit mode. One of these is used as the instruction pointer,\nwhile the other is loaded into CS and becomes the new code segment. In 32-\nbit mode the first field is expanded to 32 bits to match the 32-bit program\ncounter (EIP).\nCalls and returns work similarly\u2014a far call pushes the return instruction\npointer and return segment on the stack and loads both the instruction pointer\nand the code segment. A far return pops both the instruction pointer and the code\nsegment from the stack. Programmers or compiler writers must be sure to always\nuse the same type of call and return for a procedure\u2014a near return does not work\nwith a far call, and vice versa.\nString instructions are part of the 8080 ancestry of the 80x86 and are not\ncommonly executed in most programs.\nFigure K.34 lists some of the integer 80x86 instructions. Many of the\ninstructions are available in both byte and word formats.\nn\no\nit\nc\nn\nu\nF\nn\no\nit\nc\nu\nrt\ns\nn\nI\nJE name\nJMP name\nIP\nname\nCALLF name, seg\nSP\nSP\u20132;M[SS:SP]\n \nIP+5;SP\nSP\u20132;\nPUSH SI\nSP\nSP\u20132;M[SS:SP]\nSI\nPOP DI\nDI\nM[SS:SP];SP\nSP+2\nADD AX,#6765\nAX\nAX+6765\nSHL BX,1\nBX\nBX1..15## 0\nTEST DX,#42\nSet CC flags with DX & 42\nMOVSB\nM[ES:DI]\n8M[DS:SI];DI \nDI+1;SI\nSI+1\nMOVW BX,[DI+45]\nBX\n16M[DS:DI+45]\nM[SS:SP]  CS;IP\nname;CS\nseg; \nif equal(CC) {IP\nname};IP\u2013128 \n \n name\n IP+128\nFigure K.33 Some typical 80x86 instructions and their functions. A list of frequent\noperations appears in Figure K.34. We use the abbreviation SR:X to indicate the forma-\ntion of an address with segment register SR and offset X. This effective address corre-\nsponding to SR:X is (SR<<4)+X. The CALLF saves the IP of the next instruction and\nthe current CS on the stack.\nK.3\nThe Intel 80x86\n\u25a0\nK-37"
    },
    {
        "page": 1307,
        "text": "80x86 Floating-Point Operations\nIntel provided a stack architecture with its floating-point instructions: loads push\nnumbers onto the stack, operations find operands in the top two elements of the\nstacks, and stores can pop elements off the stack, just as the stack example in\nFigure A.31 on page A-4 suggests.\ng\nn\ni\nn\na\ne\nM\nn\no\nit\nc\nu\nrt\ns\nn\nI\nControl\nConditional and unconditional branches\nJNZ, JZ\nJump if condition to IP + 8-bit offset; JNE (for JNZ) and JE (for JZ) are alternative names\nJMP, JMPF\nUnconditional jump\u20148- or 16-bit offset intrasegment (near) and intersegment (far) versions\nCALL, CALLF\nSubroutine call\u201416-bit offset; return address pushed; near and far versions\nRET, RETF\nPops return address from stack and jumps to it; near and far versions\nLOOP\nLoop branch\u2014decrement CX; jump to IP + 8-bit displacement if CX \u00a6 0\nData transfer\nMove data between registers or between register and memory\nMOV\nMove between two registers or between register and memory\nPUSH\nPush source operand on stack\nPOP\nPop operand from stack top to a register\nLES\nLoad ES and one of the GPRs from memory\nArithmetic/logical\nArithmetic and logical operations using the data registers and memory\nADD\nAdd source to destination; register-memory format\nSUB\nSubtract source from destination; register-memory format\nCMP\nCompare source and destination; register-memory format\nSHL\nShift left\nSHR\nShift logical right\nRCR\nRotate right with carry as fill\nCBW\nConvert byte in AL to word in AX\nTEST\nLogical AND of source and destination sets flags\nINC\nIncrement destination; register-memory format\nDEC\nDecrement destination; register-memory format\nOR\nLogical OR; register-memory format\nXOR\nExclusive OR; register-memory format\nString instructions\nMove between string operands; length given by a repeat prefix\nMOVS\nCopies from string source to destination; may be repeated\nLODS\nLoads a byte or word of a string into the A register\nFigure K.34 Some typical operations on the 80x86. Many operations use register-memory format, where either the\nsource or the destination may be memory and the other may be a register or immediate operand.\nK-38\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1308,
        "text": "Intel supplemented this stack architecture with instructions and addressing\nmodes that allow the architecture to have some of the benefits of a register-memory\nmodel. In addition to finding operands in the top two elements of the stack, one\noperand can be in memory or in one of the seven registers below the top of the stack.\nThis hybrid is still a restricted register-memory model, however, in that loads\nalways move data to the top of the stack while incrementing the top of stack pointer\nand stores can only move the top of stack to memory. Intel uses the notation ST to\nindicate thetop of stack,and ST(i) to representthe ith registerbelow thetop of stack.\nOne novel feature of this architecture is that the operands are wider in the reg-\nister stack than they are stored in memory, and all operations are performed at this\nwide internal precision. Numbers are automatically converted to the internal 80-bit\nformat on a load and converted back to the appropriate size on a store. Memory\ndata can be 32-bit (single-precision) or 64-bit (double-precision) floating-point\nnumbers, called real by Intel. The register-memory version of these instructions\nwill then convert the memory operand to this Intel 80-bit format before performing\nthe operation. The data transfer instructions also will automatically convert 16- and\n32-bit integers to reals, and vice versa, for integer loads and stores.\nThe 80x86 floating-point operations can be divided into four major classes:\n1. Data movement instructions, including load, load constant, and store\n2. Arithmetic instructions, including add, subtract, multiply, divide, square root,\nand absolute value\n3. Comparison, including instructions to send the result to the integer CPU so that\nit can branch\n4. Transcendental instructions, including sine, cosine, log, and exponentiation\nFigure K.35 shows some of the 60 floating-point operations. We use the curly\nbrackets {} to show optional variations of the basic operations: {I} means there\nis an integer version of the instruction, {P} means this variation will pop one\noperand off the stack after the operation, and {R} means reverse the sense of\nthe operands in this operation.\nNot all combinations are provided. Hence,\nF{I}SUB{R}{P}\nrepresents these instructions found in the 80x86:\nFSUB\nFISUB\nFSUBR\nFISUBR\nFSUBP\nFSUBRP\nK.3\nThe Intel 80x86\n\u25a0\nK-39"
    },
    {
        "page": 1309,
        "text": "There are no pop or reverse pop versions of the integer subtract instructions.\nNote that we get even more combinations when including the operand modes\nfor these operations. The floating-point add has these options, ignoring the integer\nand pop versions of the instruction:\nFADD\nBoth operands are in the in stack, and the result replaces the top of\nstack.\nFADD\nST(i)\nOne source operand is ith register below the top of stack, and the\nresult replaces the top of stack.\nFADD\nST(i),ST\nOne source operand is the top of stack, and the result replaces ith\nregister below the top of stack.\nFADD\nmem32\nOne source operand is a 32-bit location in memory, and the result\nreplaces the top of stack.\nFADD\nmem64\nOne source operand is a 64-bit location in memory, and the result\nreplaces the top of stack.\nAs mentioned earlier SSE2 presents a model of IEEE floating-point registers.\n80x86 Instruction Encoding\nSaving the worst for last, the encoding of instructions in the 8086 is complex, with\nmany different instruction formats. Instructions may vary from 1 byte, when there\nare no operands, to up to 6 bytes, when the instruction contains a 16-bit immediate\ne\nr\na\np\nm\no\nC\ncit\ne\nm\nh\ntir\nA\nr\ne\nfs\nn\na\nrt\na\nt\na\nD\nTranscendental\nF{I}LD mem/ST(i)\nF{I}ADD{P}mem/ST(i)\nF{I}COM{P}{P}\nFPATAN\nF{I}ST{P} mem/ST(i)\nF{I}SUB{R}{P}mem/ST(i)\nF{I}UCOM{P}{P}\nF2XM1\nS\nO\nC\nF\nFSTSW AX/mem\nI}MUL{P}mem/ST(i)\n{\nF\nI\nP\nD\nL\nF\nN\nA\nT\nP\nF\nF{I}DIV{R}{P}mem/ST(i) \n1\nD\nL\nF\nM\nE\nR\nP\nF\nT\nR\nQ\nS\nF\nZ\nD\nL\nF\nIN\nS\nF\nS\nB\nA\nF\nX\n2\nL\nY\nF\nFRNDINT\nFigure K.35 The floating-point instructions of the 80x86. The first column shows the data transfer instructions,\nwhich move data to memory or to one of the registers below the top of the stack. The last three operations push\nconstants on the stack: pi, 1.0, and 0.0. The second column contains the arithmetic operations described above. Note\nthat the last three operate only on the top of stack. The third column is the compare instructions. Since there are no\nspecial floating-point branch instructions, the result of the compare must be transferred to the integer CPU via the\nFSTSW instruction, either into the AX register or into memory, followed by an SAHF instruction to set the condition\ncodes. The floating-point comparison can then be tested using integer branch instructions. The final column gives the\nhigher-level floating-point operations.\nK-40\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1310,
        "text": "and uses 16-bit displacement addressing. Prefix instructions increase 8086 instruc-\ntion length beyond the obvious sizes.\nThe 80386 additions expand the instruction size even further, as Figure K.36\nshows. Both the displacement and immediate fields can be 32 bits long, two more\nprefixes are possible, the opcode can be 16 bits long, and the scaled index mode spec-\nifier adds another 8 bits. The maximum possible 80386 instruction is 17 bytes long.\nFigure K.37 shows the instruction format for several of the example instruc-\ntions in Figure K.33. The opcode byte usually contains a bit saying whether the\noperand is a byte wide or the larger size, 16 bits or 32 bits depending on the mode.\nFor some instructions, the opcode may include the addressing mode and the reg-\nister; this is true in many instructions that have the form register\n register op immediate. Other instructions use a \u201cpostbyte\u201d or extra\nopcode byte, labeled \u201cmod, reg, r/m\u201d in Figure K.36, which contains the addres-\nsing mode information. This postbyte is used for many of the instructions that\naddress memory. The based with scaled index uses a second postbyte, labeled\n\u201csc, index, base\u201d in Figure K.36.\nThe floating-point instructions are encoded in the escape opcode of the 8086\nand the postbyte address specifier. The memory operations reserve 2 bits to decide\nSeg. override\nOpcode\nmod, reg, r/m\nDisp8\nDisp16\nDisp24\nImm8\nImm16\nDisp32\nImm24\nImm32\nOpcode ext.\nsc, index, base\nAddr. override\nSize override\nPrefixes\nAddress\nspecifiers\nDisplacement\nImmediate\nOpcode\nRepeat\nLock\nFigure K.36 The instruction format of the 8086 (black type) and the extensions for\nthe 80386 (shaded type). Every field is optional except the opcode.\nK.3\nThe Intel 80x86\n\u25a0\nK-41"
    },
    {
        "page": 1311,
        "text": "whether the operand is a 32- or 64-bit real or a 16- or 32-bit integer. Those same 2\nbits are used in versions that do not access memory to decide whether the stack\nshould be popped after the operation and whether the top of stack or a lower reg-\nister should get the result.\nAlas, you cannot separate the restrictions on registers from the encoding of the\naddressing modes in the 80x86. Hence, Figures K.38 and K.39 show the encoding\nof the two postbyte address specifiers for both 16- and 32-bit mode.\nJE\na.  JE PC + displacement\nr\ne\nb\nm\nu\nn\n t\nn\ne\nm\ng\ne\nS\nF\nL\nL\nA\nC\nOffset\nb.  CALLF\nc.  MOV  BX, [DI + 45]\nPUSH\nd.  PUSH SI\nADD\nw\ne.  ADD AX, #6765\nSHL\nr-r\npostbyte\nv/w\nf.  SHL BX, 1\ng.  TEST DX, #42\nReg\n4\n4\n8\n6\n8\n8\n6\n1\n6\n1\n8\n2\n5\n3\n4\n1\n6\n1\n3\nConstant\n6\n2\n8\n7\n1\n8\n8\nCondition\nDisplacement\nt\nn\ne\nm\ne\nc\nalp\nsi\nD\nw\n/\nd\nV\nO\nM\nr-m\npostbyte\nTEST\nPostbyte\nImmediate\nw\nReg\nFigure K.37 Typical 8086 instruction formats. The encoding of the postbyte is shown\nin Figure K.38. Many instructions contain the 1-bit field w, which says whether the oper-\nation is a byte or a word. Fields of the form v/w or d/w are a d-field or v-field followed by\nthe w-field. The d-field in MOV is used in instructions that may move to or from memory\nand shows the direction of the move. The field v in the SHL instruction indicates a\nvariable-length shift; variable-length shifts use a register to hold the shift count. The\nADD instruction shows a typical optimized short encoding usable only when the first\noperand is AX. Overall instructions may vary from 1 to 6 bytes in length.\nK-42\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1312,
        "text": "w = 1\nmod = 0\nmod = 1\nmod = 2\nreg\nw = 0 16b 32b r/m\n16b\n32b\n16b\n32b\n16b\n32b\nmod = 3 \n0\nA L\nA X\nEAX\n0\n1\nC L\nC X\nECX\n1\n2\nD L\nD X\nEDX\n2\n3\nB L\nB X\nEBX\n3\n4\nA H\nSP\nESP\n4\nSI+disp16 (sib)+disp8\n\"\n5\nC H\nB P\nEBP\n5\nDI+disp8\nEBP+disp8\nDI+disp16\n\"\n6\nD H\nSI\nESI\n6\nBP+disp8\nESI+disp8\n\"\n7\nB H\nD I\nEDI\n7\na\naddr=BX+SI\naddr=BX+DI\naddr=BP+SI\naddr=BP+SI\naddr=SI\nddr=DI\naddr=disp16\naddr=BX\n=ED X\n=EBX\n=(si)b\n=disp32\n=ESI\n=ED I\nBX+disp8\nEDI+disp8\nSI+disp8\nBP+disp16\nBX+disp16\n(sib)+disp32\nEBP+disp32\nESI+disp32\nEDI+disp32\n\"\nsame\nsame\nsame\nsame\nsame\n=ECX\n=EAX\naddr  as \naddr as \naddr as \naddr as \nas\nmod= 0\nmod= 0\nmod= 0\nmod= 0\nreg\n+ disp 8\n+ disp 8\n+ disp1 6\n+ disp3 2\nfield\nFigure K.38 The encoding of the first address specifier of the 80x86, mod, reg, r/m. The first four columns show the\nencoding of the 3-bit reg field, which depends on the w bit from the opcode and whether the machine is in 16- or 32-\nbit mode. The remaining columns explain the mod and r/m fields. The meaning of the 3-bit r/m field depends on the\nvalue in the 2-bit mod field and the address size. Basically, the registers used in the address calculation are listed in the\nsixth and seventh columns, under mod \u00bc 0, with mod \u00bc 1 adding an 8-bit displacement and mod \u00bc 2 adding a 16- or\n32-bit displacement, depending on the address mode. The exceptions are r/m \u00bc 6 when mod \u00bc 1 or mod \u00bc 2 in 16-bit\nmode selects BP plus the displacement; r/m \u00bc 5 when mod \u00bc 1 or mod \u00bc 2 in 32-bit mode selects EBP plus displace-\nment; and r/m \u00bc 4 in 32-bit mode when mod \u00a63 (sib) means use the scaled index mode shown in Figure K.39. When\nmod \u00bc 3, the r/m field indicates a register, using the same encoding as the reg field combined with the w bit.\ne\ns\na\nB\nx\ne\nd\nn\nI\nX\nA\nE\nX\nA\nE\n0\nX\nC\nE\nX\nC\nE\n1\nX\nD\nE\nX\nD\nE\n2\nX\nB\nE\nX\nB\nE\n3\nP\nS\nE\nx\ne\nd\nni\no\nN\n4\n2\n3\np\nsid\n,0\n=\nd\no\nm\nfI\nP\nB\nE\n5\nIf mod \u00a6 0, EBP\nI\nS\nE\nI\nS\nE\n6\nI\nD\nE\nI\nD\nE\n7\nFigure K.39 Based plus scaled index mode address specifier found in the 80386. This\nmode is indicated by the (sib) notation in Figure K.38. Note that this mode expands the\nlist of registers to be used in other modes: Register indirect using ESP comes from Scale\n\u00bc 0, Index \u00bc 4, and Base \u00bc 4, and base displacement with EBP comes from Scale \u00bc 0,\nIndex \u00bc 5, and mod \u00bc 0. The two-bit scale field is used in this formula of the effective\naddress: Base register + 2Scale \u0003 Index register."
    },
    {
        "page": 1313,
        "text": "Putting It All Together: Measurements\nof Instruction Set Usage\nIn this section, we present detailed measurements for the 80x86 and then compare\nthe measurements to MIPS for the same programs. To facilitate comparisons\namong dynamic instruction set measurements, we use a subset of the SPEC92 pro-\ngrams. The 80x86 results were taken in 1994 using the Sun Solaris FORTRAN and\nC compilers V2.0 and executed in 32-bit mode. These compilers were comparable\nin quality to the compilers used for MIPS.\nRemember that these measurements depend on the benchmarks chosen and the\ncompiler technology used. Although we feel that the measurements in this section\nare reasonably indicative of the usage of these architectures, other programs may\nbehave differently from any of the benchmarks here, and different compilers may\nyield different results. In doing a real instruction set study, the architect would want\nto have a much larger set of benchmarks, spanning as wide an application range as\npossible, and consider the operating system and its usage of the instruction set.\nSingle-user benchmarks like those measured here do not necessarily behave in\nthe same fashion as the operating system.\nWe start with an evaluation of the features of the 80x86 in isolation, and later\ncompare instruction counts with those of DLX.\nMeasurements of 80x86 Operand Addressing\nWe start with addressing modes. Figure K.40 shows the distribution of the operand\ntypes in the 80x86. These measurements cover the \u201csecond\u201d operand of the oper-\nation; for example,\nmov EAX, [45]\ncounts as a single memory operand. If the types of the first operand were counted,\nthe percentage of register usage would increase by about a factor of 1.5.\nThe 80x86 memory operands are divided into their respective addressing\nmodes in Figure K.41. Probably the biggest surprise is the popularity of the\nInteger average\nFP average\n%\n5\n4\nretsig\ne\nR\n%\n6\n1\netaid\ne\nm\nm\nI\n%\n2\n2\n%\n6\n%\n2\n7\n%\n9\n3\ny\nr\no\nm\ne\nM\nFigure K.40 Operand type distribution for the average of five SPECint92 programs\n(compress, eqntott, espresso, gcc, li) and the average of five SPECfp92 programs\n(doduc, ear, hydro2d, mdljdp2, su2cor).\nK-44\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1314,
        "text": "addressing modes added by the 80386, the last four rows of the figure. They\naccount for about half of all the memory accesses. Another surprise is the popu-\nlarity of direct addressing. On most other machines, the equivalent of the direct\naddressing mode is rare. Perhaps the segmented address space of the 80x86 makes\ndirect addressing more useful, since the address is relative to a base address from\nthe segment register.\nThese addressing modes largely determine the size of the Intel instructions.\nFigure K.42 shows the distribution of instruction sizes. The average number of\nbytes per instruction for integer programs is 2.8, with a standard deviation of\n1.5, and 4.1 with a standard deviation of 1.9 for floating-point programs. The dif-\nference in length arises partly from the differences in the addressing modes: Integer\nprograms rely more on the shorter register indirect and 8-bit displacement addres-\nsing modes, while floating-point programs more frequently use the 80386 addres-\nsing modes with the longer 32-bit displacements.\nGiven that the floating-point instructions have aspects of both stacks and reg-\nisters, how are they used? Figure K.43 shows that, at least for the compilers used in\nthis measurement, the stack model of execution is rarely followed. (See Section L.3\nfor a historical explanation of this observation.)\nFinally, Figures K.44 and K.45 show the instruction mixes for 10 SPEC92\nprograms.\nComparative Operation Measurements\nFigures K.46 and K.47 show the number of instructions executed for each of the 10\nprograms on the 80x86 and the ratio of instruction execution compared with that\nAddressing mode\nInteger average\nFP average\n%\n3\n1\ntc\nerid\nni\nretsig\ne\nR\n%\n1\n3\n.p\nsid\ntib\n-\n8\n+\ne\ns\na\nB\n%\n9\n.p\nsid\ntib\n-\n2\n3\n+\ne\ns\na\nB\n%\n0\nd\ne\nx\ne\nd\nn\nI\n%\n0\n.p\nsid\ntib\n-\n8\n+\nd\ne\nx\ne\nd\nni\nd\ne\ns\na\nB\n%\n0\n.p\nsid\ntib\n-\n2\n3\n+\nd\ne\nx\ne\nd\nni\nd\ne\ns\na\nB\n%\n2\n2\nd\ne\nx\ne\nd\nni\nd\nela\nc\ns\n+\ne\ns\na\nB\nBase + scaled indexed + 8-bit disp.\n0%\nBase + scaled indexed + 32-bit disp.\n4%\n%\n3\n%\n5\n1\n%\n5\n2\n%\n0\n%\n0\n%\n1\n%\n7\n8%\n4%\n%\n7\n3\n%\n0\n2\ntc\nerid\ntib\n-\n2\n3\nFigure K.41 Operand addressing mode distribution by program. This chart does not\ninclude addressing modes used by branches or control instructions.\nK.3\nThe Intel 80x86\n\u25a0\nK-45"
    },
    {
        "page": 1315,
        "text": "for DLX: Numbers less than 1.0 mean that the 80x86 executes fewer instructions\nthan DLX. The instruction count is surprisingly close to DLX for many integer\nprograms, as you would expect a load-store instruction set architecture like\nDLX to execute more instructions than a register-memory architecture like the\n80x86. The floating-point programs always have higher counts for the 80x86,\ndoduc\near\nhydro2d\nmdljdp2\nsu2cor\nFP average\nStack (2nd operand ST (1\nRegister (2nd operand ST(i), i \n))\n1.1%\n0.0%\n0.0%\n0.2%\n0.6%\n0.4%\n> 1)\n17.3%\n63.4%\n14.2%\n7.1%\n30.7%\n26.5%\n%\n1.3\n7\n%\n7.8\n6\n%\n7.2\n9\n%\n8.5\n8\n%\n6.6\n3\n%\n6.1\n8\ny\nr\no\nm\ne\nM\nOption\nFigure K.43 The percentage of instructions for the floating-point operations (add, sub, mul, div) that use each of\nthe three options for specifying a floating-point operand on the 80x86. The three options are (1) the strict stack\nmodel of implicit operands on the stack, (2) register version naming an explicit operand that is not one of the top two\nelements of the stack, and (3) memory operand.\nPercentage of instructions at each length\nInstruction lengths\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n4%\n2%\nFloating-point\naverage\nInteger average\n8%\n39%\n4%\n6%\n7%\n5%\n18%\n25%\n19%\n40%\n10%\n14%\n0%\n20%\n40%\n60%\nFigure K.42 Averages of the histograms of 80x86 instruction lengths for five SPE-\nCint92 programs and for five SPECfp92 programs, all running in 32-bit mode.\nK-46\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1316,
        "text": "presumably due to the lack of floating-point registers and the use of a stack\narchitecture.\nAnother question is the total amount of data traffic for the 80x86 versus DLX,\nsince the 80x86 can specify memory operands as part of operations while DLX\ncan only access via loads and stores. Figures K.46 and K.47 also show the data\nreads, data writes, and data read-modify-writes for these 10 programs. The total\nInstruction\ndoduc\near\nhydro2d\nmdljdp2\nsu2cor\nFP average\n%\n0\n2\n%\n6.7\n2\n%\n6.7\n2\n%\n0.8\n1\n%\n5.6\n%\n9.8\nd\na\no\nL\n%\n8\n%\n8.7\n%\n8.7\n%\n5.1\n1\n%\n1.3\n%\n4.2\n1\ner\not\nS\n%\n0\n1\n%\n8.8\n%\n8.8\n%\n6.4\n1\n%\n6.6\n%\n4.5\nd\nd\nA\n%\n3\n%\n4.2\n%\n4.2\n%\n3.3\n%\n4.2\n%\n0.1\nb\nu\nS\nMul\n0%\nDiv\n0%\n%\n2\n%\n0.1\n%\n0.1\n%\n8.0\n%\n1.5\n%\n8.1\nera\np\nm\no\nC\nMov reg-reg\n3.2%\n0.1%\n1.8%\n2.3%\n2.3%\n2%\n%\n0\n%\n5.1\n%\n4.0\nm\nm\ni\nd\na\no\nL\nCond. branch\n5.4%\n8.2%\n5.1%\n2.7%\n2.7%\n5%\nUncond branch\n0.8%\n0.4%\n1.3%\n0.3%\n0.3%\n1%\n%\n0\n%\n1.0\n%\n1.0\n%\n6.1\n%\n5.0\nlla\nC\n%\n0\n%\n1.0\n%\n1.0\n%\n6.1\n%\n5.0\ntc\nerid\nni\np\nm\nj,n\nr\nute\nR\n%\n2\n%\n5.2\n%\n5.2\n%\n5.4\n%\n1.1\ntfih\nS\nAND\n0.8%\n0.8%\n0.7%\n1.3%\n1.3%\n1%\nOR\n%\n0\n%\n1.0\n%\n1.0\n%\n1.0\nOther (XOR, not, . . .)\n0%\n%\n4\n1\n%\n6.2\n1\n%\n6.2\n1\n%\n1.9\n%\n5.2\n2\n%\n1.4\n1\nP\nF\nd\na\no\nL\n%\n7\n%\n6.6\n%\n6.6\n%\n1.4\n%\n4.1\n1\n%\n6.8\nP\nF\ner\not\nS\n%\n5\n%\n6.6\n%\n6.6\n%\n4.1\n%\n1.6\n%\n8.5\nP\nF\nd\nd\nA\n%\n3\n%\n9.2\n%\n9.2\n%\n1.3\n%\n7.2\n%\n2.2\nP\nF\nb\nu\nS\n%\n9\n%\n0.2\n1\n%\n0.2\n1\n%\n1.4\n%\n0.8\n%\n9.8\nP\nF\nlu\nM\n%\n0\n%\n2.0\n%\n2.0\n%\n8.0\n%\n1.2\nP\nF\nvi\nD\nCompare FP\n9.4%\n6.9%\n10.8%\n0.5%\n0.5%\n5%\nMov reg-reg FP\n2.5%\n0.8%\n0.3%\n0.8%\n0.8%\n1%\nOther (abs, sqrt, . . .)\n3.9%\n3.8%\n4.1%\n0.8%\n0.8%\n2%\nFigure K.44 80x86 instruction mix for five SPECfp92 programs.\nK.3\nThe Intel 80x86\n\u25a0\nK-47"
    },
    {
        "page": 1317,
        "text": "accesses ratio to DLX of each memory access type is shown in the bottom\nrows, with the read-modify-write counting as one read and one write. The\n80x86 performs about two to four times as many data accesses as DLX for\nfloating-point programs, and 1.25 times as many for integer programs. Finally,\nFigure K.48 shows the percentage of instructions in each category for 80x86\nand DLX.\nInstruction\ncompress\neqntott\nespresso\ngcc (cc1)\nli\nInt. average\n%\n2\n2\n%\n3.3\n2\n%\n9.4\n2\n%\n9.1\n2\n%\n5.8\n1\n%\n8.0\n2\nd\na\no\nL\n%\n2\n1\n%\n7.8\n1\n%\n6.6\n1\n%\n3.8\n%\n2.3\n%\n8.3\n1\ner\not\nS\n%\n8\n%\n1.6\n%\n6.7\n%\n5\n1.8\n%\n8.8\n%\n3.0\n1\nd\nd\nA\n%\n5\n%\n6.3\n%\n9.2\n%\n5.3\n%\n6.0\n1\n%\n0.7\nb\nu\nS\n%\n0\n%\n1.0\nlu\nM\nDiv\n0%\nCompare\n8.2%\n27.7%\n15.3%\n13.5%\n7.7%\n16%\nMov reg-reg\n7.9%\n0.6%\n5.0%\n4.2%\n7.8%\n4%\n%\n0\n%\n4.0\n%\n6.0\n%\n2.0\n%\n5.0\nm\nm\ni\nd\na\no\nL\nCond. branch\n15.5%\n28.6%\n18.9%\n17.4%\n15.4%\n20%\nUncond. branch\n1.2%\n0.2%\n0.9%\n2.2%\n2.2%\n1%\n%\n1\n%\n2.3\n%\n5.1\n%\n7.0\n%\n4.0\n%\n5.0\nlla\nC\nReturn, jmp indirect\n0.5%\n0.4%\n0.7%\n1.5%\n3.2%\n1%\n%\n1\n%\n7.1\n%\n5.2\n%\n8.3\ntfih\nS\nAND\n8.4%\n1.0%\n8.7%\n4.5%\n8.4%\n6%\nOR\n%\n1\n%\n4.0\n%\n4.0\n%\n7.2\n%\n6.0\nOther (XOR\n%\n1\n%\n1.0\n%\n2.2\n%\n9.0\n)...,to\nn\n,\nLoad FP\n0%\nStore FP\n0%\nAdd FP\n0%\nSub FP\n0%\nMul FP\n0%\nDiv FP\n0%\nCompare FP\n0%\nMov reg-reg FP\n0%\nOther (abs, sqrt, . . .)\n0%\nFigure K.45 80x86 instruction mix for five SPECint92 programs.\nK-48\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1318,
        "text": "Concluding Remarks\nBeauty is in the eye of the beholder.\nOld Adage\nAs we have seen, \u201corthogonal\u201d is not a term found in the Intel architectural dictio-\nnary. To fully understand which registers and which addressing modes are avail-\nable, you need to see the encoding of all addressing modes and sometimes the\nencoding of the instructions.\ncompress\neqntott\nespresso\ngcc (cc1)\nli\nInt. avg.\nInstructions executed on 80x86 (millions)\n2226\n1203\n2216\n3770\n5020\nInstructions executed ratio to DLX\n0.61\n1.74\n0.85\n0.96\n0.98\n1.03\nData reads on 80x86 (millions)\n589\n229\n622\n1079\n1459\nData writes on 80x86 (millions)\n311\n39\n191\n661\n981\nData read-modify-writes on 80x86 (millions)\n26\n1\n129\n48\n48\nTotal data reads on 80x86 (millions)\n615\n230\n751\n1127\n1507\n0\n1.1\n4\n9.0\n5\n2.1\n8\n3.1\n9\n0.1\n5\n8.0\nX\nL\nD\not\noitar\nd\na\ner\nata\nD\nTotal data writes on 80x86 (millions)\n338\n40\n319\n709\n1029\n5\n1.3\n0\n2.1\n5\n2.1\n9\n3.2\n6\n2.9\n7\n6.1\nX\nL\nD\not\noitar\netir\nw\nata\nD\nTotal data accesses on 80x86 (millions)\n953\n269\n1070\n1836\n2536\n3\n2.1\n3\n0.1\n5\n2.1\n8\n5.1\n5\n2.1\n3\n0.1\nX\nL\nD\not\noitar\nss\ne\nc\nc\na\nata\nD\nFigure K.46 Instructions executed and data accesses on 80x86 and ratios compared to DLX for five SPECint92\nprograms.\ndoduc\near\nhydro2d\nmdljdp2\nsu2cor\nFP average\nInstructions executed on 80x86 (millions)\n1223\n15,220\n13,342\n6197\n6197\nInstructions executed ratio to DLX\n1.19\n1.19\n2.53\n2.09\n1.62\n1.73\nData reads on 80x86 (millions)\n515\n6007\n5501\n3696\n3643\nData writes on 80x86 (millions)\n260\n2205\n2085\n892\n892\nData read-modify-writes on 80x86 (millions)\n1\n0\n189\n124\n124\nTotal data reads on 80x86 (millions)\n517\n6007\n5690\n3820\n3767\n1\n5.3\n1\n9.3\n7\n7.4\n8\n4.4\n6\n3.2\n4\n0.2\nX\nL\nD\not\noitar\nd\na\ner\nata\nD\nTotal data writes on 80x86 (millions)\n261\n2205\n2274\n1015\n1015\n3\nX\nL\nD\not\noitar\netir\nw\nata\nD\n.68\n33.25\n38.74\n16.74\n9.35\n20.35\nTotal data accesses on 80x86 (millions)\n778\n8212\n7965\n4835\n4782\n5\n3.4\n7\n4.4\n3\n7.5\n9\n9.5\n4\n1.3\n0\n4.2\nX\nL\nD\not\noitar\nss\ne\nc\nc\na\nata\nD\nFigure K.47 Instructions executed and data accesses for five SPECfp92 programs on 80x86 and ratio to DLX.\nK.3\nThe Intel 80x86\n\u25a0\nK-49"
    },
    {
        "page": 1319,
        "text": "Some argue that the inelegance of the 80x86 instruction set is unavoidable, the\nprice that must be paid for rampant success by any architecture. We reject that\nnotion. Obviously, no successful architecture can jettison features that were added\nin previous implementations, and over time some features may be seen as unde-\nsirable. The awkwardness of the 80x86 began at its core with the 8086 instruction\nset and was exacerbated by the architecturally inconsistent expansions of the 8087,\n80286, and 80386.\nA counterexample is the IBM 360/370 architecture, which is much older than\nthe 80x86. It dominates the mainframe market just as the 80x86 dominates the PC\nmarket. Due undoubtedly to a better base and more compatible enhancements, this\ninstruction set makes much more sense than the 80x86 more than 30 years after its\nfirst implementation.\nFor better or worse, Intel had a 16-bit microprocessor years before its compet-\nitors\u2019 more elegant architectures, and this head start led to the selection of the 8086\nas the CPU for the IBM PC. What it lacks in style is made up in quantity, making\nthe 80x86 beautiful from the right perspective.\nThe saving grace of the 80x86 is that its architectural components are not too\ndifficult to implement, as Intel has demonstrated by rapidly improving perfor-\nmance of integer programs since 1978. High floating-point performance is a larger\nchallenge in this architecture.\nK.4\nThe VAX Architecture\nVAX: the most successful minicomputer design in industry history . . . the VAX was\nprobably the hacker\u2019s favorite machine . . . . Especially noted for its large,\nassembler-programmer-friendly instruction set\u2014an asset that became a liability\nafter the RISC revolution.\nEric Raymond\nThe New Hacker\u2019s Dictionary (1991)\nInteger average \nFP average\nCategory\nx86\nDLX\nx86\nDLX\nTotal data transfer\n34%\n36%\n28%\n2%\nTotal integer arithmetic\n34%\n31%\n16%\n12%\nTotal control\n24%\n20%\n6%\n10%\nTotal logical\n8%\n13%\n3%\n2%\nTotal FP data transfer\n0%\n0%\n22%\n33%\nTotal FP arithmetic\n0%\n0%\n25%\n41%\nFigure K.48 Percentage of instructions executed by category for 80x86 and DLX for\nthe averages of five SPECint92 and SPECfp92 programs of Figures K.46 and K.47.\nK-50\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1320,
        "text": "Introduction\nTo enhance your understanding of instruction set architectures, we chose the VAX as\nthe representative Complex Instruction Set Computer (CISC) because it is so differ-\nent from MIPS and yet still easy to understand. By seeing two such divergent styles,\nwe are confident that you will be able to learn other instruction sets on your own.\nAt the time the VAX was designed, the prevailing philosophy was to create\ninstruction sets that were close to programming languages in order to simplify\ncompilers. For example, because programming languages had loops, instruction\nsets should have loop instructions. As VAX architect William Strecker said\n(\u201cVAX-11/780\u2014A Virtual Address Extension to the PDP-11 Family,\u201d AFIPS\nProc., National Computer Conference, 1978):\nA major goal of the VAX-11 instruction set was to provide for effective compiler\ngenerated code. Four decisions helped to realize this goal: 1) A very regular and\nconsistent treatment of operators . . . . 2) An avoidance of instructions unlikely\nto be generated by a compiler . . . . 3) Inclusions of several forms of common\noperators . . . . 4) Replacement of common instruction sequences with single\ninstructions . . . . Examples include procedure calling, multiway branching, loop\ncontrol, and array subscript calculation.\nRecall that DRAMs of the mid-1970s contained less than 1/1000th the capacity\nof today\u2019s DRAMs, so code space was also critical. Hence, another prevailing phi-\nlosophy was to minimize code size, which is de-emphasized in fixed-length\ninstruction sets like MIPS. For example, MIPS address fields always use 16 bits,\neven when the address is very small. In contrast, the VAX allows instructions to be\na variable number of bytes, so there is little wasted space in address fields.\nWhole books have been written just about the VAX, so this VAX extension\ncannot be exhaustive. Hence, the following sections describe only a few of its\naddressing modes and instructions. To show the VAX instructions in action, later\nsections show VAX assembly code for two C procedures. The general style will be\nto contrast these instructions with the MIPS code that you are already familiar with.\nThe differing goals for VAX and MIPS have led to very different architectures.\nThe VAX goals, simple compilers and code density, led to the powerful addressing\nmodes, powerful instructions, and efficient instruction encoding. The MIPS goals\nwere high performance via pipelining, ease of hardware implementation, and com-\npatibility with highly optimizing compilers. The MIPS goals led to simple instruc-\ntions, simple addressing modes, fixed-length instruction formats, and a large\nnumber of registers.\nVAX Operands and Addressing Modes\nThe VAX is a 32-bit architecture, with 32-bit-wide addresses and 32-bit-wide reg-\nisters. Yet, the VAX supports many other data sizes and types, as Figure K.49\nshows. Unfortunately, VAX uses the name \u201cword\u201d to refer to 16-bit quantities;\nin this text, a word means 32 bits. Figure K.49 shows the conversion between\nK.4\nThe VAX Architecture\n\u25a0\nK-51"
    },
    {
        "page": 1321,
        "text": "the MIPS data type names and the VAX names. Be careful when reading about\nVAX instructions, as they refer to the names of the VAX data types.\nThe VAX provides sixteen 32-bit registers. The VAX assembler uses the\nnotation r0, r1, . . . , r15 to refer to these registers, and we will stick to that\nnotation. Alas, 4 of these 16 registers are effectively claimed by the instruction set\narchitecture. For example, r14 is the stack pointer (sp) and r15 is the program\ncounter (pc). Hence, r15 cannot be used as a general-purpose register, and using\nr14 is very difficult because it interferes with instructions that manipulate the\nstack. The other dedicated registers are r12, used as the argument pointer (ap),\nand r13, used as the frame pointer (fp); their purpose will become clear later.\n(Like MIPS, the VAX assembler accepts either the register number or the register\nname.)\nVAX addressing modes include those discussed in Appendix A, which has all\nthe MIPS addressing modes: register, displacement, immediate, and PC-relative.\nMoreover, all these modes can be used for jump addresses or for data addresses.\nBut that\u2019s not all the addressing modes. To reduce code size, the VAX has three\nlengths of addresses for displacement addressing: 8-bit, 16-bit, and 32-bit\naddresses called, respectively, byte displacement, word displacement, and long\ndisplacement addressing. Thus, an address can be not only as small as possible\nbut also as large as necessary; large addresses need not be split, so there is no equiv-\nalent to the MIPS lui instruction (see Figure A.24 on page A-37).\nThose are still not all the VAX addressing modes. Several have a deferred\noption, meaning that the object addressed is only the address of the real object,\nrequiring another memory access to get the operand. This addressing mode is\ncalled indirect addressing in other machines. Thus, register deferred, autoincre-\nment deferred, and byte/word/long displacement deferred are other addressing\nmodes to choose from. For example, using the notation of the VAX assembler,\nBits\nData type\nMIPS name\nVAX name\n08\nInteger\nByte\nByte\n16\nInteger\nHalf word\nWord\n32\nInteger\nWord\nLong word\n32\nFloating point\nSingle precision\nF_floating\n64\nInteger\nDouble word\nQuad word\n64\nFloating point\nDouble precision\nD_floating or G_floating\n8n\nCharacter string\nCharacter\nCharacter \nFigure K.49 VAX data types, their lengths, and names. The first letter of the VAX type\n(b, w, l, f, q, d, g, c) is often used to complete an instruction name. Examples of move\ninstructions include movb, movw, movl, movf, movq, movd, movg, and movc3.\nEach move instruction transfers an operand of the data type indicated by the letter\nfollowing mov.\nK-52\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1322,
        "text": "r1 means the operand is register 1 and (r1) means the operand is the location in\nmemory pointed to by r1.\nThere is yet another addressing mode. Indexed addressing automatically con-\nverts the value in an index operand to the proper byte address to add to the rest of\nthe address. For a 32-bit word, we needed to multiply the index of a 4-byte quantity\nby 4 before adding it to a base address. Indexed addressing, called scaled addres-\nsing on some computers, automatically multiplies the index of a 4-byte quantity by\n4 as part of the address calculation.\nTo cope with such a plethora of addressing options, the VAX architecture\nseparates the specification of the addressing mode from the specification of\nthe operation. Hence, the opcode supplies the operation and the number of oper-\nands, and each operand has its own addressing mode specifier. Figure K.50\nshows the name, assembler notation, example, meaning, and length of the address\nspecifier.\nThe VAX style of addressing means that an operation doesn\u2019t know where its\noperands come from; a VAX add instruction can have three operands in registers,\nthree operands in memory, or any combination of registers and memory\noperands.\nAddressing mode\nname\nSyntax\nExample\nMeaning\nLength of address\nspeci\ufb01er in bytes\nLiteral\n#value\n#\u20131\n)e\nula\nv\nd\ne\nn\ngis\ntib\n-\n6\n(\n1\n1\n\u2013\ne\nht\nf\no\nhtg\nn\nel\n+\n1\n0\n0\n1\n0\n0\n1\n#\ne\nula\nv\n#\netaid\ne\nm\nm\nI\nimmediate\n1\n3\nr\n3\nr\nn\nr\nretsig\ne\nR\nRegister deferred\n(rn)\n(r3)\nMemory[r3]\n1\nByte/word/long \ndisplacement\nDisplacement (rn)\n100(r3)\nMemory[r3 + 100]\n1 + length of the\ndisplacement\nByte/word/long \ndisplacement deferred\n@displacement (rn)\n@100(r3) Memory[Memory [r3 + 100]]\n1 + length of the\ndisplacement\nIndexed (scaled)\nBase mode [rx]\n(r3)[r4]\nMemory[r3 + r4\nd]\n(where d is data size in bytes)\n1 + length of base \naddressing mode\nAutoincrement\n(rn)+\n(r3)+\nMemory[r3]; r3 = r3 + d\n1\nAutodecrement\n\u2013 (rn)\n\u2013(r3)\nr3 = r3 \u2013 d; Memory[r3]\n1\nAutoincrement deferred\n@(rn)+\n@(r3)+\nMemory[Memory[r3]]; r3 = r3 + d 1\nFigure K.50 Definition and length of the VAX operand specifiers. The length of each addressing mode is 1 byte\nplus the length of any displacement or immediate field needed by the mode. Literal mode uses a special 2-bit tag and\nthe remaining 6 bits encode the constant value. If the constant is too big, it must use the immediate addressing mode.\nNote that the length of an immediate operand is dictated by the length of the data type indicated in the opcode, not\nthe value of the immediate. The symbol d in the last four modes represents the length of the data in bytes; d is 4 for\n32-bit add.\nK.4\nThe VAX Architecture\n\u25a0\nK-53"
    },
    {
        "page": 1323,
        "text": "Example\nHow long is the following instruction?\naddl3 r1,737(r2),(r3)[r4]\nThe name addl3 means a 32-bit add instruction with three operands. Assume the\nlength of the VAX opcode is 1 byte.\nAnswer\nThe first operand specifier\u2014r1\u2014indicates register addressing and is 1 byte long.\nThe second operand specifier\u2014737(r2)\u2014indicates displacement addressing\nand has two parts: The first part is a byte that specifies the word displacement\naddressing mode and base register (r2); the second part is the 2-byte-long dis-\nplacement (737). The third operand specifier\u2014(r3)[r4]\u2014also has two parts:\nThe first byte specifies register deferred addressing mode ((r3)), and the second\nbyte specifies the Index register and the use of indexed addressing ([r4]). Thus, the\ntotal length of the instruction is 1 + (1) + (1 + 2) + (1 + 1) \u00bc 7 bytes.\nIn this example instruction, we show the VAX destination operand on the left and\nthe source operands on the right, just as we show MIPS code. The VAX assembler\nactually expects operands in the opposite order, but we felt it would be less con-\nfusing to keep the destination on the left for both machines. Obviously, left or right\norientation is arbitrary; the only requirement is consistency.\nElaboration\nBecause the PC is 1 of the 16 registers that can be selected in a VAX addressing\nmode, 4 of the 22 VAX addressing modes are synthesized from other addressing\nmodes. Using the PC as the chosen register in each case, immediate addressing is\nreally autoincrement, PC-relative is displacement, absolute is autoincrement\ndeferred, and relative deferred is displacement deferred.\nEncoding VAX Instructions\nGiven the independence of the operations and addressing modes, the encoding of\ninstructions is quite different from MIPS.\nVAX instructions begin with a single byte opcode containing the operation and\nthe number of operands. The operands follow the opcode. Each operand begins\nwith a single byte, called the address specifier, that describes the addressing mode\nfor that operand. For a simple addressing mode, such as register addressing, this\nbyte specifies the register number as well as the mode (see the rightmost column\nin Figure K.50). In other cases, this initial byte can be followed by many more\nbytes to specify the rest of the address information.\nAs a specific example, let\u2019s show the encoding of the add instruction from the\nexample on page K-24:\naddl3 r1,737(r2),(r3)[r4]\nAssume that this instruction starts at location 201.\nFigure K.51 shows the encoding. Note that the operands are stored in memory\nin opposite order to the assembly code above. The execution of VAX instructions\nK-54\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1324,
        "text": "begins with fetching the source operands, so it makes sense for them to come first.\nOrder is not important in fixed-length instructions like MIPS, since the source and\ndestination operands are easily found within a 32-bit word.\nThe first byte, at location 201, is the opcode. The next byte, at location 202, is a\nspecifier for the index mode using register r4. Like many of the other specifiers,\nthe left 4 bits of the specifier give the mode and the right 4 bits give the register\nused in that mode. Since addl3 is a 4-byte operation, r4 will be multiplied by 4\nand added to whatever address is specified next. In this case it is register deferred\naddressing using register r3. Thus, bytes 202 and 203 combined define the third\noperand in the assembly code.\nThe following byte, at address 204, is a specifier for word displacement addres-\nsing using register r2 as the base register. This specifier tells the VAX that the fol-\nlowing two bytes, locations 205 and 206, contain a 16-bit address to be added to r2.\nThe final byte of the instruction gives the destination operand, and this specifier\nselects register addressing using register r1.\nSuch variability in addressing means that a single VAX operation can have\nmany different lengths; for example, an integer add varies from 3 bytes to 19 bytes.\nVAX implementations must decode the first operand before they can find the sec-\nond, and so implementors are strongly tempted to take 1 clock cycle to decode each\noperand; thus, this sophisticated instruction set architecture can result in higher\nclock cycles per instruction, even when using simple addresses.\nVAX Operations\nIn keeping with its philosophy, the VAX has a large number of operations as well\nas a large number of addressing modes. We review a few here to give the flavor of\nthe machine.\nGiven the power of the addressing modes, the VAX move instruction performs\nseveral operations found in other machines. It transfers data between any two\naddressable locations and subsumes load, store, register-register moves, and\nByte address\nContents at each byte\nMachine code\n1\nc\n3ld\nd\na\ng\nniniatn\no\nc\ne\nd\no\nc\np\nO\n1\n0\n2\nhex\n4\n4\n]\n4\nr[\nr\no\nf\nreific\ne\np\ns\ne\nd\no\nm\nx\ne\nd\nn\nI\n2\n0\n2\nhex\n203\nRegister indirect mode specifier for (r3)\n63hex\n204\nWord displacement mode specifier using r2 as base\nc2hex\n1\ne\n7\n3\n7\ntn\nats\nn\no\nc\ntib\n-\n6\n1\ne\nh\nT\n5\n0\n2\nhex\n2\n0\n6\n0\n2\nhex\n1\n5\n1\nr\nr\no\nf\nreific\ne\np\ns\ne\nd\no\nm\nretsig\ne\nR\n7\n0\n2\nhex\nFigure K.51 The encoding of the VAX instruction addl3 r1,737(r2),(r3)[r4], assuming\nit starts at address 201. To satisfy your curiosity, the right column shows the actual VAX\nencoding in hexadecimal notation. Note that the 16-bit constant 737ten takes 2 bytes.\nK.4\nThe VAX Architecture\n\u25a0\nK-55"
    },
    {
        "page": 1325,
        "text": "memory-memory moves as special cases. The first letter of the VAX data type (b,\nw, l, f, q, d, g, c in Figure K.49) is appended to the acronym mov to determine the\nsize of the data. One special move, called move address, moves the 32-bit address\nof the operand rather than the data. It uses the acronym mova.\nThe arithmetic operations of MIPS are also found in the VAX, with two major\ndifferences. First, the type of the data is attached to the name. Thus, addb, addw,\nand addl operate on 8-bit, 16-bit, and 32-bit data in memory or registers, respec-\ntively; MIPS has a single add instruction that operates only on the full 32-bit reg-\nister. The second difference is that to reduce code size the add instruction specifies\nthe number of unique operands; MIPS always specifies three even if one operand is\nredundant. For example, the MIPS instruction\nadd $1, $1, $2\ntakes 32 bits like all MIPS instructions, but the VAX instruction\naddl2 r1, r2\nuses r1 for both the destination and a source, taking just 24 bits: 8 bits for the\nopcode and 8 bits each for the two register specifiers.\nNumber of Operations\nNow we can show how VAX instruction names are formed:\noperation\n\u00f0\n\u00de datatype\n\u00f0\n\u00de 2\n3\n\u0001 \u0003\nThe operation add works with data types byte, word, long, float, and double and\ncomes in versions for either 2 or 3 unique operands, so the following instructions\nare all found in the VAX:\naddb2\naddw2\naddl2\naddf2\naddd2\naddb3\naddw3\naddl3\naddf3\naddd3\nAccounting for all addressing modes (but ignoring register numbers and immediate\nvalues) and limiting to just byte, word, and long, there are more than 30,000 ver-\nsions of integer add in the VAX; MIPS has just 4!\nAnother reason for the large number of VAX instructions is the instructions\nthat either replace sequences of instructions or take fewer bytes to represent a sin-\ngle instruction. Here are four such examples (* means the data type):\nVAX operation\nExample\nMeaning\nclr*\nclrl r3\nr3 = 0\ninc*\nincl r3\nr3 = r3+1\ndec*\ndecl r3\nr3 = r3 \u22121\npush*\npushl r3\nsp = sp \u22124; Memory[sp] = r3;\nK-56\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1326,
        "text": "The push instruction in the last row is exactly the same as using the move instruc-\ntion with autodecrement addressing on the stack pointer:\nmovl \u2013 (sp), r3\nBrevity is the advantage of pushl: It is 1 byte shorter since sp is implied.\nBranches, Jumps, and Procedure Calls\nThe VAX branch instructions are related to the arithmetic instructions because the\nbranch instructions rely on condition codes. Condition codes are set as a side effect\nof an operation, and they indicate whether the result is positive, negative, or zero or\nif an overflow occurred. Most instructions set the VAX condition codes according\nto their result; instructions without results, such as branches, do not. The VAX con-\ndition codes are N (Negative), Z (Zero), V (oVerflow), and C (Carry). There is also\na compare instruction cmp* just to set the condition codes for a subsequent branch.\nThe VAX branch instructions include all conditions. Popular branch instruc-\ntions include beql(=), bneq(6\u00bc), blss(<), bleq(\u0004), bgtr(>),\nand bgeq(\u0005), which do just what you would expect. There are also unconditional\nbranches whose name is determined by the size of the PC-relative offset. Thus,\nbrb (branch byte) has an 8-bit displacement, and brw (branch word) has a 16-\nbit displacement.\nThe final major category we cover here is the procedure call and return instruc-\ntions. Unlike the MIPS architecture, these elaborate instructions can take dozens of\nclock cycles to execute. The next two sections show how they work, but we need to\nexplain the purpose of the pointers associated with the stack manipulated by calls\nand ret. The stack pointer, sp, is just like the stack pointer in MIPS; it points to the\ntop of the stack. The argument pointer, ap, points to the base of the list of arguments\nor parameters in memory that are passed to the procedure. The frame pointer, fp,\npoints to the base of the local variables of the procedure that are kept in memory (the\nstack frame). The VAX call and return instructions manipulate these pointers to\nmaintain the stack in proper condition across procedure calls and to provide conve-\nnient base registers to use when accessing memory operands. As we shall see, call\nand return also save and restore the general-purpose registers as well as the program\ncounter. Figure K.52 gives a further sampling of the VAX instruction set.\nAn Example to Put It All Together: swap\nTo see programming in VAX assembly language, we translate two C procedures,\nswap and sort. The C code for swap is reproduced in Figure K.53. The next\nsection covers sort.\nWe describe the swap procedure in three general steps of assembly language\nprogramming:\n1. Allocate registers to program variables.\n2. Produce code for the body of the procedure.\n3. Preserve registers across the procedure invocation.\nK.4\nThe VAX Architecture\n\u25a0\nK-57"
    },
    {
        "page": 1327,
        "text": "g\nn\ni\nn\na\ne\nm\nn\no\nit\nc\nu\nrt\ns\nn\nI\ne\nl\np\nm\na\nx\nE\ne\np\ny\nt\nn\no\nit\nc\nu\nrt\ns\nn\nI\nn byte, half-word, word, or double-word operands; * is data type\nmov*\nMove between two operands\nor word, extending it with zeros\nan operand; data type is last\non integer or logical bytes, half words (16 bits), words (32 bits); * is data \ntype \n size of data type\ns\ne\nh\nc\nn\nar\nb\nla\nn\noitid\nn\no\nc\nn\nu\nd\nn\na\nla\nn\noitid\nn\no\nC\nlo\nrtn\no\nC\n branch not equal\nl, branch greater than or equal\nnd; branch if result\nsecond operand\n case selector\nents on stack (see \u201cA Longer \nExample: sort\u201d on page K-33)\nRTRAN-style parameter list\nreturn address (like MIPS jal)\nData transfers\nMove data betwee\nArithmetic/logical\nOperations \nProcedure\nCall/return from procedure\nFloating point\nFloating-point operations on D, F, G, and H formats\nformat floating numbers\nn D-format floating numbers\non F-format floating point\ne of coefficients in F format\ns\nn\noitare\np\no\nlaic\ne\np\nS\nre\nht\nO\n redundancy check\nmovzb*\nMove a byte to a half word \nmova*\nMove the 32-bit address of \npush*\nPush operand onto stack\nadd*_\nAdd with 2 or 3 operands\ncmp*\nCompare and set condition codes\ntst*\nCompare to zero and set condition codes\nash*\nArithmetic shift\nclr*\nClear\ncvtb*\nSign-extend byte to\nbeql, bneq\nBranch equal,\nbleq, bgeq\nBranch less than or equa\nbrb, brw\nUnconditional branch with an 8-bit or 16-bit address\njmp\nJump using any addressing mode to specify target\naobleq\nAdd one to opera\ncase_\nJump based on\ncalls\nCall procedure with argum\ncallg\nCall procedure with FO\njsb\nJump to subroutine, saving \nret\nReturn from procedure call\naddd_\nAdd double-precision D-\nsubd_\nSubtract double-precisio\nmulf_\nMultiply single-precisi\npolyf\nEvaluate a polynomial using tabl\ncrc\nCalculate cyclic\ninsque\nInsert a queue entry into a queue\nFigure K.52 Classes of VAX instructions with examples. The asterisk stands for multiple data types: b, w, l, d, f, g, h,\nand q. The underline, as in addd_, means there are 2-operand (addd2) and 3-operand (addd3) forms of this\ninstruction.\nK-58\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1328,
        "text": "The VAX code for these procedures is based on code produced by the VMS C\ncompiler using optimization.\nRegister Allocation for swap\nIn contrast to MIPS, VAX parameters are normally allocated to memory, so this\nstep of assembly language programming is more properly called \u201cvariable alloca-\ntion.\u201d The standard VAX convention on parameter passing is to use the stack. The\ntwo parameters, v[] and k, can be accessed using register ap, the argument\npointer: The address 4(ap) corresponds to v[] and 8(ap) corresponds to k.\nRemember that with byte addressing the address of sequential 4-byte words differs\nby 4. The only other variable is temp, which we associate with register r3.\nCode for the Body of the Procedure swap\nThe remaining lines of C code in swap are\ntemp = v[k];\nv[k] = v[k + 1];\nv[k + 1] = temp;\nSince this program uses v[] and k several times, to make the programs run faster\nthe VAX compiler first moves both parameters into registers:\nmovl r2, 4(ap) ;r2 = v[]\nmovl r1, 8(ap) ;r1 = k\nNote that we follow the VAX convention of using a semicolon to start a comment;\nthe MIPS comment symbol # represents a constant operand in VAX assembly\nlanguage.\nswap(int v[], int k)\n{\n   int temp;\n   temp = v[k];\n   v[k] = v[k + 1];\n   v[k + 1] = temp;\n}\nFigure K.53 A C procedure that swaps two locations in memory. This procedure will\nbe used in the sorting example in the next section.\nK.4\nThe VAX Architecture\n\u25a0\nK-59"
    },
    {
        "page": 1329,
        "text": "The VAX has indexed addressing, so we can use index k without converting it\nto a byte address. The VAX code is then straightforward:\nmovl\nr3, (r2)[r1]\n;r3 (temp) = v[k]\naddl3 r0, #1,8(ap)\n;r0 = k + 1\nmovl\n(r2)[r1],(r2)[r0] ;v[k] = v[r0] (v[k + 1])\nmovl\n(r2)[r0],r3\n;v[k + 1] = r3 (temp)\nUnlike the MIPS code, which is basically two loads and two stores, the key VAX\ncode is one memory-to-register move, one memory-to-memory move, and one\nregister-to-memory move. Note that the addl3 instruction shows the flexibility\nof the VAX addressing modes: It adds the constant 1 to a memory operand and\nplaces the result in a register.\nNow we have allocated storage and written the code to perform the operations\nof the procedure. The only missing item is the code that preserves registers across\nthe routine that calls swap.\nPreserving Registers across Procedure Invocation of swap\nThe VAX has a pair of instructions that preserve registers, calls and ret. This\nexample shows how they work.\nThe VAX C compiler uses a form of callee convention. Examining the code\nabove, we see that the values in registers r0, r1, r2, and r3 must be saved\nso that they can later be restored. The calls instruction expects a 16-bit mask\nat the beginning of the procedure to determine which registers are saved: if bit i\nis set in the mask, then register i is saved on the stack by the calls instruction.\nIn addition, calls saves this mask on the stack to allow the return instruction\n(ret) to restore the proper registers. Thus, the calls executed by the caller does\nthe saving, but the callee sets the call mask to indicate what should be saved.\nOne of the operands for calls gives the number of parameters being passed,\nso that calls can adjust the pointers associated with the stack: the argument\npointer (ap), frame pointer (fp), and stack pointer (sp). Of course, calls also\nsaves the program counter so that the procedure can return!\nThus, to preserve these four registers for swap, we just add the mask at the\nbeginning of the procedure, letting the calls instruction in the caller do all the work:\n.word ^m<r0,r1,r2,r3> ;set bits in mask for 0,1,2,3\nThis directive tells the assembler to place a 16-bit constant with the proper bits set\nto save registers r0 through r3.\nThe return instruction undoes the work of calls. When finished, ret sets the\nstack pointer from the current frame pointer to pop everything calls placed on\nthe stack. Along the way, it restores the register values saved by calls, including\nthose marked by the mask and old values of the fp, ap, and pc.\nK-60\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1330,
        "text": "To complete the procedure swap, we just add one instruction:\nret\n;restore registers and return\nThe Full Procedure swap\nWe are now ready for the whole routine. Figure K.54 identifies each block of code\nwith its purpose in the procedure, with the MIPS code on the left and the VAX code\non the right. This example shows the advantage of the scaled indexed addressing\nand the sophisticated call and return instructions of the VAX in reducing the num-\nber of lines of code. The 17 lines of MIPS assembly code became 8 lines of VAX\nassembly code. It also shows that passing parameters in memory results in extra\nmemory accesses.\nKeep in mind that the number of instructions executed is not the same as per-\nformance; the fallacy on page K-38 makes this point.\nNote that VAX software follows a convention of treating registers r0 and r1\nas temporaries that are not saved across a procedure call, so the VMS C compiler\ndoes include registers r0 and r1 in the register saving mask. Also, the C compiler\nshould have used r1 instead of 8(ap) in the addl3 instruction; such examples\ninspire computer architects to try to write compilers!\nMIPS versus VAX\nSaving register\nProcedure body\nRestoring registers\nswap: addi\n$29,$29, \u201312\nsw\n $2, 0($29)\nsw\n$15, 4($29)\nsw\n$16, 8($29)\nswap: .word ^m<r0,r1,r2,r3>\nmuli\n$2, $5,4\nadd\n$2, $4,$2\nlw\n$15, 0($2)\nlw\n$16, 4($2)\nsw\n$16, 0($2)\nsw\n$15, 4($2)\nmovl\nr2, 4(a)\nmovl\nr1, 8(a) \nmovl\nr3, (r2)[r1]\naddl3\nr0, #1,8(ap)\nmovl\n(r2)[r1],(r2)[r0]\nmovl\n(r2)[r0],r3\nlw\n $2, 0($29)\nlw\n$15, 4($29)\nlw\n$16, 8($29)\naddi\n$29,$29, 12\nProcedure return\nret\n$31\njr\nFigure K.54 MIPS versus VAX assembly code of the procedure swap in Figure K.53\non page K-30.\nK.4\nThe VAX Architecture\n\u25a0\nK-61"
    },
    {
        "page": 1331,
        "text": "A Longer Example: sort\nWe show the longer example of the sort procedure. Figure K.55 shows the C ver-\nsion of the program. Once again we present this procedure in several steps, con-\ncluding with a side-by-side comparison to MIPS code.\nRegister Allocation for sort\nThe two parameters of the procedure sort, v and n, are found in the stack in loca-\ntions 4(ap) and 8(ap), respectively. The two local variables are assigned to regis-\nters: i to r6 and j to r4. Because the two parameters are referenced frequently in the\ncode, the VMS C compiler copies the address of these parameters into registers\nupon entering the procedure:\nmoval\nr7,8(ap)\n;move\naddress\nof\nn\ninto\nr7\nmoval\nr5,4(ap)\n;move\naddress\nof\nv\ninto\nr5\nIt would seem that moving the value of the operand to a register would be more\nuseful than its address, but once again we bow to the decision of the VMS C com-\npiler. Apparently the compiler cannot be sure that v and n don\u2019t overlap in memory.\nCode for the Body of the sort Procedure\nThe procedure body consists of two nested for loops and a call to swap, which\nincludes parameters. Let\u2019s unwrap the code from the outside to the middle.\nThe Outer Loop\nThe first translation step is the first for loop:\nfor (i = 0; i < n; i = i + 1) {\nRecall that the C for statement has three parts: initialization, loop test, and iteration\nincrement. It takes just one instruction to initialize i to 0, the first part of the for\nstatement:\nclrl\nr6\n;i = 0\nsort (int v[], int n)\n{\nint i, j;\nfor (i = 0; i < n; i = i + 1) {\nfor (j = i \u2013 1; j >= 0 && v[j] > v[j + 1]; j = j \u2013 1)\n{ swap(v,j);\n}\n}\n}\nFigure K.55 A C procedure that performs a bubble sort on the array v.\nK-62\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1332,
        "text": "It also takes just one instruction to increment i, the last part of the for:\nincl\nr6\n;i = i + 1\nThe loop should be exited if i < n is false, or said another way, exit the loop if\ni \u0005 n. This test takes two instructions:\nfor1tst: cmpl r6,(r7) ;compare r6 and memory[r7] (i:n)\nbgeq exit1\n;go to exit1 if r6 \u0005 mem[r7] (i \u0005 n)\nNote that cmpl sets the condition codes for use by the conditional branch\ninstruction bgeq.\nThe bottom of the loop just jumps back to the loop test:\nbrb\nfor1tst\n;branch to test of outer loop\nexit1:\nThe skeleton code of the first for loop is then\nclrl\nr6\n;i = 0\nfor1tst: cmpl\nr6,(r7) ;compare r6 and memory[r7] (i:n)\nbgeq exit1\n;go to exit1 if r6 \u0005 mem[r7] (i \u0005 n)\n...\n(body of first for loop)\n...\nincl\nr6\n;i = i + 1\nbrb\nfor1tst ;branch to test of outer loop\nexit1:\nThe Inner Loop\nThe second for loop is\nfor (j = i \u2013 1; j >= 0 && v[j] > v[j + 1]; j = j \u2013 1) {\nThe initialization portion of this loop is again one instruction:\nsubl3\nr4,r6,#1\n;j = i \u2013 1\nThe decrement of j is also one instruction:\ndecl\nr4\n;j = j \u2013 1\nThe loop test has two parts. We exit the loop if either condition fails, so the first test\nmust exit the loop if it fails (j < 0):\nfor2tst:blss\nexit2\n;go to exit2 if r4 < 0 (j < 0)\nNotice that there is no explicit comparison. The lack of comparison is a benefit of\ncondition codes, with the conditions being set as a side effect of the prior instruc-\ntion. This branch skips over the second condition test.\nThe second test exits if v[j] > v[j + 1] is false, or exits if v[j] \u0004 v[j + 1].\nFirst we load v and put j + 1 into registers:\nmovl\nr3,(r5)\n;r3 = Memory[r5] (r3 = v)\naddl3\nr2,r4,#1\n;r2 = r4 + 1 (r2 = j + 1)\nK.4\nThe VAX Architecture\n\u25a0\nK-63"
    },
    {
        "page": 1333,
        "text": "Register indirect addressing is used to get the operand pointed to by r5.\nOnce again the index addressing mode means we can use indices without con-\nverting to the byte address, so the two instructions for v[j] \u0004 v[j + 1] are\ncmpl (r3)[r4],(r3)[r2]\n;v[r4] : v[r2] (v[j]:v[j + 1])\nbleq exit2\n;go to exit2 if v[j] \u0004 v[j + 1]\nThe bottom of the loop jumps back to the full loop test:\nbrb\nfor2tst #\njump to test of inner loop\nCombining the pieces, the second for loop looks like this:\nsubl3 r4,r6, #1\n;j = i \u2013 1\nfor2tst:\nblss\nexit2\n;go to exit2 if r4 < 0 (j < 0)\nmovl\nr3,(r5)\n;r3 = Memory[r5] (r3 = v)\naddl3 r2,r4,#1\n;r2 = r4 + 1 (r2 = j + 1)\ncmpl\n(r3)[r4],(r3)[r2];v[r4] : v[r2]\nbleq\nexit2\n;go to exit2 if v[j] \u00f0 [j+1]\n...\n(body of second for loop) ...\ndecl\nr4\n;j = j \u2013 1\nbrb\nfor2tst\n;jump to test of inner loop\nexit2:\nNotice that the instruction blss (at the top of the loop) is testing the condition\ncodes based on the new value of r4 (j), set either by the subl3 before entering\nthe loop or by the decl at the bottom of the loop.\nThe Procedure Call\nThe next step is the body of the second for loop:\nswap(v,j);\nCalling swap is easy enough:\ncalls\n#2,swap\nThe constant 2 indicates the number of parameters pushed on the stack.\nPassing Parameters\nThe C compiler passes variables on the stack, so we pass the parameters to swap\nwith these two instructions:\npushl\n(r5)\n;first swap parameter is v\npushl\nr4\n;second swap parameter is j\nRegister indirect addressing is used to get the operand of the first instruction.\nPreserving Registers across Procedure Invocation of sort\nThe only remaining code is the saving and restoring of registers using the callee\nsave convention. This procedure uses registers r2 through r7, so we add a mask\nwith those bits set:\nK-64\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1334,
        "text": ".word ^m<r2,r3,r4,r5,r6,r7>; set mask for registers 2-7\nSince ret will undo all the operations, we just tack it on the end of the procedure.\nThe Full Procedure sort\nNow we put all the pieces together in Figure K.56. To make the code easier to fol-\nlow, once again we identify each block of code with its purpose in the procedure\nand list the MIPS and VAX code side by side. In this example, 11 lines of the sort\nprocedure in C become the 44 lines in the MIPS assembly language and 20 lines in\nVAX assembly language. The biggest VAX advantages are in register saving and\nrestoring and indexed addressing.\nFallacies and Pitfalls\nThe ability to simplify means to eliminate the unnecessary so that the necessary\nmay speak.\nHans Hoffman\nSearch for the Real (1967)\nFallacy\nIt is possible to design a flawless architecture.\nAll architecture design involves trade-offs made in the context of a set of hardware\nand software technologies. Over time those technologies are likely to change, and\ndecisions that may have been correct at one time later look like mistakes. For exam-\nple, in 1975 the VAX designers overemphasized the importance of code size effi-\nciency and underestimated how important ease of decoding and pipelining would\nbe 10 years later. And, almost all architectures eventually succumb to the lack of\nsufficient address space. Avoiding these problems in the long run, however, would\nprobably mean compromising the efficiency of the architecture in the short run.\nFallacy\nAn architecture with flaws cannot be successful.\nThe IBM 360 is often criticized in the literature\u2014the branches are not PC-relative,\nand the address is too small in displacement addressing. Yet, the machine has been\nan enormous success because it correctly handled several new problems. First, the\narchitecture has a large amount of address space. Second, it is byte addressed and\nhandles bytes well. Third, it is a general-purpose register machine. Finally, it is sim-\nple enough to be efficiently implemented across a wide performance and cost range.\nThe Intel 8086 provides an even more dramatic example. The 8086 architecture\nis the only widespread architecture in existence today that is not truly a general-\npurpose register machine. Furthermore, the segmented address space of the\n8086 causes major problems for both programmers and compiler writers. Never-\ntheless, the 8086 architecture\u2014because of its selection as the microprocessor in the\nIBM PC\u2014has been enormously successful.\nK.4\nThe VAX Architecture\n\u25a0\nK-65"
    },
    {
        "page": 1335,
        "text": "MIPS versus VAX\nSaving registers\nsort:\naddi $29,$29, \u201336\nsw\n$15, 0($29)\nsw\n$16, 4($29)\nsw\n$17, 8($29)\nsw\n$18,12($29)\nsw\n$19,16($29)\nsw\n$20,20($29)\nsw\n$24,24($29)\nsw\n$25,28($29)\nsw\n$31,32($29)\nsort:\n.word ^m<r2,r3,r4,r5,r6,r7>\nProcedure body\nMove parameters\nmove $18, $4\nmove $20, $5\nmoval\nr7,8(ap)\nmoval\nr5,4(ap)\nOuter loop\nadd\n$19, $0, $0\nfor1tst: slt\n$8, $19, $20\nbeq\n$8, $0, exit1\nclrl\nr6\nfor1tst:\ncmpl\nr6,(r7)\nbgeq\nexit1\n$17, $19, \u20131\naddi\np\no\nol\nre\nn\nn\nI\nfor2tst: slti $8, $17, 0\nbne\n$8, $0, exit2\nmuli $15, $17, 4\nadd\n$16, $18, $15\nlw\n$24, 0($16)\nlw\n$25, 4($16)\nslt\n$8, $25, $24\nbeq\n$8, $0, exit2\nfor2tst:\nsubl3\nr4,r6,#1\nblss\nexit2\nmovl\nr3,(r5)\naddl3\nr2,r4,#1 \ncmpl\n(r3)[r4],(r3)[r2]\nbleq\nexit2\nPass parameters\nand call\nmove $4, $18\nmove $5, $17\njal\nswap\npushl\n(r5)\npushl\n r4\ncalls\n#2,swap\np\no\nol\nre\nn\nn\nI\nOuter loop\nexit2:\naddi $19, $19, 1\n$17, $17, \u20131\naddi\nj\nfor2tst\ndecl\nr4\nbrb\nfor2tst\nj\nfor1tst\nexit2:\nincl\nr6\nbrb\nfor1tst\nRestoring registers\nexit1:\nlw\n$15,0($29)\nlw\n$16, 4($29)\nlw\n$17, 8($29)\nlw\n$18,12($29)\nlw\n$19,16($29)\nlw\n$20,20($29)\nlw\n$24,24($29)\nlw\n$25,28($29)\nlw\n$31,32($29)\naddi $29,$29, 36\nProcedure return\nret\nexit1:\n$31\njr\nFigure K.56 MIPS32 versus VAX assembly version of procedure sort in Figure K.55 on page K-33."
    },
    {
        "page": 1336,
        "text": "Fallacy\nThe architecture that executes fewer instructions is faster.\nDesigners of VAX machines performed a quantitative comparison of VAX and\nMIPS for implementations with comparable organizations, the VAX 8700 and\nthe MIPS M2000. Figure K.57 shows the ratio of the number of instructions exe-\ncuted and the ratio of performance measured in clock cycles. MIPS executes about\ntwice as many instructions as the VAX while the MIPS M2000 has almost three\ntimes the performance of the VAX 8700.\nConcluding Remarks\nThe Virtual Address eXtension of the PDP-11 architecture \u2026 provides a virtual\naddress of about 4.3 gigabytes which, even given the rapid improvement of mem-\nory technology, should be adequate far into the future.\nWilliam Strecker\n\u201cVAX-11/780\u2014A Virtual Address Extension to the PDP-11 Family,\u201d\nAFIPS Proc., National Computer Conference (1978)\nWe have seen that instruction sets can vary quite dramatically, both in how they\naccess operands and in the operations that can be performed by a single instruction.\nFigure K.58 compares instruction usage for both architectures for two programs;\neven very different architectures behave similarly in their use of instruction classes.\n3\n3.5\n4\n2.5\n2\n1.5\n1\n0.5\n0\nspice\nmatrix\nnasa7\nfpppp\nInstructions executed\nPerformance\ntomcatv\ndoduc\nespresso\neqntott\nli\nMIPS/VAX\nNumber of bits of displacement\nFigure K.57 Ratio of MIPS M2000 to VAX 8700 in instructions executed and perfor-\nmance in clock cycles using SPEC89 programs. On average, MIPS executes a little over\ntwice as many instructions as the VAX, but the CPI for the VAX is almost six times the\nMIPS CPI, yielding almost a threefold performance advantage. (Based on data from \u201cPer-\nformance from Architecture: Comparing a RISC and CISC with Similar Hardware Orga-\nnization,\u201d by D. Bhandarkar and D. Clark, in Proc. Symp. Architectural Support for\nProgramming Languages and Operating Systems IV, 1991.)\nK.4\nThe VAX Architecture\n\u25a0\nK-67"
    },
    {
        "page": 1337,
        "text": "A product of its time, the VAX emphasis on code density and complex oper-\nations and addressing modes conflicts with the current emphasis on easy decoding,\nsimple operations and addressing modes, and pipelined performance.\nWith more than 600,000 sold, the VAX architecture has had a very successful\nrun. In 1991, DEC made the transition from VAX to Alpha.\nOrthogonality is key to the VAX architecture; the opcode is independent of the\naddressing modes, which are independent of the data types and even the number of\nunique operands. Thus, a few hundred operations expand to hundreds of thousands of\ninstructionswhenaccountingforthedatatypes,operandcounts,andaddressingmodes.\nExercises\nK.1\n[3] <K.4> The following VAX instruction decrements the location pointed to be\nregister r5:\ndecl (r5)\nWhat is the single MIPS instruction, or if it cannot be represented in a single\ninstruction, the shortest sequence of MIPS instructions, that performs the same\noperation? What are the lengths of the instructions on each machine?\nK.2\n[5] <K.4> This exercise is the same as Exercise K.1, except this VAX instruction\nclears a location using autoincrement deferred addressing:\nclrl @(r5)+\nK.3\n[5] <K.4> This exercise is the same as Exercise K.1, except this VAX instruction\nadds 1 to register r5, placing the sum back in register r5, compares the sum to reg-\nister r6, and then branches to L1 if r5 < r6:\naoblss r6, r5, L1 # r5 = r5 + 1; if (r5 < r6) goto L1.\nK.4\n[5] <K.4> Show the single VAX instruction, or minimal sequence of instructions,\nfor this C statement:\na = b + 100;\nAssume a corresponds to register r3 and b corresponds to register r4.\nK.5\n[10] <K.4> Show the single VAX instruction, or minimal sequence of instruc-\ntions, for this C statement:\nx[i + 1] = x[i] + c;\nAssume c corresponds to register r3, i to register r4, and x is an array of 32-bit\nwords beginning at memory location 4,000,000ten.\nProgram\nMachine\nBranch\nArithmetic/\n logical\nData \ntransfer\nFloating\npoint\nTotals\ngcc\nVAX\n30%\n40%\n19%\n89%\nMIPS\n24%\n35%\n27%\n86%\nspice\nVAX\n18%\n23%\n15%\n23%\n79%\nMIPS\n04%\n29%\n35%\n15%\n83%\nFigure K.58 The frequency of instruction distribution for two programs on VAX\nand MIPS.\nK-68\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1338,
        "text": "K.5\nThe IBM 360/370 Architecture for Mainframe Computers\nIntroduction\nThe term \u201ccomputer architecture\u201d was coined by IBM in 1964 for use with the IBM\n360. Amdahl, Blaauw, and Brooks [1964] used the term to refer to the programmer-\nvisible portion of the instruction set. They believed that a family of machines of the\nsame architecture should be able to run the same software. Although this idea may\nseem obvious to us today, it was quite novel at the time. IBM, even though it was the\nleading company inthe industry, had fivedifferent architectures beforethe360.Thus,\nthe notion of a company standardizing on a single architecture was a radical one. The\n360 designers hoped that six different divisions of IBM could be brought together by\ndefining a common architecture. Their definition of architecture was\n\u2026 the structure of a computer that a machine language programmer must\nunderstand to write a correct (timing independent) program for that machine.\nThe term \u201cmachine language programmer\u201d meant that compatibility would\nhold, even in assembly language, while \u201ctiming independent\u201d allowed different\nimplementations.\nThe IBM 360 was introduced in 1964 with six models and a 25:1 performance\nratio. Amdahl, Blaauw, and Brooks [1964] discussed the architecture of the IBM\n360 and the concept of permitting multiple object-code-compatible implementa-\ntions. The notion of an instruction set architecture as we understand it today\nwas the most important aspect of the 360. The architecture also introduced several\nimportant innovations, now in wide use:\n1. 32-bit architecture\n2. Byte-addressable memory with 8-bit bytes\n3. 8-, 16-, 32-, and 64-bit data sizes\n4. 32-bit single-precision and 64-bit double-precision floating-point data\nIn1971,IBM shippedthefirst System/370 (models155 and 165), whichincluded\na number of significant extensions of the 360, as discussed by Case and Padegs\n[1978], who also discussed the early history of System/360. Themost important addi-\ntion was virtual memory, though virtual memory 370 s did not ship until 1972, when\na virtual memory operating system was ready. By 1978, the high-end 370 was several\nhundred times faster than the low-end 360 s shipped 10 years earlier. In 1984, the 24-\nbit addressingmodel built intothe IBM 360 neededtobeabandoned,and the 370-XA\n(eXtended Architecture) was introduced. While old 24-bit programs could be sup-\nported without change, several instructions could not function in the same manner\nwhen extended to a 32-bit addressing model (31-bit addresses supported) because\nthey would not produce 31-bit addresses. Converting the operating system, which\nwas written mostly in assembly language, was no doubt the biggest task.\nSeveral studies of the IBM 360 and instruction measurement have been made.\nShustek\u2019s thesis [1978] is the best known and most complete study of the 360/370\narchitecture. He made several observations about instruction set complexity that\nK.5\nThe IBM 360/370 Architecture for Mainframe Computers\n\u25a0\nK-69"
    },
    {
        "page": 1339,
        "text": "were not fully appreciated until some years later. Another important study of the\n360 is the Toronto study by Alexander and Wortman [1975] done on an IBM 360\nusing 19 XPL programs.\nSystem/360 Instruction Set\nThe 360 instruction set is shown in the following tables, organized by instruction\ntype and format. System/370 contains 15 additional user instructions.\nInteger/Logical and Floating-Point R-R Instructions\nThe * indicates the instruction is floating point, and may be either D (double pre-\ncision) or E (single precision).\nn\no\nit\np\nir\nc\ns\ne\nD\nn\no\nit\nc\nu\nrt\ns\nn\nI\nretsig\ner\nla\ncig\nol\nd\nd\nA\nALR\nretsig\ner\nd\nd\nA\nAR\nn\noitid\nd\na\nP\nF\nA*R\nretsig\ner\nla\ncig\nol\nera\np\nm\no\nC\nCLR\nretsig\ner\nera\np\nm\no\nC\nCR\nera\np\nm\no\nc\nP\nF\nC*R\nretsig\ner\ne\ndivi\nD\nDR\ne\ndivid\nP\nF\nD*R\ne\nvla\nh\nP\nF\nH*R\nretsig\ner\ntn\ne\nm\nelp\nm\no\nc\nd\na\no\nL\nLCR\ntn\ne\nm\nelp\nm\no\nc\nd\na\no\nL\nLC*R\nretsig\ner\ne\nvita\ng\ne\nn\nd\na\no\nL\nLNR\ne\nvita\ng\ne\nn\nd\na\no\nL\nLN*R\nretsig\ner\ne\nvitis\no\np\nd\na\no\nL\nLPR\ne\nvitis\no\np\nd\na\no\nL\nLP*R\nretsig\ner\nd\na\no\nL\nLR\nretsig\ner\nP\nF\nd\na\no\nL\nL*R\nretsig\ner\nts\net\nd\nn\na\nd\na\no\nL\nLTR\nretsig\ner\nP\nF\nts\net\nd\nn\na\nd\na\no\nL\nLT*R\nretsig\ner\nylpitlu\nM\nMR\nylpitlu\nm\nP\nF\nM*R\nretsig\ner\nd\nn\nA\nNR\nretsig\ner\nr\nO\nOR\nl\ntc\nartb\nu\nS\nSLR\nogical register\nretsig\ner\ntc\nartb\nu\nS\nSR\nn\noitc\nartb\nu\ns\nP\nF\nS*R\nretsig\ner\nr\no\ne\nvis\nulc\nx\nE\nXR\nK-70\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1340,
        "text": "Branches and Status Setting R-R Instructions\nThese are R-R format instructions that either branch or set some system status; sev-\neral of them are privileged and legal only in supervisor mode.\nn\no\nit\np\nir\nc\ns\ne\nD\nn\no\nit\nc\nu\nrt\ns\nn\nI\nBALR\nBranch and link\nBCTR\nBranch on count\nBCR\nBranch/condition\nISK\nInsert key\nSPM\nSet program mask\nSSK\nSet storage key\nSVC\nSupervisor call\nBranches/Logical and Floating-Point Instructions\u2014RX Format\nThese are all RX format instructions. The symbol \u201c+\u201d means either a word oper-\nation (and then stands for nothing) or H (meaning half word); for example, A+\nstands for the two opcodes A and AH. The \u201c*\u201d represents D or E, standing for\ndouble- or single-precision floating point.\nn\no\nit\np\nir\nc\ns\ne\nD\nn\no\nit\nc\nu\nrt\ns\nn\nI\nA+\nAdd\nA*\nFP add\nAL\nAdd logical\nC+\nCompare\nC*\nFP compare\nCL\nCompare logical\ne\ndivi\nD\nD\nD*\nFP divide\nL+\nLoad\nL*\nLoad FP register\nM+\nMultiply\nM*\nFP multiply\nd\nn\nA\nN\nr\nO\nO\nS+\nSubtract\nS*\nFP subtract\nSL\nSubtract logical\nST+\nStore\nST*\nStore FP register\nr\no\ne\nvis\nulc\nx\nE\nX\nK.5\nThe IBM 360/370 Architecture for Mainframe Computers\n\u25a0\nK-71"
    },
    {
        "page": 1341,
        "text": "Branches and Special Loads and Stores\u2014RX Format\nn\no\nit\np\nir\nc\ns\ne\nD\nn\no\nit\nc\nu\nrt\ns\nn\nI\nk\nnil d\nn\na h\nc\nn\nar\nB\nBAL\nn\noitid\nn\no\nc h\nc\nn\nar\nB\nBC\ntn\nu\no\nc n\no h\nc\nn\nar\nB\nBCT\ny\nra\nnib\n-tre\nv\nn\no\nC\nCVB\nla\nm\nic\ne\nd\n-tre\nv\nn\no\nC\nCVD\netu\nc\ne\nx\nE\nEX\nretc\nara\nh\nc tre\ns\nn\nI\nIC\nss\ner\nd\nd\na d\na\no\nL\nLA\nretc\nara\nh\nc er\not\nS\nSTC\nRS and SI Format Instructions\nThese are the RS and SI format instructions. The symbol \u201c*\u201d may be A (arith-\nmetic) or L (logical).\nn\no\nit\np\nir\nc\ns\ne\nD\nn\no\nit\nc\nu\nrt\ns\nn\nI\nh\ngih/h\nc\nn\nar\nB\nBXH\nla\nu\nq\ne-\nw\nol/h\nc\nn\nar\nB\nBXLE\nig\nol era\np\nm\no\nC\nCLI\ncal immediate\nO\n/I tla\nH\nHIO\nW\nS\nP\n d\na\no\nL\nLPSW\nelpitlu\nm\n d\na\no\nL\nLM\netaid\ne\nm\nm\ni e\nv\no\nM\nMVI\netaid\ne\nm\nm\ni d\nn\nA\nNI\netaid\ne\nm\nm\ni r\nO\nOI\ntc\nerid d\na\ne\nR\nRDD\nO\n/I trat\nS\nSIO\nL\n/\nA\n tfel tfih\nS\nSL*\nL\n/\nA\n elb\nu\no\nd tfel tfih\nS\nSLD*\nL\n/\nA\n th\ngir tfih\nS\nSR*\nL\n/\nA\n elb\nu\no\nd th\ngir tfih\nS\nSRD*\nk\ns\na\nm\n \nm\nets\ny\ns te\nS\nSSM\nelpitlu\nm\n er\not\nS\nSTM\nle\nn\nn\na\nh\nc ts\ne\nT\nTCH\nO\n/I ts\ne\nT\nTIO\nk\ns\na\nm\n re\nd\nn\nu ts\ne\nT\nTM\nte\ns-\nd\nn\na-ts\ne\nT\nTS\ntc\nerid etir\nW\nWRD\netaid\ne\nm\nm\ni r\no e\nvis\nulc\nx\nE\nXI\nK-72\n\u25a0\nAppendix K Survey of Instruction Set Architectures"
    },
    {
        "page": 1342,
        "text": "SS Format Instructions\nThese are add decimal or string instructions.\nn\no\nit\np\nir\nc\ns\ne\nD\nn\no\nit\nc\nu\nrt\ns\nn\nI\nd\ne\nk\nc\na\np\nd\nd\nA\nAP\nsra\nh\nc\nla\ncig\nol\nera\np\nm\no\nC\nCLC\nd\ne\nk\nc\na\np\nera\np\nm\no\nC\nCP\nd\ne\nk\nc\na\np\ne\ndivi\nD\nDP\ntid\nE\nED\nk\nra\nm\nd\nn\na\ntid\nE\nEDMK\nd\ne\nk\nc\na\np\nylpitlu\nM\nMP\nretc\nara\nh\nc\ne\nv\no\nM\nMVC\ncire\nm\nu\nn\ne\nv\no\nM\nMVN\nte\nsff\no\nhti\nw\ne\nv\no\nM\nMVO\ne\nn\no\nz\ne\nv\no\nM\nMVZ\nsretc\nara\nh\nc\nd\nn\nA\nNC\nsretc\nara\nh\nc\nr\nO\nOC\nr \u2192 decimal)\netc\nara\nh\nC\n(\nk\nc\na\nP\nPACK\nd\ne\nk\nc\na\np\ntc\nartb\nu\nS\nSP\netals\nn\nar\nT\nTR\nts\net\nd\nn\na\netals\nn\nar\nT\nTRT\nk\nc\na\np\nn\nU\nUNPK\nsretc\nara\nh\nc\nr\no\ne\nvis\nulc\nx\nE\nXC\nd\ne\nk\nc\na\np\nd\nd\na\nd\nn\na\no\nre\nZ\nZAP\n360 Detailed Measurements\nFigure K.59 shows the frequency of instruction usage for four IBM 360 programs.\nK.5\nThe IBM 360/370 Architecture for Mainframe Computers\n\u25a0\nK-73"
    },
    {
        "page": 1343,
        "text": "Instruction\nPLIC\nFORTGO\nPLIGO\nCOBOLGO\nAverage\n%\n6\n1\n%\n6\n1\n%\n5\n%\n3\n1\n%\n2\n3\nlo\nrt\nn\no\nC\n%\n5\n1\n%\n4\n1\n%\n5\n%\n3\n1\n%\n8\n2\nBC, BCR\n%\n1\n%\n2\n%\n3\nBAL, BALR\n%\n0\n1\n%\n1\n2\n%\n7\n1\n%\n3\nA, AR\n%\n3\n%\n7\n%\n3\nSR\n%\n2\n%\n3\n%\n6\nSLL\n%\n2\n%\n1\n%\n1\n%\n8\nLA\n%\n2\n%\n7\nCLI\nNI\n7%\n2%\n%\n3\n%\n0\n%\n4\n%\n4\n%\n5\nC\n%\n2\n%\n3\n%\n1\n%\n3\nTM\n%\n1\n%\n2\nMH\nArithmetic/logical\n29%\n35%\n29%\n9%\n26%\n%\n3\n3\n%\n0\n2\n%\n6\n5\n%\n0\n4\n%\n7\n1\nr\nefs\nn\na\nrt\na\nt\na\nD\n%\n9\n1\n%\n9\n1\n%\n8\n2\n%\n3\n2\n%\n7\nL, LR\n%\n5\n%\n1\n%\n6\n1\n%\n2\nMVI\n%\n3\n%\n7\n%\n3\nST\n%\n2\n%\n2\n%\n7\nLD\n%\n2\n%\n2\n%\n7\nSTD\n%\n1\n%\n3\nLPDR\n%\n1\n%\n3\nLH\n%\n1\n%\n2\nIC\n%\n0\n%\n1\nLTR\n%\n2\n%\n7\nt\nn\nio\np\ng\nn\nit\na\nol\nF\n%\n1\n%\n3\nAD\n%\n1\n%\n3\nMDR\n%\n1\n1\n%\n0\n4\n%\n4\ng\nn\nirts\n,la\nm\nic\ne\nD\n%\n3\n%\n7\n%\n4\nMVC\nAP\n11%\n3%\nZAP\n9%\n2%\nCVD\n5%\n1%\nMP\n3%\n1%\nCLC\n3%\n1%\nCP\n2%\n1%\nED\n1%\n0%\n%\n8\n8\n%\n5\n8\n%\n0\n9\n%\n5\n9\n%\n2\n8\nla\nt\no\nT\nFigure K.59 Distribution of instruction execution frequencies for the four 360 programs. All instructions with a fre-\nquency of execution greater than 1.5% are included. Immediate instructions, which operate on only a single byte, are\nincluded in the section that characterized their operation, rather than with the long character-string versions of the\nsame operation. By comparison, the average frequencies for the major instruction classes of the VAX are 23% (con-\ntrol), 28% (arithmetic), 29% (data transfer), 7% (floating point), and 9% (decimal). Once again, a 1% entry in the aver-\nage column can occur because of entries in the constituent columns. These programs are a compiler for the\nprogramming language PL-I and runtime systems for the programming languages FORTRAN, PL/I, and Cobol."
    },
    {
        "page": 1344,
        "text": "K.6\nHistorical Perspective and References\nSection L.4 (available online) features a discussion on the evolution of instruction\nsets and includes references for further reading and exploration of related topics.\nAcknowledgments\nWe would like to thank the following people for comments on drafts of this survey:\nProfessor Steven B. Furber, University of Manchester; Dr. Dileep Bhandarkar,\nIntel Corporation; Dr. Earl Killian, Silicon Graphics/MIPS; and Dr. Hiokazu\nTakata, Mitsubishi Electric Corporation.\nAcknowledgments\n\u25a0\nK-75"
    },
    {
        "page": 1345,
        "text": ""
    },
    {
        "page": 1346,
        "text": "L\nAdvanced Concepts on\nAddress Translation\nby Abhishek Bhattacharjee\nAppendix L is available online at https://www.elsevier.com/books/computer-\narchitecture/hennessy/978-0-12-811905-1"
    },
    {
        "page": 1347,
        "text": "M.1\nIntroduction\nM-2\nM.2\nThe Early Development of Computers (Chapter 1)\nM-2\nM.3\nThe Development of Memory Hierarchy and Protection\n(Chapter 2 and Appendix B)\nM-9\nM.4\nThe Evolution of Instruction Sets (Appendices A, J, and K)\nM-17\nM.5\nThe Development of Pipelining and Instruction-Level Parallelism\n(Chapter 3 and Appendices C and H)\nM-27\nM.6\nThe Development of SIMD Supercomputers, Vector Computers,\nMultimedia SIMD Instruction Extensions, and Graphical\nProcessor Units (Chapter 4)\nM-45\nM.7\nThe History of Multiprocessors and Parallel Processing\n(Chapter 5 and Appendices F, G, and I)\nM-55\nM.8\nThe Development of Clusters (Chapter 6)\nM-74\nM.9\nHistorical Perspectives and References\nM-79\nM.10\nThe History of Magnetic Storage, RAID, and I/O Buses (Appendix D)\nM-84"
    },
    {
        "page": 1348,
        "text": "M\nHistorical Perspectives and\nReferences\nIf \u2026 history \u2026 teaches us anything, it is that man in his quest for\nknowledge and progress is determined and cannot be deterred.\nJohn F. Kennedy\nAddress at Rice University (1962)\nThose who cannot remember the past are condemned to repeat it.\nGeorge Santayana\nThe Life of Reason (1905), Vol. 2, Chapter 3"
    },
    {
        "page": 1349,
        "text": "M.1\nIntroduction\nThis appendix provides historical background on some of the key ideas presented\nin the chapters. We may trace the development of an idea through a series of\nmachines or describe significant projects. If you are interested in examining the\ninitial development of an idea or machine or are interested in further reading,\nreferences are provided at the end of each section.\nSection M.2 starts us off with the invention of the digital computer and corre-\nsponds to Chapter 1. Section M.3, on memory hierarchy, corresponds to Chapter 2\nand Appendix B. Section M.4, on instruction set architecture, covers Appendices\nA, J, and K. Section M.5, on pipelining and instruction-level parallelism, corre-\nsponds to Chapter 3 and Appendices C and H. Section M.6, on data-level paral-\nlelism in vector, SIMD, and GPU architectures, corresponds to Chapter 4.\nSection M.7, on multiprocessors and parallel programming, covers Chapter 5\nand Appendices F, G, and I. Section M.8, on the development of clusters, covers\nChapter 6. Finally, Section M.9, on I/O, corresponds to Appendix D.\nM.2\nThe Early Development of Computers (Chapter 1)\nIn this historical section, we discuss the early development of digital computers\nand the development of performance measurement methodologies.\nThe First General-Purpose Electronic Computers\nJ. Presper Eckert and John Mauchly at the Moore School of the University of\nPennsylvania built the world\u2019s first fully operational electronic general-purpose\ncomputer. This machine, called ENIAC (Electronic Numerical Integrator and\nCalculator), was funded by the U.S. Army and became operational during World\nWar II, but it was not publicly disclosed until 1946. ENIAC was used for comput-\ning artillery firing tables. The machine was enormous\u2014100 feet long, 8\u00bd feet\nhigh, and several feet wide. Each of the 20 ten-digit registers was 2 feet long.\nIn total, there were 18,000 vacuum tubes.\nAlthough the size was three orders of magnitude bigger than the size of the\naverage machines built today, it was more than five orders of magnitude slower,\nwith an add taking 200 microseconds. The ENIAC provided conditional jumps\nand was programmable, which clearly distinguished it from earlier calculators.\nProgramming was done manually by plugging up cables and setting switches\nand required from a half hour to a whole day. Data were provided on punched\ncards. The ENIAC was limited primarily by a small amount of storage and tedious\nprogramming.\nIn 1944, John von Neumann was attracted to the ENIAC project. The group\nwanted to improve the way programs were entered and discussed storing programs\nas numbers; von Neumann helped crystallize the ideas and wrote a memo propos-\ning a stored-program computer called EDVAC (Electronic Discrete Variable\nM-2\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1350,
        "text": "Automatic Computer). Herman Goldstine distributed the memo and put von Neu-\nmann\u2019s name on it, much to the dismay of Eckert and Mauchly, whose names were\nomitted. This memo has served as the basis for the commonly used term von\nNeumann computer. Several early inventors in the computer field believe that this\nterm gives too much credit to von Neumann, who conceptualized and wrote up the\nideas, and too little to the engineers, Eckert and Mauchly, who worked on the\nmachines. Like most historians, your authors (winners of the 2000 IEEE von\nNeumann Medal) believe that all three individuals played a key role in developing\nthe stored-program computer. Von Neumann\u2019s role in writing up the ideas, in gen-\neralizing them, and in thinking about the programming aspects was critical in trans-\nferring the ideas to a wider audience.\nIn 1946, Maurice Wilkes of Cambridge University visited the Moore School\nto attend the latter part of a series of lectures on developments in electronic com-\nputers. When he returned to Cambridge, Wilkes decided to embark on a project\nto build a stored-program computer named EDSAC (Electronic Delay Storage\nAutomatic Calculator). (The EDSAC used mercury delay lines for its memory;\nhence, the phrase \u201cdelay storage\u201d in its name.) The EDSAC became operational\nin 1949 and was the world\u2019s first full-scale, operational, stored-program computer\n[Wilkes, Wheeler, and Gill 1951; Wilkes 1985, 1995]. (A small prototype called\nthe Mark I, which was built at the University of Manchester and ran in 1948, might\nbe called the first operational stored-program machine.) The EDSAC was an\naccumulator-based architecture. This style of instruction set architecture remained\npopular until the early 1970s. (Appendix A starts with a brief summary of the\nEDSAC instruction set.)\nIn 1947, Mauchly took the time to help found the Association for Computing\nMachinery. He served as the ACM\u2019s first vice-president and second president.\nThat same year, Eckert and Mauchly applied for a patent on electronic computers.\nThe dean of the Moore School, by demanding that the patent be turned over to the\nuniversity, may have helped Eckert and Mauchly conclude that they should leave.\nTheir departure crippled the EDVAC project, which did not become operational\nuntil 1952.\nGoldstine left to join von Neumann at the Institute for Advanced Study at\nPrinceton in 1946. Together with Arthur Burks, they issued a report based on the\n1944 memo [Burks, Goldstine, and von Neumann 1946]. The paper led to the\nIAS machine built by Julian Bigelow at Princeton\u2019s Institute for Advanced Study.\nIt had a total of 1024 40-bit words and was roughly 10 times faster than ENIAC. The\ngroup thought aboutusesforthe machine, publishedasetofreports,and encouraged\nvisitors. These reports and visitors inspired the development of a number of new\ncomputers, including the first IBM computer, the 701, which was based on the\nIAS machine. The paper by Burks, Goldstine, and von Neumann was incredible\nfor the period. Reading it today, you would never guess this landmark paper was\nwritten more than 50 years ago, as most of the architectural concepts seen in modern\ncomputers are discussed there (e.g., see the quote at the beginning of Chapter 2).\nIn the same time period as ENIAC, Howard Aiken was designing an electro-\nmechanical computer called the Mark-I at Harvard. The Mark-I was built by a team\nM.2\nThe Early Development of Computers\n\u25a0\nM-3"
    },
    {
        "page": 1351,
        "text": "of engineers from IBM. He followed the Mark-I with a relay machine, the Mark-II,\nand a pair of vacuum tube machines, the Mark-III and Mark-IV. The Mark-III\nand Mark-IV were built after the first stored-program machines. Because they\nhad separate memories for instructions and data, the machines were regarded as\nreactionary by the advocates of stored-program computers. The term Harvard\narchitecture was coined to describe this type of machine. Though clearly different\nfrom the original sense, this term is used today to apply to machines with a single\nmain memory but with separate instruction and data caches.\nThe Whirlwind project [Redmond and Smith 1980] began at MIT in 1947 and\nwas aimed at applications in real-time radar signal processing. Although it led to\nseveral inventions, its overwhelming innovation was the creation of magnetic core\nmemory, the first reliable and inexpensive memory technology. Whirlwind had\n2048 16-bit words of magnetic core. Magnetic cores served as the main memory\ntechnology for nearly 30 years.\nImportant Special-Purpose Machines\nDuring World War II, major computing efforts in both Great Britain and the United\nStates focused on special-purpose code-breaking computers. The work in Great\nBritain was aimed at decrypting messages encoded with the German Enigma coding\nmachine. This work, which occurred at a location called Bletchley Park, led to two\nimportant machines. The first, an electromechanical machine, conceived of by Alan\nTuring, was called BOMB [see Good in Metropolis, Howlett, and Rota 1980]. The\nsecond, much larger and electronic machine, conceived and designed by Newman\nand Flowers, was called COLOSSUS [see Randall in Metropolis, Howlett, and Rota\n1980]. These were highly specialized cryptanalysis machines, which played a vital\nrole in the war by providing the ability to read coded messages, especially those\nsent to U-boats. The work at Bletchley Park was highly classified (indeed, some\nof it is still classified), so its direct impact on the development of ENIAC, EDSAC,\nand other computers is difficult to trace, but it certainly had an indirect effect in\nadvancing the technology and gaining understanding of the issues.\nSimilar work on special-purpose computers for cryptanalysis went on in the\nUnited States. The most direct descendent of this effort was the company Engineer-\ning Research Associates (ERA) [see Thomash in Metropolis, Howlett, and Rota\n1980], which was founded after the war to attempt to commercialize on the key\nideas. ERA built several machines that were sold to secret government agencies,\nand it was eventually purchased by Sperry-Rand, which had earlier purchased the\nEckert Mauchly Computer Corporation.\nAnother early set of machines that deserves credit was a group of special-\npurpose machines built by Konrad Zuse in Germany in the late 1930s and early\n1940s [see Bauer and Zuse in Metropolis, Howlett, and Rota 1980]. In addition\nto producing an operating machine, Zuse was the first to implement floating point,\nwhich von Neumann claimed was unnecessary! His early machines used a\nmechanical store that was smaller than other electromechanical solutions of the\nM-4\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1352,
        "text": "time. His last machine was electromechanical but, because of the war, was never\ncompleted.\nAn important early contributor to the development of electronic computers was\nJohn Atanasoff, who built a small-scale electronic computer in the early 1940s [Ata-\nnasoff1940].Hismachine,designedatIowaStateUniversity,wasaspecial-purpose\ncomputer (called the ABC, for Atanasoff Berry Computer) that was never\ncompletely operational. Mauchly briefly visited Atanasoff before he built ENIAC,\nand several of Atanasoff\u2019s ideas (e.g., using binary representation) likely influenced\nMauchly. The presence of the Atanasoff machine, delays in filing the ENIAC pat-\nents (the work was classified, and patents could not be filed until after the war), and\nthe distribution of von Neumann\u2019s EDVAC paper were used to break the Eckert\u2013\nMauchly patent [Larson 1973]. Though controversy still rages over Atanasoff\u2019s\nrole, Eckert and Mauchly are usually given credit for building the first working,\ngeneral-purpose, electronic computer [Stern 1980]. Atanasoff, however, demon-\nstrated several important innovations included in later computers. Atanasoff\ndeserves much credit for his work, and he might fairly be given credit for the world\u2019s\nfirst special-purpose electronic computer and for possibly influencing Eckert and\nMauchly.\nCommercial Developments\nIn December 1947, Eckert and Mauchly formed Eckert-Mauchly Computer\nCorporation. Their first machine, the BINAC, was built for Northrop and was\nshown in August 1949. After some financial difficulties, the Eckert-Mauchly Com-\nputer Corporation was acquired by Remington-Rand, later called Sperry-Rand.\nSperry-Rand merged the Eckert-Mauchly acquisition, ERA, and its tabulating\nbusiness to form a dedicated computer division, called UNIVAC. UNIVAC deliv-\nered its first computer, the UNIVAC I, in June 1951. The UNIVAC I sold for\n$250,000 and was the first successful commercial computer\u201448 systems were\nbuilt! Today, this early machine, along with many other fascinating pieces of com-\nputer lore, can be seen at the Computer History Museum in Mountain View,\nCalifornia. Other places where early computing systems can be visited include\nthe Deutsches Museum in Munich and the Smithsonian Institution in Washington,\nD.C., as well as numerous online virtual museums.\nIBM, which earlier had been in the punched card and office automation busi-\nness, didn\u2019t start building computers until 1950. The first IBM computer, the IBM\n701 based on von Neumann\u2019s IAS machine, shipped in 1952 and eventually sold\n19 units [see Hurd in Metropolis, Howlett, and Rota 1980]. In the early 1950s,\nmany people were pessimistic about the future of computers, believing that the\nmarket and opportunities for these \u201chighly specialized\u201d machines were quite lim-\nited. Nonetheless, IBM quickly became the most successful computer company.\nTheir focus on reliability and customer- and market-driven strategies were key.\nAlthough the 701 and 702 were modest successes, IBM\u2019s follow-up machines,\nthe 650, 704, and 705 (delivered in 1954 and 1955) were significant successes,\neach selling from 132 to 1800 computers.\nM.2\nThe Early Development of Computers\n\u25a0\nM-5"
    },
    {
        "page": 1353,
        "text": "Several books describing the early days of computing have been written by the\npioneers [Goldstine 1972; Wilkes 1985, 1995], as well as Metropolis, Howlett, and\nRota [1980], which is a collection of recollections by early pioneers. There are\nnumerous independent histories, often built around the people involved [Slater\n1987], as well as a journal, Annals of the History of Computing, devoted to the\nhistory of computing.\nDevelopment of Quantitative Performance Measures:\nSuccesses and Failures\nIn the earliest days of computing, designers set performance goals\u2014ENIAC was to\nbe 1000 times faster than the Harvard Mark-I, and the IBM Stretch (7030) was to\nbe 100 times faster than the fastest machine in existence. What wasn\u2019t clear,\nthough, was how this performance was to be measured. In looking back over\nthe years, it is a consistent theme that each generation of computers obsoletes\nthe performance evaluation techniques of the prior generation.\nThe original measure of performance was time to perform an individual oper-\nation, such as addition. Since most instructions took the same execution time, the\ntiming of one gave insight into the others. As the execution times of instructions in\na machine became more diverse, however, the time for one operation was no longer\nuseful for comparisons. To take these differences into account, an instruction mix\nwas calculated by measuring the relative frequency of instructions in a computer\nacross many programs. The Gibson mix [Gibson 1970] was an early popular\ninstruction mix. Multiplying the time for each instruction times its weight in the\nmix gave the user the average instruction execution time. (If measured in clock\ncycles, average instruction execution time is the same as average cycles per instruc-\ntion.) Since instruction sets were similar, this was a more accurate comparison than\nadd times. From average instruction execution time, then, it was only a small step\nto MIPS (as we have seen, the one is the inverse of the other). MIPS had the virtue\nof being easy for the layperson to understand.\nAs CPUs became more sophisticated and relied on memory hierarchies and\npipelining, there was no longer a single execution time per instruction; MIPS could\nnot be calculated from the mix and the manual. The next step was benchmarking\nusing kernels and synthetic programs. Curnow and Wichmann [1976] created the\nWhetstone synthetic program by measuring scientific programs written in Algol\n60. This program was converted to FORTRAN and was widely used to character-\nize scientific program performance. An effort with similar goals to Whetstone, the\nLivermore FORTRAN Kernels, was made by McMahon [1986] and researchers at\nLawrence Livermore Laboratory in an attempt to establish a benchmark for super-\ncomputers. These kernels, however, consisted of loops from real programs.\nAs it became clear that using MIPS to compare architectures with different\ninstruction sets would not work, a notion of relative MIPS was created. When\nthe VAX-11/780 was ready for announcement in 1977, DEC ran small benchmarks\nthat were also run on an IBM 370/158. IBM marketing referred to the 370/158 as a\nM-6\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1354,
        "text": "1 MIPS computer, and, because the programs ran at the same speed, DEC market-\ning called the VAX-11/780 a 1 MIPS computer. Relative MIPS for a machine M\nwas defined based on some reference machine as:\nMIPSM \u00bc\nPerformanceM\nPerformancereference\nMIPSreference\nThe popularity of the VAX-11/780 made it a popular reference machine for relative\nMIPS, especially since relative MIPS for a 1 MIPS computer is easy to calculate: If\na machine was five times faster than the VAX-11/780, for that benchmark its rating\nwould be 5 relative MIPS. The 1 MIPS rating was unquestioned for 4 years, until\nJoel Emer of DEC measured the VAX-11/780 under a time-sharing load. He found\nthat the VAX-11/780 native MIPS rating was 0.5. Subsequent VAXes that ran 3\nnative MIPS for some benchmarks were therefore called 6 MIPS machines because\nthey ran six times faster than the VAX-11/780. By the early 1980s, the term MIPS\nwas almost universally used to mean relative MIPS.\nThe 1970s and 1980s marked the growth of the supercomputer industry, which\nwas defined by high performance on floating-point-intensive programs. Average\ninstruction time and MIPS were clearly inappropriate metrics for this industry,\nhence the invention of MFLOPS (millions of floating-point operations per\nsecond), which effectively measured the inverse of execution time for a bench-\nmark. Unfortunately, customers quickly forget the program used for the rating,\nand marketing groups decided to start quoting peak MFLOPS in the supercomputer\nperformance wars.\nSPEC (System Performance and Evaluation Cooperative) was founded in the\nlate 1980s to try to improve the state of benchmarking and make a more valid basis\nfor comparison. The group initially focused on workstations and servers in the\nUNIX marketplace, and these remain the primary focus of these benchmarks today.\nThe first release of SPEC benchmarks, now called SPEC89, was a substantial\nimprovement in the use of more realistic benchmarks. SPEC2006 still dominates\nprocessor benchmarks almost two decades later.\nReferences\nAmdahl, G. M. [1967]. \u201cValidity of the single processor approach to achieving\nlarge scale computing capabilities,\u201d Proc. AFIPS Spring Joint Computer Conf.,\nApril 18\u201320, 1967, Atlantic City, N.J., 483\u2013485.\nAtanasoff, J. V. [1940]. \u201cComputing machine for the solution of large systems of\nlinear equations,\u201d Internal Report, Iowa State University, Ames.\nAzizi, O., Mahesri, A., Lee, B. C., Patel, S. J., & Horowitz, M. [2010]. Energy-\nperformance tradeoffs in processor architecture and circuit design: a marginal\ncost analysis. Proc. International Symposium on Computer Architecture, 26-36.\nBell, C. G. [1984]. \u201cThe mini and micro industries,\u201d IEEE Computer 17:10 (Octo-\nber), 14\u201330.\nBell, C. G., J. C. Mudge, and J. E. McNamara [1978]. A DEC View of Computer\nEngineering, Digital Press, Bedford, Mass.\nM.2\nThe Early Development of Computers\n\u25a0\nM-7"
    },
    {
        "page": 1355,
        "text": "Burks, A. W., H. H. Goldstine, and J. von Neumann [1946]. \u201cPreliminary\ndiscussion of the logical design of an electronic computing instrument,\u201d Report\nto the U.S. Army Ordnance Department, p. 1; also appears in Papers of John von\nNeumann, W. Aspray and A. Burks, eds., MIT Press, Cambridge, Mass., and\nTomash Publishers, Los Angeles, Calif., 1987, 97\u2013146.\nCurnow, H. J., and B. A. Wichmann [1976]. \u201cA synthetic benchmark,\u201d The\nComputer J. 19:1, 43\u201349.\nDally, William J., \u201cHigh Performance Hardware for Machine Learning,\u201d Cadence\nEmbedded Neural Network Summit, February 9, 2016. http://ip.cadence.\ncom/uploads/presentations/1000AM_Dally_Cadence_ENN.pdf\nFlemming, P. J., and J. J. Wallace [1986]. \u201cHow not to lie with statistics: The cor-\nrect way to summarize benchmarks results,\u201d Communications of the ACM 29:3\n(March), 218\u2013221.\nFuller, S. H., and W. E. Burr [1977]. \u201cMeasurement and evaluation of alternative\ncomputer architectures,\u201d Computer 10:10 (October), 24\u201335.\nGibson, J. C. [1970]. \u201cThe Gibson mix,\u201d Rep. TR. 00.2043, IBM Systems Devel-\nopment Division, Poughkeepsie, N.Y. (research done in 1959).\nGoldstine, H. H. [1972]. The Computer: From Pascal to von Neumann, Princeton\nUniversity Press, Princeton, N.J.\nGray, J., and C. van Ingen [2005]. Empirical Measurements of Disk Failure Rates\nand Error Rates, MSR-TR-2005-166, Microsoft Research, Redmond, Wash.\nJain, R. [1991]. The Art of Computer Systems Performance Analysis: Techniques\nfor Experimental Design, Measurement, Simulation, and Modeling, Wiley,\nNew York.\nKembel, R. [2000]. \u201cFibre Channel: A comprehensive introduction,\u201d Internet\nWeek (April).\nLarson, E. R. [1973]. \u201cFindings of fact, conclusions of law, and order for judg-\nment,\u201d File No. 4-67, Civ. 138, Honeywell v. Sperry-Rand and Illinois Scientific\nDevelopment, U.S. District Court for the State of Minnesota, Fourth Division\n(October 19).\nLubeck, O., J. Moore, and R. Mendez [1985]. \u201cA benchmark comparison of\nthree supercomputers: Fujitsu VP-200, Hitachi S810/20, and Cray X-MP/2,\u201d\nComputer 18:12 (December), 10\u201324.\nLandstrom, B. [2014]. \u201cThe Cost Of Downtime,\u201d http://www.interxion.com/blogs/\n2014/07/the-cost-of-downtime/\nMcMahon, F. M. [1986]. The Livermore FORTRAN Kernels: A Computer Test of\nNumerical Performance Range, Tech. Rep. UCRL-55745, Lawrence Livermore\nNational Laboratory, University of California, Livermore.\nMetropolis, N., J. Howlett, and G. C. Rota, eds. [1980]. A History of Computing in\nthe Twentieth Century, Academic Press, New York.\nMukherjee S. S., C. Weaver, J. S. Emer, S. K. Reinhardt, and T. M. Austin [2003].\n\u201cMeasuring architectural vulnerability factors,\u201d IEEE Micro 23:6, 70\u201375.\nOliker, L., A. Canning, J. Carter, J. Shalf, and S. Ethier [2004]. \u201cScientific com-\nputations on modern parallel vector systems,\u201d Proc. ACM/IEEE Conf. on\nSupercomputing, November 6\u201312, 2004, Pittsburgh, Penn., 10.\nM-8\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1356,
        "text": "Patterson, D. [2004]. \u201cLatency lags bandwidth,\u201d Communications of the ACM\n47:10 (October), 71\u201375.\nRedmond, K. C., and T. M. Smith [1980]. Project Whirlwind\u2014The History of a\nPioneer Computer, Digital Press, Boston.\nShurkin, J. [1984]. Engines of the Mind: A History of the Computer, W. W. Norton,\nNew York.\nSlater, R. [1987]. Portraits in Silicon, MIT Press, Cambridge, Mass.\nSmith, J. E. [1988]. \u201cCharacterizing computer performance with a single number,\u201d\nCommunications of the ACM 31:10 (October), 1202\u20131206.\nSPEC. [1989]. SPEC Benchmark Suite Release 1.0 (October 2).\nSPEC. [1994]. SPEC Newsletter (June).\nStern, N. [1980]. \u201cWho invented the first electronic digital computer?\u201d Annals\nof the History of Computing 2:4 (October), 375\u2013376.\nTouma, W. R. [1993]. The Dynamics of the Computer Industry: Modeling the\nSupply of Workstations and Their Components, Kluwer Academic, Boston.\nWeicker, R. P. [1984]. \u201cDhrystone: A synthetic systems programming bench-\nmark,\u201d Communications of the ACM 27:10 (October), 1013\u20131030.\nWilkes,\nM.\nV.\n[1985].\nMemoirs of\na Computer Pioneer,\nMIT\nPress,\nCambridge, Mass.\nWilkes, M. V. [1995]. Computing Perspectives, Morgan Kaufmann, San\nFrancisco.\nWilkes, M. V., D. J. Wheeler, and S. Gill [1951]. The Preparation of Programs\nfor an Electronic Digital Computer, Addison-Wesley, Cambridge, Mass.\nM.3\nThe Development of Memory Hierarchy and Protection\n(Chapter 2 and Appendix B)\nAlthough the pioneers of computing knew of the need for a memory hierarchy and\ncoined the term, the automatic management of two levels was first proposed\nby Kilburn et al. [1962]. It was demonstrated with the Atlas computer at the\nUniversity of Manchester. This computer appeared the year before the IBM 360\nwas announced. Although IBM planned for its introduction with the next genera-\ntion (System/370), the operating system TSS was not up to the challenge in 1970.\nVirtual memory was announced for the 370 family in 1972, and it was for this com-\nputer that the term translation lookaside buffer was coined [Case and Padegs\n1978]. The only computers today without virtual memory are a few supercom-\nputers, embedded processors, and older personal computers.\nBoth the Atlas and the IBM 360 provided protection on pages, and the GE\n645 was the first system to provide paged segmentation. The earlier Burroughs\ncomputers provided virtual memory using segmentation, similar to the seg-\nmented address scheme of the Intel 8086. The 80286, the first 80x86 to have\nthe protection mechanisms described in Appendix C, was inspired by the\nMultics protection software that ran on the GE 645. Over time, computers\nM.3\nThe Development of Memory Hierarchy and Protection\n\u25a0\nM-9"
    },
    {
        "page": 1357,
        "text": "evolved more elaborate mechanisms. The most elaborate mechanism was capa-\nbilities, which attracted the greatest interest in the late 1970s and early 1980s\n[Fabry 1974; Wulf, Levin, and Harbison 1981]. Wilkes [1982], one of the early\nworkers on capabilities, had this to say:\nAnyone who has been concerned with an implementation of the type just\ndescribed [capability system], or has tried to explain one to others, is likely to feel\nthat complexity has got out of hand. It is particularly disappointing that the\nattractive idea of capabilities being tickets that can be freely handed around\nhas become lost \u2026.\nCompared with a conventional computer system, there will inevitably be a cost\nto be met in providing a system in which the domains of protection are small and\nfrequently changed. This cost will manifest itself in terms of additional hardware,\ndecreased runtime speed, and increased memory occupancy. It is at present an\nopen question whether, by adoption of the capability approach, the cost can\nbe reduced to reasonable proportions. [p. 112]\nToday there is little interest in capabilities either from the operating systems or the\ncomputer architecture communities, despite growing interest in protection and\nsecurity.\nBell and Strecker [1976] reflected on the PDP-11 and identified a small address\nspace as the only architectural mistake that is difficult to recover from. At the time\nof the creation of PDP-11, core memories were increasing at a very slow rate. In\naddition, competition from 100 other minicomputer companies meant that DEC\nmight not have a cost-competitive product if every address had to go through\nthe 16-bit data path twice, hence the architect\u2019s decision to add only 4 more address\nbits than found in the predecessor of the PDP-11.\nThe architects of the IBM 360 were aware of the importance of address size and\nplanned for the architecture to extend to 32 bits of address. Only 24 bits were used\nin the IBM 360, however, because the low-end 360 models would have been even\nslower with the larger addresses in 1964. Unfortunately, the architects didn\u2019t reveal\ntheir plans to the software people, and programmers who stored extra information\nin the upper 8 \u201cunused\u201d address bits foiled the expansion effort. (Apple made a\nsimilar mistake 20 years later with the 24-bit address in the Motorola 68000, which\nrequired a procedure to later determine \u201c32-bit clean\u201d programs for the Macintosh\nwhen later 68000s used the full 32-bit virtual address.) Virtually every computer\nsince then will check to make sure the unused bits stay unused and trap if the bits\nhave the wrong value.\nAs mentioned in the text, system virtual machines were pioneered at IBM as\npart of its investigation into virtual memory. IBM\u2019s first computer with virtual\nmemory was the IBM 360/67, introduced in 1967. IBM researchers wrote the\nprogram CP-67 that created the illusion of several independent 360 computers.\nThey then wrote an interactive, single-user operating system called CMS that\nran on these virtual machines. CP-67 led to the product VM/370, and today\nIBM sells z/VM for its mainframe computers [Meyer and Seawright 1970;\nVan Vleck 2005].\nM-10\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1358,
        "text": "A few years after the Atlas paper, Wilkes published the first paper describing\nthe concept of a cache [1965]:\nThe use is discussed of a fast core memory of, say, 32,000 words as slave to a\nslower core memory of, say, one million words in such a way that in practical cases\nthe effective access time is nearer that of the fast memory than that of the slow\nmemory. [p. 270]\nThis two-page paper describes a direct-mapped cache. Although this is the first\npublication on caches, the first implementation was probably a direct-mapped\ninstruction cache built at the University of Cambridge. It was based on tunnel diode\nmemory, the fastest form of memory available at the time. Wilkes stated that G.\nScarott suggested the idea of a cache memory.\nSubsequent to that publication, IBM started a project that led to the first com-\nmercial computer with a cache, the IBM 360/85 [Liptay 1968]. Gibson [1967]\ndescribed how to measure program behavior as memory traffic as well as miss rate\nand showed how the miss rate varies between programs. Using a sample of 20 pro-\ngrams (each with 3 million references!), Gibson also relied on average memory\naccess time to compare systems with and without caches. This precedent is more\nthan 40 years old, and yet many used miss rates until the early 1990s.\nConti, Gibson, and Pitkowsky [1968] described the resulting performance of\nthe 360/85. The 360/91 outperforms the 360/85 on only 3 of the 11 programs\nin the paper, even though the 360/85 has a slower clock cycle time (80 ns versus\n60 ns), less memory interleaving (4 versus 16), and a slower main memory (1.04\nmicrosecond versus 0.75 microsecond). This paper was also the first to use the term\ncache.\nOthers soon expanded the cache literature. Strecker [1976] published the first\ncomparative cache design paper examining caches for the PDP-11. Smith [1982]\nlater published a thorough survey paper that used the terms spatial locality and\ntemporal locality; this paper has served as a reference for many computer\ndesigners.\nAlthough most studies relied on simulations, Clark [1983] used a hardware\nmonitor to record cache misses of the VAX-11/780 over several days. Clark\nand Emer [1985] later compared simulations and hardware measurements for\ntranslations.\nHill [1987] proposed the three C\u2019s used in Appendix B to explain cache\nmisses. Jouppi [1998] retrospectively said that Hill\u2019s three C\u2019s model led directly\nto his invention of the victim cache to take advantage of faster direct-mapped\ncaches and yet avoid most of the cost of conflict misses. Sugumar and Abraham\n[1993] argued that the baseline cache for the three C\u2019s model should use\noptimal replacement; this would eliminate the anomalies of least recently used\n(LRU)-based miss classification and allow conflict misses to be broken down into\nthose caused by mapping and those caused by a nonoptimal replacement\nalgorithm.\nOne of the first papers on nonblocking caches was by Kroft [1981]. Kroft\n[1998] later explained that he was the first to design a computer with a cache at\nM.3\nThe Development of Memory Hierarchy and Protection\n\u25a0\nM-11"
    },
    {
        "page": 1359,
        "text": "Control Data Corporation, and when using old concepts for new mechanisms he hit\nupon the idea of allowing his two-ported cache to continue to service other\naccesses on a miss.\nBaer and Wang [1988] did one of the first examinations of the multilevel inclu-\nsion property. Wang, Baer, and Levy [1989] then produced an early paper on per-\nformance evaluation of multilevel caches. Later, Jouppi and Wilton [1994]\nproposed multilevel exclusion for multilevel caches on chip.\nIn addition to victim caches, Jouppi [1990] also examined prefetching via\nstreaming buffers. His work was extended by Farkas, Jouppi, and Chow [1995]\nto streaming buffers that work well with nonblocking loads and speculative exe-\ncution for in-order processors, and later Farkas et al. [1997] showed that, while out-\nof-order processors can tolerate unpredictable latency better, they still benefit.\nThey also refined memory bandwidth demands of stream buffers.\nProceedings of the Symposium on Architectural Support for Compilers and\nOperating Systems (ASPLOS) and the International Computer Architecture Sym-\nposium (ISCA) from the 1990s are filled with papers on caches. (In fact, some\nwags claimed ISCA really stood for the International Cache Architecture\nSymposium.)\nChapter 2 relies on the measurements of SPEC2000 benchmarks collected by\nCantin and Hill [2001]. There are several other papers used in Chapter 2 that\nare cited in the captions of the figures that use the data: Agarwal and Pudar\n[1993]; Barroso, Gharachorloo, and Bugnion [1998]; Farkas and Jouppi [1994];\nJouppi [1990]; Lam, Rothberg, and Wolf [1991]; Lebeck and Wood [1994];\nMcCalpin [2005]; Mowry, Lam, and Gupta [1992]; and Torrellas, Gupta, and\nHennessy [1992].\nReferences\nAgarwal, A. [1987]. \u201cAnalysis of Cache Performance for Operating Systems and\nMultiprogramming,\u201d Ph.D. thesis, Tech. Rep. No. CSL-TR-87-332, Stanford\nUniversity, Palo Alto, Calif.\nAgarwal, A., and S. D. Pudar [1993]. \u201cColumn-associative caches: A technique for\nreducing the miss rate of direct-mapped caches,\u201d 20th Annual Int\u2019l. Symposium\non Computer Architecture (ISCA), May 16\u201319, 1993, San Diego, Calif.\n(Computer Architecture News 21:2 (May), 179\u2013190).\nBaer, J.-L., and W.-H. Wang [1988]. \u201cOn the inclusion property for multi-level\ncache hierarchies,\u201d Proc. 15th Annual Int\u2019l. Symposium on Computer Architec-\nture (ISCA), May 30\u2013June 2, 1988, Honolulu, Hawaii, 73\u201380.\nBarham, P., B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho, and R. Neugebauer\n[2003]. \u201cXen and the art of virtualization,\u201d Proc. of the 19th ACM Symposium on\nOperating Systems Principles, October 19\u201322, 2003, Bolton Landing, N.Y.\nBarroso, L. A., K. Gharachorloo, and E. Bugnion [1998]. \u201cMemory system char-\nacterization of commercial workloads,\u201d Proc. 25th Annual Int\u2019l. Symposium on\nComputer Architecture (ISCA), July 3\u201314, 1998, Barcelona, Spain, 3\u201314.\nM-12\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1360,
        "text": "Bell, C. G., and W. D. Strecker [1976]. \u201cComputer structures: What have we\nlearned from the PDP-11?\u201d Proc. Third Annual Int\u2019l. Symposium on Computer\nArchitecture (ISCA), January 19\u201321, 1976, Tampa, Fla., 1\u201314.\nBhandarkar, D. P. [1995]. Alpha Architecture Implementations, Digital Press,\nNewton, Mass.\nBorg, A., R. E. Kessler, and D. W. Wall [1990]. \u201cGeneration and analysis of very\nlong address traces,\u201d Proc. 17th Annual Int\u2019l. Symposium on Computer Archi-\ntecture (ISCA), May 28\u201331, 1990, Seattle, Wash., 270\u2013279.\nCantin, J. F., and M. D. Hill [2001]. \u201cCache performance for selected SPEC\nCPU2000\nbenchmarks,\u201d\nhttp://www.cs.wisc.edu/multifacet/misc/\nspec2000cache-data/.\nCantin, J., and M. Hill [2003]. \u201cCache performance for SPEC CPU2000 bench-\nmarks, version 3.0,\u201d http://www.cs.wisc.edu/multifacet/misc/spec2000cache-\ndata/index.html.\nCase, R. P., and A. Padegs [1978]. \u201cThe architecture of the IBM System/370,\u201d\nCommunications of the ACM 21:1, 73\u201396. Also appears in D. P. Siewiorek,\nC. G. Bell, and A. Newell, Computer Structures: Principles and Examples,\nMcGraw-Hill, New York, 1982, 830\u2013855.\nClark, B., T. Deshane, E. Dow, S. Evanchik, M. Finlayson, J. Herne, and J. Neefe\nMatthews [2004]. \u201cXen and the art of repeated research,\u201d Proc. USENIX Annual\nTechnical Conf., June 27\u2013July 2, 2004, Boston, 1135\u20131144.\nClark, D. W. [1983]. \u201cCache performance of the VAX-11/780,\u201d ACM Trans. on\nComputer Systems 1:1, 24\u201337.\nClark, D. W., and J. S. Emer [1985]. \u201cPerformance of the VAX-11/780 translation\nbuffer: Simulation and measurement,\u201d ACM Trans. on Computer Systems 3:1\n(February), 31\u201362.\nCompaq Computer Corporation. [1999]. Compiler Writer\u2019s Guide for the Alpha\n21264, Order Number EC-RJ66A-TE, June.\nConti, C., D. H. Gibson, and S. H. Pitkowsky [1968]. \u201cStructural aspects of the\nSystem/360 Model 85. Part I. General organization,\u201d IBM Systems J. 7:1, 2\u201314.\nCrawford, J., and P. Gelsinger [1988]. Programming the 80386, Sybex,\nAlameda, Calif.\nCvetanovic, Z., and R. E. Kessler [2000]. \u201cPerformance analysis of the\nAlpha 21264-based Compaq ES40 system,\u201d Proc. 27th Annual Int\u2019l. Sympo-\nsium on Computer Architecture (ISCA), June 10\u201314, 2000, Vancouver, Canada,\n192\u2013202.\nFabry, R. S. [1974]. \u201cCapability based addressing,\u201d Communications of the ACM\n17:7 (July), 403\u2013412.\nFarkas, K. I., P. Chow, N. P. Jouppi, and Z. Vranesic [1997]. \u201cMemory-system\ndesign considerations for dynamically-scheduled processors,\u201d Proc. 24th\nAnnual Int\u2019l. Symposium on Computer Architecture (ISCA), June 2\u20134, 1997,\nDenver, Colo., 133\u2013143.\nFarkas, K. I., and N. P. Jouppi [1994]. \u201cComplexity/performance trade-offs with\nnon-blocking loads,\u201d Proc. 21st Annual Int\u2019l. Symposium on Computer Archi-\ntecture (ISCA), April 18\u201321, 1994, Chicago.\nM.3\nThe Development of Memory Hierarchy and Protection\n\u25a0\nM-13"
    },
    {
        "page": 1361,
        "text": "Farkas, K. I., N. P. Jouppi, and P. Chow [1995]. \u201cHow useful are non-blocking\nloads, stream buffers and speculative execution in multiple issue processors?\u201d\nProc. First IEEE Symposium on High-Performance Computer Architecture,\nJanuary 22\u201325, 1995, Raleigh, N.C., 78\u201389.\nGao, Q. S. [1993]. \u201cThe Chinese remainder theorem and the prime memory\nsystem,\u201d 20th Annual Int\u2019l. Symposium on Computer Architecture (ISCA),\nMay 16\u201319, 1993, San Diego, Calif. (Computer Architecture News 21:2\n(May), 337\u2013340).\nGee, J. D., M. D. Hill, D. N. Pnevmatikatos, and A. J. Smith [1993]. \u201cCache per-\nformance of the SPEC92 benchmark suite,\u201d IEEE Micro 13:4 (August), 17\u201327.\nGibson, D. H. [1967]. \u201cConsiderations in block-oriented systems design,\u201d AFIPS\nConf. Proc. 30, 75\u201380.\nHandy, J. [1993]. The Cache Memory Book, Academic Press, Boston.\nHeald, R., K. Aingaran, C. Amir, M. Ang, M. Boland, A. Das, P. Dixit, G. Goulds-\nberry, J. Hart, T. Horel, W.-J. Hsu, J. Kaku, C. Kim, S. Kim, F. Klass, H. Kwan,\nR. Lo, H. McIntyre, A. Mehta, D. Murata, S. Nguyen, Y.-P. Pai, S. Patel, K.\nShin, K. Tam, S. Vishwanthaiah, J. Wu, G. Yee, and H. You [2000]. \u201cImple-\nmentation of third-generation SPARC V9 64-b microprocessor,\u201d ISSCC Digest\nof Technical Papers, 412\u2013413 and slide supplement.\nHill, M. D. [1987]. \u201cAspects of Cache Memory and Instruction Buffer Perfor-\nmance,\u201d Ph.D. thesis, Tech. Rep. UCB/CSD 87/381, Computer Science Divi-\nsion, University of California, Berkeley.\nHill, M. D. [1988]. \u201cA case for direct mapped caches,\u201d Computer 21:12 (Decem-\nber), 25\u201340.\nHorel, T., and G. Lauterbach [1999]. \u201cUltraSPARC-III: Designing third-\ngeneration 64-bit performance,\u201d IEEE Micro 19:3 (May\u2013June), 73\u201385.\nHughes, C. J., P. Kaul, S. V. Adve, R. Jain, C. Park, and J. Srinivasan [2001].\n\u201cVariability in the execution of multimedia applications and implications for\narchitecture,\u201d Proc. 28th Annual Int\u2019l. Symposium on Computer Architecture\n(ISCA), June 30\u2013July 4, 2001, Goteborg, Sweden, 254\u2013265.\nIEEE. [2005]. \u201cIntel virtualization technology, computer,\u201d IEEE Computer Society\n38:5 (May), 48\u201356.\nJouppi, N. P. [1990]. \u201cImproving direct-mapped cache performance by the addi-\ntion of a small fully-associative cache and prefetch buffers,\u201d Proc. 17th Annual\nInt\u2019l. Symposium on Computer Architecture (ISCA), May 28\u201331, 1990, Seattle,\nWash., 364\u2013373.\nJouppi, N. P. [1998]. \u201cRetrospective: Improving direct-mapped cache performance\nby the addition of a small fully-associative cache and prefetch buffers,\u201d in\nG. S. Sohi, ed., 25 Years of the International Symposia on Computer Architec-\nture (Selected Papers), ACM, New York, 71\u201373.\nJouppi, N. P., and S. J. E. Wilton [1994]. \u201cTrade-offs in two-level on-chip cach-\ning,\u201d Proc. 21st Annual Int\u2019l. Symposium on Computer Architecture (ISCA),\nApril 18\u201321, 1994, Chicago, 34\u201345.\nKessler, R. E. [1999]. \u201cThe Alpha 21264 microprocessor,\u201d IEEE Micro 19:2\n(March/April), 24\u201336.\nM-14\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1362,
        "text": "Kilburn, T., D. B. G. Edwards, M. J. Lanigan, and F. H. Sumner [1962]. \u201cOne-level\nstorage system,\u201d IRE Trans. on Electronic Computers EC-11 (April) 223\u2013235.\nAlso appears in D. P. Siewiorek, C. G. Bell, and A. Newell, Computer\nStructures: Principles and Examples, McGraw-Hill, New York, 1982, 135\u2013148.\nKroft, D. [1981]. \u201cLockup-free instruction fetch/prefetch cache organization,\u201d\nProc. Eighth Annual Int\u2019l. Symposium on Computer Architecture (ISCA),\nMay 12\u201314, 1981, Minneapolis, Minn., 81\u201387.\nKroft, D. [1998]. \u201cRetrospective: Lockup-free instruction fetch/prefetch cache\norganization,\u201d in G. S. Sohi, ed., 25 Years of the International Symposia on\nComputer Architecture (Selected Papers), ACM, New York, 20\u201321.\nKunimatsu, A., N. Ide, T. Sato, Y. Endo, H. Murakami, T. Kamei, M. Hirano, F.\nIshihara, H. Tago, M. Oka, A. Ohba, T. Yutaka, T. Okada, and M. Suzuoki\n[2000]. \u201cVector unit architecture for emotion synthesis,\u201d IEEE Micro 20:2\n(March\u2013April), 40\u201347.\nLam, M. S., E. E. Rothberg, and M. E. Wolf [1991]. \u201cThe cache performance\nand optimizations of blocked algorithms,\u201d Proc. Fourth Int\u2019l. Conf. on Archi-\ntectural Support for Programming Languages and Operating Systems\n(ASPLOS), April 8\u201311, 1991, Santa Clara, Calif. (SIGPLAN Notices 26:4\n(April), 63\u201374).\nLebeck, A. R., and D. A. Wood [1994]. \u201cCache profiling and the SPEC bench-\nmarks: A case study,\u201d Computer 27:10 (October), 15\u201326.\nLiptay, J. S. [1968]. \u201cStructural aspects of the System/360 Model 85. Part II. The\ncache,\u201d IBM Systems J. 7:1, 15\u201321.\nLuk, C.-K., and T. C Mowry [1999]. \u201cAutomatic compiler-inserted prefetching for\npointer-based applications,\u201d IEEE Trans. on Computers, 48:2 (February), 134\u2013\n141.\nMcCalpin, J. D. [2005]. \u201cSTREAM: Sustainable Memory Bandwidth in High Per-\nformance Computers,\u201d www.cs.virginia.edu/stream/.\nMcFarling, S. [1989]. \u201cProgram optimization for instruction caches,\u201d Proc. Third\nInt\u2019l. Conf. on Architectural Support for Programming Languages and Operat-\ning Systems (ASPLOS), April 3\u20136, 1989, Boston, 183\u2013191.\nMenon, A., J. Renato Santos, Y. Turner, G. Janakiraman, and W. Zwaenepoel\n[2005]. \u201cDiagnosing performance overheads in the xen virtual machine environ-\nment,\u201d Proc. First ACM/USENIX Int\u2019l. Conf. on Virtual Execution Environ-\nments, June 11\u201312, 2005, Chicago, 13\u201323.\nMeyer, R. A., and L. H. Seawright [1970]. \u201cA virtual machine time sharing sys-\ntem,\u201d IBM Systems J. 9:3, 199\u2013218.\nMowry, T. C., S. Lam, and A. Gupta [1992]. \u201cDesign and evaluation of a compiler\nalgorithm for prefetching,\u201d Proc. Fifth Int\u2019l. Conf. on Architectural Support for\nProgramming Languages and Operating Systems (ASPLOS), October 12\u201315,\n1992, Boston (SIGPLAN Notices 27:9 (September), 62\u201373).\nOka, M., and M. Suzuoki [1999]. \u201cDesigning and programming the emotion\nengine,\u201d IEEE Micro 19:6 (November\u2013December), 20\u201328.\nPabst, T. [2000]. \u201cPerformance Showdown at 133 MHz FSB\u2014The Best Platform\nfor Coppermine,\u201d www6.tomshardware.com/mainboard/00q1/000302/.\nM.3\nThe Development of Memory Hierarchy and Protection\n\u25a0\nM-15"
    },
    {
        "page": 1363,
        "text": "Palacharla, S., and R. E. Kessler [1994]. \u201cEvaluating stream buffers as a secondary\ncache replacement,\u201d Proc. 21st Annual Int\u2019l. Symposium on Computer Architec-\nture (ISCA), April 18\u201321, 1994, Chicago, 24\u201333.\nPrzybylski, S. A. [1990]. Cache Design: A Performance-Directed Approach,\nMorgan Kaufmann, San Francisco.\nPrzybylski, S. A., M. Horowitz, and J. L. Hennessy [1988]. \u201cPerformance trade-\noffs in cache design,\u201d Proc. 15th Annual Int\u2019l. Symposium on Computer Archi-\ntecture (ISCA), May 30\u2013June 2, 1988, Honolulu, Hawaii, 290\u2013298.\nReinman, G., and N. P. Jouppi. [1999]. \u201cExtensions to CACTI.\u201d\nRobin, J., and C. Irvine [2000]. \u201cAnalysis of the Intel Pentium\u2019s ability to support a\nsecure virtual machine monitor,\u201d Proc. USENIX Security Symposium, August\n14\u201317, 2000, Denver, Colo.\nSaavedra-Barrera, R. H. [1992]. \u201cCPU Performance Evaluation and Execution\nTime Prediction Using Narrow Spectrum Benchmarking,\u201d Ph.D. dissertation,\nUniversity of California, Berkeley.\nSamples, A. D., and P. N. Hilfinger [1988]. Code Reorganization for Instruction\nCaches, Tech. Rep. UCB/CSD 88/447, University of California, Berkeley.\nSites, R. L. (ed.) [1992]. Alpha Architecture Reference Manual, Digital Press,\nBurlington, Mass.\nSkadron, K., and D. W. Clark [1997]. \u201cDesign issues and tradeoffs for write\nbuffers,\u201d Proc. Third Int\u2019l. Symposium on High-Performance Computer Archi-\ntecture, February 1\u20135, 1997, San Antonio, Tex., 144\u2013155.\nSmith, A. J. [1982]. \u201cCache memories,\u201d Computing Surveys 14:3 (September),\n473\u2013530.\nSmith, J. E., and J. R. Goodman [1983]. \u201cA study of instruction cache organiza-\ntions and replacement policies,\u201d Proc. 10th Annual Int\u2019l. Symposium on\nComputer Architecture (ISCA), June 5\u20137, 1982, Stockholm, Sweden, 132\u2013137.\nStokes, J. [2000]. \u201cSound and Vision: A Technical Overview of the Emotion\nEngine,\u201d http://arstechnica.com/hardware/reviews/2000/02/ee.ars.\nStrecker, W. D. [1976]. \u201cCache memories for the PDP-11?\u201d Proc. Third Annual\nInt\u2019l. Symposium on Computer Architecture (ISCA), January 19\u201321, 1976,\nTampa, Fla., 155\u2013158.\nSugumar, R. A., and S. G. Abraham [1993]. \u201cEfficient simulation of caches under\noptimal replacement with applications to miss characterization,\u201d Proc. ACM\nSIGMETRICS Conf. on Measurement and Modeling of Computer Systems,\nMay 17\u201321, 1993, Santa Clara, Calif., 24\u201335.\nTarjan, D., S. Thoziyoor, and N. Jouppi [2006]. CACTI 4.0. Technical Report\nHPL-2006-86, HP Laboratories.\nTorrellas, J., A. Gupta, and J. Hennessy [1992]. \u201cCharacterizing the caching and\nsynchronization performance of a multiprocessor operating system,\u201d Proc. Fifth\nInt\u2019l. Conf. on Architectural Support for Programming Languages and Operat-\ning Systems (ASPLOS), October 12\u201315, 1992, Boston (SIGPLAN Notices 27:9\n(September), 162\u2013174).\nVan Vleck, T. [2005]. \u201cThe IBM 360/67 and CP/CMS,\u201d http://www.multicians.\norg/thvv/360-67.html.\nM-16\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1364,
        "text": "Wang, W.-H., J.-L. Baer, and H. M. Levy [1989]. \u201cOrganization and performance\nof a two-level virtual-real cache hierarchy,\u201d Proc. 16th Annual Int\u2019l. Symposium\non Computer Architecture (ISCA), May 28\u2013June 1, 1989, Jerusalem, 140\u2013148.\nWilkes, M. [1965]. \u201cSlave memories and dynamic storage allocation,\u201d IEEE\nTrans. Electronic Computers EC-14:2 (April), 270\u2013271.\nWilkes, M. V. [1982]. \u201cHardware support for memory protection: Capability\nimplementations,\u201d Proc. Symposium on Architectural Support for Program-\nming Languages and Operating Systems (ASPLOS), March 1\u20133, 1982, Palo\nAlto, Calif., 107\u2013116.\nWulf, W. A., R. Levin, and S. P. Harbison [1981]. Hydra/C.mmp: An Experimental\nComputer System, McGraw-Hill, New York.\nM.4\nThe Evolution of Instruction Sets (Appendices A, J, and K)\nOne\u2019s eyebrows should rise whenever a future architecture is developed with a\nstack- or register-oriented instruction set.\nMeyers [1978, p. 20]\nThe earliest computers, including the UNIVAC I, the EDSAC, and the IAS\ncomputers, were accumulator-based computers. The simplicity of this type of\ncomputer made it the natural choice when hardware resources were very con-\nstrained. The first general-purpose register computer was the Pegasus, built by\nFerranti, Ltd., in 1956. The Pegasus had eight general-purpose registers, with\nR0 always being zero. Block transfers loaded the eight registers from the drum\nmemory.\nStack Architectures\nIn 1963, Burroughs delivered the B5000. The B5000 was perhaps the first com-\nputer to seriously consider software and hardware-software trade-offs. Barton\nand the designers at Burroughs made the B5000 a stack architecture (as described\nin Barton [1961]). Designed to support high-level languages such as ALGOL, this\nstack architecture used an operating system (MCP) written in a high-level lan-\nguage. The B5000 was also the first computer from a U.S. manufacturer to support\nvirtual memory. The B6500, introduced in 1968 (and discussed in Hauck and Dent\n[1968]), added hardware-managed activation records. In both the B5000 and\nB6500, the top two elements of the stack were kept in the processor and the rest\nof the stack was kept in memory. The stack architecture yielded good code density,\nbut only provided two high-speed storage locations. The authors of both the orig-\ninal IBM 360 paper [Amdahl, Blaauw, and Brooks 1964] and the original PDP-11\npaper [Bell et al. 1970] argued against the stack organization. They cited three\nmajor points in their arguments against stacks:\nM.4\nThe Evolution of Instruction Sets\n\u25a0\nM-17"
    },
    {
        "page": 1365,
        "text": "\u25a0\nPerformance is derived from fast registers, not the way they are used.\n\u25a0\nThe stack organization is too limiting and requires many swap and copy\noperations.\n\u25a0\nThe stack has a bottom, and when placed in slower memory there is a\nperformance loss.\nStack-based hardware fell out of favor in the late 1970s and, except for the Intel\n80x86 floating-point architecture, essentially disappeared; for example, except for\nthe 80x86, none of the computers listed in the SPEC report uses a stack.\nIn the 1990s, however, stack architectures received a shot in the arm with the\nsuccess of the Java Virtual Machine (JVM). The JVM is a software interpreter for\nan intermediate language produced by Java compilers, called Java bytecodes\n[Lindholm and Yellin 1999]. The purpose of the interpreter is to provide software\ncompatibility across many platforms, with the hope of \u201cwrite once, run every-\nwhere.\u201d Although the slowdown is about a factor of 10 due to interpretation, there\nare times when compatibility is more important than performance, such as when\ndownloading a Java \u201capplet\u201d into an Internet browser.\nAlthough a few have proposed hardware to directly execute the JVM instruc-\ntions (see McGhan and O\u2019Connor [1998]), thus far none of these proposals has\nbeen significant commercially. The hope instead is that just-in-time (JIT) Java\ncompilers\u2014which compile during runtime to the native instruction set of the\ncomputer running the Java program\u2014will overcome the performance penalty\nof interpretation. The popularity of Java has also led to compilers that compile\ndirectly into the native hardware instruction sets, bypassing the illusion of the\nJava bytecodes.\nComputer Architecture Defined\nIBM coined the term computer architecture in the early 1960s. Amdahl, Blaauw,\nand Brooks [1964] used the term to refer to the programmer-visible portion of the\nIBM 360 instruction set. They believed that a family of computers of the same\narchitecture should be able to run the same software. Although this idea may seem\nobvious to us today, it was quite novel at that time. IBM, although it was the lead-\ning company in the industry, had five different architectures before the 360; thus,\nthe notion of a company standardizing on a single architecture was a radical one.\nThe 360 designers hoped that defining a common architecture would bring six\ndifferent divisions of IBM together. Their definition of architecture was\n\u2026 the structure of a computer that a machine language programmer must\nunderstand to write a correct (timing independent) program for that machine.\nThe term machine language programmer meant that compatibility would hold,\neven in machine language, while timing independent allowed different implemen-\ntations. This architecture blazed the path for binary compatibility, which others\nhave followed.\nM-18\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1366,
        "text": "The IBM 360 was the first computer to sell in large quantities with both byte\naddressing using 8-bit bytes and general-purpose registers. The 360 also had\nregister-memory and limited memory-memory instructions. Appendix K summa-\nrizes this instruction set.\nIn 1964, Control Data delivered the first supercomputer, the CDC 6600. As\nThornton [1964] discussed, he, Cray, and the other 6600 designers were among\nthe first to explore pipelining in depth. The 6600 was the first general-purpose,\nload-store computer. In the 1960s, the designers of the 6600 realized the need\nto simplify architecture for the sake of efficient pipelining. Microprocessor and\nminicomputer designers largely neglected this interaction between architectural\nsimplicity and implementation during the 1970s, but it returned in the 1980s.\nHigh-Level Language Computer Architecture\nIn the late 1960s and early 1970s, people realized that software costs were growing\nfaster than hardware costs. McKeeman [1967] argued that compilers and operating\nsystems were getting too big and too complex and taking too long to develop.\nBecause of inferior compilers and the memory limitations of computers, most\nsystems programs at the time were still written in assembly language. Many\nresearchers proposed alleviating the software crisis by creating more powerful,\nsoftware-oriented architectures. Tanenbaum [1978] studied the properties of\nhigh-level languages. Like other researchers, he found that most programs are sim-\nple. He argued that architectures should be designed with this in mind and that they\nshould optimize for program size and ease of compilation. Tanenbaum proposed a\nstack computer with frequency-encoded instruction formats to accomplish these\ngoals; however, as we have observed, program size does not translate directly\nto cost-performance, and stack computers faded out shortly after this work.\nStrecker\u2019s article [1978] discusses how he and the other architects at DEC\nresponded to this by designing the VAX architecture. The VAX was designed\nto simplify compilation of high-level languages. Compiler writers had complained\nabout the lack of complete orthogonality in the PDP-11. The VAX architecture\nwas designed to be highly orthogonal and to allow the mapping of a high-level\nlanguage statement into a single VAX instruction. Additionally, the VAX\ndesigners tried to optimize code size because compiled programs were often too\nlarge for available memories. Appendix K summarizes this instruction set.\nThe VAX-11/780 was the first computer announced in the VAX series. It is\none of the most successful\u2014and most heavily studied\u2014computers ever built.\nThe cornerstone of DEC\u2019s strategy was a single architecture, VAX, running a sin-\ngle operating system, VMS. This strategy worked well for over 10 years. The large\nnumber of papers reporting instruction mixes, implementation measurements, and\nanalysis of the VAX makes it an ideal case study [Clark and Levy 1982; Wiecek\n1982]. Bhandarkar and Clark [1991] gave a quantitative analysis of the disadvan-\ntages of the VAX versus a RISC computer, essentially a technical explanation for\nthe demise of the VAX.\nM.4\nThe Evolution of Instruction Sets\n\u25a0\nM-19"
    },
    {
        "page": 1367,
        "text": "While the VAX was being designed, a more radical approach, called high-level\nlanguage computer architecture (HLLCA), was being advocated in the research\ncommunity. This movement aimed to eliminate the gap between high-level lan-\nguages and computer hardware\u2014what Gagliardi [1973] called the \u201csemantic\ngap\u201d\u2014by bringing the hardware \u201cup to\u201d the level of the programming language.\nMeyers [1982] provided a good summary of the arguments and a history of high-\nlevel language computer architecture projects. HLLCA never had a significant\ncommercial impact. The increase in memory size on computers eliminated the code\nsize problems arising from high-level languages and enabled operating systems to\nbe written in high-level languages. The combination of simpler architectures\ntogether with software offered greater performance and more flexibility at lower\ncost and lower complexity.\nReduced Instruction Set Computers\nIn the early 1980s, the direction of computer architecture began to swing away from\nproviding high-level hardware support for languages. Ditzel and Patterson [1980]\nanalyzed the difficulties encountered by the high-level language architectures\nand argued that the answer lay in simpler architectures. In another paper [Patterson\nand Ditzel 1980], these authors first discussed the idea of Reduced Instruction Set\nComputers (RISCs) and presented the argument for simpler architectures. Clark and\nStrecker [1980], who were VAX architects, rebutted their proposal.\nThe simple load-store computers such as MIPS are commonly called RISC\narchitectures. The roots of RISC architectures go back to computers like the\n6600, where Thornton, Cray, and others recognized the importance of instruction\nset simplicity in building a fast computer. Cray continued his tradition of keeping\ncomputers simple in the CRAY-1. Commercial RISCs are built primarily on the\nwork of three research projects: the Berkeley RISC processor, the IBM 801,\nand the Stanford MIPS processor. These architectures have attracted enormous\nindustrial interest because of claims of a performance advantage of anywhere from\ntwo to five times over other computers using the same technology.\nBegun in 1975, the IBM project was the first to start but was the last to become\npublic. The IBM computer was designed as a 24-bit ECL minicomputer, while the\nuniversity projects were both MOS-based, 32-bit microprocessors. John Cocke is\nconsidered the father of the 801 design. He received both the Eckert\u2013Mauchly and\nTuring awards in recognition of his contribution. Radin [1982] described the high-\nlights of the 801 architecture. The 801 was an experimental project that was never\ndesigned to be a product. In fact, to keep down costs and complexity, the computer\nwas built with only 24-bit registers.\nIn 1980, Patterson and his colleagues at Berkeley began the project that was to\ngive this architectural approach its name (see Patterson and Ditzel [1980]). They\nbuilt two computers called RISC-I and RISC-II. Because the IBM project was not\nwidely known or discussed, the role played by the Berkeley group in promoting the\nRISC approach was critical to acceptance of the technology. They also built one of\nM-20\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1368,
        "text": "the first instruction caches to support hybrid-format RISCs (see Patterson et al.\n[1983]). It supported 16-bit and 32-bit instructions in memory but 32 bits in the\ncache. The Berkeley group went on to build RISC computers targeted toward\nSmalltalk, described by Ungar et al. [1984], and LISP, described by Taylor\net al. [1986].\nIn 1981, Hennessy and his colleagues at Stanford published a description of the\nStanford MIPS computer. Efficient pipelining and compiler-assisted scheduling of\nthe pipeline were both important aspects of the original MIPS design. MIPS stood\nfor Microprocessor without Interlocked Pipeline Stages, reflecting the lack of\nhardware to stall the pipeline, as the compiler would handle dependencies.\nThese early RISC computers\u2014the 801, RISC-II, and MIPS\u2014had much in\ncommon. Both university projects were interested in designing a simple computer\nthat could be built in VLSI within the university environment. All three computers\nused a simple load-store architecture and fixed-format 32-bit instructions, and\nemphasized efficient pipelining. Patterson [1985] described the three computers\nand the basic design principles that have come to characterize what a RISC com-\nputer is, and Hennessy [1984] provided another view of the same ideas, as well as\nother issues in VLSI processor design.\nIn 1985, Hennessy published an explanation of the RISC performance advan-\ntage and traced its roots to a substantially lower CPI\u2014under 2 for a RISC processor\nand over 10 for a VAX-11/780 (though not with identical workloads). A paper by\nEmer and Clark [1984] characterizing VAX-11/780 performance was instrumental\nin helping the RISC researchers understand the source of the performance advan-\ntage seen by their computers.\nSince the university projects finished up, in the 1983\u20131984 time frame, the\ntechnology has been widely embraced by industry. Many manufacturers of the\nearly computers (those made before 1986) claimed that their products were RISC\ncomputers. These claims, however, were often born more of marketing ambition\nthan of engineering reality.\nIn 1986, the computer industry began to announce processors based on the tech-\nnology explored by the three RISC research projects. Moussouris et al. [1986]\ndescribed the MIPS R2000 integer processor, while Kane\u2019s book [1986] provides\na complete description of the architecture. Hewlett-Packard converted their existing\nminicomputer line to RISC architectures; Lee [1989] described the HP Precision\nArchitecture. IBM never directly turned the 801 into a product. Instead, the ideas\nwere adopted for a new, low-end architecture that was incorporated in the IBM\nRT-PC and described in a collection of papers [Waters 1986]. In 1990, IBM\nannounced a new RISC architecture (the RS 6000), which is the first superscalar\nRISC processor. In 1987, Sun Microsystems began delivering computers based\non the SPARC architecture, a derivative of the Berkeley RISC-II processor; SPARC\nis described in Garner et al. [1988]. The PowerPC joined the forces of Apple, IBM,\nand Motorola. Appendix K summarizes several RISC architectures.\nTo help resolve the RISC versus traditional design debate, designers of VAX\nprocessors later performed a quantitative comparison of VAX and a RISC proces-\nsor for implementations with comparable organizations. Their choices were the\nM.4\nThe Evolution of Instruction Sets\n\u25a0\nM-21"
    },
    {
        "page": 1369,
        "text": "VAX 8700 and the MIPS M2000. The differing goals for VAX and MIPS have led\nto very different architectures. The VAX goals, simple compilers and code density,\nled to powerful addressing modes, powerful instructions, efficient instruction\nencoding, and few registers. The MIPS goals were high performance via pipelin-\ning, ease of hardware implementation, and compatibility with highly optimizing\ncompilers. These goals led to simple instructions, simple addressing modes,\nfixed-length instruction formats, and a large number of registers.\nFigure M.1 shows the ratio of the number of instructions executed, the ratio of\nCPIs, and the ratio of performance measured in clock cycles. Since the organiza-\ntions were similar, clock cycle times were assumed to be the same. MIPS executes\nabout twice as many instructions as the VAX, while the CPI for the VAX is about\nsix times larger than that for the MIPS. Hence, the MIPS M2000 has almost\nthree times the performance of the VAX 8700. Furthermore, much less hardware\nis needed to build the MIPS processor than the VAX processor. This cost-\nperformance gap is the reason why the company that used to make the VAX intro-\nduced a MIPS-based product and then has dropped the VAX completely and\nswitched to Alpha, which is quite similar to MIPS. Bell and Strecker [1998]\nsummarized the debate inside the company. Today, DEC, once the second largest\ncomputer company and the major success of the minicomputer industry, exists\nonly as remnants within HP and Intel.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nli\neqntott\nespresso\ndoduc\ntomcatv\nfpppp\nnasa7\nmatrix\nspice\nPerformance\nratio\nInstructions\nexecuted ratio\nCPI ratio\nSPEC89 benchmarks\nMIPS/VAX\nFigure M.1 Ratio of MIPS M2000 to VAX 8700 in instructions executed and performance in clock cycles using\nSPEC89 programs. On average, MIPS executes a little over twice as many instructions as the VAX, but the CPI for\nthe VAX is almost six times the MIPS CPI, yielding almost a threefold performance advantage. (Based on data from\nBhandarkar and Clark [1991].)\nM-22\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1370,
        "text": "Looking back, only one Complex Instruction Set Computer (CISC) instruction\nset survived the RISC/CISC debate, and that one had binary compatibility with PC\nsoftware. The volume of chips is so high in the PC industry that there is a sufficient\nrevenue stream to pay the extra design costs\u2014and sufficient resources due to\nMoore\u2019s law\u2014to build microprocessors that translate from CISC to RISC inter-\nnally. Whatever loss in efficiency occurred (due to longer pipeline stages and big-\nger die size to accommodate translation on the chip) was overcome by the\nenormous volume and the ability to dedicate IC processing lines specifically to this\nproduct.\nInterestingly, Intel also concluded that the future of the 80x86 line was doubt-\nful. They created the IA-64 architecture to support 64-bit addressing and to\nmove to a RISC-style instruction set. The embodiment of the IA-64 (see Huck\net al. [2000]) architecture in the Itanium-1 and Itanium-2 has been a mixed suc-\ncess. Although high performance has been achieved for floating-point applica-\ntions, the integer performance was never impressive. In addition, the Itanium\nimplementations have been large in transistor count and die size and power\nhungry. The complexity of the IA-64 instruction set, standing at least in partial\nconflict with the RISC philosophy, no doubt contributed to this area and power\ninefficiency.\nAMD decided instead to just stretch the architecture from a 32-bit address to a\n64-bit address, much as Intel had done when the 80386 stretched it from a 16-bit\naddress to a 32-bit address. Intel later followed AMD\u2019s example. In the end, the\ntremendous marketplace advantage of the 80x86 presence was too much even\nfor Intel, the owner of this legacy, to overcome!\nReferences\nAlexander, W. G., and D. B. Wortman [1975]. \u201cStatic and dynamic characteristics\nof XPL programs,\u201d IEEE Computer 8:11 (November), 41\u201346.\nAmdahl, G. M., G. A. Blaauw, and F. P. Brooks, Jr. [1964]. \u201cArchitecture of the\nIBM System 360,\u201d IBM J. Research and Development 8:2 (April), 87\u2013101.\nBarton, R. S. [1961]. \u201cA new approach to the functional design of a computer,\u201d\nProc. Western Joint Computer Conf., May 9\u201311, 1961, Los Angeles, Calif.,\n393\u2013396.\nBell, G., R. Cady, H. McFarland, B. DeLagi, J. O\u2019Laughlin, R. Noonan, and W.\nWulf [1970]. \u201cA new architecture for mini-computers: The DEC PDP-11,\u201d\nProc. AFIPS SJCC, May 5\u20137, 1970, Atlantic City, N.J., 657\u2013675.\nBell, G., and W. D. Strecker [1998]. \u201cComputer structures: What have we learned\nfrom the PDP-11?\u201d in G. S. Sohi, ed., 25 Years of the International Symposia on\nComputer Architecture (Selected Papers), ACM, New York, 138\u2013151.\nBhandarkar, D. P. [1995]. Alpha Architecture and Implementations, Digital Press,\nNewton, Mass.\nBhandarkar, D., and D. W. Clark [1991]. \u201cPerformance from architecture: Com-\nparing a RISC and a CISC with similar hardware organizations,\u201d Proc. Fourth\nM.4\nThe Evolution of Instruction Sets\n\u25a0\nM-23"
    },
    {
        "page": 1371,
        "text": "Int\u2019l. Conf. on Architectural Support for Programming Languages and Operat-\ning Systems (ASPLOS), April 8\u201311, 1991, Palo Alto, Calif., 310\u2013319.\nBier, J. [1997]. \u201cThe evolution of DSP processors,\u201d paper presented at University\nof California, Berkeley, November 14.\nBoddie, J. R. [2000]. \u201cHistory of DSPs,\u201d www.lucent.com/micro/dsp/dsphist.html.\nCase, R. P., and A. Padegs [1978]. \u201cThe architecture of the IBM System/370,\u201d\nCommunications of the ACM 21:1, 73\u201396.\nChow, F. C. [1983]. \u201cA Portable Machine-Independent Global Optimizer\u2014\nDesign and Measurements,\u201d Ph.D. thesis, Stanford University, Palo Alto, Calif.\nClark, D., and H. Levy [1982]. \u201cMeasurement and analysis of instruction set use in\nthe VAX-11/780,\u201d Proc. Ninth Annual Int\u2019l. Symposium on Computer Architec-\nture (ISCA), April 26\u201329, 1982, Austin, Tex., 9\u201317.\nClark, D., and W. D. Strecker [1980]. \u201cComments on \u2018the case for the\nreduced instruction set computer,\u2019\u201d Computer Architecture News 8:6 (October),\n34\u201338.\nCrawford, J., and P. Gelsinger [1988]. Programming the 80386, Sybex Books,\nAlameda, Calif.\nDarcy, J. D., and D. Gay [1996]. \u201cFLECKmarks: Measuring floating point\nperformance using a full IEEE compliant arithmetic benchmark,\u201d CS 252 class\nproject, University of California, Berkeley (see http://www.sonic.net/\u0003jddarcy/\nResearch/fleckmrk.pdf).\nDigital Semiconductor. [1996]. Alpha Architecture Handbook, Version 3, Digital\nPress, Maynard, Mass.\nDitzel, D. R., and D. A. Patterson [1980]. \u201cRetrospective on high-level language\ncomputer architecture,\u201d Proc. Seventh Annual Int\u2019l. Symposium on Computer\nArchitecture (ISCA), May 6\u20138, 1980, La Baule, France, 97\u2013104.\nEmer, J. S., and D. W. Clark [1984]. \u201cA characterization of processor performance\nin the VAX-11/780,\u201d Proc. 11th Annual Int\u2019l. Symposium on Computer Archi-\ntecture (ISCA), June 5\u20137, 1984, Ann Arbor, Mich., 301\u2013310.\nFurber, S. B. [2000]. ARM system-on-chip architecture. Addison-Wesley,\nBoston, Mass.\nGagliardi, U. O. [1973]. \u201cReport of workshop 4\u2014software-related advances in\ncomputer hardware,\u201d Proc. Symposium on the High Cost of Software, Septem-\nber 17\u201319, 1973, Monterey, Calif., 99\u2013120.\nGame, M., and A. Booker [1999]. \u201cCodePack code compression for PowerPC pro-\ncessors,\u201d MicroNews, 5:1.\nGarner, R., A. Agarwal, F. Briggs, E. Brown, D. Hough, B. Joy, S. Kleiman, S.\nMuchnick, M. Namjoo, D. Patterson, J. Pendleton, and R. Tuck [1988]. \u201cScal-\nable processor architecture (SPARC),\u201d Proc. IEEE COMPCON, February 29\u2013\nMarch 4, 1988, San Francisco, 278\u2013283.\nHauck, E. A., and B. A. Dent [1968]. \u201cBurroughs\u2019 B6500/B7500 stack mecha-\nnism,\u201d Proc. AFIPS SJCC, April 30\u2013May 2, 1968, Atlantic City, N.J., 245\u2013251.\nHennessy, J. [1984]. \u201cVLSI processor architecture,\u201d IEEE Trans. on Computers C-\n33:11 (December), 1221\u20131246.\nM-24\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1372,
        "text": "Hennessy, J. [1985]. \u201cVLSI RISC processors,\u201d VLSI Systems Design 6:10 (Octo-\nber), 22\u201332.\nHennessy, J., N. Jouppi, F. Baskett, and J. Gill [1981]. \u201cMIPS: A VLSI processor\narchitecture,\u201d in CMU Conference on VLSI Systems and Computations, Com-\nputer Science Press, Rockville, Md.\nHewlett-Packard. [1994]. PA-RISC 2.0 Architecture Reference Manual, 3rd ed.,\nHewlett-Packard, Palo Alto, Calif.\nHitachi. [1997]. SuperH RISC Engine SH7700 Series Programming Manual, Hita-\nchi, Santa Clara, Calif.\nHuck, J. et al. [2000]. \u201cIntroducing the IA-64 Architecture\u201d IEEE Micro, 20:5,\n(September\u2013October), 12\u201323.\nIBM. [1994]. The PowerPC Architecture, Morgan Kaufmann, San Francisco.\nIntel. [2001]. \u201cUsing MMX instructions to convert RGB to YUV color conver-\nsion,\u201d\ncedar.intel.com/cgi-bin/ids.dll/content/content.jsp?cntKey\u00bcLegacy::\nirtm_AP548_9996&cntType\u00bcIDS_EDITORIAL.\nKahan, J. [1990]. \u201cOn the advantage of the 8087\u2019s stack,\u201d unpublished course\nnotes, Computer Science Division, University of California, Berkeley.\nKane, G. [1986]. MIPS R2000 RISC Architecture, Prentice Hall, Englewood\nCliffs, N.J.\nKane, G. [1996]. PA-RISC 2.0 Architecture, Prentice Hall, Upper Saddle River, N.J.\nKane, G., and J. Heinrich [1992]. MIPS RISC Architecture, Prentice Hall, Engle-\nwood Cliffs, N.J.\nKissell, K. D. [1997]. \u201cMIPS16: High-density for the embedded market,\u201d Proc.\nReal Time Systems \u201997, June 15, 1997, Las Vegas, Nev.\nKozyrakis, C. [2000]. \u201cVector IRAM: A media-oriented vector processor with\nembedded DRAM,\u201d paper presented at Hot Chips 12, August 13\u201315, 2000, Palo\nAlto, Calif, 13\u201315.\nLee, R. [1989]. \u201cPrecision architecture,\u201d Computer 22:1 (January), 78\u201391.\nLevy, H., and R. Eckhouse [1989]. Computer Programming and Architecture: The\nVAX, Digital Press, Boston.\nLindholm, T., and F. Yellin [1999]. The Java Virtual Machine Specification, 2nd\ned., Addison-Wesley, Reading, Mass.\nLunde, A. [1977]. \u201cEmpirical evaluation of some features of instruction set proces-\nsor architecture,\u201d Communications of the ACM 20:3 (March), 143\u2013152.\nMagenheimer, D. J., L. Peters, K. W. Pettis, and D. Zuras [1988]. \u201cInteger multi-\nplication and division on the HP precision architecture,\u201d IEEE Trans. on Com-\nputers 37:8, 980\u2013990.\nMcGhan, H., and M. O\u2019Connor [1998]. \u201cPicoJava: A direct execution engine for\nJava bytecode,\u201d Computer 31:10 (October), 22\u201330.\nMcKeeman, W. M. [1967]. \u201cLanguage directed computer design,\u201d Proc. AFIPS\nFall Joint Computer Conf., November 14\u201316, 1967, Washington, D.C., 413\u2013\n417.\nMeyers, G. J. [1978]. \u201cThe evaluation of expressions in a storage-to-storage archi-\ntecture,\u201d Computer Architecture News 7:3 (October), 20\u201323.\nM.4\nThe Evolution of Instruction Sets\n\u25a0\nM-25"
    },
    {
        "page": 1373,
        "text": "Meyers, G. J. [1982]. Advances in Computer Architecture, 2nd ed., Wiley,\nNew York.\nMIPS. [1997]. MIPS16 Application Specific Extension Product Description.\nMitsubishi. [1996]. Mitsubishi 32-Bit Single Chip Microcomputer M32R Family\nSoftware Manual, Mitsubishi, Cypress, Calif.\nMorse, S., B. Ravenal, S. Mazor, and W. Pohlman [1980]. \u201cIntel microproces-\nsors\u20148080 to 8086,\u201d Computer 13:10 (October).\nMoussouris, J., L. Crudele, D. Freitas, C. Hansen, E. Hudson, S. Przybylski, T.\nRiordan, and C. Rowen [1986]. \u201cA CMOS RISC processor with integrated\nsystem\nfunctions,\u201d\nProc.\nIEEE\nCOMPCON,\nMarch\n3\u20136,\n1986,\nSan\nFrancisco, 191.\nMuchnick, S. S. [1988]. \u201cOptimizing compilers for SPARC,\u201d Sun Technology 1:3\n(Summer), 64\u201377.\nPalmer, J., and S. Morse [1984]. The 8087 Primer, John Wiley & Sons, New\nYork, 93.\nPatterson, D. [1985]. \u201cReduced instruction set computers,\u201d Communications of the\nACM 28:1 (January), 8\u201321.\nPatterson, D. A., and D. R. Ditzel [1980]. \u201cThe case for the reduced instruction set\ncomputer,\u201d Computer Architecture News 8:6 (October), 25\u201333.\nPatterson, D. A., P. Garrison, M. Hill, D. Lioupis, C. Nyberg, T. Sippel, and K. Van\nDyke [1983]. \u201cArchitecture of a VLSI instruction cache for a RISC,\u201d 10th\nAnnual Int\u2019l. Conf. on Computer Architecture Conf. Proc., June 13\u201316,\n1983, Stockholm, Sweden, 108\u2013116.\nRadin, G. [1982]. \u201cThe 801 minicomputer,\u201d Proc. Symposium Architectural Sup-\nport for Programming Languages and Operating Systems (ASPLOS), March 1\u2013\n3, 1982, Palo Alto, Calif., 39\u201347.\nRiemens, A., K. A. Vissers, R. J. Schutten, F. W. Sijstermans, G. J. Hekstra, and G.\nD. La Hei [1999]. \u201cTrimedia CPU64 application domain and benchmark suite,\u201d\nProc. IEEE Int\u2019l. Conf. on Computer Design: VLSI in Computers and\nProcessors (ICCD\u201999), October 10\u201313, 1999, Austin, Tex., 580\u2013585.\nRopers, A., H. W. Lollman, and J. Wellhausen [1999]. DSPstone: Texas Instru-\nments TMS320C54x, Tech. Rep. Nr. IB 315 1999/9-ISS-Version 0.9, Aachen\nUniversity of Technology, Aaachen, Germany (www.ert.rwth-aachen.de/\nProjekte/Tools/coal/dspstone_c54x/index.html).\nShustek, L. J. [1978]. \u201cAnalysis and Performance of Computer Instruction Sets,\u201d\nPh.D. dissertation, Stanford University, Palo Alto, Calif.\nSilicon Graphics. [1996]. MIPS V Instruction Set (see http://www.sgi.com/MIPS/\narch/ISA5/#MIPSV_indx).\nSites, R. L., and R. Witek, eds. [1995]. Alpha Architecture Reference Manual, 2nd\ned., Digital Press, Newton, Mass.\nStrauss, W. [1998]. \u201cDSP Strategies 2002,\u201d www.usadata.com/market_research/\nspr_05/spr_r127-005.htm.\nStrecker, W. D. [1978]. \u201cVAX-11/780: A virtual address extension of the PDP-11\nfamily,\u201d Proc. AFIPS National Computer Conf., June 5\u20138, 1978, Anaheim,\nCalif., 47, 967\u2013980.\nM-26\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1374,
        "text": "Sun Microsystems. [1989]. The SPARC Architectural Manual, Version 8, Part No.\n800-1399-09, Sun Microsystems, Santa Clara, Calif.\nTanenbaum, A. S. [1978]. \u201cImplications of structured programming for machine\narchitecture,\u201d Communications of the ACM 21:3 (March), 237\u2013246.\nTaylor, G., P. Hilfinger, J. Larus, D. Patterson, and B. Zorn [1986]. \u201cEvaluation of\nthe SPUR LISP architecture,\u201d Proc. 13th Annual Int\u2019l. Symposium on Computer\nArchitecture (ISCA), June 2\u20135, 1986, Tokyo.\nTexas Instruments [2000]. \u201cHistory of innovation: 1980s,\u201d www.ti.com/corp/docs/\ncompany/history/1980s.shtml.\nThornton, J. E. [1964]. \u201cParallel operation in Control Data 6600,\u201d Proc. AFIPS Fall\nJoint Computer Conf., Part II, October 27\u201329, 1964, San Francisco, 26, 33\u201340.\nUngar, D., R. Blau, P. Foley, D. Samples, and D. Patterson [1984]. \u201cArchitecture\nof SOAR: Smalltalk on a RISC,\u201d Proc. 11th Annual Int\u2019l. Symposium on\nComputer Architecture (ISCA), June 5\u20137, 1984, Ann Arbor, Mich., 188\u2013197.\nvan Eijndhoven, J. T. J., F. W. Sijstermans, K. A. Vissers, E. J. D. Pol, M. I. A.\nTromp, P. Struik, R. H. J. Bloks, P. van der Wolf, A. D. Pimentel, and H. P. E.\nVranken [1999]. \u201cTrimedia CPU64 architecture,\u201d Proc. IEEE Int\u2019l. Conf. on\nComputer Design: VLSI in Computers and Processors (ICCD\u201999), October\n10\u201313, 1999, Austin, Tex., 586\u2013592.\nWakerly, J. [1989]. Microcomputer Architecture and Programming, Wiley,\nNew York.\nWaters, F. (ed.) [1986]. IBM RT Personal Computer Technology, SA 23-1057,\nIBM, Austin, Tex.\nWeaver, D. L., and T. Germond [1994]. The SPARC Architectural Manual,\nVersion 9,\nPrentice Hall, Englewood Cliffs, N.J.\nWeiss, S., and J. E. Smith [1994]. Power and PowerPC, Morgan Kaufmann,\nSan Francisco.\nWiecek, C. [1982]. \u201cA case study of the VAX 11 instruction set usage for compiler\nexecution,\u201d Proc. Symposium on Architectural Support for Programming Lan-\nguages and Operating Systems (ASPLOS), March 1\u20133, 1982, Palo Alto, Calif.,\n177\u2013184.\nWulf, W. [1981]. \u201cCompilers and computer architecture,\u201d Computer 14:7 (July),\n41\u201347.\nM.5\nThe Development of Pipelining and Instruction-Level\nParallelism (Chapter 3 and Appendices C and H)\nEarly Pipelined CPUs\nThe first general-purpose pipelined processor is considered to be Stretch, the IBM\n7030. Stretch followed the IBM 704 and had a goal of being 100 times faster than\nthe 704. The goal was a stretch from the state of the art at that time, hence the\nM.5\nThe Development of Pipelining and Instruction-Level Parallelism\n\u25a0\nM-27"
    },
    {
        "page": 1375,
        "text": "nickname.Theplanwas toobtain afactorof1.6fromoverlappingfetch,decode, and\nexecute, using a four-stage pipeline. Bloch [1959] and Bucholtz [1962] described\nthe design and engineering trade-offs, including the use of ALU bypasses.\nA series of general pipelining descriptions that appeared in the late 1970s and\nearly 1980s provided most of the terminology and described most of the basic tech-\nniquesusedinsimplepipelines.ThesesurveysincludeKeller[1975],Ramamoorthy\nand Li [1977], and Chen [1980], as well as Kogge [1981], whose book is devoted\nentirely to pipelining. Davidson and his colleagues [1971, 1975] developed the\nconcept of pipeline reservation tables as a design methodology for multicycle pipe-\nlines with feedback (also described in Kogge [1981]). Many designers use a varia-\ntion of these concepts, in either designing pipelines or in creating software to\nschedule them.\nThe RISC processors were originally designed with ease of implementation\nand pipelining in mind. Several of the early RISC papers, published in the early\n1980s, attempt to quantify the performance advantages of the simplification in\ninstruction set. The best analysis, however, is a comparison of a VAX and a MIPS\nimplementation published by Bhandarkar and Clark in 1991, 10 years after the first\npublished RISC papers (see Figure M.1). After 10 years of arguments about the\nimplementation benefits of RISC, this paper convinced even the most skeptical\ndesigners of the advantages of a RISC instruction set architecture.\nJ. E. Smith and his colleagues have written a number of papers examining\ninstruction issue, exception handling, and pipeline depth for high-speed scalar\nCPUs. Kunkel and Smith [1986] evaluated the impact of pipeline overhead and\ndependences on the choice of optimal pipeline depth; they also provided an excel-\nlent discussion of latch design and its impact on pipelining. Smith and Pleszkun\n[1988] evaluated a variety of techniques for preserving precise exceptions. Weiss\nand Smith [1984] evaluated a variety of hardware pipeline scheduling and instruc-\ntion issue techniques.\nThe MIPS R4000 was one of the first deeply pipelined microprocessors and is\ndescribed by Killian [1991] and by Heinrich [1993]. The initial Alpha implemen-\ntation (the 21064) has a similar instruction set and similar integer pipeline struc-\nture, with more pipelining in the floating-point unit.\nThe Introduction of Dynamic Scheduling\nIn 1964, CDC delivered the first CDC 6600. The CDC 6600 was unique in many\nways. In addition to introducing scoreboarding, the CDC 6600 was the first pro-\ncessor to make extensive use of multiple functional units. It also had peripheral\nprocessors that used multithreading. The interaction between pipelining and\ninstruction set design was understood, and a simple, load-store instruction set\nwas used to promote pipelining. The CDC 6600 also used an advanced packaging\ntechnology. Thornton [1964] described the pipeline and I/O processor architecture,\nincluding the concept of out-of-order instruction execution. Thornton\u2019s book\n[1970] provides an excellent description of the entire processor, from technology\nM-28\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1376,
        "text": "to architecture, and includes a foreword by Cray. (Unfortunately, this book is\ncurrently out of print.) The CDC 6600 also has an instruction scheduler for the\nFORTRAN compilers, described by Thorlin [1967].\nThe IBM 360 Model 91: A Landmark Computer\nThe IBM 360/91 introduced many new concepts, including tagging of data, reg-\nister renaming, dynamic detection of memory hazards, and generalized forward-\ning. Tomasulo\u2019s algorithm is described in his 1967 paper. Anderson, Sparacio,\nand Tomasulo [1967] described other aspects of the processor, including the\nuse of branch prediction. Many of the ideas in the 360/91 faded from use for nearly\n25 years before being broadly resurrected in the 1990s. Unfortunately, the 360/91\nwas not successful, and only a handful were sold. The complexity of the design\nmade it late to the market and allowed the Model 85, which was the first IBM\nprocessor with a cache, to outperform the 91.\nBranch-Prediction Schemes\nThe 2-bit dynamic hardware branch-prediction scheme was described by J. E.\nSmith [1981]. Ditzel and McLellan [1987] described a novel branch-target buffer\nfor CRISP, which implements branch folding. The correlating predictor we exam-\nine was described by Pan, So, and Rameh [1992]. Yeh and Patt [1992, 1993] gen-\neralized the correlation idea and described multilevel predictors that use branch\nhistories for each branch, similar to the local history predictor used in the\n21264. McFarling\u2019s tournament prediction scheme, which he refers to as a com-\nbined predictor, is described in his 1993 technical report. There are a variety of\nmore recent papers on branch prediction based on variations in the multilevel\nand correlating predictor ideas. Kaeli and Emma [1991] described return address\nprediction, and Evers et al. [1998] provided an in-depth analysis of multilevel pre-\ndictors. The data shown in Chapter 3 are from Skadron et al. [1999]. There are\nseveral schemes for prediction that may offer some additional benefit beyond tour-\nnament predictors. Eden and Mudge [1998] and Jimenez and Lin [2002] have\ndescribed such approaches.\nThe Development of Multiple-Issue Processors\nIBM did pioneering work on multiple issue. In the 1960s, a project called ACS was\nunder way in California. It included multiple-issue concepts, a proposal for dynamic\nscheduling (although with a simpler mechanism than Tomasulo\u2019s scheme, which\nused backup registers), and fetching down both branch paths. The project originally\nstartedasanewarchitecture to follow Stretch and surpassthe CDC 6600/6800.ACS\nstarted in New York but was moved to California, later changed to be S/360 com-\npatible, and eventually canceled. John Cocke was one of the intellectual forces\nbehind the team that included a number of IBM veterans and younger contributors,\nM.5\nThe Development of Pipelining and Instruction-Level Parallelism\n\u25a0\nM-29"
    },
    {
        "page": 1377,
        "text": "many of whom went on to other important roles in IBM and elsewhere: Jack Ber-\ntram, Ed Sussenguth, Gene Amdahl, Herb Schorr, Fran Allen, Lynn Conway,\nand Phil Dauber, among others. While the compiler team published many of their\nideas and had great influence outside IBM, the architecture ideas were not widely\ndisseminated at that time. The most complete accessible documentation of this\nimportant project is at www.cs.clemson.edu/\u0003mark/acs.html, which includes inter-\nviews with the ACS veterans and pointers to other sources. Sussenguth [1999] is a\ngood overview of ACS.\nMost of the early multiple-issue processors that actually reached the market\nfollowed an LIW or VLIW design approach. Charlesworth [1981] reported on\nthe Floating Point Systems AP-120B, one of the first wide-instruction processors\ncontaining multiple operations per instruction. Floating Point Systems applied the\nconcept of software pipelining both in a compiler and by handwriting assembly\nlanguage libraries to use the processor efficiently. Because the processor was an\nattached processor, many of the difficulties of implementing multiple issue in\ngeneral-purpose processors (for example, virtual memory and exception handling)\ncould be ignored.\nOne of the interesting approaches used in early VLIW processors, such as the\nAP-120B and i860, was the idea of a pipeline organization that requires operations\nto be \u201cpushed through\u201d a functional unit and the results to be caught at the end of the\npipeline. In such processors, operations advance only when another operation\npushes them from behind (in sequence). Furthermore, an instruction specifies the\ndestination for an instruction issued earlier that will be pushed out of the pipeline\nwhen this new operation is pushed in. Such an approach has the advantage that it\ndoes not specify a result destination when an operation first issues but only when\nthe result register is actually written. This separation eliminates the need to detect\nwrite after write (WAW) and write after read (WAR) hazards in the hardware.\nThe disadvantage is that it increases code size since no-ops may be needed to push\nresults out when there is a dependence on an operation that is still in the pipeline and\nno other operations of that type are immediately needed. Instead of the \u201cpush-and-\ncatch\u201d approach used in these two processors, almost all designers have chosen to\nuse self-draining pipelines that specify the destination in the issuing instruction and\nin which an issued instruction will complete without further action. The advantages\nin code density and simplifications in code generation seem to outweigh the advan-\ntages of the more unusual structure.\nSeveral research projects introduced some form of multiple issue in the mid-\n1980s. For example, the Stanford MIPS processor had the ability to place two\noperations in a single instruction, although this capability was dropped in com-\nmercial variants of the architecture, primarily for performance reasons. Along\nwith his colleagues at Yale, Fisher [1983] proposed creating a processor with\na very wide instruction (512 bits) and named this type of processor a VLIW.\nCode was generated for the processor using trace scheduling, which Fisher\n[1981] had developed originally for generating horizontal microcode. The imple-\nmentation of trace scheduling for the Yale processor is described by Fisher et al.\n[1984] and by Ellis [1986].\nM-30\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1378,
        "text": "Although IBM canceled ACS, active research in the area continued in the\n1980s. More than 10 years after ACS was canceled, John Cocke made a new\nproposal for a superscalar processor that dynamically made issue decisions; he\nand Tilak Agerwala described the key ideas in several talks in the mid-1980s\nand coined the term superscalar. He called the design America; it is described\nby Agerwala and Cocke [1987]. The IBM Power1 architecture (the RS/6000 line)\nis based on these ideas (see Bakoglu et al. [1989]).\nJ. E. Smith [1984] and his colleagues at Wisconsin proposed the decoupled\napproach that included multiple issue with limited dynamic pipeline scheduling.\nA key feature of this processor is the use of queues to maintain order among a class\nof instructions (such as memory references) while allowing it to slip behind or\nahead of another class of instructions. The Astronautics ZS-1 described by Smith\net al. [1987] embodies this approach with queues to connect the load-store unit and\nthe operation units. The Power2 design uses queues in a similar fashion. J. E. Smith\n[1989] also described the advantages of dynamic scheduling and compared that\napproach to static scheduling.\nThe concept of speculation has its roots in the original 360/91, which per-\nformed a very limited form of speculation. The approach used in recent processors\ncombines the dynamic scheduling techniques of the 360/91 with a buffer to allow\nin-order commit. Smith and Pleszkun [1988] explored the use of buffering to\nmaintain precise interrupts and described the concept of a reorder buffer. Sohi\n[1990] described adding renaming and dynamic scheduling, making it possible\nto use the mechanism for speculation. Patt and his colleagues were early propo-\nnents of aggressive reordering and speculation. They focused on checkpoint and\nrestart mechanisms and pioneered an approach called HPSm, which is also an\nextension of Tomasulo\u2019s algorithm [Hwu and Patt 1986].\nThe use of speculation as a technique in multiple-issue processors was evalu-\nated by Smith, Johnson, and Horowitz [1989] using the reorder buffer technique;\ntheir goal was to study available ILP in nonscientific code using speculation and\nmultiple issue. In a subsequent book, Johnson [1990] described the design of a\nspeculative superscalar processor. Johnson later led the AMD K-5 design, one\nof the first speculative superscalars.\nIn parallel with the superscalar developments, commercial interest in VLIW\napproaches also increased. The Multiflow processor (see Colwell et al. [1987])\nwas based on the concepts developed at Yale, although many important refine-\nments were made to increase the practicality of the approach. Among these was\na control-lable store buffer that provided support for a form of speculation.\nAlthough more than 100 Multiflow processors were sold, a variety of problems,\nincluding the difficulties of introducing a new instruction set from a small company\nand competition from commercial RISC microprocessors that changed the\neconomics in the mini-computer market, led to the failure of Multiflow as a\ncompany.\nAround the same time as Multiflow, Cydrome was founded to build a VLIW-\nstyle processor (see Rau et al. [1989]), which was also unsuccessful commercially.\nDehnert, Hsu, and Bratt [1989] explained the architecture and performance of the\nM.5\nThe Development of Pipelining and Instruction-Level Parallelism\n\u25a0\nM-31"
    },
    {
        "page": 1379,
        "text": "Cydrome Cydra 5, a processor with a wide-instruction word that provides dynamic\nregister renaming and additional support for software pipelining. The Cydra 5 is a\nunique blend of hardware and software, including conditional instructions and\nregister rotation, aimed at extracting ILP. Cydrome relied on more hardware than\nthe Multiflow processor and achieved competitive performance primarily on\nvector-style codes. In the end, Cydrome suffered from problems similar to those\nof Multiflow and was not a commercial success. Both Multiflow and Cydrome,\nalthough unsuccessful as commercial entities, produced a number of people with\nextensive experience in exploiting ILP as well as advanced compiler technology;\nmany of those people have gone on to incorporate their experience and the pieces\nof the technology in newer processors. Fisher and Rau [1993] edited a comprehen-\nsive collection of papers covering the hardware and software of these two impor-\ntant processors.\nRau had also developed a scheduling technique called polycyclic scheduling,\nwhich is a basis for most software-pipelining schemes (see Rau, Glaeser, and\nPicard [1982]). Rau\u2019s work built on earlier work by Davidson and his colleagues\non the design of optimal hardware schedulers for pipelined processors. Other his-\ntorical LIW processors have included the Apollo DN 10000 and the Intel i860, both\nof which could dual-issue FP and integer operations.\nCompiler Technology and Hardware Support for Scheduling\nLoop-level parallelism and dependence analysis were developed primarily by D.\nKuck and his colleagues at the University of Illinois in the 1970s. They also coined\nthe commonly used terminology of antidependence and output dependence and\ndeveloped several standard dependence tests, including the GCD and Banerjee\ntests. The latter test was named after Uptal Banerjee and comes in a variety of fla-\nvors. Recent work on dependence analysis has focused on using a variety of exact\ntests ending with a linear programming algorithm called Fourier\u2013Motzkin. D.\nMaydan and W. Pugh both showed that the sequences of exact tests were a prac-\ntical solution.\nIn the area of uncovering and scheduling ILP, much of the early work was con-\nnected to the development of VLIW processors, described earlier. Lam [1988]\ndeveloped algorithms for software pipelining and evaluated their use on Warp,\na wide-instruction-word processor designed for special-purpose applications.\nWeiss and Smith [1987] compared software pipelining versus loop unrolling as\ntechniques for scheduling code on a pipelined processor. Rau [1994] developed\nmodulo scheduling to deal with the issues of software-pipelining loops and simul-\ntaneously handling register allocation.\nSupport for speculative code scheduling was explored in a variety of contexts,\nincluding several processors that provided a mode in which exceptions were\nignored, allowing more aggressive scheduling of loads (e.g., the MIPS TFP pro-\ncessor [Hsu 1994]). Several groups explored ideas for more aggressive hardware\nsupport for speculative code scheduling. For example, Smith, Horowitz, and Lam\nM-32\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1380,
        "text": "[1992] created a concept called boosting that contains a hardware facility for sup-\nporting speculation but provides a checking and recovery mechanism, similar to\nthose in IA-64 and Crusoe. The sentinel scheduling idea, which is also similar\nto the speculate-and-check approach used in both Crusoe and the IA-64 architec-\ntures, was developed jointly by researchers at the University of Illinois and HP\nLaboratories (see Mahlke et al. [1992]).\nIn the early 1990s, Wen-Mei Hwu and his colleagues at the University of Illi-\nnois developed a compiler framework, called IMPACT (see Chang et al. [1991]),\nfor exploring the interaction between multiple-issue architectures and compiler\ntechnology. This project led to several important ideas, including superblock\nscheduling (see Hwu et al. [1993]), extensive use of profiling for guiding a variety\nof optimizations (e.g., procedure inlining), and the use of a special buffer (similar\nto the ALAT or program-controlled store buffer) for compile-aided memory con-\nflict detection (see Gallagher et al. [1994]). They also explored the performance\ntrade-offs between partial and full support for predication in Mahlke et al. [1995].\nThe early RISC processors all had delayed branches, a scheme inspired from\nmicroprogramming, and several studies on compile time branch prediction were\ninspired by delayed branch mechanisms. McFarling and Hennessy [1986] did a\nquantitative comparison of a variety of compile time and runtime branch-\nprediction schemes. Fisher and Freudenberger [1992] evaluated a range of compile\ntime branch-prediction schemes using the metric of distance between mispredic-\ntions. Ball and Larus [1993] and Calder et al. [1997] described static prediction\nschemes using collected program behavior.\nEPIC and the IA-64 Development\nThe roots of the EPIC approach lie in earlier attempts to build LIW and VLIW\nmachines\u2014especially those at Cydrome and Multiflow\u2014and in a long history\nof compiler work that continued after these companies failed at HP, the University\nof Illinois, and elsewhere. Insights gained from that work led designers at HP to\npropose a VLIW-style, 64-bit architecture to follow the HP PA RISC architecture.\nIntel was looking for a new architecture to replace the x86 (now called IA-32)\narchitecture and to provide 64-bit capability. In 1995, they formed a partnership\nto design a new architecture, IA-64 (see Huck et al. [2000]), and build processors\nbased on it. Itanium (see Sharangpani and Arora [2000]) is the first such processor.\nIn 2002, Intel introduced the second-generation IA-64 design, the Itanium 2\n(see McNairy and Soltis [2003] and McCormick and Knies [2002]).\nStudies of ILP and Ideas to Increase ILP\nA series of early papers, including Tjaden and Flynn [1970] and Riseman and\nFoster [1972], concluded that only small amounts of parallelism could be available\nat the instruction level without investing an enormous amount of hardware. These\npapers dampened the appeal of multiple instruction issue for more than 10 years.\nM.5\nThe Development of Pipelining and Instruction-Level Parallelism\n\u25a0\nM-33"
    },
    {
        "page": 1381,
        "text": "Nicolau and Fisher [1984] published a paper based on their work with trace sched-\nuling and asserted the presence of large amounts of potential ILP in scientific\nprograms.\nSince then there have been many studies of the available ILP. Such studies\nhave been criticized because they presume some level of both hardware support\nand compiler technology. Nonetheless, the studies are useful to set expectations\nas well as to understand the sources of the limitations. Wall has participated in sev-\neral such studies, including Jouppi and Wall [1989] and Wall [1991, 1993].\nAlthough the early studies were criticized as being conservative (e.g., they didn\u2019t\ninclude speculation), the last study is by far the most ambitious study of ILP to date\nand the basis for the data in Section 3.10. Sohi and Vajapeyam [1989] provided\nmeasurements of available parallelism for wide-instruction-word processors.\nSmith, Johnson, and Horowitz [1989] also used a speculative superscalar processor\nto study ILP limits. At the time of their study, they anticipated that the processor\nthey specified was an upper bound on reasonable designs. Recent and upcoming\nprocessors, however, are likely to be at least as ambitious as their processor.\nSkadron et al. [1999] examined the performance trade-offs and limitations in a\nprocessor comparable to the most aggressive processors in 2005, concluding that\nthe larger window sizes will not make sense without significant improvements on\nbranch prediction for integer programs.\nLam and Wilson [1992] looked at the limitations imposed by speculation and\nshowed that additional gains are possible by allowing processors to speculate in\nmultiple directions, which requires more than one PC. (Such schemes cannot\nexceed what perfect speculation accomplishes, but they help close the gap between\nrealistic prediction schemes and perfect prediction.) Wall\u2019s 1993 study includes a\nlimited evaluation of this approach (up to eight branches are explored).\nGoing Beyond the Data Flow Limit\nOne other approach that has been explored in the literature is the use of value pre-\ndiction. Value prediction can allow speculation based on data values. There have\nbeen a number of studies of the use of value prediction. Lipasti and Shen published\ntwo papers in 1996 evaluating the concept of value prediction and its potential\nimpact on ILP exploitation. Calder, Reinman, and Tullsen [1999] explored the idea\nof selective value prediction. Sodani and Sohi [1997] approached the same prob-\nlem from the viewpoint of reusing the values produced by instructions. Moshovos\net al. [1997] showed that deciding when to speculate on values, by tracking\nwhether such speculation has been accurate in the past, is important to achieving\nperformance gains with value speculation. Moshovos and Sohi [1997] and Chrysos\nand Emer [1998] focused on predicting memory dependences and using this infor-\nmation to eliminate the dependence through memory. Gonz\u00e1lez and Gonz\u00e1lez\n[1998], Babbay and Mendelson [1998], and Calder, Reinman, and Tullsen\n[1999] are more recent studies of the use of value prediction. This area is currently\nhighly active, with new results being published in every conference.\nM-34\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1382,
        "text": "Recent Advanced Microprocessors\nTheyears 1994 and1995 sawtheannouncement ofwide superscalar processors (three\normore issues perclock)by every major processorvendor:IntelPentiumProand Pen-\ntium II (these processors share the same core pipeline architecture, described by Col-\nwellandSteck[1995]);AMDK-5,K-6,andAthlon;SunUltraSPARC(seeLauterbach\nandHorel[1999]);Alpha21164(seeEdmondsonetal.[1995])and21264(seeKessler\n[1999]);MIPS R10000andR12000(see Yeager [1996]); PowerPC 603, 604,and620\n(see Diep, Nelson, and Shen [1995]); and HP 8000 (Kumar [1997]). The latter part of\nthedecade(1996\u20132000)sawsecondgenerationsofmanyoftheseprocessors(Pentium\nIII, AMD Athlon, and Alpha 21264, among others). The second generation, although\nsimilar in issue rate, could sustain a lower CPI and provided much higher clock rates.\nAll included dynamic scheduling, and they almost universally supported speculation.\nIn practice, many factors, including the implementation technology, the memory hier-\narchy, the skill of the designers, and the type of applications benchmarked, all play a\nrole in determining which approach is best.\nThe periodfrom 2000to 2005 was dominated bythree trends among superscalar\nprocessors: the introduction of higher clock rates achieved through deeper pipelin-\ning (e.g., in the Pentium 4; see Hinton et al. [2001]), the introduction of multithread-\ning by IBM in the Power 4 and by Intel in the Pentium 4 Extreme, and the beginning\nofthemovementtomulticorebyIBMinthePower4,AMDinOpteron(seeKeltcher\net al. [2003]), and most recently by Intel (see Douglas [2005]).\nMultithreading and Simultaneous Multithreading\nThe concept of multithreading dates back to one of the earliest transistorized com-\nputers, the TX-2. TX-2 is also famous for being the computer on which Ivan Suth-\nerland created Sketchpad, the first computer graphics system. TX-2 was built at\nMIT\u2019s Lincoln Laboratory and became operational in 1959. It used multiple\nthreads to support fast context switching to handle I/O functions. Clark [1957]\ndescribed the basic architecture, and Forgie [1957] described the I/O architecture.\nMultithreading was also used in the CDC 6600, where a fine-grained multithread-\ning scheme with interleaved scheduling among threads was used as the architecture\nof the I/O processors. The HEP processor, a pipelined multiprocessor designed by\nDenelcor and shipped in 1982, used fine-grained multithreading to hide the pipe-\nline latency as well as to hide the latency to a large memory shared among all the\nprocessors. Because the HEP had no cache, this hiding of memory latency was\ncritical. Burton Smith, one of the primary architects, described the HEP architec-\nture in a 1978 paper, and Jordan [1983] published a performance evaluation. The\nTERA processor extends the multithreading ideas and is described by Alverson\net al. in a 1992 paper. The Niagara multithreading approach is similar to those\nof the HEP and TERA systems, although Niagara employs caches reducing the\nneed for thread-based latency hiding.\nIn the late 1980s and early 1990s, researchers explored the concept of coarse-\ngrained multithreading (also called block multithreading) as a way to tolerate\nM.5\nThe Development of Pipelining and Instruction-Level Parallelism\n\u25a0\nM-35"
    },
    {
        "page": 1383,
        "text": "latency, especially in multiprocessor environments. The SPARCLE processor in\nthe Alewife system used such a scheme, switching threads whenever a highlatency\nexceptional event, such as a long cache miss, occurred. Agarwal et al. described\nSPARCLE in a 1993 paper. The IBM Pulsar processor uses similar ideas.\nBy the early 1990s, several research groups had arrived at two key insights.\nFirst, they realized that fine-grained multithreading was needed to get the max-\nimum performance benefit, since in a coarse-grained approach, the overhead of\nthread switching and thread start-up (e.g., filling the pipeline from the new\nthread) negated much of the performance advantage (see Laudon, Gupta, and\nHorowitz [1994]). Second, several groups realized that to effectively use large\nnumbers of functional units would require both ILP and thread-level parallelism\n(TLP). These insights led to several architectures that used combinations of multi-\nthreading and multiple issue. Wolfe and Shen [1991] described an architecture\ncalled XIMD that statically interleaves threads scheduled for a VLIW processor.\nHirata et al. [1992] described a proposed processor for media use that combines a\nstatic superscalar pipeline with support for multithreading; they reported speed-\nups from combining both forms of parallelism. Keckler and Dally [1992] com-\nbined static scheduling of ILP and dynamic scheduling of threads for a processor\nwith multiple functional units. The question of how to balance the allocation of\nfunctional units between ILP and TLP and how to schedule the two forms of par-\nallelism remained open.\nWhen it became clear in the mid-1990s that dynamically scheduled supersca-\nlars would be delivered shortly, several research groups proposed using the\ndynamic scheduling capability to mix instructions from several threads on the\nfly. Yamamoto et al. [1994] appear to have published the first such proposal,\nthough the simulation results for their multithreaded superscalar architecture use\nsimplistic assumptions. This work was quickly followed by Tullsen, Eggers,\nand Levy [1995], who provided the first realistic simulation assessment and coined\nthe term simultaneous multithreading. Subsequent work by the same group\ntogether with industrial coauthors addressed many of the open questions about\nSMT. For example, Tullsen et al. [1996] addressed questions about the challenges\nof scheduling ILP versus TLP. Lo et al. [1997] provided an extensive discussion of\nthe SMT concept and an evaluation of its performance potential, and Lo et. al.\n[1998] evaluated database performance on an SMT processor. Tuck and Tullsen\n[2003] reviewed the performance of SMT on the Pentium 4.\nThe IBM Power4 introduced multithreading (see Tendler et al. [2002]), while\nthe Power5 used simultaneous multithreading. Mathis et al. [2005] explored\nthe performance of SMT in the Power5, while Sinharoy et al. [2005] described\nthe system architecture.\nReferences\nAgarwal, A., J. Kubiatowicz, D. Kranz, B.-H. Lim, D. Yeung, G. D\u2019Souza, and M.\nParkin [1993]. \u201cSparcle: An evolutionary processor design for large-scale mul-\ntiprocessors,\u201d IEEE Micro 13 (June), 48\u201361.\nM-36\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1384,
        "text": "Agerwala, T., and J. Cocke [1987]. High Performance Reduced Instruction Set\nProcessors, Tech. Rep. RC12434, IBM Thomas Watson Research Center,\nYorktown Heights, N.Y.\nAlverson, G., R. Alverson, D. Callahan, B. Koblenz, A. Porterfield, and B. Smith\n[1992]. \u201cExploiting heterogeneous parallelism on a multithreaded multiproces-\nsor,\u201d Proc. ACM/IEEE Conf. on Supercomputing, November 16\u201320, 1992,\nMinneapolis, Minn., 188\u2013197.\nAnderson, D. W., F. J. Sparacio, and R. M. Tomasulo [1967]. \u201cThe IBM 360\nModel 91: Processor philosophy and instruction handling,\u201d IBM J. Research\nand Development 11:1 (January), 8\u201324.\nAustin, T. M., and G. Sohi [1992]. \u201cDynamic dependency analysis of ordinary pro-\ngrams,\u201d Proc. 19th Annual Int\u2019l. Symposium on Computer Architecture (ISCA),\nMay 19\u201321, 1992, Gold Coast, Australia, 342\u2013351.\nBabbay, F., and A. Mendelson [1998]. \u201cUsing value prediction to increase the\npower of speculative execution hardware,\u201d ACM Trans. on Computer Systems\n16:3 (August), 234\u2013270.\nBakoglu, H. B., G. F. Grohoski, L. E. Thatcher, J. A. Kaeli, C. R. Moore, D. P.\nTattle, W. E. Male, W. R. Hardell, D. A. Hicks, M. Nguyen Phu, R. K. Montoye,\nW. T. Glover, and S. Dhawan [1989]. \u201cIBM second-generation RISC processor\norganization,\u201d Proc. IEEE Int\u2019l. Conf. on Computer Design, October, Rye\nBrook, N.Y., 138\u2013142.\nBall, T., and J. Larus [1993]. \u201cBranch prediction for free,\u201d Proc. ACM SIG-\nPLAN\u201993 Conference on Programming Language Design and Implementation\n(PLDI), June 23\u201325, 1993, Albuquerque, N.M., 300\u2013313.\nBhandarkar, D., and D. W. Clark [1991]. \u201cPerformance from architecture: Com-\nparing a RISC and a CISC with similar hardware organizations,\u201d Proc. Fourth\nInt\u2019l. Conf. on Architectural Support for Programming Languages and Operat-\ning Systems (ASPLOS), April 8\u201311, 1991, Palo Alto, Calif., 310\u2013319.\nBhandarkar, D., and J. Ding [1997]. \u201cPerformance characterization of the Pentium\nPro processor,\u201d Proc. Third Int\u2019l. Symposium on High Performance Computer\nArchitecture, February 1\u20135, 1997, San Antonio, Tex., 288\u2013297.\nBloch, E. [1959]. \u201cThe engineering design of the Stretch computer,\u201d Proc. Eastern\nJoint Computer Conf., December 1\u20133, 1959, Boston, Mass., 48\u201359.\nBucholtz, W. [1962]. Planning a Computer System: Project Stretch, McGraw-Hill,\nNew York.\nCalder, B., D. Grunwald, M. Jones, D. Lindsay, J. Martin, M. Mozer, and B. Zorn\n[1997]. \u201cEvidence-based static branch prediction using machine learning,\u201d\nACM Trans. Program. Lang. Syst. 19:1, 188\u2013222.\nCalder, B., G. Reinman, and D. M. Tullsen [1999]. \u201cSelective value prediction,\u201d\nProc. 26th Annual Int\u2019l. Symposium on Computer Architecture (ISCA),\nMay 2\u20134, 1999, Atlanta, Ga.\nChang, P. P., S. A. Mahlke, W. Y. Chen, N. J. Warter, and W. W. Hwu [1991].\n\u201cIMPACT:\nAn\narchitectural\nframework\nfor\nmultiple-instruction-issue\nprocessors,\u201d Proc. 18th Annual Int\u2019l. Symposium on Computer Architecture\n(ISCA), May 27\u201330, 1991, Toronto, Canada, 266\u2013275.\nM.5\nThe Development of Pipelining and Instruction-Level Parallelism\n\u25a0\nM-37"
    },
    {
        "page": 1385,
        "text": "Charlesworth, A. E. [1981]. \u201cAn approach to scientific array processing: The archi-\ntecture design of the AP-120B/FPS-164 family,\u201d Computer 14:9 (September),\n18\u201327.\nChen, T. C. [1980]. \u201cOverlap and parallel processing,\u201d in Introduction to Com-\nputer Architecture, H. Stone, ed., Science Research Associates, Chicago,\n427\u2013486.\nChrysos, G. Z., and J. S. Emer [1998]. \u201cMemory dependence prediction using store\nsets,\u201d Proc. 25th Annual Int\u2019l. Symposium on Computer Architecture (ISCA),\nJuly 3\u201314, 1998, Barcelona, Spain, 142\u2013153.\nClark, D. W. [1987]. \u201cPipelining and performance in the VAX 8800 processor,\u201d\nProc. Second Int\u2019l. Conf. on Architectural Support for Programming Languages\nand Operating Systems (ASPLOS), October 5\u20138, 1987, Palo Alto, Calif., 173\u2013177.\nClark, W. A. [1957]. \u201cThe Lincoln TX-2 computer development,\u201d Proc. Western\nJoint Computer Conference, February 26\u201328, 1957, Los Angeles, 143\u2013145.\nColwell, R. P., and R. Steck [1995]. \u201cA 0.6 \u03bcm BiCMOS processor with dynamic\nexecution.\u201d Proc. of IEEE Int\u2019l. Symposium on Solid State Circuits (ISSCC),\nFebruary 15\u201317, 1995, San Francisco, 176\u2013177.\nColwell, R. P., R. P. Nix, J. J. O\u2019Donnell, D. B. Papworth, and P. K. Rodman\n[1987]. \u201cA VLIW architecture for a trace scheduling compiler,\u201d Proc. Second\nInt\u2019l. Conf. on Architectural Support for Programming Languages and Operat-\ning Systems (ASPLOS), October 5\u20138, 1987, Palo Alto, Calif., 180\u2013192.\nCvetanovic, Z., and R. E. Kessler [2000]. \u201cPerformance analysis of the Alpha\n21264-based Compaq ES40 system,\u201d 27th Annual Int\u2019l. Symposium on Com-\nputer Architecture (ISCA), June 10\u201314, 2000, Vancouver, Canada, 192\u2013202.\nDavidson, E. S. [1971]. \u201cThe design and control of pipelined function generators,\u201d\nProc. IEEE Conf. on Systems, Networks, and Computers, January 19\u201321, 1971,\nOaxtepec, Mexico, 19\u201321.\nDavidson, E. S., A. T. Thomas, L. E. Shar, and J. H. Patel [1975]. \u201cEffective con-\ntrol for pipelined processors,\u201d Proc. IEEE COMPCON, February 25\u201327, 1975,\nSan Francisco, 181\u2013184.\nDehnert, J. C., P. Y.-T. Hsu, and J. P. Bratt [1989]. \u201cOverlapped loop support on\nthe Cydra 5,\u201d Proc. Third Int\u2019l. Conf. on Architectural Support for Program-\nming Languages and Operating Systems (ASPLOS), April 3\u20136, 1989, Boston,\nMass., 26\u201339.\nDiep, T. A., C. Nelson, and J. P. Shen [1995]. \u201cPerformance evaluation of the\nPowerPC 620 microarchitecture,\u201d Proc. 22nd Annual Int\u2019l. Symposium on Com-\nputer Architecture (ISCA), June 22\u201324, 1995, Santa Margherita, Italy.\nDitzel, D. R., and H. R. McLellan [1987]. \u201cBranch folding in the CRISP micro-\nprocessor: Reducing the branch delay to zero,\u201d Proc. 14th Annual Int\u2019l. Sympo-\nsium on Computer Architecture (ISCA), June 2\u20135, 1987, Pittsburgh, Penn., 2\u20137.\nDouglas, J. [2005]. \u201cIntel 8xx series and Paxville Xeon-MP Microprocessors,\u201d\npaper presented at Hot Chips 17, August 14\u201316, 2005, Stanford University, Palo\nAlto, Calif.\nEden, A., and T. Mudge [1998]. \u201cThe YAGS branch prediction scheme,\u201d Proc. of\nthe 31st Annual ACM/IEEE Int\u2019l. Symposium on Microarchitecture, November\n30\u2013December 2, 1998, Dallas, Tex., 69\u201380.\nM-38\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1386,
        "text": "Edmondson, J. H., P. I. Rubinfield, R. Preston, and V. Rajagopalan [1995].\n\u201cSuperscalar instruction execution in the 21164 Alpha microprocessor,\u201d IEEE\nMicro 15:2, 33\u201343.\nEllis, J. R. [1986]. Bulldog: A Compiler for VLIW Architectures, MIT Press,\nCambridge, Mass.\nEmer, J. S., and D. W. Clark [1984]. \u201cA characterization of processor performance\nin the VAX-11/780,\u201d Proc. 11th Annual Int\u2019l. Symposium on Computer Archi-\ntecture (ISCA), June 5\u20137, 1984, Ann Arbor, Mich., 301\u2013310.\nEvers, M., S. J. Patel, R. S. Chappell, and Y. N. Patt [1998]. \u201cAn analysis of cor-\nrelation and predictability: What makes two-level branch predictors work,\u201d\nProc. 25th Annual Int\u2019l. Symposium on Computer Architecture (ISCA), July\n3\u201314, 1998, Barcelona, Spain, 52\u201361.\nFisher, J. A. [1981]. \u201cTrace scheduling: A technique for global microcode compac-\ntion,\u201d IEEE Trans. on Computers 30:7 (July), 478\u2013490.\nFisher, J. A. [1983]. \u201cVery long instruction word architectures and ELI-512,\u201d 10th\nAnnual Int\u2019l. Symposium on Computer Architecture (ISCA), June 5\u20137, 1982,\nStockholm, Sweden, 140\u2013150.\nFisher, J. A., and S. M. Freudenberger [1992]. \u201cPredicting conditional branches\nfrom previous runs of a program,\u201d Proc. Fifth Int\u2019l. Conf. on Architectural Sup-\nport for Programming Languages and Operating Systems (ASPLOS), October\n12\u201315, 1992, Boston, 85\u201395.\nFisher, J. A., and B. R. Rau [1993]. Journal of Supercomputing, January\n(special issue).\nFisher, J. A., J. R. Ellis, J. C. Ruttenberg, and A. Nicolau [1984]. \u201cParallel proces-\nsing: A smart compiler and a dumb processor,\u201d Proc. SIGPLAN Conf. on Com-\npiler Construction, June 17\u201322, 1984, Montreal, Canada, 11\u201316.\nForgie, J. W. [1957]. \u201cThe Lincoln TX-2 input-output system,\u201d Proc. Western\nJoint Computer Conference, February 26\u201328, 1957, Los Angeles, 156\u2013160.\nFoster, C. C., and E. M. Riseman [1972]. \u201cPercolation of code to enhance parallel\ndispatching and execution,\u201d IEEE Trans. on Computers C-21:12 (December),\n1411\u20131415.\nGallagher, D. M., W. Y. Chen, S. A. Mahlke, J. C. Gyllenhaal, and W.W. Hwu\n[1994]. \u201cDynamic memory disambiguation using the memory conflict buffer,\u201d\nProc. Sixth Int\u2019l. Conf. on Architectural Support for Programming Languages\nand Operating Systems (ASPLOS), October 4\u20137, Santa Jose, Calif., 183\u2013193.\nGonz\u00e1lez, J., and A. Gonz\u00e1lez [1998]. \u201cLimits of instruction level parallelism with\ndata speculation,\u201d Proc. Vector and Parallel Processing (VECPAR) Conf., June\n21\u201323, 1998, Porto, Portugal, 585\u2013598.\nHeinrich, J. [1993]. MIPS R4000 User\u2019s Manual, Prentice Hall, Englewood Cliffs,\nN.J.\nHinton, G., D. Sager, M. Upton, D. Boggs, D. Carmean, A. Kyker, and P. Roussel\n[2001]. \u201cThe microarchitecture of the Pentium 4 processor,\u201d Intel Technology\nJournal, February.\nHirata, H., K. Kimura, S. Nagamine, Y. Mochizuki, A. Nishimura, Y. Nakase, and\nT. Nishizawa [1992]. \u201cAn elementary processor architecture with simultaneous\ninstruction issuing from multiple threads,\u201d Proc. 19th Annual Int\u2019l. Symposium\nM.5\nThe Development of Pipelining and Instruction-Level Parallelism\n\u25a0\nM-39"
    },
    {
        "page": 1387,
        "text": "on Computer Architecture (ISCA), May 19\u201321, 1992, Gold Coast, Australia,\n136\u2013145.\nHopkins, M. [2000]. \u201cA critical look at IA-64: Massive resources, massive ILP, but\ncan it deliver?\u201d Microprocessor Report, February.\nHsu, P. [1994]. \u201cDesigning the TFP microprocessor,\u201d IEEE Micro 18:2\n(April), 2333.\nHuck, J. et al. [2000]. \u201cIntroducing the IA-64 Architecture\u201d IEEE Micro, 20:5\n(September\u2013October), 12\u201323.\nHwu, W.-M., and Y. Patt [1986]. \u201cHPSm, a high performance restricted data flow\narchitecture having minimum functionality,\u201d 13th Annual Int\u2019l. Symposium on\nComputer Architecture (ISCA), June 2\u20135, 1986, Tokyo, 297\u2013307.\nHwu, W. W., S. A. Mahlke, W. Y. Chen, P. P. Chang, N. J. Warter, R. A. Bring-\nmann, R. O. Ouellette, R. E. Hank, T. Kiyohara, G. E. Haab, J. G. Holm, and D.\nM. Lavery [1993]. \u201cThe superblock: An effective technique for VLIW and\nsuperscalar compilation,\u201d J. Supercomputing 7:1, 2 (March), 229\u2013248.\nIBM. [1990]. \u201cThe IBM RISC System/6000 processor\u201d (collection of papers), IBM\nJ. Research and Development 34:1 (January).\nJimenez, D. A., and C. Lin [2002]. \u201cNeural methods for dynamic branch\nprediction,\u201d ACM Trans. Computer Sys 20:4 (November), 369\u2013397.\nJohnson, M. [1990]. Superscalar Microprocessor Design, Prentice Hall, Engle-\nwood Cliffs, N.J.\nJordan, H. F. [1983]. \u201cPerformance measurements on HEP\u2014a pipelined MIMD\ncomputer,\u201d Proc. 10th Annual Int\u2019l. Symposium on Computer Architecture\n(ISCA), June 5\u20137, 1982, Stockholm, Sweden, 207\u2013212.\nJouppi, N. P., and D. W. Wall [1989]. \u201cAvailable instruction-level parallelism for\nsuperscalar and superpipelined processors,\u201d Proc. Third Int\u2019l. Conf. on Archi-\ntectural Support for Programming Languages and Operating Systems\n(ASPLOS), April 3\u20136, 1989, Boston, 272\u2013282.\nKaeli, D. R., and P. G. Emma [1991]. \u201cBranch history table prediction of moving\ntarget branches due to subroutine returns,\u201d Proc. 18th Annual Int\u2019l. Symposium\non Computer Architecture (ISCA), May 27\u201330, 1991, Toronto, Canada, 34\u201342.\nKeckler, S. W., and W. J. Dally [1992]. \u201cProcessor coupling: Integrating compile\ntime and runtime scheduling for parallelism,\u201d Proc. 19th Annual Int\u2019l.\nSymposium on Computer Architecture (ISCA), May 19\u201321, 1992, Gold Coast,\nAustralia, 202\u2013213.\nKeller, R. M. [1975]. \u201cLook-ahead processors,\u201d ACM Computing Surveys 7:4\n(December), 177\u2013195.\nKeltcher, C. N., K. J. McGrath, A. Ahmed, and P. Conway [2003]. \u201cThe AMD\nOpteron processor for multiprocessor servers,\u201d IEEE Micro 23:2 (March\u2013\nApril), 66\u201376.\nKessler, R. [1999]. \u201cThe Alpha 21264 microprocessor,\u201d IEEE Micro 19:2 (March/\nApril) 24\u201336.\nKillian, E. [1991]. \u201cMIPS R4000 technical overview\u201364 bits/100 MHz or bust,\u201d\nHot Chips III Symposium Record, August 26\u201327, 1991, Stanford University,\nPalo Alto, Calif., 1.6\u20131.19.\nM-40\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1388,
        "text": "Kogge, P. M. [1981]. The Architecture of Pipelined Computers, McGraw-Hill,\nNew York.\nKumar, A. [1997]. \u201cThe HP PA-8000 RISC CPU,\u201d IEEE Micro 17:2 (March/\nApril).\nKunkel, S. R., and J. E. Smith [1986]. \u201cOptimal pipelining in supercomputers,\u201d\nProc. 13th Annual Int\u2019l. Symposium on Computer Architecture (ISCA), June\n2\u20135, 1986, Tokyo, 404\u2013414.\nLam, M. [1988]. \u201cSoftware pipelining: An effective scheduling technique for\nVLIW processors,\u201d SIGPLAN Conf. on Programming Language Design and\nImplementation, June 22\u201324, 1988, Atlanta, Ga., 318\u2013328.\nLam, M. S., and R. P. Wilson [1992]. \u201cLimits of control flow on parallelism,\u201d\nProc. 19th Annual Int\u2019l. Symposium on Computer Architecture (ISCA), May\n19\u201321, 1992, Gold Coast, Australia, 46\u201357.\nLaudon, J., A. Gupta, and M. Horowitz [1994]. \u201cInterleaving: A multithreading\ntechnique targeting multiprocessors and workstations,\u201d Proc. Sixth Int\u2019l. Conf.\non Architectural Support for Programming Languages and Operating Systems\n(ASPLOS), October 4\u20137, San Jose, Calif., 308\u2013318.\nLauterbach, G., and T. Horel [1999]. \u201cUltraSPARC-III: Designing third generation\n64-bit performance,\u201d IEEE Micro 19:3 (May/June).\nLipasti, M. H., and J. P. Shen [1996]. \u201cExceeding the dataflow limit via value pre-\ndiction,\u201d Proc. 29th Int\u2019l. Symposium on Microarchitecture, December 2\u20134,\n1996, Paris, France.\nLipasti, M. H., C. B. Wilkerson, and J. P. Shen [1996]. \u201cValue locality and load\nvalue prediction,\u201d Proc. Seventh Conf. on Architectural Support for Program-\nming Languages and Operating Systems (ASPLOS), October 1\u20135, 1996,\nCambridge, Mass., 138\u2013147.\nLo, J., L. Barroso, S. Eggers, K. Gharachorloo, H. Levy, and S. Parekh [1998]. \u201cAn\nanalysis of database workload performance on simultaneous multithreaded pro-\ncessors,\u201d Proc. 25th Annual Int\u2019l. Symposium on Computer Architecture\n(ISCA), July 3\u201314, 1998, Barcelona, Spain, 39\u201350.\nLo, J., S. Eggers, J. Emer, H. Levy, R. Stamm, and D. Tullsen [1997].\n\u201cConverting thread-level parallelism into instruction-level parallelism via\nsimultaneous multithreading,\u201d ACM Trans. on Computer Systems 15:2\n(August), 322\u2013354.\nMahlke, S. A., W. Y. Chen, W.-M. Hwu, B. R. Rau, and M. S. Schlansker [1992].\n\u201cSentinel scheduling for VLIW and superscalar processors,\u201d Proc. Fifth Int\u2019l.\nConf. on Architectural Support for Programming Languages and Operating\nSystems (ASPLOS), October 12\u201315, 1992, Boston, 238\u2013247.\nMahlke, S. A., R. E. Hank, J. E. McCormick, D. I. August, and W. W. Hwu [1995].\n\u201cA comparison of full and partial predicated execution support for ILP proces-\nsors,\u201d Proc. 22nd Annual Int\u2019l. Symposium on Computer Architecture (ISCA),\nJune 22\u201324, 1995, Santa Margherita, Italy, 138\u2013149.\nMathis, H. M., A. E. Mercias, J. D. McCalpin, R. J. Eickemeyer, and S. R. Kunkel\n[2005]. \u201cCharacterization of the multithreading (SMT) efficiency in Power5,\u201d\nIBM J. of Research and Development, 49:4/5 (July/September), 555\u2013564.\nM.5\nThe Development of Pipelining and Instruction-Level Parallelism\n\u25a0\nM-41"
    },
    {
        "page": 1389,
        "text": "McCormick, J., and A. Knies [2002]. \u201cA brief analysis of the SPEC CPU2000\nbenchmarks on the Intel Itanium 2 processor,\u201d paper presented at Hot Chips\n14, August 18\u201320, 2002, Stanford University, Palo Alto, Calif.\nMcFarling, S. [1993]. Combining Branch Predictors, WRL Technical Note\nTN-36, Digital Western Research Laboratory, Palo Alto, Calif.\nMcFarling, S., and J. Hennessy [1986]. \u201cReducing the cost of branches,\u201d Proc.\n13th Annual Int\u2019l. Symposium on Computer Architecture (ISCA), June 2\u20135,\n1986, Tokyo, 396\u2013403.\nMcNairy, C., and D. Soltis [2003]. \u201cItanium 2 processor microarchitecture,\u201d IEEE\nMicro 23:2 (March\u2013April), 44\u201355.\nMoshovos, A., and G. S. Sohi [1997]. \u201cStreamlining inter-operation memory\ncommunication via data dependence prediction,\u201d Proc. 30th Annual Int\u2019l.\nSymposium on Microarchitecture, December 1\u20133, Research Triangle Park, N.C.,\n235\u2013245.\nMoshovos, A., S. Breach, T. N. Vijaykumar, and G. S. Sohi [1997]. \u201cDynamic\nspeculation and synchronization of data dependences,\u201d Proc. 24th Annual Int\u2019l.\nSymposium on Computer Architecture (ISCA), June 2\u20134, 1997, Denver, Colo.\nNicolau, A., and J. A. Fisher [1984]. \u201cMeasuring the parallelism available for very\nlong instruction word architectures,\u201d IEEE Trans. on Computers C-33:11\n(November), 968\u2013976.\nPan, S.-T., K. So, and J. T. Rameh [1992]. \u201cImproving the accuracy of dynamic\nbranch prediction using branch correlation,\u201d Proc. Fifth Int\u2019l. Conf. on Archi-\ntectural Support for Programming Languages and Operating Systems\n(ASPLOS), October 12\u201315, 1992, Boston, 76\u201384.\nPostiff, M.A., D. A. Greene, G. S. Tyson, and T. N. Mudge [1999]. \u201cThe limits of\ninstruction level parallelism in SPEC95 applications,\u201d Computer Architecture\nNews 27:1 (March), 31\u201340.\nRamamoorthy, C. V., and H. F. Li [1977]. \u201cPipeline architecture,\u201d ACM\nComputing Surveys 9:1 (March), 61\u2013102.\nRau, B. R. [1994]. \u201cIterative modulo scheduling: An algorithm for software pipe-\nlining loops,\u201d Proc. 27th Annual Int\u2019l. Symposium on Microarchitecture,\nNovember 30\u2013December 2, 1994, San Jose, Calif., 63\u201374.\nRau, B. R., C. D. Glaeser, and R. L. Picard [1982]. \u201cEfficient code generation for\nhorizontal architectures: Compiler techniques and architectural support,\u201d Proc.\nNinth Annual Int\u2019l. Symposium on Computer Architecture (ISCA), April 26\u201329,\n1982, Austin, Tex., 131\u2013139.\nRau, B. R., D. W. L. Yen, W. Yen, and R. A. Towle [1989]. \u201cThe Cydra 5 depart-\nmental supercomputer: Design philosophies, decisions, and trade-offs,\u201d IEEE\nComputers 22:1 (January), 12\u201334.\nRiseman, E. M., and C. C. Foster [1972]. \u201cPercolation of code to enhance\nparalleled dispatching and execution,\u201d IEEE Trans. on Computers C-21:12\n(December), 1411\u20131415.\nRymarczyk, J. [1982]. \u201cCoding guidelines for pipelined processors,\u201d Proc. Sym-\nposium Architectural Support for Programming Languages and Operating Sys-\ntems (ASPLOS), March 1\u20133, 1982, Palo Alto, Calif., 12\u201319.\nM-42\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1390,
        "text": "Sharangpani, H., and K. Arora [2000]. \u201cItanium Processor Microarchitecture,\u201d\nIEEE Micro, 20:5 (September\u2013October), 24\u201343.\nSinharoy, B., R. N. Koala, J. M. Tendler, R. J. Eickemeyer, and J. B. Joyner\n[2005]. \u201cPOWER5 system microarchitecture,\u201d IBM J. of Research and Devel-\nopment, 49:4\u20135, 505\u2013521.\nSites, R. [1979]. Instruction Ordering for the CRAY-1 Computer, Tech. Rep.\n78-CS-023, Dept. of Computer Science, University of California, San Diego.\nSkadron, K., P. S. Ahuja, M. Martonosi, and D. W. Clark [1999]. \u201cBranch\nprediction, instruction-window size, and cache size: Performance tradeoffs\nand simulation techniques,\u201d IEEE Trans. on Computers, 48:11 (November).\nSmith, A., and J. Lee [1984]. \u201cBranch prediction strategies and branch-target\nbuffer design,\u201d Computer 17:1 (January), 6\u201322.\nSmith, B. J. [1978]. \u201cA pipelined, shared resource MIMD computer,\u201d Proc. Int\u2019l.\nConf. on Parallel Processing (ICPP), August, Bellaire, Mich., 6\u20138.\nSmith, J. E. [1981]. \u201cA study of branch prediction strategies,\u201d Proc. Eighth Annual\nInt\u2019l. Symposium on Computer Architecture (ISCA), May 12\u201314, 1981, Minne-\napolis, Minn., 135\u2013148.\nSmith, J. E. [1984]. \u201cDecoupled access/execute computer architectures,\u201d ACM\nTrans. on Computer Systems 2:4 (November), 289\u2013308.\nSmith, J. E. [1989]. \u201cDynamic instruction scheduling and the Astronautics ZS-1,\u201d\nComputer 22:7 (July), 21\u201335.\nSmith, J. E., and A. R. Pleszkun [1988]. \u201cImplementing precise interrupts\nin pipelined processors,\u201d IEEE Trans. on Computers 37:5 (May), 562\u2013573.\n(This paper is based on an earlier paper that appeared in Proc. 12th Annual\nInt\u2019l. Symposium on Computer Architecture (ISCA), June 17\u201319, 1985, Boston,\nMass.)\nSmith, J. E., G. E. Dermer, B. D. Vanderwarn, S. D. Klinger, C. M. Rozewski,\nD. L. Fowler, K. R. Scidmore, and J. P. Laudon [1987]. \u201cThe ZS-1 central pro-\ncessor,\u201d Proc. Second Int\u2019l. Conf. on Architectural Support for Programming\nLanguages and Operating Systems (ASPLOS), October 5\u20138, 1987, Palo Alto,\nCalif., 199\u2013204.\nSmith, M. D., M. Horowitz, and M. S. Lam [1992]. \u201cEfficient superscalar perfor-\nmance through boosting,\u201d Proc. Fifth Int\u2019l. Conf. on Architectural Support for\nProgramming Languages and Operating Systems (ASPLOS), October 12\u201315,\n1992, Boston, 248\u2013259.\nSmith, M. D., M. Johnson, and M. A. Horowitz [1989]. \u201cLimits on multiple\ninstruction issue,\u201d Proc. Third Int\u2019l. Conf. on Architectural Support for Pro-\ngramming Languages and Operating Systems (ASPLOS), April 3\u20136, 1989, Bos-\nton, 290\u2013302.\nSodani, A., and G. Sohi [1997]. \u201cDynamic instruction reuse,\u201d Proc. 24th Annual\nInt\u2019l. Symposium on Computer Architecture (ISCA), June 2\u20134, 1997,\nDenver, Colo.\nSohi, G. S. [1990]. \u201cInstruction issue logic for high-performance, interruptible,\nmultiple functional unit, pipelined computers,\u201d IEEE Trans. on Computers\n39:3 (March), 349\u2013359.\nM.5\nThe Development of Pipelining and Instruction-Level Parallelism\n\u25a0\nM-43"
    },
    {
        "page": 1391,
        "text": "Sohi, G. S., and S. Vajapeyam [1989]. \u201cTradeoffs in instruction format design for\nhorizontal architectures,\u201d Proc. Third Int\u2019l. Conf. on Architectural Support for\nProgramming Languages and Operating Systems (ASPLOS), April 3\u20136, 1989,\nBoston, 15\u201325.\nSussenguth, E. [1999]. \u201cIBM\u2019s ACS-1 Machine,\u201d IEEE Computer 22:11\n(November).\nTendler, J. M., J. S. Dodson, J. S. Fields, Jr., H. Le, and B. Sinharoy [2002].\n\u201cPower4 system microarchitecture,\u201d IBM J. of Research and Development,\n46:1, 5\u201326.\nThorlin, J. F. [1967]. \u201cCode generation for PIE (parallel instruction execution)\ncomputers,\u201d Proc. Spring Joint Computer Conf., April 18\u201320, 1967, Atlantic\nCity, N.J., 27.\nThornton, J. E. [1964]. \u201cParallel operation in the Control Data 6600,\u201d Proc. AFIPS\nFall Joint Computer Conf., Part II, October 27\u201329, 1964, San Francisco, 26,\n33\u201340.\nThornton, J. E. [1970]. Design of a Computer, the Control Data 6600, Scott, Fores-\nman, Glenview, Ill.\nTjaden, G. S., and M. J. Flynn [1970]. \u201cDetection and parallel execution of\nindependent instructions,\u201d IEEE Trans. on Computers C-19:10 (October),\n889\u2013895.\nTomasulo, R. M. [1967]. \u201cAn efficient algorithm for exploiting multiple arithmetic\nunits,\u201d IBM J. Research and Development 11:1 (January), 25\u201333.\nTuck, N., and D. Tullsen [2003]. \u201cInitial observations of the simultaneous multi-\nthreading Pentium 4 processor,\u201d Proc. 12th Int. Conf. on Parallel Architectures\nand Compilation Techniques (PACT\u201903), September 27\u2013October 1, New\nOrleans, La., 26\u201334.\nTullsen, D. M., S. J. Eggers, and H. M. Levy [1995]. \u201cSimultaneous multithread-\ning: Maximizing on-chip parallelism,\u201d Proc. 22nd Annual Int\u2019l. Symposium on\nComputer Architecture (ISCA), June 22\u201324, 1995, Santa Margherita, Italy, 392\u2013\n403.\nTullsen, D. M., S. J. Eggers, J. S. Emer, H. M. Levy, J. L. Lo, and R. L. Stamm\n[1996]. \u201cExploiting choice: Instruction fetch and issue on an implementable\nsimultaneous multithreading processor,\u201d Proc. 23rd Annual Int\u2019l. Symposium\non Computer Architecture (ISCA), May 22\u201324, 1996, Philadelphia, Penn.,\n191\u2013202.\nWall, D. W. [1991]. \u201cLimits of instruction-level parallelism,\u201d Proc. Fourth Int\u2019l.\nConf. on Architectural Support for Programming Languages and Operating\nSystems (ASPLOS), April 8\u201311, 1991, Palo Alto, Calif., 248\u2013259.\nWall, D. W. [1993]. Limits of Instruction-Level Parallelism, Research Rep. 93/6,\nWestern Research Laboratory, Digital Equipment Corp., Palo Alto, Calif.\nWeiss, S., and J. E. Smith [1984]. \u201cInstruction issue logic for pipelined supercom-\nputers,\u201d Proc. 11th Annual Int\u2019l. Symposium on Computer Architecture (ISCA),\nJune 5\u20137, 1984, Ann Arbor, Mich., 110\u2013118.\nWeiss, S., and J. E. Smith [1987]. \u201cA study of scalar compilation techniques for\npipelined supercomputers,\u201d Proc. Second Int\u2019l. Conf. on Architectural Support\nM-44\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1392,
        "text": "for Programming Languages and Operating Systems (ASPLOS), October 5\u20138,\n1987, Palo Alto, Calif., 105\u2013109.\nWilson, R. P., and M. S. Lam [1995]. \u201cEfficient context-sensitive pointer analysis\nfor C programs,\u201d Proc. ACM SIGPLAN\u201995 Conf. on Programming Language\nDesign and Implementation, June 18\u201321, 1995, La Jolla, Calif., 1\u201312.\nWolfe, A., and J. P. Shen [1991]. \u201cA variable instruction stream extension to the\nVLIW architecture,\u201d Proc. Fourth Int\u2019l. Conf. on Architectural Support for Pro-\ngramming Languages and Operating Systems (ASPLOS), April 8\u201311, 1991,\nPalo Alto, Calif., 2\u201314.\nYamamoto, W., M. J. Serrano, A. R. Talcott, R. C. Wood, and M. Nemirosky\n[1994]. \u201cPerformance estimation of multistreamed, superscalar processors,\u201d\nProc. 27th Annual Hawaii Int\u2019l. Conf. on System Sciences, January 4\u20137,\n1994, Maui, 195\u2013204.\nYeager, K. [1996]. \u201cThe MIPS R10000 superscalar microprocessor,\u201d IEEE Micro\n16:2 (April), 28\u201340.\nYeh, T., and Y. N. Patt [1992]. \u201cAlternative implementations of two-level adaptive\nbranch prediction,\u201d Proc. 19th Annual Int\u2019l. Symposium on Computer Architec-\nture (ISCA), May 19\u201321, 1992, Gold Coast, Australia, 124\u2013134.\nYeh, T., and Y. N. Patt [1993]. \u201cA comparison of dynamic branch predictors that\nuse two levels of branch history,\u201d Proc. 20th Annual Int\u2019l. Symposium on Com-\nputer Architecture (ISCA), May 16\u201319, 1993, San Diego, Calif., 257\u2013266.\nM.6\nThe Development of SIMD Supercomputers, Vector\nComputers, Multimedia SIMD Instruction Extensions,\nand Graphical Processor Units (Chapter 4)\nIn this historical section, we start with perhaps the most infamous supercomputer,\nthe Illiac IV, as a representative of the early SIMD (Single Instruction, Multiple\nData) architectures and then move to perhaps the most famous supercomputer,\nthe Cray-1, as a representative of vector architectures. The next step is Multimedia\nSIMD Extensions, which got its name in part due to an advertising campaign\ninvolving the \u201cBunny People,\u201d a disco-dancing set of workers in cleansuits on a\nsemiconductor fabrication line. We conclude with the history of GPUs, which is\nnot quite as colorful.\nSIMD Supercomputers\nThe cost of a general multiprocessor is, however, very high and further design\noptions were considered which would decrease the cost without seriously degrad-\ning the power or efficiency of the system. The options consist of recentralizing one\nof the three major components. \u2026 Centralizing the [control unit] gives rise to the\nbasic organization of [an] \u2026 array processor such as the Illiac IV.\nBouknight et al. [1972]\nM.6\nThe Development of SIMD Supercomputers, Vector Computers, Multimedia\n\u25a0\nM-45"
    },
    {
        "page": 1393,
        "text": "\u2026 with Iliac IV, programming the machine was very difficult and the architecture\nprobably was not very well suited to some of the applications we were trying to\nrun. The key idea was that I did not think we had a very good match in Iliac IV\nbetween applications and architecture.\nDavid Kuck\nSoftware designer for the Illiac IV and\nearly pioneer in parallel software\nDavid Kuck\nAn oral history conducted in 1991 by Andrew Goldstein,\nIEEE History Center, New Brunswick, N.J.\nThe SIMD model was one of the earliest models of parallel computing, dating back\nto the first large-scale multiprocessor, the Illiac IV. Rather than pipelining the data\ncomputation as in vector architectures, these machines had an array of functional\nunits; hence, they might be considered array processors.\nThe earliest ideas on SIMD-style computers are from Unger [1958] and Slot-\nnick, Borck, and McReynolds [1962]. Slotnick\u2019s Solomon design formed the basis\nof the Illiac IV, perhaps the most infamous of the supercomputer projects.\nAlthough successful in pushing several technologies that proved useful in later\nprojects, it failed as a computer. Costs escalated from the $8 million estimate in\n1966 to $31 million by 1972, despite construction of only a quarter of the planned\nmultiprocessor. (In 2011 dollars, that was an increase from $54M to $152M.)\nActual performance was at best 15 MFLOPS versus initial predictions of 1000\nMFLOPS for the full system [Hord 1982]. Delivered to NASA Ames Research\nin 1972, the computer required three more years of engineering before it was\nusable. These events slowed investigation of SIMD, but Danny Hillis [1985] resus-\ncitated this style in the Connection Machine, which had 65,536 1-bit processors.\nThe basic trade-off in SIMD multiprocessors is performance of a processor ver-\nsus number of processors. SIMD supercomputers of the 1980s emphasized a large\ndegree of parallelism over performance of the individual processors. The Connec-\ntion Multiprocessor 2, for example, offered 65,536 single-bit-wide processors,\nwhile the Illiac IV planned for 64 64-bit processors. Massively parallel SIMD mul-\ntiprocessors relied on interconnection or communication networks to exchange\ndata between processing elements.\nAfter being resurrected in the 1980s, first by Thinking Machines and then by\nMasPar, the SIMD model faded away as supercomputers for two main reasons.\nFirst, it is too inflexible. A number of important problems were not data parallel,\nand the architecture did not scale down in a competitive fashion; that is, small-scale\nSIMD multiprocessors often have worse cost-performance compared with that of\nthe alternatives. Second, SIMD could not take advantage of the tremendous per-\nformance and cost advantages of SISD (Single Instruction, Single Data) micropro-\ncessor technology of the 1980s, which was doubling in performance every\n18 months. Instead of leveraging this low-cost technology, designers of SIMD\nmultiprocessors had to build custom processors for their multiprocessors.\nM-46\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1394,
        "text": "Vector Computers\nI\u2019m certainly not inventing vector processors. There are three kinds that I know of\nexisting today. They are represented by the Illiac-IV, the (CDC) Star processor, and\nthe TI (ASC) processor. Those three were all pioneering processors. \u2026 One of the\nproblems of being a pioneer is you always make mistakes and I never, never want\nto be a pioneer. It\u2019s always best to come second when you can look at the mistakes\nthe pioneers made.\nSeymour Cray\nPublic lecture at Lawrence Livermore Laboratories\non the introduction of the Cray-1 (1976)\nThe first vector processors were the Control Data Corporation (CDC) STAR-100\n(see Hintz and Tate [1972]) and the Texas Instruments ASC (see Watson [1972]),\nboth announced in 1972. Both were memory-memory vector processors. They had\nrelatively slow scalar units\u2014the STAR used the same units for scalars and\nvectors\u2014making the scalar pipeline extremely deep. Both processors had high\nstart-up overhead and worked on vectors of several hundred to several thousand\nelements. The crossover between scalar and vector could be over 50 elements.\nIt appears that not enough attention was paid to the role of Amdahl\u2019s law on these\ntwo processors.\nSeymour Cray, who worked on the 6600 and the 7600 at CDC, founded Cray\nResearch and introduced the Cray-1 in 1976 (see Russell [1978]). The Cray-1 used\na vector-register architecture to lower start-up overhead significantly and to reduce\nmemory bandwidth requirements. He also had efficient support for non-unit stride\nand invented chaining. Most importantly, the Cray-1 was the fastest scalar proces-\nsor in the world at that time. This matching of good scalar and vector performance\nwas probably the most significant factor in making the Cray-1 a success. Some\ncustomers bought the processor primarily for its outstanding scalar performance.\nMany subsequent vector processors are based on the architecture of this first\ncommercially successful vector processor. Baskett and Keller [1977] provided a\ngood evaluation of the Cray-1.\nIn 1981, CDC started shipping the CYBER 205 (see Lincoln [1982]). The 205\nhad the same basic architecture as the STAR but offered improved performance all\naround as well as expandability of the vector unit with up to four lanes, each with\nmultiple functional units and a wide load-store pipe that provided multiple words\nper clock. The peak performance of the CYBER 205 greatly exceeded the perfor-\nmance of the Cray-1; however, on real programs, the performance difference was\nmuch smaller.\nIn 1983, Cray Research shipped the first Cray X-MP (see Chen [1983]). With\nan improved clock rate (9.5 ns versus 12.5 ns on the Cray-1), better chaining\nsupport (allowing vector operations with RAW dependencies to operate in paral-\nlel), and multiple memory pipelines, this processor maintained the Cray Research\nlead in supercomputers. The Cray-2, a completely new design configurable with up\nto four processors, was introduced later. A major feature of the Cray-2 was the use\nM.6\nThe Development of SIMD Supercomputers, Vector Computers, Multimedia\n\u25a0\nM-47"
    },
    {
        "page": 1395,
        "text": "of DRAM, which made it possible to have very large memories at the time. The\nfirst Cray-2, with its 256M word (64-bit words) memory, contained more memory\nthan the total of all the Cray machines shipped to that point! The Cray-2 had a much\nfaster clock than the X-MP, but also much deeper pipelines; however, it lacked\nchaining, had enormous memory latency, and had only one memory pipe per pro-\ncessor. In general, the Cray-2 was only faster than the Cray X-MP on problems that\nrequired its very large main memory.\nThat same year, processor vendors from Japan entered the supercomputer mar-\nketplace. First were the Fujitsu VP100 and VP200 (see Miura and Uchida [1983]),\nand later came the Hitachi S810 and the NEC SX/2 (see Watanabe [1987]). These\nprocessors proved to be close to the Cray X-MP in performance. In general, these\nthree processors had much higher peak performance than the Cray X-MP. How-\never, because of large start-up overhead, their typical performance was often lower\nthan that of the Cray X-MP. The Cray X-MP favored a multiple-processor\napproach, first offering a two-processor version and later a four-processor version.\nIn contrast, the three Japanese processors had expandable vector capabilities.\nIn 1988, Cray Research introduced the Cray Y-MP\u2014a bigger and faster ver-\nsion of the X-MP. The Y-MP allowed up to eight processors and lowered the cycle\ntime to 6 ns. With a full complement of eight processors, the Y-MP was generally\nthe fastest supercomputer, though the single-processor Japanese supercomputers\ncould be faster than a one-processor Y-MP. In late 1989, Cray Research was split\ninto two companies, both aimed at building high-end processors available in the\nearly 1990s. Seymour Cray headed the spin-off, Cray Computer Corporation, until\nits demise in 1995. Their initial processor, the Cray-3, was to be implemented in\ngallium arsenide, but they were unable to develop a reliable and cost-effective\nimplementation technology. Shortly before his tragic death in a car accident in\n1996, Seymour Cray started yet another company to develop high-performance\nsystems but this time using commodity components.\nCray Research focused on the C90, a new high-end processor with up to 16\nprocessors and a clock rate of 240 MHz. This processor was delivered in 1991.\nIn 1993, Cray Research introduced their first highly parallel processor, the\nT3D, employing up to 2048 Digital Alpha21064 microprocessors. In 1995, they\nannounced the availability of both a new low-end vector machine, the J90, and\na high-end machine, the T90. The T90 was much like the C90, but with a clock\nthat was twice as fast (460 MHz), using three-dimensional packaging and optical\nclock distribution.\nIn 1995, Cray Research was acquired by Silicon Graphics. In 1998, it released\nthe SV1 system, which grafted considerably faster CMOS processors onto the J90\nmemory system. It also added a data cache for vectors to each CPU to help meet the\nincreased memory bandwidth demands. Silicon Graphics sold Cray Research to\nTera Computer in 2000, and the joint company was renamed Cray Inc.\nThe Japanese supercomputer makers continued to evolve their designs. In\n2001, the NEC SX/5 was generally held to be the fastest available vector super-\ncomputer, with 16 lanes clocking at 312 MHz and with up to 16 processors sharing\nthe same memory. The NEC SX/6, released in 2001, was the first commercial\nsingle-chip\nvector\nmicroprocessor,\nintegrating\nan\nout-of-order\nquad-issue\nM-48\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1396,
        "text": "superscalar processor, scalar instruction and data caches, and an eight-lane vector\nunit on a single die [Kitagawa et al. 2003]. The Earth Simulator is constructed from\n640 nodes connected with a full crossbar, where each node comprises eight SX-6\nvector microprocessors sharing a local memory. The SX-8, released in 2004,\nreduces the number of lanes to four but increases the vector clock rate to 2\nGHz. The scalar unit runs at a slower 1 GHz clock rate, a common pattern in vector\nmachines where the lack of hazards simplifies the use of deeper pipelines in the\nvector unit.\nIn 2002, Cray Inc. released the X1 based on a completely new vector ISA. The\nX1 SSP processor chip integrates an out-of-order superscalar with scalar caches\nrunning at 400 MHz and a two-lane vector unit running at 800 MHz. When four\nSSP chips are ganged together to form an MSP, the resulting peak vector perfor-\nmance of 12.8 GFLOPS is competitive with the contemporary NEC SX machines.\nThe X1E enhancement, delivered in 2004, raises the clock rates to 565 and 1130\nMHz, respectively. Many of the ideas were borrowed from the Cray T3E design,\nwhich is a MIMD (Multiple Instruction, Multiple Data) computer that uses off-the-\nshelf microprocessors. X1 has a new instruction set with a larger number of reg-\nisters and with memory distributed locally with the processor in shared address\nspace. The out-of-order scalar unit and vector units are decoupled, so that the scalar\nunit can get ahead of the vector unit. Vectors become shorter when the data are\nblocked to utilize the MSP caches, which is not a good match to an eight-lane vec-\ntor unit. To handle these shorter vectors, each processor with just two vector lanes\ncan work on a different loop.\nThe Cray X2 was announced in 2007, and it may prove to be the last Cray vec-\ntor architecture to be built, as it\u2019s difficult to justify the investment in new silicon\ngiven the size of the market. The processor has a 1.3 GHz clock rate and 8 vector\nlanes for a processor peak performance of 42 GFLOP/sec for single precision. It\nincludes both L1 and L2 caches. Each node is a 4-way SMP with up to 128 GBytes\nof DRAM, and the maximum size is 8K nodes.\nThe NEC SX-9 has up to 16 processors per node, with each processor having\n8 lanes and running at 3.2 GHz. It was announced in 2008. The peak double pre-\ncision vector performance is 102 GFLOP/sec. The 16 processor SMP can have\n1024 GBytes of DRAM. The maximum size is 512 nodes.\nThe basis for modern vectorizing compiler technology and the notion of data\ndependence was developed by Kuck and his colleagues [1974] at the University of\nIllinois. Padua and Wolfe [1986] gave a good overview of vectorizing compiler\ntechnology.\nMultimedia SIMD Instruction Extensions\nWhat could a computer hardware company \u2026 possibly have in common with\ndisco dancing. A lot, if one goes by an advertisement campaign released by\nthe world\u2019s largest microprocessor company \u2026 Intel, in 1997.\nIBS Center for Management Research\n\u201cDancing Its Way Towards Leadership,\u201d 2002\nM.6\nThe Development of SIMD Supercomputers, Vector Computers, Multimedia\n\u25a0\nM-49"
    },
    {
        "page": 1397,
        "text": "Going through the history books, the 1957 TX-2 had partitioned ALUs to support\nmedia of the time, but these ideas faded away to be rediscovered 30 years later in\nthe personal computer era. Since every desktop microprocessor by definition has\nits own graphical displays, as transistor budgets increased it was inevitable that\nsupport would be added for graphics operations. Many graphics systems use 8 bits\nto represent each of the 3 primary colors plus 8 bits for a transparency of a pixel.\nThe addition of speakers and microphones for teleconferencing and video games\nsuggested support of sound as well. Audio samples need more than 8 bits of pre-\ncision, but 16 bits are sufficient.\nEvery microprocessor has special support so that bytes and half words take up\nless space when stored in memory, but due to the infrequency of arithmetic oper-\nations on these data sizes in typical integer programs, there is little support\nbeyond data transfers. The Intel i860 was justified as a graphical accelerator\nwithin the company. Its architects recognized that many graphics and audio appli-\ncations would perform the same operation on vectors of these data [Atkins 1991;\nKohn 1989]. Although a vector unit was beyond the transistor budget of the i860\nin 1989, by partitioning the carry chains within a 64-bit ALU, it could perform\nsimultaneous operations on short vectors of eight 8-bit operands, four 16-bit oper-\nands, or two 32-bit operands. The cost of such partitioned ALUs was small.\nApplications that lend themselves to such support include MPEG (video), video\ngames (3D graphics), digital photography, and teleconferencing (audio and image\nprocessing).\nLike a virus, over time such multimedia support has spread to nearly every\ndesktop microprocessor. HP was the first successful desktop RISC to include\nsuch support, but soon every other manufacturer had their own take on the idea\nin the 1990s.\nThese extensions were originally called subword parallelism or vector. Since\nIntel marketing used SIMD to describe the MMX extension of the 80x86\nannounced in 1996, that became the popular name, due in part to a successful tele-\nvision advertising campaign involving disco dancers wearing clothing modeled\nafter the cleansuits worn in semiconductor fabrication lines.\nGraphical Processor Units\nIt\u2019s been almost three years since GPU computing broke into the mainstream of\nHPC with the introduction of NVIDIA\u2019s CUDA API in September 2007. Adoption of\nthe technology since then has proceeded at a surprisingly strong and steady\npace. Many organizations that began with small pilot projects a year or two\nago have moved on to enterprise deployment, and GPU accelerated machines\nare now represented on the TOP500 list starting at position two. The relatively\nrapid adoption of CUDA by a community not known for the rapid adoption of\nmuch of anything is a noteworthy signal. Contrary to the accepted wisdom that\nGPU computing is more difficult, I believe its success thus far signals that it is no\nmore complicated than good CPU programming. Further, it more clearly and\nsuccinctly expresses the parallelism of a large class of problems leading to code\nM-50\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1398,
        "text": "that is easier to maintain, more scalable and better positioned to map to future\nmany-core architectures.\nVincent Natol\n\u201cKudos for CUDA,\u201d HPCwire (2010)\n3D graphics pipeline hardware evolved from the large expensive systems of the\nearly 1980s to small workstations and then to PC accelerators in the mid- to late\n1990s. During this period, three major transitions occurred:\n\u25a0\nPerformance-leading graphics subsystems declined in price from $50,000 to\n$200.\n\u25a0\nPerformance increased from 50 million pixels per second to 1 billion pixels per\nsecond and from 100,000 vertices per second to 10 million vertices per second.\n\u25a0\nNative hardware capabilities evolved from wireframe (polygon outlines) to\nflat-shaded (constant color) filled polygons, to smooth-shaded (interpolated\ncolor) filled polygons, to full-scene anti-aliasing with texture mapping and\nrudimentary multitexturing.\nScalable GPUs\nScalability has been an attractive feature of graphics systems from the beginning.\nWorkstation graphics systems gave customers a choice in pixel horse-power by vary-\ning the number of pixel processor circuit boards installed. Prior to the mid-1990s PC\ngraphicsscalingwasalmostnonexistent.Therewasoneoption\u2014theVGAcontroller.\nAs 3D-capable accelerators appeared, the market had room for a range of offerings.\n3dfx introduced multiboard scaling with the original SLI (Scan Line Interleave) on\ntheir Voodoo2, which held the performance crown for its time (1998). Also in\n1998, NVIDIA introduced distinct products as variants on a single architecture with\nRiva TNT Ultra (high-performance) and Vanta (low-cost), first by speed binning and\npackaging, then with separate chip designs (GeForce 2 GTS and GeForce 2 MX). At\npresent,foragivenarchitecturegeneration,fourorfiveseparateGPUchipdesignsare\nneeded to cover the range of desktop PC performance and price points. In addition,\nthere are separate segments in notebook and workstation systems. After acquiring\n3dfx, NVIDIA continued the multi-GPU SLI concept in 2004, starting with GeForce\n6800\u2014providing multi-GPU scalability transparently to the programmer and to the\nuser.Functionalbehaviorisidenticalacrossthescalingrange;oneapplicationwillrun\nunchanged on any implementation of an architectural family.\nGraphics Pipelines\nEarly graphics hardware was configurable, but not programmable by the applica-\ntion developer. With each generation, incremental improvements were offered;\nhowever, developers were growing more sophisticated and asking for more new\nfeatures than could be reasonably offered as built-in fixed functions. The NVIDIA\nM.6\nThe Development of SIMD Supercomputers, Vector Computers, Multimedia\n\u25a0\nM-51"
    },
    {
        "page": 1399,
        "text": "GeForce 3, described by Lindholm et al. [2001], took the first step toward true gen-\neral shader programmability. It exposed to the application developer what had been\nthe private internal instruction set of the floating-point vertex engine. This coin-\ncided with the release of Microsoft\u2019s DirectX 8 and OpenGL\u2019s vertex shader exten-\nsions. Later GPUs, at the time of DirectX 9, extended general programmability and\nfloating-point capability to the pixel fragment stage and made texture available at\nthe vertex stage. The ATI Radeon 9700, introduced in 2002, featured a program-\nmable 24-bit floating-point pixel fragment processor programmed with DirectX 9\nand OpenGL. The GeForce FX added 32-bit floating-point pixel processors. This\nwas part of a general trend toward unifying the functionality of the different stages,\nat least as far as the application programmer was concerned. NVIDIA\u2019s GeForce\n6800 and 7800 series were built with separate processor designs and separate hard-\nware dedicated to the vertex and to the fragment processing. The XBox 360 intro-\nduced an early unified processor GPU in 2005, allowing vertex and pixel shaders to\nexecute on the same processor.\nGPGPU: An Intermediate Step\nAs DirectX 9-capable GPUs became available, some researchers took notice of the\nraw performance growth path of GPUs and began to explore the use of GPUs to\nsolve complex parallel problems. DirectX 9 GPUs had been designed only to\nmatch the features required by the graphics API. To access the computational\nresources, a programmer had to cast their problem into native graphics operations.\nFor example, to run many simultaneous instances of a pixel shader, a triangle had\nto be issued to the GPU (with clipping to a rectangle shape if that was what was\ndesired). Shaders did not have the means to perform arbitrary scatter operations to\nmemory. The only way to write a result to memory was to emit it as a pixel color\nvalue and configure the framebuffer operation stage to write (or blend, if desired)\nthe result to a two-dimensional framebuffer. Furthermore, the only way to get a\nresult from one pass of computation to the next was to write all parallel results\nto a pixel framebuffer, then use that framebuffer as a texture map as input to\nthe pixel fragment shader of the next stage of the computation. Mapping general\ncomputations to a GPU in this era was quite awkward. Nevertheless, intrepid\nresearchers demonstrated a handful of useful applications with painstaking efforts.\nThis field was called \u201cGPGPU\u201d for general-purpose computing on GPUs.\nGPU Computing\nWhile developing the Tesla architecture for the GeForce 8800, NVIDIA realized\nits potential usefulness would be much greater if programmers could think of the\nGPU as a processor. NVIDIA selected a programming approach in which program-\nmers would explicitly declare the data-parallel aspects of their workload.\nFor the DirectX 10 generation, NVIDIA had already begun work on a highef-\nficiency floating-point and integer processor that could run a variety of\nM-52\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1400,
        "text": "simultaneous workloads to support the logical graphics pipeline. This processor\nwas designed to take advantage of the common case of groups of threads executing\nthe same code path. NVIDIA added memory load and store instructions with inte-\nger byte addressing to support the requirements of compiled C programs. It intro-\nduced the thread block (cooperative thread array), grid of thread blocks, and barrier\nsynchronization to dispatch and manage highly parallel computing work. Atomic\nmemory operations were added. NVIDIA developed the CUDA C/C++ compiler,\nlibraries, and runtime software to enable programmers to readily access the new\ndata-parallel computation model and develop applications.\nTo create a vendor-neutral GPU programming language, a large number of com-\npanies are creating compilers for the OpenCL language, which has many of the fea-\ntures of CUDA but which runs on many more platforms. In 2011, the performance is\nmuch higher if you write CUDA code for GPUs than if you write OpenCL code.\nAMD\u2019s acquisition of ATI, the second leading GPU vendor, suggests a spread\nof GPU computing. The AMD Fusion architecture, announced just as this edition\nwas being finished, is an initial merger between traditional GPUs and traditional\nCPUs. NVIDIA also announced Project Denver, which combines an ARM scalar\nprocessor with NVIDIA GPUs in a single address space. When these systems are\nshipped, it will be interesting to learn just how tightly integrated they are and the\nimpact of integration on performance and energy of both data parallel and graphics\napplications.\nReferences\nSIMD Supercomputers\nBouknight, W. J., S. A. Deneberg, D. E. McIntyre, J. M. Randall, A. H. Sameh, and\nD. L. Slotnick [1972]. \u201cThe Illiac IV system,\u201d Proc. IEEE 60:4, 369\u2013379. Also\nappears in D. P. Siewiorek, C. G. Bell, and A. Newell, Computer Structures:\nPrinciples and Examples, McGraw-Hill, New York, 1982, 306\u2013316.\nHillis, W. D. [1985]. The Connection Multiprocessor, MIT Press, Cambridge, Mass.\nHord, R. M. [1982]. The Illiac-IV, The First Supercomputer, Computer Science\nPress, Rockville, Md.\nSlotnick, D. L., W. C. Borck, and R. C. McReynolds [1962]. \u201cThe Solomon com-\nputer,\u201d Proc. AFIPS Fall Joint Computer Conf., December 4\u20136, 1962,\nPhiladelphia, Penn., 97\u2013107.\nUnger, S. H. [1958]. \u201cA computer oriented towards spatial problems,\u201d Proc. Insti-\ntute of Radio Engineers 46:10 (October), 1744\u20131750.\nVector Architecture\nAsanovic, K. [1998]. \u201cVector Microprocessors,\u201d Ph.D. thesis, Computer Science\nDivision, University of California, Berkeley.\nBaskett, F., and T. W. Keller [1977]. \u201cAn Evaluation of the Cray-1 Processor,\u201d in\nHigh Speed Computer and Algorithm Organization, D. J. Kuck, D. H. Lawrie,\nand A. H. Sameh, eds., Academic Press, San Diego, Calif., 71\u201384.\nM.6\nThe Development of SIMD Supercomputers, Vector Computers, Multimedia\n\u25a0\nM-53"
    },
    {
        "page": 1401,
        "text": "Chen, S. [1983]. \u201cLarge-scale and high-speed multiprocessor system for scientific\napplications,\u201d Proc. NATO Advanced Research Workshop on High Speed Com-\nputing, June 20\u201322, Julich, West Germany. Also in K. Hwang, ed., \u201cSuperpro-\ncessors: Design and applications,\u201d IEEE, August, 59\u201373, 1984.\nFlynn, M. J. [1966]. \u201cVery high-speed computing systems,\u201d Proc. IEEE 54:12\n(December), 1901\u20131909.\nGebis, J. and Patterson, D. [2007]. \u201cEmbracing and extending 20th-century\ninstruction set architectures,\u201d IEEE Computer, 40:4 (April), 68\u201375.\nHintz, R. G., and D. P. Tate [1972]. \u201cControl data STAR-100 processor design,\u201d\nProc. IEEE COMPCON, September 12\u201314, 1972, San Francisco, 1\u20134.\nKitagawa, K., S. Tagaya, Y. Hagihara, and Y. Kanoh [2003]. \u201cA hardware over-\nview of SX-6 and SX-7 supercomputer,\u201d NEC Research and Development Jour-\nnal 44:1 (January), 2\u20137.\nKozyrakis, C., and D. Patterson [2002]. \u201cVector vs. superscalar and VLIW archi-\ntectures for embedded multimedia benchmarks,\u201d Proc. 35th Annual Intl. Sym-\nposium on Microarchitecture (MICRO), November 18\u201322, 2002, Istanbul,\nTurkey.\nKuck, D., P. P. Budnik, S.-C. Chen, D. H. Lawrie, R. A. Towle, R. E. Strebendt, E.\nW. Davis, Jr., J. Han, P. W. Kraska, and Y. Muraoka [1974]. \u201cMeasurements of\nparallelism in ordinary Fortran programs,\u201d Computer 7:1 (January), 37\u201346.\nLincoln, N. R. [1982]. \u201cTechnology and design trade offs in the creation of a mod-\nern supercomputer,\u201d IEEE Trans. on Computers C-31:5 (May), 363\u2013376.\nMiura, K., and K. Uchida [1983]. \u201cFACOM vector processing system: VP100/\n200,\u201d Proc. NATO Advanced Research Workshop on High Speed Computing,\nJune 20\u201322, Julich, West Germany. Also in K. Hwang, ed., \u201cSuperprocessors:\nDesign and applications,\u201d IEEE, August, 59\u201373, 1984.\nPadua, D., and M. Wolfe [1986]. \u201cAdvanced compiler optimizations for supercom-\nputers,\u201d Communications of the ACM 29:12 (December), 1184\u20131201.\nRussell, R. M. [1978]. \u201cThe Cray-1 processor system,\u201d Communications of the\nACM 21:1 (January), 63\u201372.\nVajapeyam, S. [1991]. \u201cInstruction-Level Characterization of the Cray Y-MP Pro-\ncessor,\u201d Ph.D. thesis, Computer Sciences Department, University of Wiscon-\nsin\u2013Madison.\nWatanabe, T. [1987]. \u201cArchitecture and performance of the NEC supercomputer\nSX system,\u201d Parallel Computing 5, 247\u2013255.\nWatson, W. J. [1972]. \u201cThe TI ASC\u2014a highly modular and flexible super\nprocessor architecture,\u201d Proc. AFIPS Fall Joint Computer Conf., December\n5\u20137, 1972, Anaheim, Calif., 221\u2013228.\nMultimedia SIMD\nAtkins, M. [1991]. \u201cPerformance and the i860 Microprocessor,\u201d IEEE Micro, 11:5\n(September), 24\u201327, 72\u201378.\nKohn, L., and N. Margulis [1989]. \u201cIntroducing the Intel i860 64-Bit Microproces-\nsor,\u201d IEEE Micro, 9:4 (July), 15\u201330.\nM-54\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1402,
        "text": "GPU\nAkeley, K., and T. Jermoluk [1988]. \u201cHigh-performance polygon rendering,\u201d\nProc. SIGGRAPH 88, August 1\u20135, 1988, Atlanta, Ga., 239\u201346.\nHillis, W. D., and G. L. Steele [1986]. \u201cData parallel algorithms,\u201d Communications\nof the ACM 29:12 (December), 1170\u20131183 (http://doi.acm.org/10.1145/7902.\n7903).\nIEEE 754-2008 Working Group. [2006]. DRAFT Standard for Floating-Point\nArithmetic, 754-2008 (https://doi.org/10.1109/IEEESTD.2008.4610935).\nLee, W. V., et al. [2010]. \u201cDebunking the 100X GPU vs. CPU myth: an evaluation\nof throughput computing on CPU and GPU,\u201d Proc. ISCA \u201910, June 19\u201323, 2010,\nSaint-Malo, France.\nLindholm, E., M. J. Kligard, and H. Moreton [2001]. A user-programmable vertex\nengine. In SIGGRAPH \u201901: Proceedings of the 28th annual conference on Com-\nputer graphics and interactive techniques, 149\u2013158.\nMoore, G. E. [1965]. \u201cCramming more components onto integrated circuits,\u201d Elec-\ntronics 38:8 (April 19), 114\u2013117.\nWilliams, S., A. Waterman, and D. Patterson [2009]. \u201cRoofline: An insightful\nvisual performance model for multicore architectures,\u201d Communications of\nthe ACM, 52:4 (April), 65\u201376.\nM.7\nThe History of Multiprocessors and Parallel Processing\n(Chapter 5 and Appendices F, G, and I)\nThere is a tremendous amount of history in multiprocessors; in this section, we\ndivide our discussion by both time period and architecture. We start with the SIMD\napproach and the Illiac IV. We then turn to a short discussion of some other early\nexperimental multiprocessors and progress to a discussion of some of the great\ndebates in parallel processing. Next we discuss the historical roots of the present\nmultiprocessors and conclude by discussing recent advances.\nSIMD Computers: Attractive Idea, Many Attempts,\nNo Lasting Successes\nThe cost of a general multiprocessor is, however, very high and further design\noptions were considered which would decrease the cost without seriously\ndegrading the power or efficiency of the system. The options consist of recen-\ntralizing one of the three major components. \u2026 Centralizing the [control unit]\ngives rise to the basic organization of [an] \u2026 array processor such as the\nIlliac IV.\nBouknight et al. [1972]\nM.7\nThe History of Multiprocessors and Parallel Processing\n\u25a0\nM-55"
    },
    {
        "page": 1403,
        "text": "The SIMD model was one of the earliest models of parallel computing, dating\nback to the first large-scale multiprocessor, the Illiac IV. The key idea in that mul-\ntiprocessor, as in more recent SIMD multiprocessors, is to have a single instruction\nthat operates on many data items at once, using many functional units.\nThe earliest ideas on SIMD-style computers are from Unger [1958] and Slot-\nnick, Borck, and McReynolds [1962]. Slotnick\u2019s Solomon design formed the basis\nof the Illiac IV, perhaps the most infamous of the supercomputer projects.\nAlthough successful in pushing several technologies that proved useful in later\nprojects, it failed as a computer. Costs escalated from the $8 million estimate in\n1966 to $31 million by 1972, despite construction of only a quarter of the planned\nmultiprocessor. Actual performance was at best 15 MFLOPS versus initial predic-\ntions of 1000 MFLOPS for the full system [Hord 1982]. Delivered to NASA Ames\nResearch in 1972, the computer took three more years of engineering before it was\nusable. These events slowed investigation of SIMD, but Danny Hillis [1985] resus-\ncitated this style in the Connection Machine, which had 65,636 1-bit processors.\nReal SIMD computers need to have a mixture of SISD and SIMD instructions.\nThere is an SISD host computer to perform operations such as branches and\naddress calculations that do not need parallel operation. The SIMD instructions\nare broadcast to all the execution units, each of which has its own set of registers.\nFor flexibility, individual execution units can be disabled during an SIMD instruc-\ntion. In addition, massively parallel SIMD multiprocessors rely on interconnection\nor communication networks to exchange data between processing elements.\nSIMD works best in dealing with arrays in for loops; hence, to have the opportu-\nnity for massive parallelism in SIMD there must be massive amounts of data, or data\nparallelism.SIMDisatitsweakestincasestatements,whereeachexecutionunitmust\nperform adifferent operationonits data,depending onwhatdataithas.Theexecution\nunits with the wrong data are disabled so that the proper units can continue. Such sit-\nuations essentially run at 1/nth performance, where n is the number of cases.\nThe basic trade-off in SIMD multiprocessors is performance of a processor\nversus number of processors. Recent multiprocessors emphasize a large degree\nof parallelism over performance of the individual processors. The Connection\nMultiprocessor 2, for example, offered 65,536 single-bit-wide processors, while\nthe Illiac IV had 64 64-bit processors.\nAfter being resurrected in the 1980s, first by Thinking Machines and then by\nMasPar, the SIMD model has once again been put to bed as a general-purpose mul-\ntiprocessor architecture, for two main reasons. First, it is too inflexible. A number\nof important problems cannot use such a style of multiprocessor, and the architec-\nture does not scale down in a competitive fashion; that is, small-scale SIMD\nmultiprocessors often have worse cost-performance compared with that of the\nalternatives. Second, SIMD cannot take advantage of the tremendous performance\nand cost advantages of microprocessor technology. Instead of leveraging this low-\ncost technology, designers of SIMD multiprocessors must build custom processors\nfor their multiprocessors.\nAlthough SIMD computers have departed from the scene as general-purpose\nalternatives, this style of architecture will continue to have a role in special-purpose\nM-56\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1404,
        "text": "designs. Many special-purpose tasks are highly data parallel and require a limited\nset of functional units. Thus, designers can build in support for certain operations,\nas well as hardwired interconnection paths among functional units. Such organi-\nzations are often called array processors, and they are useful for such tasks as\nimage and signal processing.\nOther Early Experiments\nIt is difficult to distinguish the first MIMD multiprocessor. Surprisingly, the first\ncomputer from the Eckert-Mauchly Corporation, for example, had duplicate units\nto improve availability. Holland [1959] gave early arguments for multiple proces-\nsors. Two of the best-documented multiprocessor projects were undertaken in the\n1970s at Carnegie Mellon University. The first of these was C.mmp [Wulf and Bell\n1972; Wulf and Harbison 1978], which consisted of 16 PDP-11s connected by a\ncrossbar switch to 16 memory units. It was among the first multiprocessors with\nmore than a few processors, and it had a shared-memory programming model.\nMuch of the focus of the research in the C.mmp project was on software, especially\nin the OS area. A later multiprocessor, Cm* [Swan et al. 1977], was a cluster-based\nmultiprocessor with a distributed memory and a nonuniform access time. The\nabsence of caches and a long remote access latency made data placement critical.\nThis multiprocessor and a number of application experiments are well described by\nGehringer, Siewiorek, and Segall [1987]. Many of the ideas in these multiproces-\nsors would be reused in the 1980s when the microprocessor made it much cheaper\nto build multiprocessors.\nGreat Debates in Parallel Processing\nThe turning away from the conventional organization came in the middle 1960s,\nwhen the law of diminishing returns began to take effect in the effort to increase\nthe operational speed of a computer. \u2026 Electronic circuits are ultimately limited in\ntheir speed of operation by the speed of light \u2026 and many of the circuits were\nalready operating in the nanosecond range.\nBouknight et al. [1972]\n\u2026 sequential computers are approaching a fundamental physical limit on their\npotential computational power. Such a limit is the speed of light \u2026\nAngel L. DeCegama\nThe Technology of Parallel Processing, Vol. I (1989)\n\u2026 today\u2019s multiprocessors \u2026 are nearing an impasse as technologies approach\nthe speed of light. Even if the components of a sequential processor could be made\nto work this fast, the best that could be expected is no more than a few million\ninstructions per second.\nDavid Mitchell\nThe Transputer: The Time Is Now (1989)\nM.7\nThe History of Multiprocessors and Parallel Processing\n\u25a0\nM-57"
    },
    {
        "page": 1405,
        "text": "The quotes above give the classic arguments for abandoning the current form\nof computing, and Amdahl [1967] gave the classic reply in support of continued\nfocus on the IBM 360 architecture. Arguments for the advantages of parallel exe-\ncution can be traced back to the 19th century [Menabrea 1842]! Yet, the effective-\nness of the multiprocessor for reducing latency of individual important programs is\nstill being explored. Aside from these debates about the advantages and limitations\nof parallelism, several hot debates have focused on how to build multiprocessors.\nIt\u2019s hard to predict the future, yet in 1989 Gordon Bell made two predictions for\n1995. We included these predictions in the first edition of the book, when the out-\ncome was completely unclear. We discuss them in this section, together with an\nassessment of the accuracy of the prediction.\nThe first was that a computer capable of sustaining a teraFLOPS\u2014one million\nMFLOPS\u2014would be constructed by 1995, using either a multicomputer with 4K\nto 32K nodes or a Connection Multiprocessor with several million processing ele-\nments [Bell 1989]. To put this prediction in perspective, each year the Gordon Bell\nPrize acknowledges advances in parallelism, including the fastest real program\n(highest MFLOPS). In 1989, the winner used an eight-processor Cray Y-MP to\nrun at 1680 MFLOPS. On the basis of these numbers, multiprocessors and pro-\ngrams would have to have improved by a factor of 3.6 each year for the fastest\nprogram to achieve 1 TFLOPS in 1995. In 1999, the first Gordon Bell prize winner\ncrossed the 1 TFLOPS bar. Using a 5832-processor IBM RS/6000 SST system\ndesigned specially for Livermore Laboratories, they achieved 1.18 TFLOPS on\na shock-wave simulation. This ratio represents a year-to-year improvement of\n1.93, which is still quite impressive.\nWhat has become recognized since the 1990s is that, although we may have the\ntechnology to build a TFLOPS multiprocessor, it is not clear that the machine is\ncost effective, except perhaps for a few very specialized and critically important\napplications related to national security. We estimated in 1990 that to achieve 1\nTFLOPS would require a machine with about 5000 processors and would cost\nabout $100 million. The 5832-processor IBM system at Livermore cost $110 mil-\nlion. As might be expected, improvements in the performance of individual micro-\nprocessors both in cost and performance directly affect the cost and performance of\nlarge-scale multiprocessors, but a 5000-processor system will cost more than 5000\ntimes the price of a desktop system using the same processor. Since that time, much\nfaster multiprocessors have been built, but the major improvements have increas-\ningly come from the processors in the past five years, rather than fundamental\nbreakthroughs in parallel architecture.\nThe second Bell prediction concerned the number of data streams in supercom-\nputers shipped in 1995. Danny Hillis believed that, although supercomputers with\na small number of data streams may be the best sellers, the biggest multiprocessors\nwould be multiprocessors with many data streams, and these would perform the\nbulk of the computations. Bell bet Hillis that in the last quarter of calendar year\n1995 more sustained MFLOPS would be shipped in multiprocessors using few\ndata streams (\u0004100) rather than many data streams (\u00051000). This bet concerned\nM-58\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1406,
        "text": "only supercomputers, defined as multiprocessors costing more than $1 million and\nused for scientific applications. Sustained MFLOPS was defined for this bet as the\nnumber of floating-point operations per month, so availability of multiprocessors\naffects their rating.\nIn 1989, when this bet was made, it was totally unclear who would win. In\n1995, a survey of the current publicly known supercomputers showed only six\nmultiprocessors in existence in the world with more than 1000 data streams,\nso Bell\u2019s prediction was a clear winner. In fact, in 1995, much smaller\nmicroprocessor-based multiprocessors (\u000420 processors) were becoming domi-\nnant. In 1995, a survey of the 500 highest-performance multiprocessors in use\n(based on Linpack ratings), called the TOP500, showed that the largest number\nof multiprocessors were bus-based shared-memory multiprocessors! By 2005,\nvarious clusters or multicomputers played a large role. For example, in the top\n25 systems, 11 were custom clusters, such as the IBM Blue Gene system or the\nCray XT3; 10 were clusters of shared-memory multiprocessors (both using distrib-\nuted and centralized memory); and the remaining 4 were clusters built using PCs\nwith an off-the-shelf interconnect.\nMore Recent Advances and Developments\nWith the primary exception of the parallel vector multiprocessors (see Appendix\nG) and more recently of the IBM Blue Gene design, all other recent MIMD\ncomputers have been built from off-the-shelf microprocessors using a bus and log-\nically central memory or an interconnection network and a distributed memory. A\nnumber of experimental multiprocessors built in the 1980s further refined and\nenhanced the concepts that form the basis for many of today\u2019s multiprocessors.\nThe Development of Bus-Based Coherent Multiprocessors\nAlthough very large mainframes were built with multiple processors in the 1960s\nand 1970s, multiprocessors did not become highly successful until the 1980s. Bell\n[1985] suggested that the key was that the smaller size of the microprocessor\nallowed the memory bus to replace the interconnection network hardware and that\nportable operating systems meant that multiprocessor projects no longer required\nthe invention of a new operating system. In his paper, Bell defined the terms mul-\ntiprocessor and multicomputer and set the stage for two different approaches to\nbuilding larger scale multiprocessors.\nThe first bus-based multiprocessor with snooping caches was the Synapse\nN+1 described by Frank [1984]. Goodman [1983] wrote one of the first papers\nto describe snooping caches. The late 1980s saw the introduction of many commer-\ncial bus-based, snooping cache architectures, including the Silicon Graphics\n4D/240 [Baskett, Jermoluk, and Solomon 1988], the Encore Multimax [Wilson\n1987], and the Sequent Symmetry [Lovett and Thakkar 1988]. The mid-1980s\nM.7\nThe History of Multiprocessors and Parallel Processing\n\u25a0\nM-59"
    },
    {
        "page": 1407,
        "text": "saw an explosion in the development of alternative coherence protocols, and\nArchibald and Baer [1986] provided a good survey and analysis, as well as refer-\nences to the original papers. Figure M.2 summarizes several snooping cache coher-\nence protocols and shows some multiprocessors that have used or are using that\nprotocol.\nThe early 1990s saw the beginning of an expansion of such systems with the\nuse of very wide, high-speed buses (the SGI Challenge system used a 256-bit,\npacket-oriented bus supporting up to 8 processor boards and 32 processors) and\nlater the use of multiple buses and crossbar interconnects\u2014for example, in the\nSun SPARCCenter and Enterprise systems (Charlesworth [1998] discussed the\ninterconnect architecture of these multiprocessors). In 2001, the Sun Enterprise\nservers represented the primary example of large-scale (>16 processors), symmet-\nric multiprocessors in active use. Today, most bus-based machines offer only four\nor so processors and switches, or alternative designs are used for eight or more.\nToward Large-Scale Multiprocessors\nIn the effort to build large-scale multiprocessors, two different directions were\nexplored: message-passing multicomputers and scalable shared-memory multipro-\ncessors. Although there had been many attempts to build mesh and hypercube-\nconnected multiprocessors, one of the first multiprocessors to successfully bring\ntogether all the pieces was the Cosmic Cube built at Caltech [Seitz 1985]. It intro-\nduced important advances in routing and interconnect technology and substantially\nName\nProtocol\ntype\nMemory write policy\nUnique feature\nMultiprocessors using\nWrite\nOnce\nWrite\ninvalidate\nWrite-back after first\nwrite\nFirst snooping protocol\ndescribed in literature\nSynapse\nN+1\nWrite\ninvalidate\nWrite-back\nExplicit state where memory is\nthe owner\nSynapse multiprocessors; first\ncache-coherent multiprocessors\navailable\nBerkeley\n(MOESI)\nWrite\ninvalidate\nWrite-back\nOwned shared state\nBerkeley SPUR multiprocessor;\nSun Enterprise servers\nIllinois\n(MESI)\nWrite\ninvalidate\nWrite-back\nClean private state; can supply\ndata from any cache with a\nclean copy\nSGI Power and Challenge series\n\u201cFirefly\u201d\nWrite\nbroadcast\nWrite-back when\nprivate, write through\nwhen shared\nMemory updated on broadcast\nNo current multiprocessors;\nSPARCCenter 2000 closest\nFigure M.2 Five snooping protocols summarized. Archibald and Baer [1986] use these names to describe the five\nprotocols, and Eggers [1989] summarizes the similarities and differences as shown in this figure. The Firefly protocol\nwas named for the experimental DEC Firefly multiprocessor, in which it appeared. The alternative names for protocols\nare based on the states they support: M\u00bcModified, E\u00bcExclusive (private clean), S\u00bcShared, I\u00bcInvalid, O\u00bcOwner\n(shared dirty).\nM-60\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1408,
        "text": "reduced the cost of the interconnect, which helped make the multicomputer viable.\nThe Intel iPSC 860, a hypercube-connected collection of i860s, was based on these\nideas. More recent multiprocessors, such as the Intel Paragon, have used networks\nwith lower dimensionality and higher individual links. The Paragon also employed\na separate i860 as a communications controller in each node, although a number of\nusers have found it better to use both i860 processors for computation as well as\ncommunication. The Thinking Multiprocessors CM-5 made use of off-the-shelf\nmicroprocessors and a fat tree interconnect (see Appendix F). It provided user-\nlevel access to the communication channel, thus significantly improving commu-\nnication latency. In 1995, these two multiprocessors represented the state of the art\nin message-passing multicomputers.\nEarly attempts at building a scalable shared-memory multiprocessor include the\nIBM RP3 [Pfister et al. 1985], the NYU Ultracomputer [Elder et al. 1985; Schwartz\n1980], the University of Illinois Cedar project [Gajksi et al. 1983], and the BBN\nButterfly and Monarch [BBN Laboratories 1986; Rettberg et al. 1990]. These\nmultiprocessors all provided variationson a nonuniform distributed-memory model\nand, hence, are distributed shared-memory (DSM) multiprocessors, but they did\nnot support cache coherence, which substantially complicated programming.\nThe RP3 and Ultracomputer projects both explored new ideas in synchronization\n(fetch-and-operate) as well as the idea of combining references in the network.\nIn all four multiprocessors, the interconnect networks turned out to be more\ncostly than the processing nodes, raising problems for smaller versions of the\nmultiprocessor. The Cray T3D/E (see Arpaci et al. [1995] for an evaluation of\nthe T3D and Scott[1996] for adescription of the T3E enhancements) builds onthese\nideas, using a noncoherent shared address space but building on the advances in\ninterconnect technology developed in the multicomputer domain (see Scott and\nThorson [1996]).\nExtending the shared-memory model with scalable cache coherence was done\nby combining a number of ideas. Directory-based techniques for cache coherence\nwere actually known before snooping cache techniques. In fact, the first cache\ncoherence protocols actually used directories, as described by Tang [1976] and\nimplemented in the IBM 3081. Censier and Feautrier [1978] described a directory\ncoherence scheme with tags in memory. The idea of distributing directories with\nthe memories to obtain a scalable implementation of cache coherence was first\ndescribed by Agarwal et al. [1988] and served as the basis for the Stanford DASH\nmultiprocessor (see Lenoski et al. [1990, 1992]), which was the first operational\ncache-coherent DSM multiprocessor. DASH was a \u201cplump\u201d node cc-NUMA\nmachine that used four-processor SMPs as its nodes, interconnecting them in a\nstyle similar to that of Wildfire but using a more scalable two-dimensional grid\nrather than a crossbar for the interconnect.\nThe Kendall Square Research KSR-1 [Burkhardt et al. 1992] was the first com-\nmercial implementation of scalable coherent shared memory. It extended the basic\nDSM approach to implement a concept called cache-only memory architecture\n(COMA), which makes the main memory a cache. In the KSR-1, memory blocks\ncould be replicated in the main memories of each node with hardware support to\nM.7\nThe History of Multiprocessors and Parallel Processing\n\u25a0\nM-61"
    },
    {
        "page": 1409,
        "text": "handletheadditionalcoherencerequirementsforthesereplicatedblocks.(TheKSR-1\nwas not strictly a pure COMA because it did not migrate the home location of a data\nitem but always kept a copy at home. Essentially, it implemented only replication.)\nManyother researchproposals [Falsafiand Wood1997; Hagersten, Landin, and Har-\nidi 1992; Saulsbury et al. 1995; Stenstr\u20acom, Joe, and Gupta 1992] for COMA-style\narchitectures and similar approaches that reduce the burden of nonuniform memory\naccessthroughmigration[Chandraetal.1994;Soundararajanetal.1998]weredevel-\noped, but there have been no further commercial implementations.\nThe Convex Exemplar implemented scalable coherent shared memory using a\ntwo-level architecture: At the lowest level, eight-processor modules are built using\na crossbar. A ring can then connect up to 32 of these modules, for a total of 256\nprocessors (see Thekkath et al. [1997] for an evaluation). Laudon and Lenoski\n[1997] described the SGI Origin, which was first delivered in 1996 and is closely\nbased on the original Stanford DASH machine, although including a number of\ninnovations for scalability and ease of programming. Origin uses a bit vector\nfor the directory structure, which is either 16 or 32 bits long. Each bit represents\na node, which consists of two processors; a coarse bit vector representation allows\neach bit to represent up to 8 nodes for a total of 1024 processors. As Galles [1996]\ndescribed, a high-performance fat hypercube is used for the global interconnect.\nHristea, Lenoski, and Keen [1997] have provided a thorough evaluation of the\nperformance of the Origin memory system.\nSeveral research prototypes were undertaken to explore scalable coherence\nwith and without multithreading. These include the MIT Alewife machine\n[Agarwal et al. 1995] and the Stanford FLASH multiprocessor [Gibson et al.\n2000; Kuskin et al. 1994].\nClusters\nClusters were probably \u201cinvented\u201d in the 1960s by customers who could not fit\nall their work on one computer or who needed a backup machine in case of failure\nof the primary machine [Pfister 1998]. Tandem introduced a 16-node cluster in\n1975. Digital followed withVAX clusters,introduced in1984.Theywereoriginally\nindependent computers that shared I/O devices, requiring a distributed operating\nsystem to coordinate activity. Soon they had communication links between com-\nputers, in part so that the computers could be geographically distributed to\nincrease availability in case of a disaster at a single site. Users log onto the cluster\nand are unaware of which machine they are running on. DEC (now HP) sold\nmore than 25,000 clusters by 1993. Other early companies were Tandem (now\nHP) and IBM (still IBM). Today, virtually every company has cluster products.\nMost of these products are aimed at availability, with performance scaling as a\nsecondary benefit.\nScientific computing on clusters emerged as a competitor to MPPs. In 1993, the\nBeowulf project started with the goal of fulfilling NASA\u2019s desire for a 1 GFLOPS\ncomputer for under $50,000. In 1994, a 16-node cluster built from off-the-shelf\nM-62\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1410,
        "text": "PCs using 80486s achieved that goal [Bell and Gray 2001]. This emphasis led to a\nvariety of software interfaces to make it easier to submit, coordinate, and debug\nlarge programs or a large number of independent programs.\nEfforts were made to reduce latency of communication in clusters as well as to\nincrease bandwidth, and several research projects worked on that problem.\n(One commercial result of the low-latency research was the VI interface standard,\nwhich has been embraced by Infiniband, discussed below.) Low latency then\nproved useful in other applications. For example, in 1997 a cluster of 100 Ultra-\nSPARC desktop computers at the University of California\u2013Berkeley, connected by\n160 MB/sec per link Myrinet switches, was used to set world records in database\nsort\u2014sorting 8.6 GB of data originally on disk in 1 minute\u2014and in cracking an\nencrypted message\u2014taking just 3.5 hours to decipher a 40-bit DES key.\nThis research project, called Network of Workstations [Anderson, Culler, and\nPatterson 1995], also developed the Inktomi search engine, which led to a startup\ncompany with the same name. Google followed the example of Inktomi to build\nsearch engines from clusters of desktop computers rather large-scale SMPs, which\nwas the strategy of the leading search engine Alta Vista that Google overtook [Brin\nand Page 1998]. In 2011, nearly all Internet services rely on clusters to serve their\nmillions of customers.\nClusters are also very popular with scientists. One reason is their low cost, so\nindividual scientists or small groups can own a cluster dedicated to their programs.\nSuch clusters can get results faster than waiting in the long job queues of the shared\nMPPs at supercomputer centers, which can stretch to weeks. For those interested in\nlearning more, Pfister [1998] wrote an entertaining book on clusters.\nRecent Trends in Large-Scale Multiprocessors\nIn the mid- to late 1990s, it became clear that the hoped for growth in the market for\nultralarge-scale parallel computing was unlikely to occur. Without this market\ngrowth, it became increasingly clear that the high-end parallel computing market\ncould not support the costs of highly customized hardware and software designed\nfor a small market. Perhaps the most important trend to come out of this observa-\ntion was that clustering would be used to reach the highest levels of performance.\nThere are now four general classes of large-scale multiprocessors:\n\u25a0\nClusters that integrate standard desktop motherboards using interconnection\ntechnology such as Myrinet or Infiniband.\n\u25a0\nMulticomputers built from standard microprocessors configured into proces-\nsingelementsandconnectedwith acustominterconnect.These include theCray\nXT3 (which used an earlier version of Cray interconnect with a simple cluster\narchitecture) and IBM Blue Gene (more on this unique machine momentarily).\n\u25a0\nClusters of small-scale shared-memory computers, possibly with vector\nsupport, which includes the Earth Simulator (which has its own journal\navailable online).\nM.7\nThe History of Multiprocessors and Parallel Processing\n\u25a0\nM-63"
    },
    {
        "page": 1411,
        "text": "\u25a0\nLarge-scale shared-memory multiprocessors, such as the Cray X1 [Dunigan\net al. 2005] and SGI Origin and Altix systems. The SGI systems have also been\nconfigured into clusters to provide more than 512 processors, although only\nmessage passing is supported across the clusters.\nThe IBM Blue Gene is the most interesting of these designs since its rationale par-\nallels the underlying causes of the recent trend toward multicore in uniprocessor\narchitectures. Blue Gene started as a research project within IBM aimed at the pro-\ntein sequencing and folding problem. The Blue Gene designers observed that\npower was becoming an increasing concern in large-scale multiprocessors and\nthat the performance/watt of processors from the embedded space was much better\nthat those in the high-end uniprocessor space. If parallelism was the route to high\nperformance, why not start with the most efficient building block and simply have\nmore of them?\nThus, Blue Gene is constructed using a custom chip that includes an embedded\nPowerPC microprocessor offering half the performance of a high-end PowerPC,\nbut at a much smaller fraction of the area of power. This allows more system func-\ntions, including the global interconnect, to be integrated onto the same die. The\nresult is a highly replicable and efficient building block, allowing Blue Gene to\nreach much larger processor counts more efficiently. Instead of using stand-alone\nmicroprocessors or standard desktop boards as building blocks, Blue Gene uses\nprocessor cores. There is no doubt that such an approach provides much greater\nefficiency. Whether the market can support the cost of a customized design and\nspecial software remains an open question.\nIn 2006, a Blue Gene processor at Lawrence Livermore with 32K processors\n(and scheduled to go to 65K in late 2005) holds a factor of 2.6 lead in Linpack\nperformance over the third-place system consisting of 20 SGI Altix 512-processor\nsystems interconnected with Infiniband as a cluster.\nBlue Gene\u2019s predecessor was an experimental machine, QCDOD, which pio-\nneered the concept of a machine using a lower-power embedded microprocessor\nand tightly integrated interconnect to drive down the cost and power consumption\nof a node.\nDevelopments in Synchronization and Consistency Models\nA wide variety of synchronization primitives have been proposed for shared-\nmemory multiprocessors. Mellor-Crummey and Scott [1991] provided an over-\nview of the issues as well as efficient implementations of important primitives,\nsuch as locks and barriers. An extensive bibliography supplies references to other\nimportant contributions, including developments in spin locks, queuing locks, and\nbarriers. Lamport [1979] introduced the concept of sequential consistency and\nwhat correct execution of parallel programs means. Dubois, Scheurich, and Briggs\n[1988] introduced the idea of weak ordering (originally in 1986). In 1990, Adve\nand Hill provided a better definition of weak ordering and also defined the concept\nof data-race-free; at the same conference, Gharachorloo and his colleagues [1990]\nintroduced release consistency and provided the first data on the performance of\nM-64\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1412,
        "text": "relaxed consistency models. More relaxed consistency models have been widely\nadopted in microprocessor architectures, including the Sun SPARC, Alpha, and\nIA-64. Adve and Gharachorloo [1996] have provided an excellent tutorial on mem-\nory consistency and the differences among these models.\nOther References\nThe concept of using virtual memory to implement a shared address space among\ndistinct machines was pioneered in Kai Li\u2019s Ivy system in 1988. There have been sub-\nsequent papers exploring hardware support issues, software mechanisms, and pro-\ngramming issues. Amza et al. [1996] described a system built on workstations using\na new consistency model, Kontothanassis et al. [1997] described a software shared-\nmemory scheme using remote writes, and Erlichson et al. [1996] described the use\nof shared virtual memory to build large-scale multiprocessors using SMPs as nodes.\nThereisanalmostunboundedamountofinformationonmultiprocessorsandmul-\nticomputers: Conferences, journal papers, and even books seem to appear faster than\nany single person can absorb the ideas. No doubt many of these papers will go unno-\nticed\u2014not unlike the past. Most of the major architecture conferences contain papers\non multiprocessors. An annual conference, Supercomputing XY (where X and Y are\nthe last two digits of the year), brings together users, architects, software developers,\nand vendors, and the proceedings are published in book, CD-ROM, and online (see\nwww.scXY.org) form. Two major journals, Journal of Parallel and Distributed Com-\nputing and the IEEE Transactions on Parallel and Distributed Systems, contain\npapers on all aspects of parallel processing. Several books focusing on parallel pro-\ncessingareincludedinthefollowingreferences,withCuller,Singh,andGupta[1999]\nbeing the most recent, large-scale effort. For years, Eugene Miya of NASA Ames\nResearch Center has collected an online bibliography of parallel-processing papers.\nThe bibliography,which nowcontainsmorethan35,000entries,isavailable onlineat\nliinwww.ira.uka.de/bibliography/Parallel/Eugene/index.html.\nIn addition to documenting the discovery of concepts now used in practice,\nthese references also provide descriptions of many ideas that have been explored\nand found wanting, as well as ideas whose time has just not yet come. Given the\nmove toward multicore and multiprocessors as the future of high-performance\ncomputer architecture, we expect that many new approaches will be explored in\nthe years ahead. A few of them will manage to solve the hardware and software\nproblems that have been the key to using multiprocessing for the past 40 years!\nReferences\nAdve, S. V., and K. Gharachorloo [1996]. \u201cShared memory consistency models: A\ntutorial,\u201d IEEE Computer 29:12 (December), 66\u201376.\nAdve, S. V., and M. D. Hill [1990]. \u201cWeak ordering\u2014a new definition,\u201d Proc.\n17th Annual Int\u2019l. Symposium on Computer Architecture (ISCA), May 28\u201331,\n1990, Seattle, Wash., 2\u201314.\nM.7\nThe History of Multiprocessors and Parallel Processing\n\u25a0\nM-65"
    },
    {
        "page": 1413,
        "text": "Agarwal, A., R. Bianchini, D. Chaiken, K. Johnson, and D. Kranz [1995]. \u201cThe\nMIT Alewife machine: Architecture and performance,\u201d 22nd Annual Int\u2019l.\nSymposium on Computer Architecture (ISCA), June 22\u201324, 1995, Santa Mar-\ngherita, Italy, 2\u201313.\nAgarwal, A., J. L. Hennessy, R. Simoni, and M. A. Horowitz [1988]. \u201cAn\nevaluation of directory schemes for cache coherence,\u201d Proc. 15th Annual Int\u2019l.\nSymposium on Computer Architecture, May 30\u2013June 2, 1988, Honolulu,\nHawaii, 280\u2013289.\nAgarwal, A., J. Kubiatowicz, D. Kranz, B.-H. Lim, D. Yeung, G. D\u2019Souza, and M.\nParkin [1993]. \u201cSparcle: An evolutionary processor design for large-scale\nmultiprocessors,\u201d IEEE Micro 13 (June), 48\u201361.\nAlles, A. [1995]. \u201cATM Internetworking,\u201d White Paper (May), Cisco Systems,\nInc., San Jose, Calif. (www.cisco.com/warp/public/614/12.html).\nAlmasi, G. S., and A. Gottlieb [1989]. Highly Parallel Computing, Benjamin/\nCummings, Redwood City, Calif.\nAlverson, G., R. Alverson, D. Callahan, B. Koblenz, A. Porterfield, and B. Smith\n[1992]. \u201cExploiting heterogeneous parallelism on a multithreaded multiproces-\nsor,\u201d Proc. ACM/IEEE Conf. on Supercomputing, November 16\u201320, 1992, Min-\nneapolis, Minn., 188\u2013197.\nAmdahl, G. M. [1967]. \u201cValidity of the single processor approach to achieving\nlarge scale computing capabilities,\u201d Proc. AFIPS Spring Joint Computer Conf.,\nApril 18\u201320, 1967, Atlantic City, N.J., 483\u2013485.\nAmza C., A. L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, W. Yu, and\nW. Zwaenepoel [1996]. \u201cTreadmarks: Shared memory computing on networks\nof workstations,\u201d IEEE Computer 29:2 (February), 18\u201328.\nAnderson, T. E., D. E. Culler, and D. Patterson [1995]. \u201cA case for NOW\n(networks of workstations),\u201d IEEE Micro 15:1 (February), 54\u201364.\nAng, B., D. Chiou, D. Rosenband, M. Ehrlich, L. Rudolph, and Arvind [1998].\n\u201cStarT-Voyager: A flexible platform for exploring scalable SMP issues,\u201d Proc.\nACM/IEEE Conf. on Supercomputing, November 7\u201313, 1998, Orlando, FL.\nArchibald, J., and J.-L. Baer [1986]. \u201cCache coherence protocols: Evaluation using\na multiprocessor simulation model,\u201d ACM Trans. on Computer Systems 4:4\n(November), 273\u2013298.\nArpaci, R. H., D. E. Culler, A. Krishnamurthy, S. G. Steinberg, and K. Yelick\n[1995]. \u201cEmpirical evaluation of the CRAY-T3D: A compiler perspective,\u201d\nProc. 22nd Annual Int\u2019l. Symposium on Computer Architecture (ISCA), June\n22\u201324, 1995, Santa Margherita, Italy.\nBaer, J.-L., and W.-H. Wang [1988]. \u201cOn the inclusion properties for multi-level\ncache hierarchies,\u201d Proc. 15th Annual Int\u2019l. Symposium on Computer Architec-\nture, May 30\u2013June 2, 1988, Honolulu, Hawaii, 73\u201380.\nBalakrishnan, H. V., N. Padmanabhan, S. Seshan, and R. H. Katz [1997]. \u201cA com-\nparison of mechanisms for improving TCP performance over wireless links,\u201d\nIEEE/ACM Trans. on Networking 5:6 (December), 756\u2013769.\nBarroso, L. A., K. Gharachorloo, and E. Bugnion [1998]. \u201cMemory system char-\nacterization of commercial workloads,\u201d Proc. 25th Annual Int\u2019l. Symposium on\nComputer Architecture (ISCA), July 3\u201314, 1998, Barcelona, Spain, 3\u201314.\nM-66\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1414,
        "text": "Baskett, F., T. Jermoluk, and D. Solomon [1988]. \u201cThe 4D-MP graphics super-\nworkstation: Computing+graphics\u00bc40 MIPS+40 MFLOPS and 10,000\nlighted polygons per second,\u201d Proc. IEEE COMPCON, February 29\u2013March\n4, 1988, San Francisco, 468\u2013471.\nBBN Laboratories. [1986]. Butterfly Parallel Processor Overview, Tech.\nRep. 6148, BBN Laboratories, Cambridge, Mass.\nBell, C. G. [1985]. \u201cMultis: A new class of multiprocessor computers,\u201d Science\n228 (April 26), 462\u2013467.\nBell, C. G. [1989]. \u201cThe future of high performance computers in science and engi-\nneering,\u201d Communications of the ACM 32:9 (September), 1091\u20131101.\nBell, C. G., and J. Gray [2001]. Crays, Clusters and Centers, Tech. Rep. MSR-TR-\n2001-76, Microsoft Research, Redmond, Wash.\nBell, C. G., and J. Gray [2002]. \u201cWhat\u2019s next in high performance computing,\u201d\nCACM, 45:2 (February), 91\u201395.\nBouknight, W. J., S. A. Deneberg, D. E. McIntyre, J. M. Randall, A. H. Sameh, and\nD. L. Slotnick [1972]. \u201cThe Illiac IV system,\u201d Proc. IEEE 60:4, 369\u2013379. Also\nappears in D. P. Siewiorek, C. G. Bell, and A. Newell, Computer Structures:\nPrinciples and Examples, McGraw-Hill, New York, 1982, 306\u2013316.\nBrain, M. [2000]. Inside a Digital Cell Phone, www.howstuffworks.com/inside-\ncell-phone.htm.\nBrewer, E. A., and B. C. Kuszmaul [1994]. \u201cHow to get good performance from\nthe CM-5 data network,\u201d Proc. Eighth Int\u2019l. Parallel Processing Symposium\n(IPPS), April 26\u201329, 1994, Cancun, Mexico.\nBrin, S., and L. Page [1998]. \u201cThe anatomy of a large-scale hypertextual\nWeb search engine,\u201d Proc. 7th Int\u2019l. World Wide Web Conf., April 14\u201318,\n1998, Brisbane, Queensland, Australia, 107\u2013117.\nBurkhardt III, H., S. Frank, B. Knobe, and J. Rothnie [1992]. Overview of the KSR1\nComputer System, Tech. Rep. KSR-TR-9202001, Kendall Square Research,\nBoston.\nCensier, L., and P. Feautrier [1978]. \u201cA new solution to coherence problems\nin multicache systems,\u201d IEEE Trans. on Computers C-27:12 (December),\n1112\u20131118.\nChandra, R., S. Devine, B. Verghese, A. Gupta, and M. Rosenblum [1994].\n\u201cScheduling and page migration for multiprocessor compute servers,\u201d Proc.\nSixth Int\u2019l. Conf. on Architectural Support for Programming Languages and\nOperating Systems (ASPLOS), October 4\u20137, 1994, San Jose, Calif., 12\u201324.\nCharlesworth, A. [1998]. \u201cStarfire: Extending the SMP envelope,\u201d IEEE Micro\n18:1 (January/February), 39\u201349.\nClark, W. A. [1957]. \u201cThe Lincoln TX-2 computer development,\u201d Proc. Western\nJoint Computer Conference, February 26\u201328, 1957, Los Angeles, 143\u2013145.\nComer, D. [1993]. Internetworking with TCP/IP, 2nd ed., Prentice Hall, Engle-\nwood Cliffs, N.J.\nCuller, D. E., J. P. Singh, and A. Gupta [1999]. Parallel Computer Architecture: A\nHardware/Software Approach, Morgan Kaufmann, San Francisco.\nDally, W. J., and C. I. Seitz [1986]. \u201cThe torus routing chip,\u201d Distributed Comput-\ning 1:4, 187\u2013196.\nM.7\nThe History of Multiprocessors and Parallel Processing\n\u25a0\nM-67"
    },
    {
        "page": 1415,
        "text": "Davie, B. S., L. L. Peterson, and D. Clark [1999]. Computer Networks: A Systems\nApproach, 2nd ed., Morgan Kaufmann, San Francisco.\nDesurvire, E. [1992]. \u201cLightwave communications: The fifth generation,\u201d\nScientific American (International Edition) 266:1 (January), 96\u2013103.\nDongarra, J., T. Sterling, H. Simon, and E. Strohmaier [2005]. \u201cHigh-performance\ncomputing: Clusters, constellations, MPPs, and future directions,\u201d Computing in\nScience & Engineering, 7:2 (March/April), 51\u201359.\nDubois, M., C. Scheurich, and F. Briggs [1988]. \u201cSynchronization, coherence, and\nevent ordering,\u201d IEEE Computer 21:2 (February), 9\u201321.\nDunigan, W., K. Vetter, K. White, and P. Worley [2005]. \u201cPerformance evaluation\nof the Cray X1 distributed shared memory architecture,\u201d IEEE Micro, January/\nFebruary, 30\u201340.\nEggers, S. [1989]. \u201cSimulation Analysis of Data Sharing in Shared Memory Mul-\ntiprocessors,\u201d Ph.D. thesis, Computer Science Division, University of Califor-\nnia, Berkeley.\nElder, J., A. Gottlieb, C. K. Kruskal, K. P. McAuliffe, L. Randolph, M. Snir, P.\nTeller, and J. Wilson [1985]. \u201cIssues related to MIMD shared-memory computers:\nThe NYU Ultracomputer approach,\u201d Proc. 12th Annual Int\u2019l. Symposium on\nComputer Architecture (ISCA), June 17\u201319, 1985, Boston, Mass., 126\u2013135.\nErlichson, A., N. Nuckolls, G. Chesson, and J. L. Hennessy [1996]. \u201cSoftFLASH:\nAnalyzing the performance of clustered distributed virtual shared memory,\u201d\nProc. Seventh Int\u2019l. Conf. on Architectural Support for Programming Lan-\nguages and Operating Systems (ASPLOS), October 1\u20135, 1996, Cambridge,\nMass., 210\u2013220.\nFalsafi, B., and D. A. Wood [1997]. \u201cReactive NUMA: A design for unifying S-\nCOMA and CC-NUMA,\u201d Proc. 24th Annual Int\u2019l. Symposium on Computer\nArchitecture (ISCA), June 2\u20134, 1997, Denver, Colo., 229\u2013240.\nFlynn, M. J. [1966]. \u201cVery high-speed computing systems,\u201d Proc. IEEE 54:12\n(December), 1901\u20131909.\nForgie, J. W. [1957]. \u201cThe Lincoln TX-2 input-output system,\u201d Proc. Western\nJoint Computer Conference, February 26\u201328, 1957, Los Angeles, 156\u2013160.\nFrank, S. J. [1984]. \u201cTightly coupled multiprocessor systems speed memory access\ntime,\u201d Electronics 57:1 (January), 164\u2013169.\nGajski, D., D. Kuck, D. Lawrie, and A. Sameh [1983]. \u201cCEDAR\u2014a large scale\nmultiprocessor,\u201d Proc. Int\u2019l. Conf. on Parallel Processing (ICPP), August,\nColumbus, Ohio, 524\u2013529.\nGalles, M. [1996]. \u201cScalable pipelined interconnect for distributed endpoint rout-\ning: The SGI SPIDER chip,\u201d Proc. IEEE HOT Interconnects \u201996, August 15\u2013\n17, 1996, Stanford University, Palo Alto, Calif.\nGehringer, E. F., D. P. Siewiorek, and Z. Segall [1987]. Parallel Processing: The\nCm* Experience, Digital Press, Bedford, Mass.\nGharachorloo, K., A. Gupta, and J. L. Hennessy [1992]. \u201cHiding memory latency\nusing dynamic scheduling in shared-memory multiprocessors,\u201d Proc. 19th\nAnnual Int\u2019l. Symposium on Computer Architecture (ISCA), May 19\u201321,\n1992, Gold Coast, Australia.\nM-68\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1416,
        "text": "Gharachorloo, K., D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. L. Hen-\nnessy [1990]. \u201cMemory consistency and event ordering in scalable shared-\nmemory multiprocessors,\u201d Proc. 17th Annual Int\u2019l. Symposium on Computer\nArchitecture (ISCA), May 28\u201331, 1990, Seattle, Wash., 15\u201326.\nGibson, J., R. Kunz, D. Ofelt, M. Horowitz, J. Hennessy, and M. Heinrich [2000].\n\u201cFLASH vs. (simulated) FLASH: Closing the simulation loop,\u201d Proc. Ninth\nInt\u2019l. Conf. on Architectural Support for Programming Languages and Operat-\ning Systems (ASPLOS), November 12\u201315, Cambridge, Mass., 49\u201358.\nGoodman, J. R. [1983]. \u201cUsing cache memory to reduce processor memory traf-\nfic,\u201d Proc. 10th Annual Int\u2019l. Symposium on Computer Architecture (ISCA),\nJune 5\u20137, 1982, Stockholm, Sweden, 124\u2013131.\nGoralski, W. [1997]. SONET: A Guide to Synchronous Optical Network, McGraw-\nHill, New York.\nGrice, C., and M. Kanellos [2000]. \u201cCell phone industry at crossroads: Go high or\nlow?\u201d CNET News (August 31), technews.netscape.com/news/0-1004-201-\n2518386-0.html?tag\u00bcst.ne.1002.tgif.sf.\nGroe, J. B., and L. E. Larson [2000]. CDMA Mobile Radio Design, Artech House,\nBoston.\nHagersten E., and M. Koster [1998]. \u201cWildFire: A scalable path for SMPs,\u201d Proc.\nFifth Int\u2019l. Symposium on High-Performance Computer Architecture, January\n9\u201312, 1999, Orlando, Fla.\nHagersten, E., A. Landin, and S. Haridi [1992]. \u201cDDM\u2014a cache-only memory\narchitecture,\u201d IEEE Computer 25:9 (September), 44\u201354.\nHill, M. D. [1998]. \u201cMultiprocessors should support simple memory consistency\nmodels,\u201d IEEE Computer 31:8 (August), 28\u201334.\nHillis,\nW.\nD.\n[1985].\nThe\nConnection\nMultiprocessor,\nMIT\nPress,\nCambridge, Mass.\nHirata, H., K. Kimura, S. Nagamine, Y. Mochizuki, A. Nishimura, Y. Nakase, and\nT. Nishizawa [1992]. \u201cAn elementary processor architecture with simultaneous\ninstruction issuing from multiple threads,\u201d Proc. 19th Annual Int\u2019l. Symposium\non Computer Architecture (ISCA), May 19\u201321, 1992, Gold Coast, Australia,\n136\u2013145.\nHockney, R. W., and C. R. Jesshope [1988]. Parallel Computers 2: Architectures,\nProgramming and Algorithms, Adam Hilger, Ltd., Bristol, England.\nHolland, J. H. [1959]. \u201cA universal computer capable of executing an arbitrary\nnumber of subprograms simultaneously,\u201d Proc. East Joint Computer Conf.\n16, 108\u2013113.\nHord, R. M. [1982]. The Illiac-IV, The First Supercomputer, Computer Science\nPress, Rockville, Md.\nHristea, C., D. Lenoski, and J. Keen [1997]. \u201cMeasuring memory hierarchy per-\nformance of cache-coherent multiprocessors using micro benchmarks,\u201d\nProc. ACM/IEEE Conf. on Supercomputing, November 15\u201321, 1997, San\nJose, Calif.\nHwang, K. [1993]. Advanced Computer Architecture and Parallel Programming,\nMcGraw-Hill, New York.\nM.7\nThe History of Multiprocessors and Parallel Processing\n\u25a0\nM-69"
    },
    {
        "page": 1417,
        "text": "IBM. [2005]. \u201cBlue Gene,\u201d IBM J. of Research and Development, 49:2/3\n(special issue).\nInfiniband Trade Association. [2001]. InfiniBand Architecture Specifications\nRelease 1.0.a, www.infinibandta.org.\nJordan, H. F. [1983]. \u201cPerformance measurements on HEP\u2014a pipelined MIMD\ncomputer,\u201d Proc. 10th Annual Int\u2019l. Symposium on Computer Architecture\n(ISCA), June 5\u20137, 1982, Stockholm, Sweden, 207\u2013212.\nKahn, R. E. [1972]. \u201cResource-sharing computer communication networks,\u201d Proc.\nIEEE 60:11 (November), 1397\u20131407.\nKeckler, S. W., and W. J. Dally [1992]. \u201cProcessor coupling: Integrating compile\ntime and runtime scheduling for parallelism,\u201d Proc. 19th Annual Int\u2019l. Sympo-\nsium on Computer Architecture (ISCA), May 19\u201321, 1992, Gold Coast, Austra-\nlia, 202\u2013213.\nKontothanassis, L., G. Hunt, R. Stets, N. Hardavellas, M. Cierniak, S.\nParthasarathy, W. Meira, S. Dwarkadas, and M. Scott [1997]. \u201cVM-based\nshared memory on low-latency, remotememory-access networks,\u201d Proc. 24th\nAnnual Int\u2019l. Symposium on Computer Architecture (ISCA), June 2\u20134, 1997,\nDenver, Colo.\nKurose, J. F., and K. W. Ross [2001]. Computer Networking: A Top-Down\nApproach Featuring the Internet, Addison-Wesley, Boston.\nKuskin, J., D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharachorloo, J. Cha-\npin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. L. Hen-\nnessy [1994]. \u201cThe Stanford FLASH multiprocessor,\u201d Proc. 21st Annual Int\u2019l.\nSymposium on Computer Architecture (ISCA), April 18\u201321, 1994, Chicago.\nLamport, L. [1979]. \u201cHow to make a multiprocessor computer that correctly exe-\ncutes multiprocess programs,\u201d IEEE Trans. on Computers C-28:9 (September),\n241\u2013248.\nLaudon, J., A. Gupta, and M. Horowitz [1994]. \u201cInterleaving: A multithreading\ntechnique targeting multiprocessors and workstations,\u201d Proc. Sixth Int\u2019l. Conf.\non Architectural Support for Programming Languages and Operating Systems\n(ASPLOS), October 4\u20137, 1994, San Jose, Calif., 308\u2013318.\nLaudon, J., and D. Lenoski [1997]. \u201cThe SGI Origin: A ccNUMA highly scalable\nserver,\u201d Proc. 24th Annual Int\u2019l. Symposium on Computer Architecture (ISCA),\nJune 2\u20134, 1997, Denver, Colo., 241\u2013251.\nLenoski, D., J. Laudon, K. Gharachorloo, A. Gupta, and J. L. Hennessy [1990].\n\u201cThe Stanford DASH multiprocessor,\u201d Proc. 17th Annual Int\u2019l. Symposium\non Computer Architecture (ISCA), May 28\u201331, 1990, Seattle, Wash., 148\u2013159.\nLenoski, D., J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. L. Hennessy,\nM. A. Horowitz, and M. Lam [1992]. \u201cThe Stanford DASH multiprocessor,\u201d\nIEEE Computer 25:3 (March), 63\u201379.\nLi, K. [1988]. \u201cIVY: A shared virtual memory system for parallel computing,\u201d\nProc. Int\u2019l. Conf. on Parallel Processing (ICCP), August, The Pennsylvania\nState University, University Park, Penn.\nLo, J., L. Barroso, S. Eggers, K. Gharachorloo, H. Levy, and S. Parekh [1998]. \u201cAn\nanalysis of database workload performance on simultaneous multithreaded\nM-70\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1418,
        "text": "processors,\u201d Proc. 25th Annual Int\u2019l. Symposium on Computer Architecture\n(ISCA), July 3\u201314, 1998, Barcelona, Spain, 39\u201350.\nLo, J., S. Eggers, J. Emer, H. Levy, R. Stamm, and D. Tullsen [1997]. \u201cConverting\nthread-level parallelism into instruction-level parallelism via simultaneous mul-\ntithreading,\u201d ACM Trans. on Computer Systems 15:2 (August), 322\u2013354.\nLovett, T., and S. Thakkar [1988]. \u201cThe Symmetry multiprocessor system,\u201d Proc.\nInt\u2019l. Conf. on Parallel Processing (ICCP), August, The Pennsylvania State\nUniversity, University Park, Penn., 303\u2013310.\nMellor-Crummey, J. M., and M. L. Scott [1991]. \u201cAlgorithms for scalable synchro-\nnization on shared-memory multiprocessors,\u201d ACM Trans. on Computer\nSystems 9:1 (February), 21\u201365.\nMenabrea, L. F. [1842]. \u201cSketch of the analytical engine invented by Charles Bab-\nbage,\u201d Biblioth\u00e8que Universelle de Gen\u00e8ve, 82 (October).\nMetcalfe, R. M. [1993]. \u201cComputer/network interface design: Lessons from Arpa-\nnet and Ethernet.\u201d IEEE J. on Selected Areas in Communications 11:2\n(February), 173\u2013180.\nMetcalfe, R. M., and D. R. Boggs [1976]. \u201cEthernet: Distributed packet switching for\nlocal computer networks,\u201d Communications of the ACM 19:7 (July), 395\u2013404.\nMitchell, D. [1989]. \u201cThe Transputer: The time is now,\u201d Computer Design (RISC\nsuppl.), 40\u201341.\nMiya, E. N. [1985]. \u201cMultiprocessor/distributed processing bibliography,\u201d Com-\nputer Architecture News 13:1, 27\u201329.\nNational Research Council. [1997]. The Evolution of Untethered Communications,\nComputer Science and Telecommunications Board, National Academy Press,\nWashington, D.C.\nNikhil, R. S., G. M. Papadopoulos, and Arvind [1992]. \u201c*T: A multithreaded mas-\nsively parallel architecture,\u201d Proc. 19th Annual Int\u2019l. Symposium on Computer\nArchitecture (ISCA), May 19\u201321, 1992, Gold Coast, Australia, 156\u2013167.\nNoordergraaf, L., and R. van der Pas [1999]. \u201cPerformance experiences on Sun\u2019s\nWildFire prototype,\u201d Proc. ACM/IEEE Conf. on Supercomputing, November\n13\u201319, 1999, Portland, Ore.\nPartridge, C. [1994]. Gigabit Networking, Addison-Wesley, Reading, Mass.\nPfister, G. F. [1998]. In Search of Clusters, 2nd ed., Prentice Hall, Upper Saddle\nRiver, N.J.\nPfister, G. F., W. C. Brantley, D. A. George, S. L. Harvey, W. J. Kleinfekder, K. P.\nMcAuliffe, E. A. Melton, V. A. Norton, and J. Weiss [1985]. \u201cThe IBM research\nparallel processor prototype (RP3): Introduction and architecture,\u201d Proc. 12th\nAnnual Int\u2019l. Symposium on Computer Architecture (ISCA), June 17\u201319,\n1985, Boston, Mass., 764\u2013771.\nReinhardt, S. K., J. R. Larus, and D. A. Wood [1994]. \u201cTempest and Typhoon:\nUser-level shared memory,\u201d Proc. 21st Annual Int\u2019l. Symposium on Computer\nArchitecture (ISCA), April 18\u201321, 1994, Chicago, 325\u2013336.\nRettberg, R. D., W. R. Crowther, P. P. Carvey, and R. S. Towlinson [1990].\n\u201cThe Monarch parallel processor hardware design,\u201d IEEE Computer 23:4\n(April), 18\u201330.\nM.7\nThe History of Multiprocessors and Parallel Processing\n\u25a0\nM-71"
    },
    {
        "page": 1419,
        "text": "Rosenblum, M., S. A. Herrod, E. Witchel, and A. Gupta [1995]. \u201cComplete com-\nputer simulation: The SimOS approach,\u201d in IEEE Parallel and Distributed\nTechnology (now called Concurrency) 4:3, 34\u201343.\nSaltzer, J. H., D. P. Reed, and D. D. Clark [1984]. \u201cEnd-to-end arguments in sys-\ntem design,\u201d ACM Trans. on Computer Systems 2:4 (November), 277\u2013288.\nSatran, J., D. Smith, K. Meth, C. Sapuntzakis, M. Wakeley, P. Von Stamwitz, R.\nHaagens, E. Zeidner, L. Dalle Ore, and Y. Klein [2001]. \u201ciSCSI,\u201d IPS Working\nGroup of IETF, Internet draft www.ietf.org/internet-drafts/draft-ietf-ips-iscsi-\n07.txt.\nSaulsbury, A., T. Wilkinson, J. Carter, and A. Landin [1995]. \u201cAn argument for\nSimple COMA,\u201d Proc. First IEEE Symposium on High-Performance Computer\nArchitectures, January 22\u201325, 1995, Raleigh, N.C., 276\u2013285.\nSchwartz, J. T. [1980]. \u201cUltracomputers,\u201d ACM Trans. on Programming\nLanguages and Systems 4:2, 484\u2013521.\nScott, S. L. [1996]. \u201cSynchronization and communication in the T3E multiproces-\nsor,\u201d Seventh Int\u2019l. Conf. on Architectural Support for Programming Languages\nand Operating Systems (ASPLOS), October 1\u20135, 1996, Cambridge, Mass., 26\u2013\n36.\nScott, S. L., and G. M. Thorson [1996]. \u201cThe Cray T3E network: Adaptive routing\nin a high-performance 3D torus,\u201d Proc. IEEE HOT Interconnects \u201996, August\n15\u201317, 1996, Stanford University, Palo Alto, Calif., 14\u2013156.\nSeitz, C. L. [1985]. \u201cThe Cosmic Cube (concurrent computing),\u201d Communications\nof the ACM 28:1 (January), 22\u201333.\nSingh, J. P., J. L. Hennessy, and A. Gupta [1993]. \u201cScaling parallel programs for\nmultiprocessors: Methodology and examples,\u201d Computer 26:7 (July), 22\u201333.\nSlotnick, D. L., W. C. Borck, and R. C. McReynolds [1962]. \u201cThe Solomon com-\nputer,\u201d Proc. AFIPS Fall Joint Computer Conf., December 4\u20136, 1962,\nPhiladelphia, Penn., 97\u2013107.\nSmith, B. J. [1978]. \u201cA pipelined, shared resource MIMD computer,\u201d Proc. Int\u2019l.\nConf. on Parallel Processing (ICPP), August, Bellaire, Mich., 6\u20138.\nSoundararajan, V., M. Heinrich, B. Verghese, K. Gharachorloo, A. Gupta, and J. L.\nHennessy [1998]. \u201cFlexible use of memory for replication/migration in\ncache-coherent DSM multiprocessors,\u201d Proc. 25th Annual Int\u2019l. Symposium\non Computer Architecture (ISCA), July 3\u201314, 1998, Barcelona, Spain, 342\u2013355.\nSpurgeon, C. [2001]. \u201cCharles Spurgeon\u2019s Ethernet Web site,\u201d www.host.ots.\nutexas.edu/ethernet/ethernet-home.html.\nStenstr\u20acom, P., T. Joe, and A. Gupta [1992]. \u201cComparative performance evaluation\nof cachecoherent NUMA and COMA architectures,\u201d Proc. 19th Annual Int\u2019l.\nSymposium on Computer Architecture (ISCA), May 19\u201321, 1992, Gold Coast,\nAustralia, 80\u201391.\nSterling, T. [2001]. Beowulf PC Cluster Computing with Windows and Beowulf\nPC Cluster Computing with Linux, MIT Press, Cambridge, Mass.\nStevens, W. R. [1994\u20131996]. TCP/IP Illustrated (three volumes), Addison-\nWesley, Reading, Mass.\nStone, H. [1991]. High Performance Computers, Addison-Wesley, New York.\nM-72\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1420,
        "text": "Swan, R. J., A. Bechtolsheim, K. W. Lai, and J. K. Ousterhout [1977]. \u201cThe imple-\nmentation of the Cm* multi-microprocessor,\u201d Proc. AFIPS National Computing\nConf., June 13\u201316, 1977, Dallas, Tex., 645\u2013654.\nSwan, R. J., S. H. Fuller, and D. P. Siewiorek [1977]. \u201cCm*\u2014a modular, multi-\nmicroprocessor,\u201d Proc. AFIPS National Computing Conf., June 13\u201316, 1977,\nDallas, Tex., 637\u2013644.\nTanenbaum, A. S. [1988]. Computer Networks, 2nd ed., Prentice Hall, Englewood\nCliffs, N.J.\nTang, C. K. [1976]. \u201cCache design in the tightly coupled multiprocessor system,\u201d\nProc. AFIPS National Computer Conf., June 7\u201310, 1976, New York, 749\u2013753.\nThacker, C. P., E. M. McCreight, B. W. Lampson, R. F. Sproull, and D. R. Boggs\n[1982]. \u201cAlto: A personal computer,\u201d in D. P. Siewiorek, C. G. Bell, and A.\nNewell, eds., Computer Structures: Principles and Examples, McGraw-Hill,\nNew York, 549\u2013572.\nThekkath, R., A. P. Singh, J. P. Singh, S. John, and J. L. Hennessy [1997]. \u201cAn\nevaluation of a commercial CC-NUMA architecture\u2014the CONVEX Exemplar\nSPP1200,\u201d Proc. 11th Int\u2019l. Parallel Processing Symposium (IPPS), April 1\u20137,\n1997, Geneva, Switzerland.\nTullsen, D. M., S. J. Eggers, J. S. Emer, H. M. Levy, J. L. Lo, and R. L. Stamm\n[1996]. \u201cExploiting choice: Instruction fetch and issue on an implementable\nsimultaneous multithreading processor,\u201d Proc. 23rd Annual Int\u2019l. Symposium\non Computer Architecture (ISCA), May 22\u201324, 1996, Philadelphia, Penn.,\n191\u2013202.\nTullsen, D. M., S. J. Eggers, and H. M. Levy [1995]. \u201cSimultaneous multithread-\ning: Maximizing on-chip parallelism,\u201d Proc. 22nd Annual Int\u2019l. Symposium on\nComputer Architecture (ISCA), June 22\u201324, 1995, Santa Margherita, Italy, 392\u2013\n403.\nUnger, S. H. [1958]. \u201cA computer oriented towards spatial problems,\u201d Proc. Insti-\ntute of Radio Engineers 46:10 (October), 1744\u20131750.\nWalrand, J. [1991]. Communication Networks: A First Course, Aksen Associates:\nIrwin, Homewood, Ill.\nWilson, A. W., Jr. [1987]. \u201cHierarchical cache/bus architecture for shared-memory\nmultiprocessors,\u201d Proc. 14th Annual Int\u2019l. Symposium on Computer Architec-\nture (ISCA), June 2\u20135, 1987, Pittsburgh, Penn., 244\u2013252.\nWolfe, A., and J. P. Shen [1991]. \u201cA variable instruction stream extension to the\nVLIW architecture.\u201d Proc. Fourth Int\u2019l. Conf. on Architectural Support for Pro-\ngramming Languages and Operating Systems (ASPLOS), April 8\u201311, 1991,\nPalo Alto, Calif., 2\u201314.\nWood, D. A., and M. D. Hill [1995]. \u201cCost-effective parallel computing,\u201d IEEE\nComputer 28:2 (February), 69\u201372.\nWulf, W., and C. G. Bell [1972]. \u201cC.mmp\u2014A multi-mini-processor,\u201d Proc. AFIPS\nFall Joint Computer Conf., December 5\u20137, 1972, Anaheim, Calif., 765\u2013777.\nWulf, W., and S. P. Harbison [1978]. \u201cReflections in a pool of processors\u2014an\nexperience report on C.mmp/Hydra,\u201d Proc. AFIPS National Computing Conf.\nJune 5\u20138, 1978, Anaheim, Calif., 939\u2013951.\nM.7\nThe History of Multiprocessors and Parallel Processing\n\u25a0\nM-73"
    },
    {
        "page": 1421,
        "text": "Yamamoto, W., M. J. Serrano, A. R. Talcott, R. C. Wood, and M. Nemirosky [1994].\n\u201cPerformance estimation of multistreamed, superscalar processors,\u201d Proc. 27th\nHawaii Int\u2019l. Conf. on System Sciences, January 4\u20137, 1994, Wailea, 195\u2013204.\nM.8\nThe Development of Clusters (Chapter 6)\nIn this section, we cover the development of clusters that were the foundation of\nwarehouse-scale computers (WSCs) and of utility computing. (Readers interested\nin learning more should start with Barroso and H\u20acolzle [2009] and the blog postings\nand talks of James Hamilton at http://perspectives.mvdirona.com.)\nClusters, the Forerunner of WSCs\nClusters were probably \u201cinvented\u201d in the 1960s by customers who could not fit\nall their work on one computer or who needed a backup machine in case of failure\nof the primary machine [Pfister 1998]. Tandem introduced a 16-node cluster in\n1975. Digital followed withVAX clusters,introduced in1984.Theywereoriginally\nindependent computers that shared I/O devices, requiring a distributed operating\nsystem to coordinate activity. Soon they had communication links between com-\nputers, in part so that the computers could be geographically distributed to increase\navailability in case of a disaster at a single site. Users log onto the cluster and are\nunaware of which machine they are running on. DEC (now HP) sold more than\n25,000 clusters by 1993. Other early companies were Tandem (now HP) and\nIBM (still IBM).Today,virtuallyevery companyhasclusterproducts.Most ofthese\nproducts are aimed at availability, with performance scaling as a secondary benefit.\nScientific computing on clusters emerged as a competitor to MPPs. In 1993, the\nBeowulf project started with the goal of fulfilling NASA\u2019s desire for a 1 GFLOPS\ncomputer for under $50,000. In 1994, a 16-node cluster built from off-the-shelf\nPCs using 80486s achieved that goal [Bell and Gray 2001]. This emphasis led\nto a variety of software interfaces to make it easier to submit, coordinate, and debug\nlarge programs or a large number of independent programs.\nEfforts were made to reduce latency of communication in clusters as well as to\nincrease bandwidth, and several research projects worked on that problem. (One\ncommercial result of the low-latency research was the VI interface standard, which\nhas been embraced by Infiniband, discussed below.) Low latency then proved use-\nful in other applications. For example, in 1997 a cluster of 100 UltraSPARC desk-\ntop computers at the University of California\u2013Berkeley, connected by 160 MB/sec\nper link Myrinet switches, was used to set world records in database sort\u2014sorting\n8.6 GB of data originally on disk in 1 minute\u2014and in cracking an encrypted mes-\nsage\u2014taking just 3.5 hours to decipher a 40-bit DES key.\nThis research project, called Network of Workstations [Anderson, Culler, and\nPatterson 1995], also developed the Inktomi search engine, which led to a start-up\nM-74\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1422,
        "text": "company with the same name. Eric Brewer led the Inktomi effort at Berkeley and\nthen at the company to demonstrate the use of commodity hardware to build com-\nputing infrastructure for Internet services. Using standardized networks within a\nrack of PC servers gave Inktomi better scalability. In contrast, the strategy of\nthe prior leading search engine Alta Vista was to build from large-scale SMPs.\nCompared to the high-performance computing work in clusters, the emphasis\nwas on a relatively large number of low-cost nodes and a clear programming\nmodel. Hence, the NOW project and Inktomi are considered the foundation of\nWSCs and Cloud Computing. Google followed the example of Inktomi technology\nwhen it took the leading search engine mantle from Inktomi just as Inktomi had\ntaken it from Alta Vista [Brin and Page 1998]. (Google\u2019s initial innovation was\nsearch quality; the WSC innovations came much later.) For many years now,\nall Internet services have relied on cluster technology to serve their millions of\ncustomers.\nUtility Computing, the Forerunner of Cloud Computing\nAs stated in the text, the earliest version of utility computing was timesharing.\nAlthough timesharing faded away over time with the creation of smaller and\ncheaper personal computers, in the last decade there have been many less than fully\nsuccessful attempts to resuscitate utility computing. Sun began selling time on Sun\nCloud at $1 per hour in 2000, HP offered a Utility Data Center in 2001, and Intel\ntried selling time on internal supercomputers in the early 2000s. Although they\nwere commercially available, few customers used them.\nA related topic is grid computing, which was originally invented so that scien-\ntific programs could be run across geographically distributed computing facilities.\nAt the time, some questioned the wisdom of this goal, setting aside how difficult it\nwould be to achieve. Grid computing tended to require very large systems running\nvery large programs, using multiple datacenters for the tasks. Single applications\ndid not really run well when geographically distributed, given the long latencies\ninherent with long distance. This first step eventually led to some conventions\nfor data access, but the grid computing community did not develop APIs that were\nuseful beyond the high-performance computing community, so the cloud comput-\ning effort shares little code or history with grid computing.\nArmbrust et al [2009] argued that, once the Internet service companies solved\nthe operational problems to work at large scale, the significant economies of scale\nthat they uncovered brought their costs down below those of smaller datacenters.\nAmazon recognized that if this cost advantage was true then Amazon should be\nable to make a profit selling this service. In 2006, Amazon announced Elastic\nCloud Computing (EC2) at $0.10 per hour per instance. The subsequent popularity\nof EC2 led other Internet companies to offer cloud computing services, such as\nGoogle App Engine and Microsoft Azure, albeit at higher abstraction levels than\nthe x86 virtual machines of Amazon Web Services. Hence, the current popularity\nof pay-as-you go computing isn\u2019t because someone recently came up with the idea;\nM.8\nThe Development of Clusters\n\u25a0\nM-75"
    },
    {
        "page": 1423,
        "text": "it\u2019s because the technology and business models have aligned so that companies\ncan make money offering a service that many people want to use. Time will tell\nwhether there will be many successful utility computing models or whether the\nindustry will converge around a single standard. It will certainly be interesting\nto watch.\nContainers\nIn the fall of 2003, many people were thinking about using containers to hold\nservers. Brewster Kahle, director and founder of the Internet Archive, gave talks\nabout how he could fit the whole archive in a single 40-foot container. His interest\nwas making copies of the Archive and distributing it around the world to ensure its\nsurvivability, thereby avoiding the fate of the Library of Alexandria that was\ndestroyed by fire in 48 B.C.E. People working with Kahle wrote a white paper\nbased on his talk in November 2003 to get more detail about what a container\ndesign would look like.\nThat same year, engineers at Google were also looking at building datacenters\nusing containers and submitted a patent on aspects of it in December 2003. The\nfirst container for a datacenter was delivered in January 2005, and Google received\nthe patent in October 2007. Google publicly revealed the use of containers in\nApril 2009.\nGreg Papadopolous of Sun Microsystems and Danny Hillis of Applied Minds\nheard Kahle\u2019s talk and designed a product called the Sun Modular Datacenter that\ndebuted in October 2006. (The project code name was Black Box, a term many\npeople still use.) This half-length (20-foot) container could hold 280 servers. This\nproduct release combined with Microsoft\u2019s announcement that they were building\na datacenter designed to hold 220 40-foot containers inspired many other compa-\nnies to offer containers and servers designed to be placed in them.\nIn a nice turn of events, in 2009 the Internet Archive migrated its data to a Sun\nModular Datacenter. A copy of the Internet Archive is now at the New Library of\nAlexandria in Egypt, near the site of the original library.\nReferences\nAnderson, T. E., D. E. Culler, and D. Patterson [1995]. \u201cA case for NOW (net-\nworks of workstations),\u201d IEEE Micro 15:1 (February), 54\u201364.\nApache Software Foundation. [2011]. Apache Hadoop project, http://hadoop.\napache.org.\nArmbrust, M., A. Fox, R. Griffith, A.D. Joseph, R. Katz, A. Konwinski, G. Lee, D.\nPatterson, A. Rabkin, I. Stoica, and M. Zaharia [2009]. Above the Clouds: A\nBerkeley View of Cloud Computing, Tech. Rep. UCB/EECS-2009-28, Univer-\nsity of California, Berkeley (http://www.eecs.berkeley.edu/Pubs/TechRpts/\n2009/EECS-2009-28.html).\nM-76\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1424,
        "text": "Barroso, L. A. [2010]. \u201cWarehouse scale computing [keynote address],\u201d Proc.\nACM SIG-MOD, June 8\u201310, 2010, Indianapolis, Ind.\nBarroso, L. A., and U. H\u20acolzle [2007]. \u201cThe case for energy-proportional comput-\ning,\u201d IEEE Computer 40:12 (December), 33\u201337.\nBarroso, L.A., and U. H\u20acolzle [2009]. \u201cThe datacenter as a computer: An introduc-\ntion to the design of warehouse-scale machines,\u201d in M. D. Hill, ed., Synthesis\nLectures on Computer Architecture, Morgan & Claypool, San Rafael, Calif.\nBarroso, L.A., Clidaras, J. and H\u20acolzle, U., 2013. The datacenter as a computer:An\nintroduction to the design of warehouse-scale machines. Synthesis lectures on\ncomputer architecture, 8(3), pp.1\u2013154.\nBarroso, L.A., Marty, M., Patterson, D., and Ranganathan, P. 2017. Attack of the\nKiller Microseconds. Communications of the ACM, 56(2).\nBell, C. G., and J. Gray [2002]. \u201cWhat\u2019s next in high performance computing,\u201d\nCommunications of the ACM 45:2 (February), 91\u201395.\nBrady, J.T., 1986. A theory of productivity in the creative process. IEEE Computer\nGraphics and Applications, 6(5), pp.25\u201334.\nBrin, S., and L. Page [1998]. \u201cThe anatomy of a large-scale hypertextual Web\nsearch engine,\u201d Proc. 7th Int\u2019l. World Wide Web Conf., April 14\u201318, 1998, Bris-\nbane, Queensland, Australia, 107\u2013117.\nCarter, J., and K. Rajamani [2010]. \u201cDesigning energy-efficient servers and data\ncenters,\u201d IEEE Computer 43:7 (July), 76\u201378.\nChang, F., J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Burrows, T.\nChandra, A. Fikes, and R. E. Gruber [2006]. \u201cBigtable: A distributed storage\nsystem for structured data,\u201d in Proc. Operating Systems Design and Implemen-\ntation (OSDI \u201906), November 6\u20138, 2006, Seattle, Wash.\nChang, J., J. Meza, P. Ranganathan, C. Bash, and A. Shah [2010]. \u201cGreen server\ndesign: Beyond operational energy to sustainability,\u201d Workshop on Power\nAware Computing and Systems (HotPower \u201910), October 4\u20136, 2010, Vancou-\nver, British Columbia.\nClark, J., 2014 Five Numbers That Illustrate the Mind-Bending Size of Amazon\u2019s\nCloud,\nBloomberg,\nhttps://www.bloomberg.com/news/2014-11-14/5-\nnumbersthat-illustrate-the-mind-bending-size-of-amazon-s-cloud.html.\nClidaras, J., C. Johnson, and B. Felderman [2010]. Private communication.\nClimate\nSavers\nComputing.\n[2007].\nEfficiency\nspecs,\nhttp://www.\nclimatesaverscomputing.org/.\nClos, C., 1953. A Study of Non-Blocking Switching Networks. Bell Labs Techni-\ncal Journal, 32(2), pp.406-424.\nDean, J. [2009]. \u201cDesigns, lessons and advice from building large distributed sys-\ntems [keynote address],\u201d Proc. 3rd ACM SIGOPS International Workshop on\nLarge Scale Distributed Systems and Middleware, Co-located with the 22nd\nACM Symposium on Operating Systems Principles (SOSP 2009), October\n10\u201311, 2009, Big Sky, Mont.\nDean, J. and Barroso, L.A., 2013. The tail at scale. Communications of the ACM,\n56(2), pp.74-80.\nM.8\nThe Development of Clusters\n\u25a0\nM-77"
    },
    {
        "page": 1425,
        "text": "Dean, J., and S. Ghemawat [2004]. \u201cMapReduce: Simplified data processing on\nlarge clusters.\u201d In Proc. Operating Systems Design and Implementation (OSDI\n\u201904), December 6\u20138, 2004, San Francisco, 137\u2013150.\nDean, J., and S. Ghemawat [2008]. \u201cMapReduce: simplified data processing on\nlarge clusters,\u201d Communications of the ACM 51:1, 107\u2013113.\nDeCandia, G., D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman, A. Pilchin,\nS. Sivasubramanian, P. Vosshall, and W. Vogels [2007]. \u201cDynamo: Amazon\u2019s\nhighly available key-value store,\u201d in Proc. 21st ACM Symposium on Operating\nSystems Principles, October 14\u201317, 2007, Stevenson, Wash.\nDoherty, W.J. and Thadhani, A.J., 1982. The economic value of rapid response\ntime. IBM Report.\nFan, X., W. Weber, and L. A. Barroso [2007]. \u201cPower provisioning for a\nwarehouse-sized computer,\u201d in Proc. 34th Annual Int\u2019l. Symposium on Com-\nputer Architecture (ISCA), June 9\u201313, 2007, San Diego, Calif.\nA. Fikes, \u201cStorage architecture and challenges,\u201d in Google Faculty Summit, 2010.\nGhemawat, S., H. Gobioff, and S.-T. Leung (2003). \u201cThe Google file system,\u201d in\nProc. 19th ACM Symposium on Operating Systems Principles, October 19\u201322,\n2003, Lake George, N.Y.\nGreenberg, A., N. Jain, S. Kandula, C. Kim, P. Lahiri, D. Maltz, P. Patel, and S.\nSengupta [2009]. \u201cVL2: A scalable and flexible data center network,\u201d in Proc.\nSIGCOMM, August 17\u201321, Barcelona, Spain.\nGonz\u00e1lez, A.and Day, M. April 27, 2016, \u201cAmazon, Microsoft invest billions as\ncomputing shifts to cloud,\u201d The Seattle Times. http://www.seattletimes.com/\nbusiness/technology/amazon-microsoft-invest-billions-as-computing-shifts-to-\ncloud/\nHamilton, J. [2009]. \u201cData center networks are in my way,\u201d Stanford Clean Slate\nCTO Summit, October 23, 2009, http://mvdirona.com/jrh/TalksAndPapers/\nJamesHamilton_CleanSlateCTO2009.pdf.\nHamilton, J. [2010]. \u201cCloud computing economies of scale,\u201d Proc. AWS Workshop\non Genomics & Cloud Computing, June 8, 2010, Seattle, Wash. (http://\nmvdirona.com/jrh/TalksAndPapers/JamesHamilton_\nGenomicsCloud20100608.pdf).\nHamilton, J., 2014. AWS Innovation at Scale, AWS Re-invent conference. https://\nwww.youtube.com/watch?v\u00bcJIQETrFC_SQ\nHamilton, J., May 2015. The Return to the Cloud, http://perspectives.mvdirona.\ncom/2015/05/the-return-to-the-cloud//\nHamilton, J., April 2017. How Many Data Centers Needed World-Wide, http://\nperspectives.mvdirona.com/2017/04/how-many-data-centers-needed-worldwide/\nH\u20acolzle, U. [2010]. \u201cBrawny cores still beat wimpy cores, most of the time,\u201d IEEE\nMicro, July/August.\nKanev, S., Darago, J.P., Hazelwood, K., Ranganathan, P., Moseley, T., Wei, G.Y.\nand Brooks, D., 2015, June. Profiling a warehouse-scale computer. ACM/\nIEEE 42nd Annual International Symposium on Computer Architecture\n(ISCA).\nM-78\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1426,
        "text": "Lang, W., J. M. Patel, and S. Shankar [2010]. \u201cWimpy node clusters: What about\nnon-wimpy workloads?\u201d Proc. Sixth Int\u2019l. Workshop on Data Management on\nNew Hardware, June 7, 2010, Indianapolis, Ind.\nLim, K., P. Ranganathan, J. Chang, C. Patel, T. Mudge, and S. Reinhardt [2008].\n\u201cUnderstanding and designing new system architectures for emerging\nwarehouse-computing environments,\u201d Proc. 35th Annual Int\u2019l. Symposium on\nComputer Architecture (ISCA), June 21\u201325, 2008, Beijing, China.\nNarayanan, D., E. Thereska, A. Donnelly, S. Elnikety, and A. Rowstron [2009].\n\u201cMigrating server storage to SSDs: Analysis of trade-offs,\u201d Proc. 4th ACM\nEuropean Conf. on Computer Systems, April 1\u20133, 2009, Nuremberg, Germany.\nPfister, G. F. [1998]. In Search of Clusters, 2nd ed., Prentice Hall, Upper Saddle\nRiver, N.J.\nPinheiro, E., W.-D. Weber, and L. A. Barroso [2007]. \u201cFailure trends in a large\ndisk drive population,\u201d Proc. 5th USENIX Conference on File and Storage\nTechnologies (FAST \u201907), February 13\u201316, 2007, San Jose, Calif.\nRanganathan, P., P. Leech, D. Irwin, and J. Chase [2006]. \u201cEnsemble-level power\nmanagement for dense blade servers,\u201d Proc. 33rd Annual Int\u2019l. Symposium on\nComputer Architecture (ISCA), June 17\u201321, 2006, Boston, Mass., 66\u201377.\nReddi, V. J., B. C. Lee, T. Chilimbi, and K. Vaid [2010]. \u201cWeb search using mobile\ncores: Quantifying and mitigating the price of efficiency,\u201d Proc. 37th Annual\nInt\u2019l. Symposium on Computer Architecture (ISCA), June 19\u201323, 2010, Saint-\nMalo, France.\nSchroeder, B., and G. A. Gibson [2007]. \u201cUnderstanding failures in petascale com-\nputers,\u201d Journal of Physics: Conference Series 78, 188\u2013198.\nSchroeder, B., E. Pinheiro, and W.-D. Weber [2009]. \u201cDRAM errors in the wild: A\nlarge-scale field study,\u201d Proc. Eleventh Int\u2019l. Joint Conf. on Measurement and\nModeling\nof\nComputer\nSystems\n(SIGMETRICS),\nJune\n15\u201319,\n2009,\nSeattle, Wash.\nSchurman, E. and J. Brutlag [2009]. \u201cThe User and Business Impact of Server\nDelays,\u201d Proc. Velocity: Web Performance and Operations Conf., June 22\u2013\n24, 2009, San Jose, Calif.\nTezzaron Semiconductor. [2004]. \u201cSoft Errors in Electronic Memory\u2014A White\nPaper, Tezzaron Semiconductor, Naperville, Ill. (http://www.tezzaron.com/\nabout/papers/soft_errors_1_1_secure.pdf).\nVahdat, A., M. Al-Fares, N. Farrington, R. N. Mysore, G. Porter, and S. Radhak-\nrishnan [2010]. \u201cScale-out networking in the data center,\u201d IEEE Micro July/\nAugust 2010.\nM.9\nHistorical Perspectives and References\nAs architects experiment with DSAs, knowing architecture history may help.\nThere are likely older architecture ideas that were unsuccessful for general-purpose\ncomputing that could nevertheless make eminent sense for domain-specific archi-\ntectures. After all, they probably did some things well, and either they might match\nM.9\nHistorical Perspectives and References\n\u25a0\nM-79"
    },
    {
        "page": 1427,
        "text": "your domain, or, conversely, your domain might omit features that were challenges\nfor these architectures. For example, both the Illiac IV (Barnes et al., 1968) from\nthe 1960s and the FPS 120a (Charlesworth, 1981) from the 1970s had two-\ndimensional arrays of processing elements, so they are proper ancestors to the\nTPU and Paintbox. Similarly, while VLIW architectures of the Multiflow (Rau\nand Fisher, 1993) and Itanium (Sharangpani and Arora, 2000) were not commer-\ncial successes for general-purpose computing, Paintbox does not have the erratic\ndata cache misses, unpredictable branches, or large code footprint that were diffi-\ncult for VLIW architectures.\nTwo survey articles document that custom neural network ASICs go back at\nleast 25 years (Ienne et al., 1996; Asanovic, 2002). For example, CNAPS chips\ncontained a 64 SIMD array of 16-bit by 8-bit multipliers, and several CNAPS chips\ncould be connected together with a sequencer (Hammerstrom, 1990). The\nSynapse-1 system was based on a custom systolic multiply-accumulate chip called\nthe MA-16, which performed sixteen 16-bit multiplications at a time (Ramacher\net al., 1991). The system concatenated MA-16 chips and had custom hardware\nto do activation functions.\nTwenty-five SPERT-II workstations, accelerated by the T0 custom ASIC, were\ndeployed starting in 1995 to do both NN training and inference for speech recog-\nnition (Asanovic et al., 1998). The 40-MHz T0 added vector instructions to the\nMIPS instruction set architecture. The eight-lane vector unit could produce up\nto sixteen 32-bit arithmetic results per clock cycle based on 8-bit and 16-bit inputs,\nmaking it 25 times faster at inference and 20 times faster at training than a SPARC-\n20 workstation. They found that 16 bits were insufficient for training, so they used\ntwo 16-bit words instead, which doubled training time. To overcome that draw-\nback, they introduced \u201cbunches\u201d (batches) of 32\u20131000 data sets to reduce time\nspent updating weights, which made it faster than training with one word but\nno batches.\nWe use the phrase Image Processing Unit for Paintbox to identify this emerg-\ning class of processor, but this is not the first use of the term. The earliest use we can\nfind is 1999, when the Sony Playstation put the name on a chip that was basically\nan MPEG2 decoder (Sony/Toshiba, 1999). In 2006, Freescale used IPU to name\npart of the i.MX31 Applications Processor, which is closer to the more generic way\nwe interpret it (Freescale as part of i.MX31 Applications Processor, 2006).\nReferences\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.\nS., Davis, A., Dean, J., Devin, M., Ghemawat, S., 2016. Tensor-flow: large-\nscale machine learning on heterogeneous distributed systems. arXiv preprint\narXiv:1603.04467.\nAdolf, R., Rama, S., Reagen, B., Wei, G.Y., Brooks, D., 2016. Fathom: reference\nworkloads for modern deep learning methods. In: IEEE International Sympo-\nsium on Workload Characterization (IISWC).\nAmodei, D., et al., 2015. Deep speech 2: end-to-end speech recognition in English\nand mandarin, arXiv:1512.02595.\nM-80\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1428,
        "text": "Asanovic, K., 2002. Programmable neurocomputing. In: Arbib, M.A. (Ed.), The\nHandbook of Brain Theory and Neural Networks, second ed. MIT Press, Cam-\nbridge, MA. ISBN: 0-262-01197-2. https://people.eecs.berkeley.edu/\u0003krste/\npapers/neurocomputing.pdf.\nAsanovic, K., Beck, A., Johnson, J., Wawrzynek, J., Kingsbury, B., Morgan, N.,\n1998. Training neural networks with Spert-II. In: Sundararajan, N., Saratchan-\ndran, P. (Eds.), Parallel Architectures for Artificial Networks: Paradigms\nand Implementations. IEEE Computer Society Press. ISBN: 0-8186-8399-6.\n(Chapter 11) https://people.eecs.berkeley.edu/\u0003krste/papers/annbook.pdf.\nBachrach, J., Vo, H., Richards, B., Lee, Y., Waterman, A., Avi\u017eienis, R.,\nWawrzynek, J., Asanovic, K., 2012. Chisel: constructing hardware in a Scala\nembedded language. In: Proceedings of the 49th Annual Design Automation\nConference, pp. 1216\u20131225.\nBarnes, G.H., Brown, R.M., Kato, M., Kuck, D.J., Slotnick, D.L., Stokes, R.,\n1968. The ILLIAC IV computer. IEEE Trans. Comput. 100 (8), 746\u2013757.\nBhattacharya, S., Lane, N.D., 2016. Sparsification and separation of deep learning\nlayers for constrained resource inference on wearables. In: Proceedings of the\n14th ACM Conference on Embedded Network Sensor Systems CD-ROM,\npp. 176\u2013189.\nBrunhaver, J., 2014. PhD thesis. Stanford.\nCanis, A., Choi, J., Aldham, M., Zhang, V., Kammoona, A., Czajkowski, T.,\nBrown, S.D., Anderson, J.H., 2013. LegUp: an open-source high-level synthesis\ntool for FPGA-based processor/accelerator systems. ACM Trans. Embed. Com-\nput. Syst. 13 (2).\nCanny, J., et al., 2015. Machine learning at the limit. In: IEEE International Con-\nference on Big Data.\nCaulfield, A.M., Chung, E.S., Putnam, A., Haselman, H.A.J.F.M., Humphrey,\nS.H.M., Daniel, P.K.J.Y.K., Ovtcharov, L.T.M.K., Lanka, M.P.L.W.S.,\nBurger, D.C.D., 2016. A cloud-scale acceleration architecture. In: MICRO\nConference.\nCharlesworth, A.E., 1981. An approach to scientific array processing: the architec-\ntural design of the AP-120B/FPS-164 family. Computer 9, 18\u201327.\nClark, J., October 26, 2015. Google Turning Its Lucrative Web Search Over to AI\nMachines. Bloomberg Technology, www.bloomberg.com.\nDally, W.J., 2002. Computer architecture is all about interconnect. In: Proceed-\nings of the 8th International Symposium High Performance Computer\nArchitecture.\nFreescale as part of i.MX31 Applications Processor, 2006. http://cache.freescale.\ncom/files/32bit/doc/white_paper/IMX31MULTIWP.pdf.\nGalal, S., Shacham, O., Brunhaver II, J.S., Pu, J., Vassiliev, A., Horowitz, M.,\n2013. FPU generator for design space exploration. In: 21st IEEE Symposium\non Computer Arithmetic (ARITH).\nHameed, R., Qadeer, W., Wachs, M., Azizi, O., Solomatnikov, A., Lee, B.C.,\nRichardson, S., Kozyrakis, C., Horowitz, M., 2010. Understanding sources of\ninefficiency in general-purpose chips. ACM SIGARCH Comput. Architect.\nNews 38 (3), 37\u201347.\nM.9\nHistorical Perspectives and References\n\u25a0\nM-81"
    },
    {
        "page": 1429,
        "text": "Hammerstrom, D., 1990. A VLSI architecture for high-performance, low-cost, on-\nchip learning. In: IJCNN International Joint Conference on Neural Networks.\nHe, K., Zhang, X., Ren, S., Sun, J., 2016. Identity mappings in deep residual net-\nworks. Also in arXiv preprint arXiv:1603.05027.\nHuang, M., Wu, D., Yu, C.H., Fang, Z., Interlandi, M., Condie, T., Cong, J., 2016.\nProgramming and runtime support to blaze FPGA accelerator deployment\nat datacenter scale. In: Proceedings of the Seventh ACM Symposium on Cloud\nComputing. ACM, pp. 456\u2013469.\nIandola, F., 2016. Exploring the Design Space of Deep Convolutional Neural Net-\nworks at Large Scale (Ph.D. dissertation). UC Berkeley.\nIenne, P., Cornu, T., Kuhn, G., 1996. Special-purpose digital hardware for neural\nnetworks: an architectural survey. J. VLSI Signal Process. Syst. Signal Image\nVideo Technol. 13 (1).\nJouppi, N., 2016. Google supercharges machine learning tasks with TPU custom\nchip. https://cloudplatform.googleblog.com.\nJouppi, N., Young, C., Patil, N., Patterson, D., Agrawal, G., et al., 2017. Datacenter\nperformance analysis of a matrix processing unit. In: 44th International Sympo-\nsium on Computer Architecture.\nKarpathy, A., et al., 2014. Large-scale video classification with convolutional neu-\nral networks. CVPR.\nKrizhevsky, A., Sutskever, I., Hinton, G., 2012. Imagenet classification with deep\nconvolutional neural networks. Adv. Neural Inf. Process. Syst.\nKung, H.T., Leiserson, C.E., 1980. Algorithms for VLSI processor arrays. Intro-\nduction to VLSI systems.\nLee, Y., Waterman, A., Cook, H., Zimmer, B., Keller, B., Puggelli, A., Kwak, J.,\nJevtic, R., Bailey, S., Blagojevic, M., Chiu, P.-F., Avizienis, R., Richards, B.,\nBachrach, J., Patterson, D., Alon, E., Nikolic, B., Asanovic, K., 2016. An agile\napproach to building RISC-V microprocessors. IEEE Micro 36 (2), 8\u201320.\nLewis-Kraus, G., 2016. The Great A.I. Awakening. New York Times Magazine.\nNielsen,\nM.,\n2016.\nNeural\nNetworks\nand\nDeep\nLearning.\nhttp://\nneuralnetworksanddeeplearning.com/.\nNvidia, 2016. Tesla GPU Accelerators For Servers. http://www.nvidia.com/object/\nteslaservers.html.\nOlofsson, A., 2011. Debunking the myth of the $100 M ASIC. EE Times. http://\nwww.eetimes.com/author.asp?section_id\u00bc36&doc_id\u00bc1266014.\nOvtcharov, K., Ruwase, O., Kim, J.Y., Fowers, J., Strauss, K., Chung, E.S., 2015a.\nAccelerating deep convolutional neural networks using specialized hardware.\nMicrosoft Research Whitepaper. https://www.microsoft.com/en-us/research/\npublication/accelerating-deepconvolutional-neural-networks-using-specialized-\nhardware/.\nOvtcharov, K., Ruwase, O., Kim, J.Y., Fowers, J., Strauss, K., Chung, E.S., 2015b.\nToward accelerating deep learning at scale using specialized hardware in the\ndatacenter. In: 2015 IEEE Hot Chips 27 Symposium.\nM-82\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1430,
        "text": "Patterson, D., Nikolic, B., 7/25/2015, Agile Design for Hardware, Parts I, II, and\nIII. EE Times, http://www.eetimes.com/author.asp?doc_id\u00bc1327239.\nPatterson, D.A., Ditzel, D.R., 1980. The case for the reduced instruction set com-\nputer. ACM SIGARCH Comput. Architect. News 8 (6), 25\u201333.\nPrabhakar, R., Koeplinger, D., Brown, K.J., Lee, H., De Sa, C., Kozyrakis, C.,\nOlukotun, K., 2016. Generating configurable hardware from parallel patterns.\nIn:ProceedingsoftheTwenty-FirstInternationalConferenceonArchitecturalSup-\nport for Programming Languages and Operating Systems. ACM, pp. 651\u2013665.\nPutnam, A., Caulfield, A.M., Chung, E.S., Chiou, D., Constantinides, K., Demme,\nJ., Esmaeilzadeh, H., Fowers, J., Gopal, G.P., Gray, J., Haselman, M., Hauck,\nS., Heil, S., Hormati, A., Kim, J.-Y., Lanka, S., Larus, J., Peterson, E., Pope, S.,\nSmith, A., Thong, J., Xiao, P.Y., Burger, D., 2014. A reconfigurable fabric for\naccelerating large-scale datacenter services. In: 41st International Symposium\non Computer Architecture.\nPutnam, A., Caulfield, A.M., Chung, E.S., Chiou, D., Constantinides, K., Demme,\nJ., Esmaeilzadeh, H., Fowers, J., Gopal, G.P., Gray, J., Haselman, M., Hauck,\nS., Heil, S., Hormati, A., Kim, J.-Y., Lanka, S., Larus, J., Peterson, E., Pope, S.,\nSmith, A., Thong, J., Xiao, P.Y., Burger, D., 2015. A reconfigurable fabric for\naccelerating large-scale datacenter services. IEEE Micro. 35 (3).\nPutnam, A., Caulfield, A.M., Chung, E.S., Chiou, D., Constantinides, K., Demme,\nJ., Esmaeilzadeh, H., Fowers, J., Gopal, G.P., Gray, J., Haselman, M., Hauck,\nS., Heil, S., Hormati, A., Kim, J.-Y., Lanka, S., Larus, J., Peterson, E., Pope, S.,\nSmith, A., Thong, J., Xiao, P.Y., Burger, D., 2016. A reconfigurable fabric for\naccelerating large-scale datacenter services. Commun. ACM.\nQadeer, W., Hameed, R., Shacham, O., Venkatesan, P., Kozyrakis, C., Horowitz,\nM.A., 2015. Convolution engine: balancing efficiency & flexibility in special-\nized computing. Commun. ACM 58 (4).\nRagan-Kelley, J., Barnes, C., Adams, A., Paris, S., Durand, F., Amarasinghe, S.,\n2013. Halide: a language and compiler for optimizing parallelism, locality, and\nrecomputation in image processing pipelines. ACM SIGPLAN Not. 48 (6),\n519\u2013530.\nRamacher, U., Beichter, J., Raab, W., Anlauf, J., Bruels, N., Hachmann, A.,\nWesseling, M., 1991. Design of a 1st generation neurocomputer. VLSI Design\nof Neural Networks. Springer, USA.\nRau, B.R., Fisher, J.A., 1993. Instruction-level parallelism. J. Supercomput. 235,\nSpringer Science & Business Media.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,\nKarpathy, A., Khosla, A., Bernstein, M., Berg, A.C., 2015. Imagenet large scale\nvisual recognition challenge. Int. J. Comput. Vis. 115 (3).\nSergio Guadarrama, 2015. BVLC googlenet. https://github.com/BVLC/caffe/tree/\nmaster/models/bvlc_googlenet.\nShao, Y.S., Brooks, D., 2015. Research infrastructures for hardware accelerators.\nSynth. Lect. Comput. Architect. 10 (4), 1\u201399.\nM.9\nHistorical Perspectives and References\n\u25a0\nM-83"
    },
    {
        "page": 1431,
        "text": "Sharangpani, H., Arora, K., 2000. Itanium processor microarchitecture. IEEE\nMicro 20 (5), 24\u201343.\nSilver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche, G.,\nSchrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman,\nS., 2016. Mastering the game of Go with deep neural networks and tree search.\nNature 529 (7587).\nSmith, J.E., 1982. Decoupled access/execute computer architectures. In: Proceed-\nings of the 11th International Symposium on Computer Architecture.\nSony/Toshiba, 1999. \u2018Emotion Engine\u2019 in PS2 (\u201cIPU is basically an MPEG2\ndecoder\u2026\u201d).\nhttp://www.cpu-collection.de/?l0\u00bcco&l1\u00bcSony&l2\u00bcEmotion\n+Engine, http://arstechnica.com/gadgets/2000/02/ee/3/.\nSteinberg, D., 2015. Full-Chip Simulations, Keys to Success. In: Proceedings of\nthe Synopsys Users Group (SNUG) Silicon Valley 2015.\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,\nVanhoucke, V., Rabinovich, A., 2015. Going deeper with convolutions. In: Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\nTensorFlow Tutorials, 2016. https://www.tensorflow.org/versions/r0.12/tutorials/\nindex.html.\nTung, L., 2016. Google Translate: \u2018This landmark update is our biggest single leap\nin\n10\nyears\u2019,\nZDNet.\nhttp://www.zdnet.com/article/google-translate-this-\nlandmarkupdate-is-our-biggest-single-leap-in-10years/.\nVanhoucke, V., Senior, A., Mao, M.Z., 2011. Improving the speed of neural net-\nworks on CPUs. https://static.googleusercontent.com/media/research.google.\ncom/en//pubs/archive/37631.pdf.\nWu, Y., Schuster, M., Chen, Z., Le, Q., Norouzi, M., Macherey, W., Krikun, M.,\nCao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X.,\nKaiser, \u0141., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G.,\nPatil, N., Wang, W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O.,\nCorrado, G., Hughes, M., Dean, J., 2016. Google\u2019s Neural Machine Translation\nSystem: Bridging the Gap between Human and Machine Translation. http://\narxiv.org/abs/1609.08144.\nM.10\nThe History of Magnetic Storage, RAID, and I/O Buses\n(Appendix D)\nMass storage is a term used there to imply a unit capacity in excess of one million\nalphanumeric characters \u2026\nHoagland [1963]\nThe variety of storage I/O and issues leads to a varied history for the rest of\nthe story. (Smotherman [1989] explored the history of I/O in more depth.) This\nsection discusses magnetic storage, RAID, and I/O buses and controllers.\nJain [1991] and Lazowska et al. [1984] are books for those interested in learning\nmore about queuing theory.\nM-84\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1432,
        "text": "Magnetic Storage\nMagnetic recording was invented to record sound, and by 1941 magnetic tape was\nable to compete with other storage devices. It was the success of the ENIAC in\n1947 that led to the push to use tapes to record digital information. Reels of mag-\nnetic tapes dominated removable storage through the 1970s. In the 1980s, the IBM\n3480 cartridge became the de facto standard, at least for mainframes. It can transfer\nat 3 MB/sec by reading 18 tracks in parallel. The capacity is just 200 MB for this 1/\n2-inch tape. The 9840 cartridge, used by StorageTek in the Powder-Horn, transfers\nat 10 MB/sec and stores 20,000 MB. This device records the tracks in a zigzag\nfashion rather than just longitudinally, so that the head reverses direction to follow\nthe track. This technique is called serpentine recording. Another 1/2-inch tape is\nDigital Linear Tape; the DLT7000 stores 35,000 MB and transfers at 5 MB/sec. Its\ncompetitor is helical scan, which rotates the head to get the increased recording\ndensity. In 2001, the 8-mm helical-scan tapes contain 20,000 MB and transfer\nat about 3 MB/sec. Whatever their density and cost, the serial nature of tapes cre-\nates an appetite for storage devices with random access.\nIn 1953, Reynold B. Johnson of IBM picked a staff of 15 scientists with the\ngoal of building a radically faster random access storage system than tape. The goal\nwas to have the storage equivalent of 50,000 standard IBM punch cards and to\nfetch the data in a single second. Johnson\u2019s disk drive design was simple but\nuntried: The magnetic read/write sensors would have to float a few thousandths\nof an inch above the continuously rotating disk. Twenty-four months later the team\nemerged with the functional prototype. It weighed 1 ton and occupied about 300\ncubic feet of space. The RAMAC-350 (Random Access Method of Accounting\nControl) used 50 platters that were 24 inches in diameter, rotated at 1200 RPM,\nwith a total capacity of 5 MB and an access time of 1 second.\nStarting with the RAMAC, IBM maintained its leadership in the disk industry,\nwith its storage headquarters in San Jose, California, where Johnson\u2019s team did its\nwork. Many of the future leaders of competing disk manufacturers started their\ncareers at IBM, and many disk companies are located near San Jose.\nAlthough RAMAC contained the first disk, a major breakthrough in magnetic\nrecording was found in later disks with air-bearing read/write heads, where\nthe head would ride on a cushion of air created by the fast-moving disk surface.\nThis cushion meant the head could both follow imperfections in the surface and\nyet be very close to the surface. Subsequent advances have come largely from\nimproved quality of components and higher precision. In 2001, heads flew 2 to\n3 microinches above the surface, whereas in the RAMAC drive they were 1000\nmicroinches away.\nMoving-head disks quickly became the dominant high-speed magnetic stor-\nage, although their high cost meant that magnetic tape continued to be used\nextensively until the 1970s. The next important development for hard disks was\nthe removable hard disk drive developed by IBM in 1962; this made it possible\nto share the expensive drive electronics and helped disks overtake tapes as the pre-\nferred storage medium. The IBM 1311 disk in 1962 had an areal density of 50,000\nM.10\nThe History of Magnetic Storage, RAID, and I/O Buses\n\u25a0\nM-85"
    },
    {
        "page": 1433,
        "text": "bits per square inch and a cost of about $800 per megabyte. IBM also invented the\nfloppy disk drive in 1970, originally to hold microcode for the IBM 370 series.\nFloppy disks became popular with the PC about 10 years later.\nThe second major disk breakthrough was the so-called Winchester disk design\nin about 1973. Winchester disks benefited from two related properties. First, inte-\ngrated circuits lowered the costs of not only CPUs but also of disk controllers and\nthe electronics to control disk arms. Reductions in the cost of the disk electronics\nmade it unnecessary to share the electronics and thus made nonremovable disks\neconomical. Since the disk was fixed and could be in a sealed enclosure, both\nthe environmental and control problems were greatly reduced. Sealing the system\nallowed the heads to fly closer to the surface, which in turn enabled increases\nin areal density. The first sealed disk that IBM shipped had two spindles, each\nwith a 30 MB disk; the moniker \u201c30-30\u201d for the disk led to the name Winchester.\n(America\u2019s most popular sporting rifle, the Winchester 94, was nicknamed the\n\u201c30-30\u201d after the caliber of its cartridge.) Winchester disks grew rapidly in popu-\nlarity in the 1980s, completely replacing removable disks by the middle of that\ndecade. Before this time, the cost of the electronics to control the disk meant that\nthe media had to be removable.\nAs mentioned in Appendix D, as DRAMs started to close the areal density gap\nand appeared to be catching up with disk storage, internal meetings at IBM called\ninto question the future of disk drives. Disk designers concluded that disks must\nimprove at 60% per year to forestall the DRAM threat, in contrast to the historical\n29% per year. The essential enabler was magnetoresistive heads, with giant\nmagnetoresistive heads enabling the current densities. Because of this competition,\nthe gap in time between when a density record is achieved in the lab and when a\ndisk is shipped with that density has closed considerably.\nThe personal computer created a market for small form factor (SFF) disk\ndrives, since the 14-inch disk drives used in mainframes were bigger than\nthe PC. In 2006, the 3.5-inch drive was the market leader, although the smaller\n2.5-inch drive required for laptop computers was significant in sales volume. It\nremains to be seen whether handheld devices such as iPODs or video cameras,\nwhich require even smaller disks, will remain significant in sales volume. For\nexample, 1.8-inch drives were developed in the early 1990s for palmtop com-\nputers, but that market chose Flash instead and 1.8-inch drives disappeared.\nRAID\nThe SFF hard disks for PCs in the 1980s led a group at Berkeley to propose redun-\ndant arrays of inexpensive disks (RAID). This group had worked on the reduced\ninstruction set computer effort and so expected much faster CPUs to become avail-\nable. They asked: What could be done with the small disks that accompanied their\nPCs? and What could be done in the area of I/O to keep up with much faster pro-\ncessors? They argued to replace one mainframe drive with 50 small drives to gain\nmuch greater performance from that many independent arms. The many small\nM-86\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1434,
        "text": "drives even offered savings in power consumption and floor space. The downside\nof many disks was much lower mean time to failure (MTTF). Hence, on their own\nthey reasoned out the advantages of redundant disks and rotating parity to address\nhow to get greater performance with many small drives yet have reliability as high\nas that of a single mainframe disk.\nThe problem they experienced when explaining their ideas was that some\nresearchers had heard of disk arrays with some form of redundancy, and they didn\u2019t\nunderstand the Berkeley proposal. Hence, the first RAID paper [Patterson, Gibson,\nand Katz 1987] is not only a case for arrays of SFF disk drives but also something\nof a tutorial and classification of existing work on disk arrays. Mirroring (RAID 1)\nhad long been used in fault-tolerant computers such as those sold by Tandem.\nThinking Machines had arrays with 32 data disks and 7 check disks using ECC\nfor correction (RAID 2) in 1987, and Honeywell Bull had a RAID 2 product even\nearlier. Also, disk arrays with a single parity disk had been used in scientific com-\nputers in the same time frame (RAID 3). Their paper then described a single parity\ndisk with support for sector accesses (RAID 4) and rotated parity (RAID 5). Chen\net al. [1994] surveyed the original RAID ideas, commercial products, and more\nrecent developments.\nUnknown to the Berkeley group, engineers at IBM working on the AS/400\ncomputer also came up with rotated parity to give greater reliability for a collection\nof large disks. IBM filed a patent on RAID 5 before the Berkeley group wrote their\npaper. Patents for RAID 1, RAID 2, and RAID 3 from several companies predate\nthe IBM RAID 5 patent, which has led to plenty of courtroom action.\nThe Berkeley paper was written before the World Wide Web, but it captured\nthe imagination of many engineers, as copies were faxed around the world. One\nengineer at what is now Seagate received seven copies of the paper from friends\nand customers. EMC had been a supplier of DRAM boards for IBM computers, but\naround 1988 new policies from IBM made it nearly impossible for EMC to con-\ntinue to sell IBM memory boards. Apparently, the Berkeley paper also crossed the\ndesks of EMC executives, and they decided to go after the market dominated by\nIBM disk storage products instead. As the paper advocated, their model was to use\nmany small drives to compete with mainframe drives, and EMC announced a\nRAID product in 1990. It relied on mirroring (RAID 1) for reliability; RAID 5\nproducts came much later for EMC. Over the next year, Micropolis offered a RAID\n3 product, Compaq offered a RAID 4 product, and Data General, IBM, and NCR\noffered RAID 5 products.\nThe RAID ideas soon spread to the rest of the workstation and server industry.\nAn article explaining RAID in Byte magazine (see Anderson [1990]) led to RAID\nproducts being offered on desktop PCs, which was something of a surprise to the\nBerkeley group. They had focused on performance with good availability, but\nhigher availability was attractive to the PC market.\nAnother surprise was the cost of the disk arrays. With redundant power sup-\nplies and fans, the ability to \u201chot swap\u201d a disk drive, the RAID hardware controller\nitself, the redundant disks, and so on, the first disk arrays cost many times the cost\nof the disks. Perhaps as a result, the \u201cinexpensive\u201d in RAID morphed into\nM.10\nThe History of Magnetic Storage, RAID, and I/O Buses\n\u25a0\nM-87"
    },
    {
        "page": 1435,
        "text": "\u201cindependent.\u201d Many marketing departments and technical writers today know of\nRAID only as \u201credundant arrays of independent disks.\u201d\nThe EMC transformation was successful; in 2006, EMC was the leading\nsupplier of storage systems, and NetApp was the leading supplier of Network-\nAttached Storage systems. RAID was a $30 billion industry in 2006, and more than\n80% of the non-PC drive sales were found in RAIDs. In recognition of their role, in\n1999 Garth Gibson, Randy Katz, and David Patterson received the IEEE Reynold\nB. Johnson Information Storage Award \u201cfor the development of Redundant Arrays\nof Inexpensive Disks (RAID).\u201d\nI/O Buses and Controllers\nThe ubiquitous microprocessor inspired not only the personal computers of the\n1970s but also the trend in the late 1980s and 1990s of moving controller functions\ninto I/O devices. I/O devices have continued this trend by moving controllers into\nthe devices themselves. These devices are called intelligent devices, and some bus\nstandards (e.g., SCSI) have been created specifically for them. Intelligent devices\ncan relax the timing constraints by handling many low-level tasks themselves and\nqueuing the results. For example, many SCSI-compatible disk drives include a\ntrack buffer on the disk itself, supporting read ahead and connect/disconnect. Thus,\non a SCSI string some disks can be seeking and others loading their track buffer\nwhile one is transferring data from its buffer over the SCSI bus. The controller in\nthe original RAMAC, built from vacuum tubes, only needed to move the head over\nthe desired track, wait for the data to pass under the head, and transfer data with\ncalculated parity. SCSI, which stands for small computer systems interface, is an\nexample of one company inventing a bus and generously encouraging other com-\npanies to build devices that would plug into it. Shugart created this bus, originally\ncalled SASI. It was later standardized by the IEEE.\nThere have been several candidates to be the successor to SCSI, with the cur-\nrent leading contender being Fibre Channel Arbitrated Loop (FC-AL). The SCSI\ncommittee continues to increase the clock rate of the bus, giving this standard a\nnew life, and SCSI is lasting much longer than some of its proposed successors.\nWith the creation of serial interfaces for SCSI (\u201cSerial Attach SCSI\u201d) and ATA\n(\u201cSerial ATA\u201d), they may have very long lives.\nPerhaps the first multivendor bus was the PDP-11 Unibus in 1970 from DEC.\nAlas, this open-door policy on buses is in contrast to companies with proprietary\nbuses using patented interfaces, thereby preventing competition from plug-\ncompatible vendors. Making a bus proprietary also raises costs and lowers the\nnumber of available I/O devices that plug into it, since such devices must have\nan interface designed just for that bus. The PCI bus pushed by Intel represented\na return to open, standard I/O buses inside computers. Its immediate successor\nis PCI-X, with Infiniband under development in 2000. Both were standardized\nby multicompany trade associations.\nM-88\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1436,
        "text": "The machines of the RAMAC era gave us I/O interrupts as well as storage\ndevices. The first machine to extend interrupts from detecting arithmetic abnor-\nmalities to detecting asynchronous I/O events is credited as the NBS DYSEAC in\n1954 [Leiner and Alexander 1954]. The following year, the first machine with\nDMA was operational, the IBM SAGE. Just as today\u2019s DMA has, the SAGE\nhad address counters that performed block transfers in parallel with CPU\noperations.\nThe early IBM 360s pioneered many of the ideas that we use in I/O systems\ntoday. The 360 was the first commercial machine to make heavy use of DMA,\nand it introduced the notion of I/O programs that could be interpreted by the device.\nChaining of I/O programs was an important feature. The concept of channels intro-\nduced in the 360 corresponds to the I/O bus of today.\nMyer and Sutherland [1968] wrote a classic paper on the trade-off of complex-\nity and performance in I/O controllers. Borrowing the religious concept of the\n\u201cwheel of reincarnation,\u201d they eventually noticed they were caught in a loop\nof continuously increasing the power of an I/O processor until it needed its own\nsimpler coprocessor. Their quote in Appendix D captures their cautionary tale.\nThe IBM mainframe I/O channels, with their I/O processors, can be thought of\nas an inspiration for Infiniband, with their processors on their Host Channel\nAdaptor cards.\nReferences\nAnderson, D. [2003]. \u201cYou don\u2019t know jack about disks,\u201d Queue 1:4 (June),\n20\u201330.\nAnderson, D., J. Dykes, and E. Riedel [2003]. \u201cSCSI vs. ATA\u2014more than an\ninterface,\u201d Proc. 2nd USENIX Conf. on File and Storage Technology (FAST\n\u201903), March 31\u2013April 2, 2003, San Francisco.\nAnderson, M. H. [1990]. \u201cStrength (and safety) in numbers (RAID, disk storage\ntechnology),\u201d Byte 15:13 (December), 337\u2013339.\nAnon. et al. [1985]. A Measure of Transaction Processing Power, Tandem Tech.\nRep. TR 85.2. Also appeared in Datamation, 31:7 (April), 112\u2013118.\nBashe, C. J., W. Buchholz, G. V. Hawkins, J. L. Ingram, and N. Rochester [1981].\n\u201cThe architecture of IBM\u2019s early computers,\u201d IBM J. Research and Develop-\nment 25:5 (September), 363\u2013375.\nBashe, C. J., L. R. Johnson, J. H. Palmer, and E. W. Pugh [1986]. IBM\u2019s Early\nComputers, MIT Press, Cambridge, Mass.\nBlaum, M., J. Brady, J. Bruck, and J. Menon [1994]. \u201cEVENODD: An optimal\nscheme for tolerating double disk failures in RAID architectures,\u201d Proc. 21st\nAnnual Int\u2019l. Symposium on Computer Architecture (ISCA), April 18\u201321,\n1994, Chicago, 245\u2013254.\nBlaum, M., J. Brady, J. Bruck, and J. Menon [1995]. \u201cEVENODD: An optimal\nscheme for tolerating double disk failures in RAID architectures,\u201d IEEE Trans.\non Computers 44:2 (February), 192\u2013202.\nM.10\nThe History of Magnetic Storage, RAID, and I/O Buses\n\u25a0\nM-89"
    },
    {
        "page": 1437,
        "text": "Blaum, M., J. Brady, J., Bruck, J. Menon, and A. Vardy [2001]. \u201cThe EVENODD\ncode and its generalization,\u201d in H. Jin, T. Cortes, and R. Buyya, eds., High Per-\nformance Mass Storage and Parallel I/O: Technologies and Applications, IEEE\n& Wiley Press, New York, 187\u2013208.\nBlaum, M., J. Bruck, and A. Vardy [1996]. \u201cMDS array codes with independent\nparity symbols,\u201d IEEE Trans. on Information Theory, IT-42 (March),\n529\u2013542.\nBrady, J. T. [1986]. \u201cA theory of productivity in the creative process,\u201d IEEE\nCG&A (May), 25\u201334.\nBrown, A., and D. A. Patterson [2000]. \u201cTowards maintainability, availability, and\ngrowth benchmarks: A case study of software RAID systems.\u201d Proc. 2000 USE-\nNIX Annual Technical Conf., June 18\u201323, San Diego, Calif.\nBucher, I. V., and A. H. Hayes [1980]. \u201cI/O performance measurement on Cray-1\nand CDC 7000 computers,\u201d Proc. Computer Performance Evaluation Users\nGroup, 16th Meeting, October 20\u201323, 1980, Orlando, Fl., 245\u2013254.\nChen, P. M., G. A. Gibson, R. H. Katz, and D. A. Patterson [1990]. \u201cAn evaluation\nof redundant arrays of inexpensive disks using an Amdahl 5890,\u201d Proc. ACM\nSIGMETRICS Conf. on Measurement and Modeling of Computer Systems, May\n22\u201325, 1990, Boulder, Colo.\nChen, P. M., and E. K. Lee [1995]. \u201cStriping in a RAID level 5 disk array,\u201d Proc.\nACM SIGMETRICS Conf. on Measurement and Modeling of Computer\nSystems, May 15\u201319, 1995, Ottawa, Canada, 136\u2013145.\nChen, P. M., E. K. Lee, G. A. Gibson, R. H. Katz, and D. A. Patterson [1994].\n\u201cRAID: High-performance, reliable secondary storage,\u201d ACM Computing Sur-\nveys 26:2 (June), 145\u2013188.\nCorbett, P., B. English, A. Goel, T. Grcanac, S. Kleiman, J. Leong, and S. Sankar\n[2004]. \u201cRow-diagonal parity for double disk failure correction,\u201d Proc. 3rd\nUSENIX Conf. on File and Storage Technology (FAST \u201904), March 31\u2013April\n2, 2004, San Francisco.\nDenehy, T. E., J. Bent, F. I. Popovici, A. C. Arpaci-Dusseau, and R. H. Arpaci-\nDusseau [2004]. \u201cDeconstructing storage arrays,\u201d Proc. 11th Int\u2019l. Conf. on\nArchitectural Support for Programming Languages and Operating Systems\n(ASPLOS), October 7\u201313, 2004, Boston, Mass., 59\u201371.\nDoherty, W. J., and R. P. Kelisky [1979]. \u201cManaging VM/CMS systems for user\neffectiveness,\u201d IBM Systems J. 18:1, 143\u2013166.\nDouceur, J. R., and W. J. Bolosky [1999]. \u201cA large scale study of file-system con-\ntents,\u201d Proc. ACM SIGMETRICS Conf. on Measurement and Modeling of Com-\nputer Systems, May 1\u20139, 1999, Atlanta, Ga., 59\u201369.\nEnriquez, P. [2001]. \u201cWhat happened to my dial tone? A study of FCC service\ndisruption reports,\u201d poster, Richard Tapia Symposium on the Celebration of\nDiversity in Computing, October 18\u201320, 2001, Houston, Tex.\nFriesenborg, S. E., and R. J. Wicks [1985]. DASD Expectations: The 3380, 3380-\n23, and MVS/XA, Tech. Bulletin GG22-9363-02, IBM Washington Systems\nCenter, Gaithersburg, Md.\nM-90\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1438,
        "text": "Gibson, G. A. [1992]. Redundant Disk Arrays: Reliable, Parallel Secondary Stor-\nage, ACM Distinguished Dissertation Series, MIT Press, Cambridge, Mass.\nGoldstein, S. [1987]. Storage Performance\u2014An Eight Year Outlook, Tech. Rep.\nTR 03.308-1, IBM Santa Teresa Laboratory, San Jose, Calif.\nGray, J. [1990]. \u201cA census of Tandem system availability between 1985 and\n1990,\u201d IEEE Trans. on Reliability, 39:4 (October), 409\u2013418.\nGray, J. (ed.) [1993]. The Benchmark Handbook for Database and Transaction\nProcessing Systems, 2nd ed., Morgan Kaufmann, San Francisco.\nGray, J., and A. Reuter [1993]. Transaction Processing: Concepts and Tech-\nniques, Morgan Kaufmann, San Francisco.\nGray, J., and D. P. Siewiorek [1991]. \u201cHigh-availability computer systems.\u201d Com-\nputer 24:9 (September), 39\u201348.\nGray, J., and C. van Ingen [2005]. Empirical Measurements of Disk Failure Rates\nand Error Rates,\u201d MSR-TR-2005-166, Microsoft Research, Redmond, Wash.\nGurumurthi, S., A. Sivasubramaniam, and V. Natarajan [2005]. Disk Drive Road-\nmap from the Thermal Perspective: A Case for Dynamic Thermal Management,\nProceedings of the International Symposium on Computer Architecture (ISCA),\nJune, 38\u201349.\nHenly, M., and B. McNutt [1989]. DASD I/O Characteristics: A Comparison of\nMVS to VM, Tech. Rep. TR 02.1550, IBM General Products Division, San\nJose, Calif.\nHewlett-Packard. [1998]. \u201cHP\u2019s \u20185NINES:5MINUTES\u2019 vision extends leadership\nand re-defines high availability in mission-critical environments,\u201d February 10,\nwww.future.enterprisecomputing.hp.com/ia64/news/5nines_vision_pr.html.\nHoagland, A. S. [1963]. Digital Magnetic Recording, Wiley, New York.\nHospodor, A. D., and A. S. Hoagland [1993]. \u201cThe changing nature of disk con-\ntrollers.\u201d Proc. IEEE 81:4 (April), 586\u2013594.\nIBM. [1982]. The Economic Value of Rapid Response Time, GE20-0752-0, IBM,\nWhite Plains, N.Y., 11\u201382.\nImprimis. [1989]. Imprimis Product Specification, 97209 Sabre Disk Drive IPI-2\nInterface 1.2 GB, Document No. 64402302, Imprimis, Dallas, Tex.\nJain, R. [1991]. The Art of Computer Systems Performance Analysis: Techniques for\nExperimental Design,Measurement, Simulation,andModeling, Wiley, New York.\nKatz, R. H., D. A. Patterson, and G. A. Gibson [1989]. \u201cDisk system architectures\nfor high performance computing,\u201d Proc. IEEE 77:12 (December), 1842\u20131858.\nKim, M. Y. [1986]. \u201cSynchronized disk interleaving,\u201d IEEE Trans. on Computers\nC-35:11 (November), 978\u2013988.\nKuhn, D. R. [1997]. \u201cSources of failure in the public switched telephone network,\u201d\nIEEE Computer 30:4 (April), 31\u201336.\nLambright, D. [2000]. \u201cExperiences in measuring the reliability of a cache-based\nstorage system,\u201d Proc. of First Workshop on Industrial Experiences with Sys-\ntems Software (WIESS 2000), Co-Located with the 4th Symposium on Operating\nSystems Design and Implementation (OSDI), October 22, 2000, San\nDiego, Calif.\nM.10\nThe History of Magnetic Storage, RAID, and I/O Buses\n\u25a0\nM-91"
    },
    {
        "page": 1439,
        "text": "Laprie, J.-C. [1985]. \u201cDependable computing and fault tolerance: Concepts and\nterminology,\u201d Proc. 15th Annual Int\u2019l. Symposium on Fault-Tolerant Comput-\ning, June 19\u201321, 1985, Ann Arbor, Mich., 2\u201311.\nLazowska, E. D., J. Zahorjan, G. S. Graham, and K. C. Sevcik [1984]. Quantitative\nSystem Performance: Computer System Analysis Using Queueing Network\nModels, Prentice Hall, Englewood Cliffs, N.J. (Although out of print, it is avail-\nable online at www.cs.washington.edu/homes/lazowska/qsp/.)\nLeiner, A. L. [1954]. \u201cSystem specifications for the DYSEAC,\u201d J. ACM 1:2\n(April), 57\u201381.\nLeiner, A. L., and S. N. Alexander [1954]. \u201cSystem organization of the DYSEAC,\u201d\nIRE Trans. of Electronic Computers EC-3:1 (March), 1\u201310.\nMaberly, N. C. [1966]. Mastering Speed Reading, New American Library,\nNew York.\nMajor, J. B. [1989]. \u201cAre queuing models within the grasp of the unwashed?\u201d\nProc. Int\u2019l. Conf. on Management and Performance Evaluation of Computer\nSystems, December 11\u201315, 1989, Reno, Nev., 831\u2013839.\nMueller, M., L. C. Alves, W. Fischer, M. L. Fair, and I. Modi [1999]. \u201cRAS strat-\negy for IBM S/390 G5 and G6,\u201d IBM J. Research and Development, 43:5\u20136\n(September\u2013November), 875\u2013888.\nMurphy, B., and T. Gent [1995]. \u201cMeasuring system and software reliability using\nan automated data collection process,\u201d Quality and Reliability Engineering\nInternational, 11:5 (September\u2013October), 341\u2013353.\nMyer, T. H., and I. E. Sutherland [1968]. \u201cOn the design of display processors,\u201d\nCommunications of the ACM, 11:6 (June), 410\u2013414.\nNational Storage Industry Consortium. [1998]. \u201cTape Roadmap,\u201d www.nsic.org.\nNelson, V. P. [1990]. \u201cFault-tolerant computing: Fundamental concepts,\u201d Com-\nputer 23:7 (July), 19\u201325.\nNyberg, C. R., T. Barclay, Z. Cvetanovic, J. Gray, and D. Lomet [1994]. \u201cAlpha-\nSort: A RISC machine sort,\u201d Proc. ACM SIGMOD, May 24\u201327, 1994,\nMinneapolis, Minn.\nOkada, S., S. Okada, Y. Matsuda, T. Yamada, and A. Kobayashi [1999].\n\u201cSystem on a chip for digital still camera,\u201d IEEE Trans. on Consumer Electron-\nics 45:3 (August), 584\u2013590.\nPatterson, D. A., G. A. Gibson, and R. H. Katz [1987]. A Case for Redundant\nArrays of Inexpensive Disks (RAID), Tech. Rep. UCB/CSD 87/391, University\nof California, Berkeley. Also appeared in Proc. ACM SIGMOD, June 1\u20133, 1988,\nChicago, 109\u2013116.\nPavan, P., R. Bez, P. Olivo, and E. Zanoni [1997]. \u201cFlash memory cells\u2014an over-\nview,\u201d Proc. IEEE 85:8 (August), 1248\u20131271.\nRobinson, B., and L. Blount [1986]. The VM/HPO 3880-23 Performance Results,\nIBM Tech. Bulletin GG66-0247-00, IBM Washington Systems Center,\nGaithersburg, Md.\nSalem, K., and H. Garcia-Molina [1986]. \u201cDisk striping,\u201d Proc. 2nd Int\u2019l. IEEE\nConf. on Data Engineering, February 5\u20137, 1986, Washington, D.C., 249\u2013259.\nM-92\n\u25a0\nAppendix M Historical Perspectives and References"
    },
    {
        "page": 1440,
        "text": "Scranton, R. A., D. A. Thompson, and D. W. Hunter [1983]. The Access Time\nMyth, Tech. Rep. RC 10197 (45223), IBM, Yorktown Heights, N.Y.\nSeagate. [2000]. Seagate Cheetah 73 Family: ST173404LW/LWV/LC/LCV Prod-\nuct Manual, Vol. 1, Seagate, Scotts Valley, Calif. (www.seagate.com/support/\ndisc/manuals/scsi/29478b.pdf).\nSmotherman, M. [1989]. \u201cA sequencing-based taxonomy of I/O systems and\nreview of historical machines,\u201d Computer Architecture News 17:5 (September),\n5\u201315. Reprinted in Computer Architecture Readings, M. D. Hill, N. P. Jouppi,\nand G. S. Sohi, eds., Morgan Kaufmann, San Francisco, 1999, 451\u2013461.\nTalagala, N. [2000]. \u201cCharacterizing Large Storage Systems: Error Behavior and\nPerformance Benchmarks,\u201d Ph.D. dissertation, Computer Science Division,\nUniversity of California, Berkeley.\nTalagala, N., and D. Patterson [1999]. An Analysis of Error Behavior in a Large\nStorage System, Tech. Report UCB//CSD-99-1042, Computer Science Divi-\nsion, University of California, Berkeley.\nTalagala, N., R. Arpaci-Dusseau, and D. Patterson [2000]. Micro-Benchmark\nBased Extraction of Local and Global Disk Characteristics, CSD-99-1063,\nComputer Science Division, University of California, Berkeley.\nTalagala, N., S. Asami, D. Patterson, R. Futernick, and D. Hart [2000]. \u201cThe art of\nmassive storage: A case study of a Web image archive,\u201d IEEE Computer\n(November), 22\u201328.\nThadhani, A. J. [1981]. \u201cInteractive user productivity,\u201d IBM Systems J. 20:4, 407\u2013\n423.\nM.10\nThe History of Magnetic Storage, RAID, and I/O Buses\n\u25a0\nM-93"
    },
    {
        "page": 1441,
        "text": "References\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S.,\nIrving, G., Isard, M., Kudlur, M., 2016. TensorFlow: A System for Large-Scale\nMachine Learning. In: OSDI (November), vol. 16, pp. 265\u2013283.\nAdolf, R., Rama, S., Reagen, B., Wei, G.Y., Brooks, D., 2016. Fathom: reference workloads\nfor modern deep learning methods. In: IEEE International Symposium on Workload\nCharacterization (IISWC).\nAdve, S.V., Gharachorloo, K., 1996. Shared memory consistency models: a tutorial. IEEE\nComput. 29 (12), 66\u201376.\nAdve, S.V., Hill, M.D., 1990. Weak ordering: a new definition. In: Proceedings of 17th\nAnnual International Symposium on Computer Architecture (ISCA), May 28\u201331,\n1990, Seattle, Washington, pp. 2\u201314.\nAgarwal, A., 1987. Analysis of Cache Performance for Operating Systems and Multipro-\ngramming (Ph.D. thesis). Tech. Rep. No. CSL-TR-87-332. Stanford University, Palo\nAlto, CA.\nAgarwal, A., 1991. Limits on interconnection network performance. IEEE Trans. Parallel\nDistrib. Syst. 2 (4), 398\u2013412.\nAgarwal, A., Pudar, S.D., 1993. Column-associative caches: a technique for reducing the\nmiss rate of direct-mapped caches. In: 20th Annual International Symposium on Com-\nputer Architecture (ISCA), May 16\u201319, 1993, San Diego, California. Also appears in\nComputer Architecture News 21:2 (May), 179\u2013190, 1993.\nAgarwal, A., Hennessy, J.L., Simoni, R., Horowitz, M.A., 1988. An evaluation of directory\nschemes for cache coherence. In: Proceedings of 15th International Symposium on\nComputer Architecture (June), pp. 280\u2013289.\nAgarwal, A., Kubiatowicz, J., Kranz, D., Lim, B.-H., Yeung, D., D\u2019Souza, G., Parkin, M.,\n1993. Sparcle: an evolutionary processor design for large-scale multiprocessors. IEEE\nMicro 13, 48\u201361.\nAgarwal, A., Bianchini, R., Chaiken, D., Johnson, K., Kranz, D., 1995. The MIT Alewife\nmachine: architecture and performance. In: International Symposium on Computer\nArchitecture (Denver, CO), June, 2\u201313.\nAgerwala, T., Cocke, J., 1987. High Performance Reduced Instruction Set Processors. IBM\nTech. Rep. RC12434, IBM, Armonk, NY.\nAkeley, K., Jermoluk, T., 1988. High-performance polygon rendering. In: Proceedings of\n15th Annual Conference on Computer Graphics and Interactive Techniques\n(SIGGRAPH 1988), August 1\u20135, 1988, Atlanta, GA, pp. 239\u2013246.\nAlexander, W.G., Wortman, D.B., 1975. Static and dynamic characteristics of XPL\nprograms. IEEE Comput. 8 (11), 41\u201346.\nAlles, A., 1995. ATM Internetworking. White Paper (May). Cisco Systems, Inc., San Jose,\nCA. www.cisco.com/warp/public/614/12.html.\nAlliant, 1987. Alliant FX/Series: Product Summary. Alliant Computer Systems Corp,\nActon, MA.\nAlmasi, G.S., Gottlieb, A., 1989. Highly Parallel Computing. Benjamin/Cummings,\nRedwood City, CA.\nR-1"
    },
    {
        "page": 1442,
        "text": "Alverson, G., Alverson, R., Callahan, D., Koblenz, B., Porterfield, A., Smith, B., 1992.\nExploiting\nheterogeneous\nparallelism\non\na\nmultithreaded\nmultiprocessor.\nIn: Proceedings of ACM/IEEE Conference on Supercomputing, November 16\u201320,\n1992, Minneapolis, MN, pp. 188\u2013197.\nAmdahl, G.M., 1967. Validity of the single processor approach to achieving large scale\ncomputing capabilities. In: Proceedings of AFIPS Spring Joint Computer Conference,\nApril 18\u201320, 1967, Atlantic City, NJ, pp. 483\u2013485.\nAmdahl, G.M., Blaauw, G.A., Brooks Jr., F.P., 1964. Architecture of the IBM System 360.\nIBM J. Res. Dev. 8 (2), 87\u2013101.\nAmodei, D., Ananthanarayanan, S., Anubhai, R., Bai, J., Battenberg, E., Case, C., Casper,\nJ., Catanzaro, B., Cheng, Q., Chen, G., Chen, J., 2016. Deep speech 2: End-\nto-end speech recognition in english and mandarin. In: International Conference on\nMachine Learning (June), pp. 173\u2013182.\nAmza, C., Cox, A.L., Dwarkadas, S., Keleher, P., Lu, H., Rajamony, R., Yu, W.,\nZwaenepoel, W., 1996. Treadmarks: shared memory computing on networks of work-\nstations. IEEE Comput. 29 (2), 18\u201328.\nAnderson, M.H., 1990. Strength (and safety) in numbers (RAID, disk storage technology).\nByte 15 (13), 337\u2013339.\nAnderson, D., 2003. You don\u2019t know jack about disks. Queue 1 (4), 20\u201330.\nAnderson, D.W., Sparacio, F.J., Tomasulo, R.M., 1967. The IBM 360 Model 91: processor\nphilosophy and instruction handling. IBM J. Res. Dev. 11 (1), 8\u201324.\nAnderson, T.E., Culler, D.E., Patterson, D., 1995. A case for NOW (networks of worksta-\ntions). IEEE Micro 15 (1), 54\u201364.\nAnderson, D., Dykes, J., Riedel, E., 2003. SCSI vs. ATA\u2014more than an interface.\nIn: Proceedings of 2nd USENIX Conference on File and Storage Technology\n(FAST\u201903), March 31\u2013April 2.\nAng, B., Chiou, D., Rosenband, D., Ehrlich, M., Rudolph, L., Arvind, A., 1998. StarT-\nVoyager: a flexible platform for exploring scalable SMP issues. In: Proceedings of\nACM/IEEE Conference on Supercomputing, November 7\u201313, 1998, Orlando, FL.\nAnjan, K.V., Pinkston, T.M., 1995. An efficient, fully-adaptive deadlock recovery scheme:\nDisha. In: Proceedings of 22nd Annual International Symposium on Computer Archi-\ntecture (ISCA), June 22\u201324, 1995, Santa Margherita, Italy.\nAnon. et al., 1985. A Measure of Transaction Processing Power. Tandem Tech. Rep.\nTR85.2. Also appears in Datamation 31:7 (April), 112\u2013118, 1985.\nApache Hadoop, 2011. http://hadoop.apache.org.\nArchibald, J., Baer, J.-L., 1986. Cache coherence protocols: evaluation using a multiproces-\nsor simulation model. ACM Trans. Comput. Syst. 4 (4), 273\u2013298.\nArmbrust, M., Fox, A., Griffith, R., Joseph, A.D., Katz, R., Konwinski, A., Lee, G., Patter-\nson, D., Rabkin, A., Stoica, I., Zaharia, M., 2009. Above the Clouds: A Berkeley View\nof Cloud Computing, Tech. Rep. UCB/EECS-2009-28, University of California, Berke-\nley. http://www.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-28.html.\nArmbrust, M., Fox, A., Griffith, R., Joseph, A.D., Katz, R., Konwinski, A., Lee, G.,\nPatterson, D., Rabkin, A., Stoica, I., Zaharia, M., 2010. A view of cloud computing.\nCommun. ACM. 53 (4), 50\u201358.\nArpaci, R.H., Culler, D.E., Krishnamurthy, A., Steinberg, S.G., Yelick, K., 1995. Empirical\nevaluation of the CRAY-T3D: a compiler perspective. In: 22nd Annual International\nSymposium on Computer Architecture (ISCA), June 22\u201324, 1995, Santa Margherita,\nItaly.\nAsanovic, K., 1998. Vector Microprocessors (Ph.D. thesis). Computer Science Division,\nUniversity of California, Berkeley.\nAsanovi\u0001c, K., 2002. Programmable neurocomputing. In: Arbib, M.A. (Ed.), The Handbook\nof Brain Theory and Neural Networks, second ed. MIT Press, Cambridge, MA. ISBN:\n0-262-01197-2. https://people.eecs.berkeley.edu/\u0001krste/papers/neurocomputing.pdf.\nR-2\n\u25a0\nReferences"
    },
    {
        "page": 1443,
        "text": "Asanovi\u0001c, K., Beck, A., Johnson, J., Wawrzynek, J., Kingsbury, B., Morgan, N., 1998.\nTraining neural networks with Spert-II. In: Sundararajan, N., Saratchandran, P.\n(Eds.), Parallel Architectures for Artificial Networks: Paradigms and Implementations.\nIEEE Computer Society Press, California, USA. ISBN: 0-8186-8399-6. (Chapter 11)\nhttps://people.eecs.berkeley.edu/\u0001krste/papers/annbook.pdf.\nAssociated Press, 2005. Gap Inc. shuts down two Internet stores for major overhaul. USA-\nTODAY.com, August 8, 2005.\nAtanasoff, J.V., 1940. Computing Machine for the Solution of Large Systems of Linear\nEquations. Internal Report. Iowa State University, Ames.\nAtkins, M., 1991. Performance and the i860 microprocessor. IEEE Micro 11 (5), 24\u201327.\n72\u201378.\nAustin, T.M., Sohi, G., 1992. Dynamic dependency analysis of ordinary programs.\nIn: Proceedings of 19th Annual International Symposium on Computer Architecture\n(ISCA), May 19\u201321, 1992, Gold Coast, Australia, pp. 342\u2013351.\nAzizi, O., Mahesri, A., Lee, B.C., Patel, S.J., Horowitz, M., 2010. Energy-performance\ntradeoffs in processor architecture and circuit design: a marginal cost analysis.\nIn: Proceedings of the International Symposium on Computer Architecture, pp. 26\u201336.\nBabbay, F., Mendelson, A., 1998. Using value prediction to increase the power of specu-\nlative execution hardware. ACM Trans. Comput. Syst. 16 (3), 234\u2013270.\nBachrach, J., Vo, H., Richards, B., Lee, Y., Waterman, A., Avi\u017eienis, R., Wawrzynek, J.,\nAsanovi\u0001c, K., 2012. Chisel: constructing hardware in a Scala embedded language.\nIn: Proceedings of the 49th Annual Design Automation Conference, pp. 1216\u20131225.\nBaer, J.-L., Wang, W.-H., 1988. On the inclusion property for multi-level cache hierarchies.\nIn: Proceedings of 15th Annual International Symposium on Computer Architecture,\nMay 30\u2013June 2, 1988, Honolulu, Hawaii, pp. 73\u201380.\nBailey, D.H., Barszcz, E., Barton, J.T., Browning, D.S., Carter, R.L., Dagum, L.,\nFatoohi, R.A., Frederickson, P.O., Lasinski, T.A., Schreiber, R.S., Simon, H.D.,\nVenkatakrishnan, V., Weeratunga, S.K., 1991. The NAS parallel benchmarks. Int.\nJ. Supercomput. Appl. 5, 63\u201373.\nBakoglu, H.B., Grohoski, G.F., Thatcher, L.E., Kaeli, J.A., Moore, C.R., Tattle, D.P.,\nMale, W.E., Hardell, W.R., Hicks, D.A., Nguyen Phu, M., Montoye, R.K.,\nGlover, W.T., Dhawan, S., 1989. IBM second-generation RISC processor organization.\nIn: Proceedings of IEEE International Conference on Computer Design, September\n30\u2013October 4, 1989, Rye, NY, pp. 138\u2013142.\nBalakrishnan, H., Padmanabhan, V.N., Seshan, S., Katz, R.H., 1997. A comparison of\nmechanisms for improving TCP performance over wireless links. IEEE/ACM Trans.\nNetw. 5 (6), 756\u2013769.\nBall, T., Larus, J., 1993. Branch prediction for free. In: Proceedings of ACM SIGPLAN\u201993\nConference on Programming Language Design and Implementation (PLDI), June\n23\u201325, 1993, Albuquerque, NM, pp. 30 0\u2013313.\nBanerjee, U., 1979. Speedup of Ordinary Programs (Ph.D. thesis). Department of Computer\nScience, University of Illinois at Urbana-Champaign.\nBarham, P., Dragovic, B., Fraser, K., Hand, S., Harris, T., Ho, A., Neugebauer, R., 2003.\nXen and the art of virtualization. In: Proceedings of the 19th ACM Symposium on Oper-\nating Systems Principles, October 19\u201322, 2003, Bolton Landing, NY.\nBarnes, G.H., Brown, R.M., Kato, M., Kuck, D.J., Slotnick, D.L., Stokes, R., 1968. The\nILLIAC IV computer. IEEE Trans. Comput. 100 (8), 746\u2013757.\nBarroso, L.A., 2010. Warehouse scale computing [keynote address]. In: Proceedings of\nACM SIGMOD, June 8\u201310, 2010, Indianapolis, IN.\nBarroso, L.A., H\u20acolzle, U., 2007. The case for energy-proportional computing. IEEE\nComput. 40 (12), 33\u201337.\nBarroso, L.A., H\u20acolzle, U., 2009. The Datacenter as a Computer: An Introduction to the\nDesign of Warehouse-Scale Machines. Morgan & Claypool, San Rafael, CA.\nReferences\n\u25a0\nR-3"
    },
    {
        "page": 1444,
        "text": "Barroso, L.A., Gharachorloo, K., Bugnion, E., 1998. Memory system characterization of\ncommercial workloads. In: Proceedings of 25th Annual International Symposium on\nComputer Architecture (ISCA), July 3\u201314, 1998, Barcelona, Spain, pp. 3\u201314.\nBarroso, L.A., Clidaras, J., H\u20acolzle, U., 2013. The datacenter as a computer: An introduction to\nthe design of warehouse-scale machines. Synth. Lect. Comput. Architect. 8 (3), 1\u2013154.\nBarroso, L.A., Marty, M., Patterson, D., Ranganathan, P., 2017. Attack of the killer micro-\nseconds. Commun. ACM 56(2).\nBarton, R.S., 1961.Anew approachto thefunctional design of acomputer. In:Proceedings of\nWestern Joint Computer Conference, May 9\u201311, 1961, Los Angeles, CA, pp. 393\u2013396.\nBashe, C.J., Buchholz, W., Hawkins, G.V., Ingram, J.L., Rochester, N., 1981. The architec-\nture of IBM\u2019s early computers. IBM J. Res. Dev. 25 (5), 363\u2013375.\nBashe, C.J., Johnson, L.R., Palmer, J.H., Pugh, E.W., 1986. IBM\u2019s Early Computers. MIT\nPress, Cambridge, MA.\nBaskett, F., Keller, T.W., 1977. An evaluation of the Cray-1 processor. In: Kuck, D.J.,\nLawrie, D.H., Sameh, A.H. (Eds.), High Speed Computer and Algorithm Organization.\nAcademic Press, San Diego, pp. 71\u201384.\nBaskett, F., Jermoluk, T., Solomon, D., 1988. The 4D-MP graphics superworkstation:\nComputing + graphics \u00bc 40 MIPS + 40 MFLOPS and 10,000 lighted polygons per sec-\nond. In: Proceedings of IEEE COMPCON, February 29\u2013March 4, 1988, San Francisco,\npp. 468\u2013471.\nBBN Laboratories, 1986. Butterfly Parallel Processor Overview, Tech. Rep. 6148. BBN\nLaboratories, Cambridge, MA.\nBell, C.G., 1984. The mini and micro industries. IEEE Comput. 17 (10), 14\u201330.\nBell, C.G., 1985. Multis: a new class of multiprocessor computers. Science 228 (6), 462\u2013467.\nBell, C.G., 1989. The future of high performance computers in science and engineering.\nCommun. ACM 32 (9), 1091\u20131101.\nBell, G., Gray, J., 2001. Crays, Clusters and Centers, Tech. Rep. MSR-TR-2001-76. Micro-\nsoft Research, Redmond, WA.\nBell, C.G., Gray, J., 2002. What\u2019s next in high performance computing? CACM 45 (2),\n91\u201395.\nBell, C.G., Newell, A., 1971. Computer Structures: Readings and Examples. McGraw-Hill,\nNew York.\nBell, C.G., Strecker, W.D., 1976. Computer structures: what have we learned from the PDP-\n11? In: Third Annual International Symposium on Computer Architecture (ISCA), Jan-\nuary 19\u201321, 1976, Tampa, FL, pp. 1\u201314.\nBell, C.G., Strecker, W.D., 1998. Computer structures: what have we learned from the PDP-\n11? In: 25 Years of the International Symposia on Computer Architecture (Selected\nPapers), ACM, New York, pp. 138\u2013151.\nBell, C.G., Cady, R., McFarland, H., DeLagi, B., O\u2019Laughlin, J., Noonan, R., Wulf, W.,\n1970. A new architecture for mini-computers: The DEC PDP-11. In: Proceedings of\nAFIPS Spring Joint Computer Conference, May 5\u2013May 7, 1970, Atlantic City, NJ,\npp. 657\u2013675.\nBell, C.G., Mudge, J.C., McNamara, J.E., 1978. A DEC View of Computer Engineering.\nDigital Press, Bedford, MA.\nBenes, V.E., 1962. Rearrangeable three stage connecting networks. Bell Syst. Tech. J.\n41, 1481\u20131492.\nBertozzi, D., Jalabert, A., Murali, S., Tamhankar, R., Stergiou, S., Benini, L., De\nMicheli, G., 2005. NoC synthesis flow for customized domain specific multiprocessor\nsystems-on-chip. IEEE Trans. Parallel Distrib. Syst. 16 (2), 113\u2013130.\nBhandarkar, D.P., 1995. Alpha Architecture and Implementations. Digital Press, Newton,\nMA.\nR-4\n\u25a0\nReferences"
    },
    {
        "page": 1445,
        "text": "Bhandarkar, D.P., Clark, D.W., 1991. Performance from architecture: comparing a RISC\nand a CISC with similar hardware organizations. In: Proceedings of Fourth International\nConference on Architectural Support for Programming Languages and Operating\nSystems (ASPLOS), April 8\u201311, 1991, Palo Alto, CA, pp. 310\u2013319.\nBhandarkar, D.P., Ding, J., 1997. Performance characterization of the Pentium Pro proces-\nsor. In: Proceedings of Third International Symposium on High-Performance Computer\nArchitecture, February 1\u2013February 5, 1997, San Antonio, TX, pp. 288\u2013297.\nBhattacharya, S., Lane, N.D., 2016. Sparsification and separation of deep learning layers for\nconstrained resource inference on wearables. In: Proceedings of the 14th ACM Confer-\nence on Embedded Network Sensor Systems CD-ROM, pp. 176\u2013189.\nBhuyan, L.N., Agrawal, D.P., 1984. Generalized hypercube and hyperbus structures for a\ncomputer network. IEEE Trans. Comput. 32 (4), 322\u2013333.\nBienia, C., Kumar, S., Jaswinder, P.S., Li, K., 2008. The Parsec Benchmark Suite: Charac-\nterization and Architectural Implications, Tech. Rep. TR-811-08. Princeton University,\nPrinceton, NJ.\nBier, J., 1997. The evolution of DSP processors. In: Presentation at University of California,\nBerkeley, November 14.\nBird, S., Phansalkar, A., John, L.K., Mericas, A., Indukuru, R., 2007. Characterization of\nperformance of SPEC CPU benchmarks on Intel\u2019s Core Microarchitecture based pro-\ncessor. In: Proceedings of 2007 SPEC Benchmark Workshop, January 21, 2007,\nAustin, TX.\nBirman, M., Samuels, A., Chu, G., Chuk, T., Hu, L., McLeod, J., Barnes, J., 1990. Devel-\noping the WRL3170/3171 SPARC floating-point coprocessors. IEEE Micro 10 (1),\n55\u201364.\nBlackburn, M., Garner, R., Hoffman, C., Khan, A.M., McKinley, K.S., Bentzur, R.,\nDiwan, A., Feinberg, D., Frampton, D., Guyer, S.Z., Hirzel, M., Hosking, A.,\nJump, M., Lee, H., Moss, J.E.B., Phansalkar, A., Stefanovic, D., VanDrunen, T.,\nvon Dincklage, D., Wiedermann, B., 2006. The DaCapo benchmarks: Java benchmark-\ning development and analysis. In: ACM SIGPLAN Conference on Object-Oriented\nProgramming, Systems, Languages, and Applications (OOPSLA), October 22\u201326,\n2006, pp. 169\u2013190.\nBlaum, M., Brady, J., Bruck, J., Menon, J., 1994. EVENODD: an optimal scheme for tol-\nerating double disk failures in RAID architectures. In: Proceedings of 21st Annual Inter-\nnational Symposium on Computer Architecture (ISCA), April 18\u201321, 1994, Chicago,\nIL, pp. 245\u2013254.\nBlaum, M., Brady, J., Bruck, J., Menon, J., 1995. EVENODD: an optimal scheme for\ntolerating double disk failures in RAID architectures. IEEE Trans. Comput. 44 (2),\n192\u2013202.\nBlaum, M., Bruck, J., Vardy, A., 1996. MDS array codes with independent parity symbols.\nIEEE Trans. Inf. Theory 42, 529\u2013542.\nBlaum, M., Brady, J., Bruck, J., Menon, J., Vardy, A., 2001. The EVENODD code and its\ngeneralization. In: Jin, H., Cortes, T., Buyya, R. (Eds.), High Performance Mass Storage\nand Parallel I/O: Technologies and Applications. Wiley-IEEE, New York, pp. 187\u2013208.\nBloch, E., 1959. The engineering design of the Stretch computer. In: 1959 Proceedings of\nthe Eastern Joint Computer Conference, December 1\u20133, 1959, Boston, MA, pp. 48\u201359.\nBoddie, J.R., 2000. History of DSPs, www.lucent.com/micro/dsp/dsphist.html.\nBoggs, D., Baktha, A., Hawkins, J., Marr, D.T., Miller, J.A., Roussel, P., et al., 2004. The\nMicroarchitecture of the Intel Pentium 4 processor on 90 nm technology. Intel Technol.\nJ. 8 (1), 7\u201323.\nBolt, K.M., 2005. Amazon sees sales rise, profit fall. Seattle Post-Intelligencer. http://\nseattlepi.nwsource.com/business/245943_techearns26.html.\nReferences\n\u25a0\nR-5"
    },
    {
        "page": 1446,
        "text": "Bordawekar, R., Bondhugula, U., Rao, R., 2010. Believe it or not!: multi-core CPUs can\nmatch GPU performance for a FLOP-intensive application! In: 19th International\nConference on Parallel Architecture and Compilation Techniques (PACT 2010).\nVienna, Austria, September 11\u201315, 2010, pp. 537\u2013538.\nBorg, A., Kessler, R.E., Wall, D.W., 1990. Generation and analysis of very long address\ntraces. In: 19th Annual International Symposium on Computer Architecture (ISCA),\nMay 19\u201321, 1992, Gold Coast, Australia, pp. 270\u2013279.\nBouknight, W.J., Deneberg, S.A., McIntyre, D.E., Randall, J.M., Sameh, A.H.,\nSlotnick, D.L., 1972. The Illiac IV system. Proc. IEEE 60 (4), 369\u2013379. Also appears\nin Siewiorek, D.P., Bell, C.G., Newell, A. 1982. Computer Structures: Principles and\nExamples. McGraw-Hill, New York, pp. 306\u2013316.\nBrady, J.T., 1986. A theory of productivity in the creative process. IEEE Comput. Graph.\nAppl. 6 (5), 25\u201334.\nBrain, M., 2000. Inside a Digital Cell Phone. www.howstuffworks.com/-inside-cellphone.\nhtm.\nBrandt, M., Brooks, J., Cahir, M., Hewitt, T., Lopez-Pineda, E., Sandness, D., 2000. The\nBenchmarker\u2019s Guide for Cray SV1 Systems. Cray Inc., Seattle, WA.\nBrent, R.P., Kung, H.T., 1982. A regular layout for parallel adders. IEEE Trans. Comput.\nC-31, 260\u2013264.\nBrewer, E.A., Kuszmaul, B.C., 1994. How to get good performance from the CM-5 data\nnetwork. In: Proceedings of Eighth International Parallel Processing Symposium, April\n26\u201327, 1994, Cancun, Mexico.\nBrin, S., Page, L., 1998. The anatomy of a large-scale hypertextual Web search engine.\nIn: Proceedings of 7th International World Wide Web Conference, April 14\u201318,\n1998, Brisbane, Queensland, Australia, pp. 107\u2013117.\nBrown, A., Patterson, D.A., 2000. Towards maintainability, availability, and growth bench-\nmarks: a case study of software RAID systems. In: Proceedings of 2000 USENIX\nAnnual Technical Conference, June 18\u201323, 2000, San Diego, CA.\nBrunhaver, J.S., 2015. Design and optimization of a stencil engine (Ph.D. dissertation).\nStanford University.\nBucher, I.Y., 1983. The computational speed of supercomputers. In: Proceedings of Inter-\nnational Conference on Measuring and Modeling of Computer Systems (SIGMETRICS\n1983), August 29\u201331, 1983, Minneapolis, MN, pp. 151\u2013165.\nBucher, I.V., Hayes, A.H., 1980. I/O performance measurement on Cray-1 and CDC 7000\ncomputers. In: Proceedings of Computer Performance Evaluation Users Group, 16th\nMeeting, NBS 500-65, pp. 245\u2013254.\nBucholtz, W., 1962. Planning a Computer System: Project Stretch. McGraw-Hill,\nNew York.\nBurgess, N., Williams, T., 1995. Choices of operand truncation in the SRT division algo-\nrithm. IEEE Trans. Comput. 44 (7), 933\u2013938.\nBurkhardt III, H., Frank, S., Knobe, B., Rothnie, J., 1992. Overview of the KSR1\nComputer\nSystem,\nTech.\nRep.\nKSR-TR-9202001.\nKendall\nSquare\nResearch,\nBoston, MA.\nBurks, A.W., Goldstine, H.H., von Neumann, J., 1946. Preliminary discussion of the logical\ndesign of an electronic computing instrument. Report to the U.S. Army Ordnance\nDepartment, p. 1; also appears in Papers of John von Neumann, Aspray, W., Burks,\nA. (Eds.), MIT Press, Cambridge, MA, and Tomash Publishers, Los Angeles, CA,\n1987, pp. 97\u2013146.\nCalder, B., Grunwald, D., Jones, M., Lindsay, D., Martin, J., Mozer, M., Zorn, B., 1997.\nEvidence-based static branch prediction using machine learning. ACM Trans. Program.\nLang. Syst. 19 (1), 188\u2013222.\nR-6\n\u25a0\nReferences"
    },
    {
        "page": 1447,
        "text": "Calder, B., Reinman, G., Tullsen, D.M., 1999. Selective value prediction. In: Proceedings of\n26th Annual International Symposium on Computer Architecture (ISCA), May 2\u20134,\n1999, Atlanta, GA.\nCallahan, D., Dongarra, J., Levine, D., 1988. Vectorizing compilers: a test suite and results.\nIn: Proceedings of ACM/IEEE Conference on Supercomputing, November 12\u201317,\n1988, Orland, FL, pp. 98\u2013105.\nCanis, A., Choi, J., Aldham, M., Zhang, V., Kammoona, A., Czajkowski, T., Brown, S.D.,\nAnderson, J.H., 2013. LegUp: an open-source high-level synthesis tool for FPGA-based\nprocessor/accelerator systems. ACM Trans. Embed. Comput. Syst. 13(2).\nCanny, J., et al., 2015. Machine learning at the limit. In: IEEE International Conference on\nBig Data.\nCantin, J.F., Hill, M.D., 2001. Cache performance for selected SPEC CPU2000 bench-\nmarks. www.jfred.org/cache-data.html.\nCantin, J.F., Hill, M.D., 2003. Cache performance for SPEC CPU2000 benchmarks, version\n3.0. www.cs.wisc.edu/multifacet/misc/spec2000cache-data/index.html.\nCarles, S., 2005. Amazon reports record Xmas season, top game picks. Gamasutra,\nDecember 27. http://www.gamasutra.com/php-bin/news_index.php?story\u00bc7630.\nCarter, J., Rajamani, K., 2010. Designing energy-efficient servers and data centers. IEEE\nComput. 43 (7), 76\u201378.\nCase, R.P., Padegs, A., 1978. The architecture of the IBM System/370. Commun. ACM\n21 (1), 73\u201396. Also appears in Siewiorek, D.P., Bell, C.G., Newell, A., 1982. Computer\nStructures: Principles and Examples. McGraw-Hill, New York, pp. 830\u2013855.\nCaulfield, A.M., Chung, E.S., Putnam, A., Haselman, H.A.J.F.M., Humphrey, S.H.M.,\nDaniel, P.K.J.Y.K., Ovtcharov, L.T.M.K., Lanka, M.P.L.W.S., Burger, D.C.D.,\n2016. A cloud-scale acceleration architecture. In: MICRO Conference.\nCensier, L., Feautrier, P., 1978. A new solution to coherence problems in multicache\nsystems. IEEE Trans. Comput. C-27 (12), 1112\u20131118.\nChandra, R., Devine, S., Verghese, B., Gupta, A., Rosenblum, M., 1994. Scheduling and\npage migration for multiprocessor compute servers. In: Sixth International Conference\non Architectural Support for Programming Languages and Operating Systems\n(ASPLOS), October 4\u20137, 1994, San Jose, CA, pp. 12\u201324.\nChang, P.P., Mahlke, S.A., Chen, W.Y., Warter, N.J., Hwu, W.W., 1991. IMPACT: an\narchitectural framework for multiple-instruction-issue processors. In: 18th Annual\nInternational Symposium on Computer Architecture (ISCA), May 27\u201330, 1991,\nToronto, Canada, pp. 266\u2013275.\nChang, F., Dean, J., Ghemawat, S., Hsieh, W.C., Wallach, D.A., Burrows, M., Chandra, T.,\nFikes, A., Gruber, R.E., 2006. Bigtable: a distributed storage system for structured data.\nIn: Proceedings of 7th USENIX Symposium on Operating Systems Design and Imple-\nmentation (OSDI\u201906), November 6\u20138, 2006, Seattle, WA.\nChang, J., Meza, J., Ranganathan, P., Bash, C., Shah, A., 2010. Green server design: beyond\noperational energy to sustainability. In: Proceedings of Workshop on Power Aware Com-\nputing and Systems (HotPower\u201910), October 3, 2010, Vancouver, British Columbia.\nCharlesworth, A.E., 1981. An approach to scientific array processing: the architectural\ndesign of the AP-120B/FPS-164 family. Computer 9, 18\u201327.\nCharlesworth, A., 1998. Starfire: extending the SMP envelope. IEEE Micro 18 (1), 39\u201349.\nChen, T.C., 1980. Overlap and parallel processing. In: Stone, H. (Ed.), Introduction to\nComputer Architecture. Science Research Associates, Chicago, pp. 427\u2013486.\nChen, S., 1983. Large-scale and high-speed multiprocessor system for scientific applica-\ntions. In: Proceedings of NATO Advanced Research Workshop on High-Speed Com-\nputing, June 20\u201322, 1983, J\u20aculich, West Germany. Also appears in Hwang, K. (Ed.),\n1984. Superprocessors: design and applications, IEEE (August), 602\u2013609.\nReferences\n\u25a0\nR-7"
    },
    {
        "page": 1448,
        "text": "Chen, P.M., Lee, E.K., 1995. Striping in a RAID level 5 disk array. In: Proceedings of ACM\nSIGMETRICS Conference on Measurement and Modeling of Computer Systems, May\n15\u201319, 1995, Ottawa, Canada, pp. 136\u2013145.\nChen, P.M., Gibson, G.A., Katz, R.H., Patterson, D.A., 1990. An evaluation of redundant\narrays of inexpensive disks using an Amdahl 5890. In: Proceedings of ACM SIG-\nMETRICS Conference on Measurement and Modeling of Computer Systems, May\n22\u201325, 1990, Boulder, CO.\nChen, P.M., Lee, E.K., Gibson, G.A., Katz, R.H., Patterson, D.A., 1994. RAID: high-\nperformance, reliable secondary storage. ACM Comput. Surv. 26 (2), 145\u2013188.\nChow, F.C., 1983. A Portable Machine-Independent Global Optimizer\u2014Design and\nMeasurements (Ph.D. thesis). Stanford University, Palo Alto, CA.\nChrysos, G.Z., Emer, J.S., 1998. Memory dependence prediction using store sets.\nIn: Proceedings of 25th Annual International Symposium on Computer Architecture\n(ISCA), July 3\u201314, 1998, Barcelona, Spain, pp. 142\u2013153.\nClark, W.A., 1957. The Lincoln TX-2 computer development. In: Proceedings of Western\nJoint Computer Conference, February 26\u201328, 1957, Los Angeles, pp. 143\u2013145.\nClark, D.W., 1983. Cache performance of the VAX-11/780. ACM Trans. Comput. Syst.\n1 (1), 24\u201337.\nClark, D.W., 1987. Pipelining and performance in the VAX 8800 processor. In: Proceedings of\nSecond International Conference on Architectural Support for Programming Languages\nand Operating Systems (ASPLOS), October 5\u20138, 1987, Palo Alto, CA, pp. 173\u2013177.\nClark, J., 2014. Five Numbers That Illustrate the Mind-Bending Size of Amazon's Cloud.\nBloomberg.\nhttps://www.bloomberg.com/news/2014-11-14/5-numbersthat-illustrate-\nthe-mind-bending-size-of-amazon-s-cloud.html.\nClark, J., October 26, 2015. Google Turning Its Lucrative Web Search Over to AI Machines.\nBloomberg Technology, www.bloomberg.com.\nClark, D.W., Emer, J.S., 1985. Performance of the VAX-11/780 translation buffer: simula-\ntion and measurement. ACM Trans. Comput. Syst. 3 (1), 31\u201362.\nClark, D., Levy, H., 1982. Measurement and analysis of instruction set use in the VAX-11/\n780. In: Proceedings of Ninth Annual International Symposium on Computer Architec-\nture (ISCA), April 26\u201329, 1982, Austin, TX, pp. 9\u201317.\nClark, D., Strecker, W.D., 1980. Comments on \u2018the case for the reduced instruction set com-\nputer\u2019. Comput. Architect. News 8 (6), 34\u201338.\nClark, B., Deshane, T., Dow, E., Evanchik, S., Finlayson, M., Herne, J., Neefe Matthews, J.,\n2004. Xen and the art of repeated research. In: Proceedings of USENIX Annual Tech-\nnical Conference, June 27\u2013July 2, 2004, pp. 135\u2013144.\nClidaras, J., Johnson, C., Felderman, B., 2010. Private communication.\nClimate Savers Computing Initiative, 2007. Efficiency Specs. http://www.climatesavers\ncomputing.org/.\nClos, C., 1953. A study of non-blocking switching networks. Bell Syst. Tech. J. 32 (2),\n406\u2013424.\nCloud, Bloomberg, n.d. https://www.bloomberg.com/news/2014-11-14/5-numbersthat-\nillustrate-the-mind-bending-size-of-amazon-s-cloud.html.\nCody, W.J., Coonen, J.T., Gay, D.M., Hanson, K., Hough, D., Kahan, W., Karpinski, R.,\nPalmer, J., Ris, F.N., Stevenson, D., 1984. A proposed radix- and word-length indepen-\ndent standard for floating-point arithmetic. IEEE Micro 4 (4), 86\u2013100.\nColwell, R.P., Steck, R., 1995. A 0.6 \u03bcm BiCMOS processor with dynamic execution.\nIn: Proceedings of IEEE International Symposium on Solid State Circuits (ISSCC),\nFebruary 15\u201317, 1995, San Francisco, pp. 176\u2013177.\nColwell, R.P., Nix, R.P., O\u2019Donnel, J.J., Papworth, D.B., Rodman, P.K., 1987. A VLIW\narchitecture for a trace scheduling compiler. In: Proceedings of Second International\nR-8\n\u25a0\nReferences"
    },
    {
        "page": 1449,
        "text": "Conference on Architectural Support for Programming Languages and Operating\nSystems (ASPLOS), October 5\u20138, 1987, Palo Alto, CA, pp. 180\u2013192.\nComer, D., 1993. Internetworking with TCP/IP, second ed. Prentice Hall, Englewood\nCliffs, NJ.\nCompaq Computer Corporation, 1999. Compiler Writer\u2019s Guide for the Alpha 21264, Order\nNumber EC-RJ66A-TE, June, www1.support.compaq.com/alpha-tools/-documentation/\ncurrent/21264_EV67/ec-rj66a-te_comp_writ_gde_for_alpha21264.pdf.\nConti, C., Gibson, D.H., Pitkowsky, S.H., 1968. Structural aspects of the System/360 Model\n85. Part I. General organization. IBM Syst. J. 7 (1), 2\u201314.\nCoonen, J., 1984. Contributions to a Proposed Standard for Binary Floating-Point\nArithmetic (Ph.D. thesis). University of California, Berkeley.\nCorbett, P., English, B., Goel, A., Grcanac, T., Kleiman, S., Leong, J., Sankar, S., 2004.\nRow-diagonal parity for double disk failure correction. In: Proceedings of 3rd USENIX\nConference on File and Storage Technology (FAST\u201904), March 31\u2013April 2, 2004, San\nFrancisco.\nCrawford, J., Gelsinger, P., 1988. Programming the 80386. Sybex Books, Alameda, CA.\nCuller, D.E., Singh, J.P., Gupta, A., 1999. Parallel Computer Architecture: A Hardware/\nSoftware Approach. Morgan Kaufmann, San Francisco.\nCurnow, H.J., Wichmann, B.A., 1976. A synthetic benchmark. Comput. J. 19 (1), 43\u201349.\nCvetanovic, Z., Kessler, R.E., 2000. Performance analysis of the Alpha 21264-based\nCompaq ES40 system. In: Proceedings of 27th Annual International Symposium on\nComputer Architecture (ISCA), June 10\u201314, 2000, Vancouver, Canada, pp. 192\u2013202.\nDally, W.J., 1990. Performance analysis of k-ary n-cube interconnection networks. IEEE\nTrans. Comput. 39 (6), 775\u2013785.\nDally, W.J., 1992. Virtual channel flow control. IEEE Trans. Parallel Distrib. Syst. 3 (2),\n194\u2013205.\nDally, W.J., 1999. Interconnect limited VLSI architecture. In: Proceedings of the Interna-\ntional Interconnect Technology Conference, May 24\u201326, 1999, San Francisco.\nDally, W.J., 2002. Computer architecture is all about interconnect. In: Proceedings of the 8th\nInternational Symposium High Performance Computer Architecture.\nDally, W.J., 2016. High Performance Hardware for Machine Learning. Cadence Embedded\nNeural\nNetwork\nSummit,\nFebruary\n9,\n2016.\nhttp://ip.cadence.com/uploads/\npresentations/1000AM_Dally_Cadence_ENN.pdf.\nDally, W.J., Seitz, C.I., 1986. The torus routing chip. Distrib. Comput. 1 (4), 187\u2013196.\nDally, W.J., Towles, B., 2001. Route packets, not wires: on-chip interconnection networks.\nIn: Proceedings of 38th Design Automation Conference, June 18\u201322, 2001, Las Vegas.\nDally, W.J., Towles, B., 2003. Principles and Practices of Interconnection Networks.\nMorgan Kaufmann, San Francisco.\nDarcy, J.D., Gay, D., 1996. FLECKmarks: measuring floating point performance using a\nfull IEEE compliant arithmetic benchmark. CS 252 class project, University of\nCalifornia, Berkeley. See http.CS.Berkeley.EDU/\u0001darcy/Projects/cs252/.\nDarley, H.M., et al., 1989. Floating Point/Integer Processor with Divide and Square Root\nFunctions, U.S. Patent 4,878,190, October 31.\nDavidson, E.S., 1971. The design and control of pipelined function generators.\nIn: Proceedings of IEEE Conference on Systems, Networks, and Computers, January\n19\u201321, 1971, Oaxtepec, Mexico, pp. 19\u201321.\nDavidson, E.S., Thomas, A.T., Shar, L.E., Patel, J.H., 1975. Effective control for pipelined\nprocessors. In: Proceedings of IEEE COMPCON, February 25\u201327, 1975, San\nFrancisco, pp. 181\u2013184.\nDavie, B.S., Peterson, L.L., Clark, D., 1999. Computer Networks: A Systems Approach,\nsecond ed. Morgan Kaufmann, San Francisco.\nReferences\n\u25a0\nR-9"
    },
    {
        "page": 1450,
        "text": "Dean, J., 2009. Designs, lessons and advice from building large distributed systems [key-\nnote address]. In: Proceedings of 3rd ACM SIGOPS International Workshop on\nLarge-Scale Distributed Systems and Middleware, Co-located with the 22nd\nACM Symposium on Operating Systems Principles, October 11\u201314, 2009, Big\nSky, Mont.\nDean, J., Barroso, L.A., 2013. The tail at scale. Commun. ACM 56 (2), 74\u201380.\nDean, J., Ghemawat, S., 2004. MapReduce: simplified data processing on large clusters.\nIn: Proceedings of Operating Systems Design and Implementation (OSDI), December\n6\u20138, 2004, San Francisco, CA, pp. 137\u2013150.\nDean, J., Ghemawat, S., 2008. MapReduce: simplified data processing on large clusters.\nCommun. ACM 51 (1), 107\u2013113.\nDeCandia, G., Hastorun, D., Jampani, M., Kakulapati, G., Lakshman, A., Pilchin, A.,\nSivasubramanian, S., Vosshall, P., Vogels, W., 2007. Dynamo: Amazon\u2019s highly avail-\nable key-value store. In: Proceedings of 21st ACM Symposium on Operating Systems\nPrinciples, October 14\u201317, 2007, Stevenson, WA.\nDehnert, J.C., Hsu, P.Y.-T., Bratt, J.P., 1989. Overlapped loop support on the Cydra 5.\nIn: Proceedings of Third International Conference on Architectural Support for\nProgramming Languages and Operating Systems (ASPLOS), April 3\u20136, 1989, Boston,\nMA, pp. 26\u201339.\nDemmel, J.W., Li, X., 1994. Faster numerical algorithms via exception handling. IEEE\nTrans. Comput. 43 (8), 983\u2013992.\nDenehy, T.E., Bent, J., Popovici, F.I., Arpaci-Dusseau, A.C., Arpaci-Dusseau, R.H., 2004.\nDeconstructing storage arrays. In: Proceedings of 11th International Conference on\nArchitectural Support for Programming Languages and Operating Systems (ASPLOS),\nOctober 7\u201313, 2004, Boston, MA, pp. 59\u201371.\nDesurvire, E., 1992. Lightwave communications: the fifth generation. Sci. Am. (Int. Ed.)\n266 (1), 96\u2013103.\nDiep, T.A., Nelson, C., Shen, J.P., 1995. Performance evaluation of the PowerPC 620\nmicroarchitecture. In: Proceedings of 22nd Annual International Symposium on\nComputer Architecture (ISCA), June 22\u201324, 1995, Santa Margherita, Italy.\nDigital Semiconductor, 1996. Alpha Architecture Handbook, Version 3. Digital Press,\nMaynard, MA.\nDitzel, D.R., McLellan, H.R., 1987. Branch folding in the CRISP microprocessor: reducing\nthe branch delay to zero. In: Proceedings of 14th Annual International Symposium on\nComputer Architecture (ISCA), June 2\u20135, 1987, Pittsburgh, PA, pp. 2\u20137.\nDitzel, D.R., Patterson, D.A., 1980. Retrospective on high-level language computer archi-\ntecture. In: Proceedings of Seventh Annual International Symposium on Computer\nArchitecture (ISCA), May 6\u20138, 1980, La Baule, France, pp. 97\u2013104.\nDoherty, W.J., Kelisky, R.P., 1979. Managing VM/CMS systems for user effectiveness.\nIBM Syst. J. 18 (1), 143\u2013166.\nDoherty, W.J., Thadhani, A.J., 1982. The economic value of rapid response time. IBM Report.\nDongarra, J.J., 1986. A survey of high performance processors. In: Proceedings of IEEE\nCOMPCON, March 3\u20136, 1986, San Francisco, pp. 8\u201311.\nDongarra, J.J., Luszczek, P., Petitet, A., 2003. The LINPACK benchmark: past, present and\nfuture. Concurr. Comput. Pract. Exp. 15 (9), 803\u2013820.\nDongarra, J., Sterling, T., Simon, H., Strohmaier, E., 2005. High-performance computing: clus-\nters, constellations, MPPs, and future directions. Comput. Sci. Eng. 7 (2), 51\u201359.\nDouceur, J.R., Bolosky, W.J., 1999. A large scale study of file-system contents.\nIn: Proceedings of ACM SIGMETRICS Conference on Measurement and Modeling\nof Computer Systems, May 1\u20139, 1999, Atlanta, GA, pp. 59\u201369.\nDouglas, J., 2005. Intel 8xx series and Paxville Xeon-MP microprocessors. In: Paper\nPresented at Hot Chips 17, August 14\u201316, 2005, Stanford University, Palo Alto, CA.\nR-10\n\u25a0\nReferences"
    },
    {
        "page": 1451,
        "text": "Duato, J., 1993. A new theory of deadlock-free adaptive routing in wormhole networks.\nIEEE Trans. Parallel Distrib. Syst. 4 (12), 1320\u20131331.\nDuato, J., Pinkston, T.M., 2001. A general theory for deadlock-free adaptive routing using a\nmixed set of resources. IEEE Trans. Parallel Distrib. Syst. 12 (12), 1219\u20131235.\nDuato, J., Yalamanchili, S., Ni, L., 2003. Interconnection Networks: An Engineering\nApproach, 2nd printing Morgan Kaufmann, San Francisco.\nDuato, J., Johnson, I., Flich, J., Naven, F., Garcia, P., Nachiondo, T., 2005a. A new scalable\nand cost-effective congestion management strategy for lossless multistage interconnec-\ntion networks. In: Proceedings of 11th International Symposium on High-Performance\nComputer Architecture, February 12\u201316, 2005, San Francisco.\nDuato, J., Lysne, O., Pang, R., Pinkston, T.M., 2005b. Part I: a theory for deadlock-free\ndynamic reconfiguration of interconnection networks. IEEE Trans. Parallel Distrib.\nSyst. 16 (5), 412\u2013427.\nDubois, M., Scheurich, C., Briggs, F., 1988. Synchronization, coherence, and event order-\ning. IEEE Comput. 21 (2), 9\u201321.\nDunigan, W., Vetter, K., White, K., Worley, P., 2005. Performance evaluation of the Cray\nX1 distributed shared memory architecture. IEEE Micro, 30\u201340.\nEden, A., Mudge, T., 1998. The YAGS branch prediction scheme. In: Proceedings of the\n31st Annual ACM/IEEE International Symposium on Microarchitecture, November\n30\u2013December 2, 1998, Dallas, TX, pp. 69\u201380.\nEdmondson, J.H., Rubinfield, P.I., Preston, R., Rajagopalan, V., 1995. Superscalar instruc-\ntion execution in the 21164 Alpha microprocessor. IEEE Micro 15 (2), 33\u201343.\nEggers, S., 1989. Simulation Analysis of Data Sharing in Shared Memory Multiprocessors\n(Ph.D. thesis). University of California, Berkeley.\nElder, J., Gottlieb, A., Kruskal, C.K., McAuliffe, K.P., Randolph, L., Snir, M., Teller, P.,\nWilson, J., 1985. Issues related to MIMD shared-memory computers: the NYU ultra-\ncomputer approach. In: Proceedings of 12th Annual International Symposium on Com-\nputer Architecture (ISCA), June 17\u201319, 1985, Boston, MA, pp. 126\u2013135.\nEllis, J.R., 1986. Bulldog: A Compiler for VLIW Architectures. MIT Press, Cambridge, MA.\nEmer, J.S., Clark, D.W., 1984. A characterization of processor performance in the VAX-11/\n780. In: Proceedings of 11th Annual International Symposium on Computer Architec-\nture (ISCA), June 5\u20137, 1984, Ann Arbor, MI, pp. 301\u2013310.\nEnriquez, P., 2001. What happened to my dial tone? A study of FCC service disruption\nreports. In: Poster, Richard Tapia Symposium on the Celebration of Diversity in\nComputing, October 18\u201320, Houston, TX.\nErlichson, A., Nuckolls, N., Chesson, G., Hennessy, J.L., 1996. SoftFLASH: analyzing the\nperformance of clustered distributed virtual shared memory. In: Proceedings of Seventh\nInternational Conference on Architectural Support for Programming Languages and\nOperating Systems (ASPLOS), October 1\u20135, 1996, Cambridge, MA, pp. 210\u2013220.\nEsmaeilzadeh, H., Cao, T., Xi, Y., Blackburn, S.M., McKinley, K.S., 2011. Looking back\non the language and hardware revolution: measured power, performance, and scaling.\nIn: Proceedings of 16th International Conference on Architectural Support for Program-\nming Languages and Operating Systems (ASPLOS), March 5\u201311, 2011, Newport\nBeach, CA.\nEsmaeilzadeh, H., Blem, E., St Amant, R., Sankaralingam, K., Burger, D., 2012. Power lim-\nitations and dark silicon challenge the future of multicore. ACM Trans. Comput. Syst.\n30 (3), 115\u2013138.\nEvers, M., Patel, S.J., Chappell, R.S., Patt, Y.N., 1998. An analysis of correlation and\npredictability: what makes two-level branch predictors work. In: Proceedings of 25th\nAnnual International Symposium on Computer Architecture (ISCA), July 3\u201314,\n1998, Barcelona, Spain, pp. 52\u201361.\nFabry, R.S., 1974. Capability based addressing. Commun. ACM 17 (7), 403\u2013412.\nReferences\n\u25a0\nR-11"
    },
    {
        "page": 1452,
        "text": "Falsafi, B., Wood, D.A., 1997. Reactive NUMA: a design for unifying S-COMA and CC-\nNUMA. In: Proceedings of 24th Annual International Symposium on Computer Archi-\ntecture (ISCA), June 2\u20134, 1997, Denver, CO, pp. 229\u2013240.\nFan, X., Weber, W., Barroso, L.A., 2007. Power provisioning for a warehouse-sized com-\nputer. In: Proceedings of 34th Annual International Symposium on Computer Architec-\nture (ISCA), June 9\u201313, 2007, San Diego, CA.\nFarkas, K.I., Jouppi, N.P., 1994. Complexity/performance trade-offs with non-blocking\nloads. In: Proceedings of 21st Annual International Symposium on Computer Architec-\nture (ISCA), April 18\u201321, 1994, Chicago.\nFarkas, K.I., Jouppi, N.P., Chow, P., 1995. How useful are non-blocking loads, stream\nbuffers and speculative execution in multiple issue processors? In: Proceedings of First\nIEEE Symposium on High-Performance Computer Architecture, January 22\u201325, 1995,\nRaleigh, NC, pp. 78\u201389.\nFarkas, K.I., Chow, P., Jouppi, N.P., Vranesic, Z., 1997. Memory-system design consider-\nations for dynamically-scheduled processors. In: Proceedings of 24th Annual Interna-\ntional Symposium on Computer Architecture (ISCA), June 2\u20134, 1997, Denver, CO,\npp. 133\u2013143.\nFazio, D., 1987. It\u2019s really much more fun building a supercomputer than it is simply invent-\ning one. In: Proceedings of IEEE COMPCON, February 23\u201327, 1987, San Francisco,\npp. 102\u2013105.\nFikes, A., 2010. Storage architecture and challenges. In: Google Faculty Summit.\nFisher, J.A., 1981. Trace scheduling: a technique for global microcode compaction. IEEE\nTrans. Comput. 30 (7), 478\u2013490.\nFisher, J.A., 1983. Very long instruction word architectures and ELI-512. In: 10th Annual\nInternational Symposium on Computer Architecture (ISCA), June 5\u20137, 1982, Stock-\nholm, Sweden, pp. 140\u2013150.\nFisher, J.A., Freudenberger, S.M., 1992. Predicting conditional branches from previous runs\nof a program. In: Proceedings of Fifth International Conference on Architectural Sup-\nport for Programming Languages and Operating Systems (ASPLOS), October 12\u201315,\n1992, Boston, MA, pp. 85\u201395.\nFisher, J.A., Rau, B.R., 1993. J. Supercomput., January (special issue).\nFisher, J.A., Ellis, J.R., Ruttenberg, J.C., Nicolau, A., 1984. Parallel processing: a smart\ncompiler and a dumb processor. In: Proceedings of SIGPLAN Conference on Compiler\nConstruction, June 17\u201322, 1984, Montreal, Canada, pp. 11\u201316.\nFlemming, P.J., Wallace, J.J., 1986. How not to lie with statistics: the correct way to sum-\nmarize benchmarks results. Commun. ACM 29 (3), 218\u2013221.\nFlynn, M.J., 1966. Very high-speed computing systems. Proc. IEEE 54 (12), 1901\u20131909.\nForgie, J.W., 1957. The Lincoln TX-2 input-output system. In: Proceedings of Western Joint\nComputerConference(February),InstituteofRadioEngineers,LosAngeles,pp.156\u2013160.\nFoster, C.C., Riseman, E.M., 1972. Percolation of code to enhance parallel dispatching and\nexecution. IEEE Trans. Comput. C-21 (12), 1411\u20131415.\nFrank, S.J., 1984. Tightly coupled multiprocessor systems speed memory access time. Elec-\ntronics 57 (1), 164\u2013169.\nFreescale as part of i.MX31 Applications Processor, 2006. http://cache.freescale.com/files/\n32bit/doc/white_paper/IMX31MULTIWP.pdf.\nFreiman, C.V., 1961. Statistical analysis of certain binary division algorithms. Proc. IRE\n49 (1), 91\u2013103.\nFriesenborg, S.E., Wicks, R.J., 1985. DASD Expectations: The 3380, 3380-23, and\nMVS/XA, Tech. Bulletin GG22-9363-02. IBM Washington Systems Center, Gaithers-\nburg, MD.\nR-12\n\u25a0\nReferences"
    },
    {
        "page": 1453,
        "text": "Fuller, S.H., Burr, W.E., 1977. Measurement and evaluation of alternative computer archi-\ntectures. Computer 10 (10), 24\u201335.\nFurber, S.B., 1996. ARM System Architecture. Addison-Wesley, Harlow, England. www.\ncs.man.ac.uk/amulet/publications/books/ARMsysArch.\nGagliardi, U.O., 1973. Report of workshop 4\u2014software-related advances in computer hard-\nware. In: Proceedings of Symposium on the High Cost of Software, September 17\u201319,\n1973, Monterey, CA, pp. 99\u2013120.\nGajski, D., Kuck, D., Lawrie, D., Sameh, A., 1983. CEDAR\u2014a large scale multiprocessor.\nIn: Proceedings of International Conference on Parallel Processing (ICPP), August,\nColumbus, Ohio, pp. 524\u2013529.\nGalal, S., Shacham, O., Brunhaver II, J.S., Pu, J., Vassiliev, A., Horowitz, M., 2013. FPU\ngenerator for design space exploration. In: 21st IEEE Symposium on Computer Arith-\nmetic (ARITH).\nGallagher, D.M., Chen, W.Y., Mahlke, S.A., Gyllenhaal, J.C., Hwu, W.W., 1994.\nDynamic memory disambiguation using the memory conflict buffer. In: Proceedings of\nSixth International Conference on Architectural Support for Programming Languages\nand Operating Systems (ASPLOS), October 4\u20137, Santa Jose, CA, pp. 183\u2013193.\nGalles, M., 1996. Scalable pipelined interconnect for distributed endpoint routing: the SGI\nSPIDER chip. In: Proceedings of IEEE HOT Interconnects\u201996, August 15\u201317, 1996,\nStanford University, Palo Alto, CA.\nGame, M., Booker, A., 1999. CodePack code compression for PowerPC processors. Micro-\nNews. 5 (1). www.chips.ibm.com/micronews/vol5_no1/codepack.html.\nGao, Q.S., 1993. The Chinese remainder theorem and the prime memory system. In: 20th\nAnnual International Symposium on Computer Architecture (ISCA), May 16\u201319, 1993,\nSan Diego, CA (Computer Architecture News 21:2 (May), pp. 337\u2013340.\nGap, 2005. Gap Inc. Reports Third Quarter Earnings. http://gapinc.com/public/documents/\nPR_Q405EarningsFeb2306.pdf.\nGap, 2006. Gap Inc. Reports Fourth Quarter and Full Year Earnings. http://-gapinc.com/\npublic/documents/Q32005PressRelease_Final22.pdf.\nGarner, R., Agarwal, A., Briggs, F., Brown, E., Hough, D., Joy, B., Kleiman, S.,\nMuchnick, S., Namjoo, M., Patterson, D., Pendleton, J., Tuck, R., 1988. Scalable pro-\ncessor architecture (SPARC). In: Proceedings of IEEE COMPCON, February\n29\u2013March 4, 1988, San Francisco, pp. 278\u2013283.\nGebis, J., Patterson, D., 2007. Embracing and extending 20th-century instruction set archi-\ntectures. IEEE Comput. 40 (4), 68\u201375.\nGee, J.D., Hill, M.D., Pnevmatikatos, D.N., Smith, A.J., 1993. Cache performance of the\nSPEC92 benchmark suite. IEEE Micro 13 (4), 17\u201327.\nGehringer, E.F., Siewiorek, D.P., Segall, Z., 1987. Parallel Processing: The Cm* Experi-\nence. Digital Press, Bedford, MA.\nGharachorloo, K., Lenoski, D., Laudon, J., Gibbons, P., Gupta, A., Hennessy, J.L., 1990.\nMemory consistency and event ordering in scalable shared-memory multiprocessors.\nIn: Proceedings of 17th Annual International Symposium on Computer Architecture\n(ISCA), May 28\u201331, 1990, Seattle, WA, pp. 15\u201326.\nGharachorloo, K., Gupta, A., Hennessy, J.L., 1992. Hiding memory latency using dynamic\nscheduling in shared-memory multiprocessors. In: Proceedings of 19th Annual Interna-\ntional Symposium on Computer Architecture (ISCA), May 19\u201321, 1992, Gold Coast,\nAustralia.\nGhemawat, S., Gobioff, H., Leung, S.-T., 2003. The Google file system. In: Proceedings of\n19th ACM Symposium on Operating Systems Principles, October 19\u201322, 2003, Bolton\nLanding, NY.\nReferences\n\u25a0\nR-13"
    },
    {
        "page": 1454,
        "text": "Gibson, D.H., 1967. Considerations in block-oriented systems design. AFIPS Conf. Proc.\n30, 75\u201380.\nGibson, J.C., 1970. The Gibson mix, Rep. TR. 00.2043. IBM Systems Development Divi-\nsion, Poughkeepsie, NY (research done in 1959).\nGibson, G.A., 1992. In: Redundant Disk Arrays: Reliable, Parallel Secondary Storage.\nACM Distinguished Dissertation Series, MIT Press, Cambridge, MA.\nGibson, J., Kunz, R., Ofelt, D., Horowitz, M., Hennessy, J., Heinrich, M., 2000.\nFLASH vs. (simulated) FLASH: Closing the simulation loop. In: Proceedings of\nNinth\nInternational\nConference\non\nArchitectural\nSupport\nfor\nProgramming\nLanguages and Operating Systems (ASPLOS), November 12\u201315, Cambridge,\nMA, pp. 49\u201358.\nGlass, C.J., Ni, L.M., 1992. The Turn Model for adaptive routing. In: 19th Annual Interna-\ntional Symposium on Computer Architecture (ISCA), May 19\u201321, 1992, Gold Coast,\nAustralia.\nGoldberg, I.B., 1967. 27 bits are not enough for 8-digit accuracy. Commun. ACM 10 (2),\n105\u2013106.\nGoldberg, D., 1991. What every computer scientist should know about floating-point arith-\nmetic. Comput. Surv. 23 (1), 5\u201348.\nGoldstein, S., 1987. Storage Performance\u2014An Eight Year Outlook, Tech. Rep. TR 03.308-\n1. Santa Teresa Laboratory, IBM Santa Teresa Laboratory, San Jose, CA.\nGoldstine, H.H., 1972. The Computer: From Pascal to von Neumann. Princeton University\nPress, Princeton, NJ.\nGonz\u00e1lez, A., Day, M., 2016. Amazon, Microsoft invest billions as computing shifts to\ncloud. The Seattle Times. http://www.seattletimes.com/business/technology/amazon-\nmicrosoft-invest-billions-as-computing-shifts-to-cloud/.\nGonz\u00e1lez, J., Gonz\u00e1lez, A., 1998. Limits of instruction level parallelism with data specula-\ntion. In: Proceedings of Vector and Parallel Processing (VECPAR) Conference, June\n21\u201323, 1998, Porto, Portugal, pp. 585\u2013598.\nGoodman, J.R., 1983. Using cache memory to reduce processor memory traffic.\nIn: Proceedings of 10th Annual International Symposium on Computer Architecture\n(ISCA), June 5\u20137, 1982, Stockholm, Sweden, pp. 124\u2013131.\nGoralski, W., 1997. SONET: A Guide to Synchronous Optical Network. McGraw-Hill,\nNew York.\nGosling, J.B., 1980. Design of Arithmetic Units for Digital Computers. Springer-Verlag,\nNew York.\nGray, J., 1990. A census of Tandem system availability between 1985 and 1990. IEEE\nTrans. Reliab. 39 (4), 409\u2013418.\nGray, J. (Ed.), 1993. The Benchmark Handbook for Database and Transaction Processing\nSystems, second ed. Morgan Kaufmann, San Francisco.\nGray, J., 2006. Sort benchmark home page. http://sortbenchmark.org/.\nGray, J., Reuter, A., 1993. Transaction Processing: Concepts and Techniques. Morgan\nKaufmann, San Francisco.\nGray, J., Siewiorek, D.P., 1991. High-availability computer systems. Computer 24 (9),\n39\u201348.\nGray, J., van Ingen, C., 2005. Empirical Measurements of Disk Failure Rates and Error\nRates, MSR-TR-2005-166. Microsoft Research, Redmond, WA.\nGreenberg, A., Jain, N., Kandula, S., Kim, C., Lahiri, P., Maltz, D., Patel, P., Sengupta, S.,\n2009. VL2: a scalable and flexible data center network. In: Proceedings of ACM SIG-\nCOMM, August 17\u201321, 2009, Barcelona, Spain.\nGrice, C., Kanellos, M., 2000. Cell phone industry at crossroads: go high or low? CNET\nNews.technews.netscape.com/news/0-1004-201-2518386-0.html?tag\u00bcst.ne.1002.tgif.sf.\nGroe, J.B., Larson, L.E., 2000. CDMA Mobile Radio Design. Artech House, Boston.\nR-14\n\u25a0\nReferences"
    },
    {
        "page": 1455,
        "text": "Gunther, K.D., 1981. Prevention of deadlocks in packet-switched data transport systems.\nIEEE Trans. Commun. 29 (4), 512\u2013524.\nHagersten, E., Koster, M., 1998. WildFire: a scalable path for SMPs. In: Proceedings of\nFifth International Symposium on High-Performance Computer Architecture, January\n9\u201312, 1999, Orlando, FL.\nHagersten, E., Landin, A., Haridi, S., 1992. DDM\u2014a cache-only memory architecture.\nIEEE Comput. 25 (9), 44\u201354.\nHamacher, V.C., Vranesic, Z.G., Zaky, S.G., 1984. Computer Organization, second ed.\nMcGraw-Hill, New York.\nHameed, R., Qadeer, W., Wachs, M., Azizi, O., Solomatnikov, A., Lee, B.C.,\nRichardson, S., Kozyrakis, C., Horowitz, M., 2010. Understanding sources of ineffi-\nciency in general-purpose chips. ACM SIGARCH Comput. Architect. News 38 (3),\n37\u201347.\nHamilton, J., 2009. Data center networks are in my way. In: Paper Presented at the\nStanford Clean Slate CTO Summit, October 23, 2009. http://mvdirona.com/jrh/-\nTalksAndPapers/JamesHamilton_CleanSlateCTO2009.pdf.\nHamilton, J., 2010. Cloud computing economies of scale. In: Paper Presented at the AWS\nWorkshop on Genomics and Cloud Computing, June 8, 2010, Seattle, WA. http://\nmvdirona.com/jrh/TalksAndPapers/JamesHamilton_GenomicsCloud20100608.pdf.\nHamilton, J., 2014. AWS Innovation at Scale, AWS Re-invent conference. https://www.\nyoutube.com/watch?v\u00bcJIQETrFC_SQ.\nHamilton, J., 2015. The Return to the Cloud. http://perspectives.mvdirona.com/2015/05/\nthe-return-to-the-cloud//.\nHamilton, J., 2017. How Many Data Centers Needed World-Wide. http://perspectives.\nmvdirona.com/2017/04/how-many-data-centers-needed-worldwide/.\nHammerstrom, D., 1990. A VLSI architecture for high-performance, low-cost, on-chip\nlearning. In: IJCNN International Joint Conference on Neural Networks.\nHandy, J., 1993. The Cache Memory Book. Academic Press, Boston.\nHauck,\nE.A.,\nDent,\nB.A.,\n1968.\nBurroughs\u2019\nB6500/B7500\nstack\nmechanism.\nIn: Proceedings of AFIPS Spring Joint Computer Conference, April 30\u2013May 2,\n1968, Atlantic City, NJ, pp. 245\u2013251.\nHe, K., Zhang, X., Ren, S., Sun, J., 2016. Identity mappings in deep residual networks. Also\nin arXiv preprint arXiv:1603.05027.\nHeald, R., Aingaran, K., Amir, C., Ang, M., Boland, M., Das, A., Dixit, P.,\nGouldsberry, G., Hart, J., Horel, T., Hsu, W.-J., Kaku, J., Kim, C., Kim, S.,\nKlass, F., Kwan, H., Lo, R., McIntyre, H., Mehta, A., Murata, D., Nguyen, S.,\nPai, Y.-P., Patel, S., Shin, K., Tam, K., Vishwanthaiah, S., Wu, J., Yee, G.,\nYou, H., 2000. Implementation of third-generation SPARC V9 64-b microprocessor.\nIn: ISSCC Digest of Technical Papers, pp. 412\u2013413.\nHeinrich, J., 1993. MIPS R4000 User\u2019s Manual. Prentice Hall, Englewood Cliffs, NJ.\nHenly, M., McNutt, B., 1989. DASD I/O Characteristics: A Comparison of MVS to VM,\nTech. Rep. TR 02.1550 (May). IBM General Products Division, San Jose, CA.\nHennessy, J., 1984. VLSI processor architecture. IEEE Trans. Comput. C-33 (11), 1221\u20131246.\nHennessy, J., 1985. VLSI RISC processors. VLSI Syst. Des. 6 (10), 22\u201332.\nHennessy, J., Jouppi, N., Baskett, F., Gill, J., 1981. MIPS: a VLSI processor architecture.\nIn: CMU Conference on VLSI Systems and Computations. Computer Science Press,\nRockville, MD.\nHewlett-Packard, 1994. PA-RISC 2.0 Architecture Reference Manual, third ed. Hewlett-\nPackard, Palo Alto, CA.\nHewlett-Packard,\n1998.\nHP\u2019s\n\u20185NINES:5MINUTES\u2019\nVision\nExtends\nLeadership\nand Redefines High Availability in Mission-Critical Environments.\nwww.future.\nenterprisecomputing.hp.com/ia64/news/5nines_vision_pr.html.\nReferences\n\u25a0\nR-15"
    },
    {
        "page": 1456,
        "text": "Hill, M.D., 1987. Aspects of Cache Memory and Instruction Buffer Performance (Ph.D.\nthesis). Tech. Rep. UCB/CSD 87/381. Computer Science Division, University of\nCalifornia, Berkeley.\nHill, M.D., 1988. A case for direct mapped caches. Computer 21 (12), 25\u201340.\nHill, M.D., 1998. Multiprocessors should support simple memory consistency models.\nIEEE Comput. 31 (8), 28\u201334.\nHillis, W.D., 1985. The Connection Multiprocessor. MIT Press, Cambridge, MA.\nHillis, W.D., Steele, G.L., 1986. Data parallel algorithms. Commun. ACM 29 (12),\n1170\u20131183.\nHinton, G., Sager, D., Upton, M., Boggs, D., Carmean, D., Kyker, A., Roussel, P., 2001.\nThe microarchitecture of the Pentium 4 processor. Intel Technol. J.\nHintz, R.G., Tate, D.P., 1972. Control data STAR-100 processor design. In: Proceedings of\nIEEE COMPCON, September 12\u201314, 1972, San Francisco, pp. 1\u20134.\nHirata, H., Kimura, K., Nagamine, S., Mochizuki, Y., Nishimura, A., Nakase, Y.,\nNishizawa, T., 1992. An elementary processor architecture with simultaneous instruction\nissuing from multiple threads. In: Proceedings of 19th Annual International Symposium\non Computer Architecture (ISCA), May 19\u201321, 1992, Gold Coast, Australia,\npp. 136\u2013145.\nHitachi, 1997. SuperH RISC Engine SH7700 Series Programming Manual. Hitachi, Santa\nClara, CA. www.halsp.hitachi.com/tech_prod/.\nHo, R., Mai, K.W., Horowitz, M.A., 2001. The future of wires. In: Proc. of the IEEE, 89. 4,\npp. 490\u2013504.\nHoagland, A.S., 1963. Digital Magnetic Recording. Wiley, New York.\nHockney, R.W., Jesshope, C.R., 1988. Parallel Computers 2: Architectures, Programming\nand Algorithms. Adam Hilger, Ltd., Bristol, England.\nHolland, J.H., 1959. A universal computer capable of executing an arbitrary number of\nsubprograms simultaneously. Proc. East Joint Comput. Conf. 16, 108\u2013113.\nHolt, R.C., 1972. Some deadlock properties of computer systems. ACM Comput. Surv.\n4 (3), 179\u2013196.\nH\u20acolzle, U., 2010. Brawny cores still beat wimpy cores, most of the time. IEEE Micro 30, 4\n(July/August).\nHopkins, M., 2000. A critical look at IA-64: massive resources, massive ILP, but can it\ndeliver? Microprocessor Rep. February.\nHord, R.M., 1982. The Illiac-IV, The First Supercomputer. Computer Science Press,\nRockville, MD.\nHorel, T., Lauterbach, G., 1999. UltraSPARC-III: designing third-generation 64-bit perfor-\nmance. IEEE Micro 19 (3), 73\u201385.\nHospodor, A.D., Hoagland, A.S., et al., 1993. The changing nature of disk controllers. Proc.\nIEEE 81 (4), 586\u2013594.\nHristea, C., Lenoski, D., Keen, J., 1997. Measuring memory hierarchy performance of\ncache-coherent multiprocessors using micro benchmarks. In: Proceedings of ACM/\nIEEE Conference on Supercomputing, November 16\u201321, 1997, San Jose, CA.\nHsu, P., 1994. Designing the TFP microprocessor. IEEE Micro 18(2).\nHuang, M., Wu, D., Yu, C.H., Fang, Z., Interlandi, M., Condie, T., Cong, J., 2016. Program-\nming and runtime support to blaze FPGA accelerator deployment at datacenter scale.\nIn: Proceedings of the Seventh ACM Symposium on Cloud Computing. ACM,\npp. 456\u2013469.\nHuck, J., et al., 2000. Introducing the IA-64 Architecture. IEEE Micro 20 (5), 12\u201323.\nHughes, C.J., Kaul, P., Adve, S.V., Jain, R., Park, C., Srinivasan, J., 2001. Variability in the\nexecution of multimedia applications and implications for architecture. In: Proceedings\nR-16\n\u25a0\nReferences"
    },
    {
        "page": 1457,
        "text": "of 28th Annual International Symposium on Computer Architecture (ISCA), June\n30\u2013July 4, 2001, Goteborg, Sweden, pp. 254\u2013265.\nHwang, K., 1979. Computer Arithmetic: Principles, Architecture, and Design. Wiley,\nNew York.\nHwang, K., 1993. Advanced Computer Architecture\nand Parallel Programming.\nMcGraw-Hill, New York.\nHwu, W.-M., Patt, Y., 1986. HPSm, a high performance restricted data flow architecture\nhaving minimum functionality. In: Proceedings of 13th Annual International Sympo-\nsium on Computer Architecture (ISCA), June 2\u20135, 1986, Tokyo, pp. 297\u2013307.\nHwu, W.W., Mahlke, S.A., Chen, W.Y., Chang, P.P., Warter, N.J., Bringmann, R.A.,\nOuellette, R.O., Hank, R.E., Kiyohara, T., Haab, G.E., Holm, J.G., Lavery, D.M.,\n1993. The superblock: an effective technique for VLIW and superscalar compilation.\nJ. Supercomput. 7 (1), 229\u2013248.\nIandola, F., 2016. Exploring the Design Space of Deep Convolutional Neural Networks at\nLarge Scale (Ph.D. dissertation). UC Berkeley.\nIBM, 1982. The Economic Value of Rapid Response Time, GE20-0752-0. IBM, White\nPlains, NY, pp. 11\u201382.\nIBM, 1990. The IBM RISC System/6000 processor. IBM J. Res. Dev. 34(1).\nIBM, 1994. The PowerPC Architecture. Morgan Kaufmann, San Francisco.\nIBM, 2005. Blue Gene. IBM J. Res. Dev. 49 (2/3) (Special issue).\nIEEE, 1985. IEEE standard for binary floating-point arithmetic. SIGPLAN Notices 22 (2),\n9\u201325.\nIEEE, 2005. Intel virtualization technology, computer. IEEE Comput. Soc. 38 (5),\n48\u201356.\nIEEE 754-2008 Working Group, 2006. DRAFT Standard for Floating-Point Arithmetic\n754-2008, https://doi.org/10.1109/IEEESTD.2008.4610935.\nIenne, P., Cornu, T., Kuhn, G., 1996. Special-purpose digital hardware for neural networks: an\narchitectural survey. J. VLSI Signal Process. Syst. Signal Image Video Technol. 13(1).\nImprimis Product Specification, 97209 Sabre Disk Drive IPI-2 Interface 1.2 GB, Document\nNo. 64402302, Imprimis, Dallas, TX.\nInfiniBand Trade Association, 2001. InfiniBand Architecture Specifications Release 1.0.a.\nwww.infinibandta.org.\nInoue, K., Ishihara, T., Murakami, K., 1999. Way-predicting set-associative cache for high\nperformance and low energy consumption. In: Proc. 1999 International Symposium on\nLow Power Electronics and Design, ACM, pp. 273\u2013275.\nIntel, 2001. Using MMX Instructions to Convert RGB to YUV Color Conversion. cedar.\nintel.com/cgi-bin/ids.dll/content/content.jsp?cntKey\u00bcLegacy::irtm_AP548_9996&\ncntType\u00bcIDS_EDITORIAL.\nInternet Retailer, 2005. The Gap launches a new site\u2014after two weeks of downtime. Inter-\nnet Retailer. http://www.internetretailer.com/2005/09/28/the-gap-launches-a-new-site-\nafter-two-weeks-of-downtime.\nJain, R., 1991. The Art of Computer Systems Performance Analysis: Techniques for\nExperimental Design, Measurement, Simulation, and Modeling. Wiley, New York.\nJantsch, A., Tenhunen, H. (Eds.), 2003. Networks on Chips. Kluwer Academic Publishers,\nThe Netherlands.\nJimenez, D.A., Lin, C., 2001. Dynamic branch prediction with perceptrons. In: Proceedings\nof the 7th International Symposium on High-Performance Computer Architecture\n(HPCA '01). IEEE, Washington, DC, pp. 197\u2013206.\nJimenez, D.A., Lin, C., 2002. Neural methods for dynamic branch prediction. ACM Trans.\nComput. Syst. 20 (4), 369\u2013397.\nReferences\n\u25a0\nR-17"
    },
    {
        "page": 1458,
        "text": "Johnson, M., 1990. Superscalar Microprocessor Design. Prentice Hall, Englewood Cliffs,\nNJ.\nJordan, H.F., 1983. Performance measurements on HEP\u2014a pipelined MIMD computer.\nIn: Proceedings of 10th Annual International Symposium on Computer Architecture\n(ISCA), June 5\u20137, 1982, Stockholm, Sweden, pp. 207\u2013212.\nJordan,K.E.,1987.Performancecomparisonoflarge-scalescientificprocessors:scalarmain-\nframes, mainframes with vector facilities, and supercomputers. Computer 20 (3), 10\u201323.\nJouppi, N.P., 1990. Improving direct-mapped cache performance by the addition of a small\nfully-associative cache and prefetch buffers. In: Proceedings of 17th Annual Interna-\ntional Symposium on Computer Architecture (ISCA), May 28\u201331, 1990, Seattle,\nWA, pp. 364\u2013373.\nJouppi,N.P.,1998.Retrospective:Improvingdirect-mappedcacheperformancebytheaddition\nof a small fully-associative cache and prefetch buffers. In: 25 Years of the International\nSymposia on Computer Architecture (Selected Papers). ACM, New York, pp. 71\u201373.\nJouppi, N., 2016. Google supercharges machine learning tasks with TPU custom chip.\nhttps://cloudplatform.googleblog.com.\nJouppi, N.P., Wall, D.W., 1989. Available instruction-level parallelism for super-scalar and\nsuperpipelined processors. In: Proceedings of Third International Conference on Archi-\ntectural Support for Programming Languages and Operating Systems (ASPLOS), April\n3\u20136, 1989, Boston, pp. 272\u2013282.\nJouppi, N.P., Wilton, S.J.E., 1994. Trade-offs in two-level on-chip caching. In: Proceedings\nof 21st Annual International Symposium on Computer Architecture (ISCA), April 18\u2013\n21, 1994, Chicago, pp. 34\u201345.\nJouppi, N., Young, C., Patil, N., Patterson, D., Agrawal, G., et al., 2017. Datacenter perfor-\nmance analysis of a matrix processing unit. In: 44th International Symposium on\nComputer Architecture.\nKaeli, D.R., Emma, P.G., 1991. Branch history table prediction of moving target branches\ndue to subroutine returns. In: Proceedings of 18th Annual International Symposium on\nComputer Architecture (ISCA), May 27\u201330, 1991, Toronto, Canada, pp. 34\u201342.\nKahan, W., 1968. 7094-II system support for numerical analysis, SHARE Secretarial\nDistribution SSD-159. Department of Computer Science, University of Toronto.\nKahan, J., 1990. On the advantage of the 8087\u2019s stack, unpublished course notes. Computer\nScience Division, University of California, Berkeley.\nKahaner, D.K., 1988. Benchmarks for \u2018real\u2019 programs. SIAM News. November.\nKahn, R.E., 1972. Resource-sharing computer communication networks. Proc. IEEE\n60 (11), 1397\u20131407.\nKane, G., 1986. MIPS R2000 RISC Architecture. Prentice Hall, Englewood Cliffs, NJ.\nKane, G., 1996. PA-RISC 2.0 Architecture. Prentice Hall, Upper Saddle River, NJ.\nKane, G., Heinrich, J., 1992. MIPS RISC Architecture. Prentice Hall, Englewood Cliffs, NJ.\nKanev, S., Darago, J.P., Hazelwood, K., Ranganathan, P., Moseley, T., Wei, G.Y.,\nBrooks, D., 2015. Profiling a warehouse-scale computer. In: ACM/IEEE 42nd Annual\nInternational Symposium on Computer Architecture (ISCA).\nKarpathy, A., et al., 2014. Large-scale video classification with convolutional neural\nnetworks. CVPR.\nKatz, R.H., Patterson, D.A., Gibson, G.A., 1989. Disk system architectures for high perfor-\nmance computing. Proc. IEEE 77 (12), 1842\u20131858.\nKeckler, S.W., Dally, W.J., 1992. Processor coupling: integrating compile time and runtime\nscheduling for parallelism. In: Proceedings of 19th Annual International Symposium on\nComputer Architecture (ISCA), May 19\u201321, 1992, Gold Coast, Australia, pp. 202\u2013213.\nKeller, R.M., 1975. Look-ahead processors. ACM Comput. Surv. 7 (4), 177\u2013195.\nR-18\n\u25a0\nReferences"
    },
    {
        "page": 1459,
        "text": "Keltcher, C.N., McGrath, K.J., Ahmed, A., Conway, P., 2003. The AMD Opteron processor\nfor multiprocessor servers. IEEE Micro 23 (2), 66\u201376.\nKembel, R., 2000. Fibre channel: a comprehensive introduction. Internet Week. April.\nKermani, P., Kleinrock, L., 1979. Virtual cut-through: a new computer communication\nswitching technique. Comput. Netw. 3, 267\u2013286.\nKessler, R., 1999. The Alpha 21264 microprocessor. IEEE Micro 19 (2), 24\u201336.\nKilburn, T., Edwards, D.B.G., Lanigan, M.J., Sumner, F.H., 1962. One-level storage sys-\ntem. IRE Trans. Electron. Comput. EC-11, 223\u2013235. Also appears in Siewiorek, D.P.,\nBell, C.G., Newell, A. 1982. Computer Structures: Principles and Examples. McGraw-\nHill, New York. pp. 135\u2013148.\nKillian, E., 1991. MIPS R4000 technical overview\u201364 bits/100 MHz or bust. In: Hot Chips\nIII Symposium Record, August 26\u201327, 1991, Stanford University, Palo Alto, CA.\npp. 1.6\u20131.19.\nKim, M.Y., 1986. Synchronized disk interleaving. IEEE Trans. Comput. 35 (11),\n978\u2013988.\nKim, K., 2005. Technology for sub-50nm DRAM and NAND flash manufacturing. In: Elec-\ntron Devices Meeting Technical Digest (December), pp. 323\u2013326.\nKissell, K.D., 1997. MIPS16: High-density for the embedded market. In: Proceedings of\nReal Time Systems\u201997, June 15, 1997, Las Vegas, Nev. www.sgi.com/MIPS/arch/\nMIPS16/MIPS16.whitepaper.pdf.\nKitagawa, K., Tagaya, S., Hagihara, Y., Kanoh, Y., 2003. A hardware overview of SX-6\nand SX-7 supercomputer. NEC Res. Dev. J. 44 (1), 2\u20137.\nKnuth, D., 1981. second ed. The Art of Computer Programming, vol. II. Addison-Wesley,\nReading, MA.\nKogge, P.M., 1981. The Architecture of Pipelined Computers. McGraw-Hill, New York.\nKohn, L., Fu, S.-W., 1989. A 1,000,000 transistor microprocessor. In: Proceedings of IEEE\nInternational Symposium on Solid State Circuits (ISSCC), February 15\u201317, 1989,\nNew York, pp. 54\u201355.\nKohn, L., Margulis, N., 1989. Introducing the Intel i860 64-Bit Microprocessor. IEEE Micro\n9 (4), 15\u201330.\nKontothanassis, L., Hunt, G., Stets, R., Hardavellas, N., Cierniak, M., Parthasarathy, S.,\nMeira, W., Dwarkadas, S., Scott, M., 1997. VM-based shared memory on low-latency,\nremote-memory-access networks. In: Proceedings of 24th Annual International\nSymposium on Computer Architecture (ISCA), June 2\u20134, 1997, Denver, CO.\nKoren, I., 1989. Computer Arithmetic Algorithms. Prentice Hall, Englewood Cliffs, NJ.\nKozyrakis, C., 2000. Vector IRAM: a media-oriented vector processor with embedded\nDRAM. In: Paper Presented at Hot Chips 12, August 13\u201315, 2000, Palo Alto, CA,\npp. 13\u201315.\nKozyrakis, C., Patterson, D., 2002. Vector vs. superscalar and VLIW architectures for\nembedded multimedia benchmarks. In: Proceedings of 35th Annual International\nSymposium on Microarchitecture (MICRO-35), November 18\u201322, 2002, Istanbul,\nTurkey.\nKrizhevsky, A., Sutskever, I., Hinton, G., 2012. Imagenet classification with deep convolu-\ntional neural networks. Adv. Neural Inf. Process. Syst.\nKroft, D., 1981. Lockup-free instruction fetch/prefetch cache organization. In: Proceedings\nof Eighth Annual International Symposium on Computer Architecture (ISCA), May\n12\u201314, 1981, Minneapolis, MN, pp. 81\u201387.\nKroft, D., 1998. Retrospective: lockup-free instruction fetch/prefetch cache organization.\nIn: 25 Years of the International Symposia on Computer Architecture (Selected Papers),\nACM, New York, pp. 20\u201321.\nReferences\n\u25a0\nR-19"
    },
    {
        "page": 1460,
        "text": "Kuck, D., Budnik, P.P., Chen, S.-C., Lawrie, D.H., Towle, R.A., Strebendt, R.E.,\nDavis Jr., E.W., Han, J., Kraska, P.W., Muraoka, Y., 1974. Measurements of parallelism\nin ordinary FORTRAN programs. Computer 7 (1), 37\u201346.\nKuhn, D.R., 1997. Sources of failure in the public switched telephone network. IEEE\nComput. 30 (4), 31\u201336.\nKumar, A., 1997. The HP PA-8000 RISC CPU. IEEE Micro 17 (2), 27\u201332.\nKung, H.T., Leiserson, C.E., 1980. Algorithms for VLSI processor arrays. Introduction to\nVLSI systems.\nKunimatsu, A., Ide, N., Sato, T., Endo, Y., Murakami, H., Kamei, T., Hirano, M.,\nIshihara, F., Tago, H., Oka, M., Ohba, A., Yutaka, T., Okada, T., Suzuoki, M.,\n2000. Vector unit architecture for emotion synthesis. IEEE Micro 20 (2), 40\u201347.\nKunkel, S.R., Smith, J.E., 1986. Optimal pipelining in supercomputers. In: Proceedings of\n13th Annual International Symposium on Computer Architecture (ISCA), June 2\u20135,\n1986, Tokyo, pp. 404\u2013414.\nKurose, J.F., Ross, K.W., 2001. Computer Networking: A Top-Down Approach Featuring\nthe Internet. Addison-Wesley, Boston.\nKuskin, J., Ofelt, D., Heinrich, M., Heinlein, J., Simoni, R., Gharachorloo, K., Chapin, J.,\nNakahira, D., Baxter, J., Horowitz, M., Gupta, A., Rosenblum, M., Hennessy, J.L.,\n1994. The Stanford FLASH multiprocessor. In: Proceedings of 21st Annual\nInternational Symposium on Computer Architecture (ISCA), April 18\u201321, 1994, Chicago.\nLam, M., 1988. Software pipelining: an effective scheduling technique for VLIW proces-\nsors. In: SIGPLAN Conference on Programming Language Design and Implementa-\ntion, June 22\u201324, 1988, Atlanta, GA, pp. 318\u2013328.\nLam, M.S., Wilson, R.P., 1992. Limits of control flow on parallelism. In: Proceedings of\n19th Annual International Symposium on Computer Architecture (ISCA), May\n19\u201321, 1992, Gold Coast, Australia, pp. 46\u201357.\nLam, M.S., Rothberg, E.E., Wolf, M.E., 1991. The cache performance and optimizations of\nblocked algorithms. In: Proceedings of Fourth International Conference on Architec-\ntural Support for Programming Languages and Operating Systems (ASPLOS), April\n8\u201311, 1991, Santa Clara, CA. (SIGPLAN Notices 26:4 (April), 63\u201374).\nLambright, D., 2000. Experiences in measuring the reliability of a cache-based storage\nsystem. In: Proceedings of First Workshop on Industrial Experiences with Systems\nSoftware (WIESS 2000), Co-Located with the 4th Symposium on Operating Systems\nDesign and Implementation (OSDI), October 22, 2000, San Diego, CA.\nLamport, L., 1979. How to make a multiprocessor computer that correctly executes multi-\nprocess programs. IEEE Trans. Comput. C-28 (9), 241\u2013248.\nLandstrom, B., 2014. The Cost of Downtime. http://www.interxion.com/blogs/2014/07/the-\ncost-of-downtime/.\nLang, W., Patel, J.M., Shankar, S., 2010. Wimpy node clusters: what about non-wimpy\nworkloads? In: Proceedings of Sixth International Workshop on Data Management\non New Hardware (DaMoN), June 7, Indianapolis, IN.\nLaprie, J.-C., 1985. Dependable computing and fault tolerance: concepts and terminology.\nIn: Proceedings of 15th Annual International Symposium on Fault-Tolerant Computing,\nJune 19\u201321, 1985, Ann Arbor, Mich, pp. 2\u201311.\nLarabel, M., 2016. Google Looks To Open Up StreamExecutor To Make GPGPU Program-\nming Easier. Phoronix, March 10. https://www.phoronix.com/.\nLarson, E.R., 1973. Findings of fact, conclusions of law, and order for judgment,\nFile No. 4-67, Civ. 138, Honeywell v. Sperry-Rand and Illinois Scientific Develop-\nment, U.S. District Court for the State of Minnesota, Fourth Division (October 19).\nLaudon, J., Lenoski, D., 1997. The SGI Origin: a ccNUMA highly scalable server.\nIn: Proceedings of 24th Annual International Symposium on Computer Architecture\n(ISCA), June 2\u20134, 1997, Denver, CO, pp. 241\u2013251.\nR-20\n\u25a0\nReferences"
    },
    {
        "page": 1461,
        "text": "Laudon, J., Gupta, A., Horowitz, M., 1994. Interleaving: a multithreading technique target-\ning multiprocessors and workstations. In: Proceedings of Sixth International Confer-\nence on Architectural Support for Programming Languages and Operating Systems\n(ASPLOS), October 4\u20137, San Jose, CA, pp. 308\u2013318.\nLauterbach, G., Horel, T., 1999. UltraSPARC-III: designing third generation 64-bit perfor-\nmance. IEEE Micro 19, 3 (May/June).\nLazowska, E.D., Zahorjan, J., Graham, G.S., Sevcik, K.C., 1984. Quantitative System\nPerformance: Computer System Analysis Using Queueing Network Models. Prentice\nHall, Englewood Cliffs, NJ (Although out of print, it is available online at www.cs.\nwashington.edu/homes/lazowska/qsp/).\nLebeck, A.R., Wood, D.A., 1994. Cache profiling and the SPEC benchmarks: a case study.\nComputer 27 (10), 15\u201326.\nLee, R., 1989. Precision architecture. Computer 22 (1), 78\u201391.\nLee, W.V., et al., 2010. Debunking the 100X GPU vs. CPU myth: an evaluation of\nthroughput computing on CPU and GPU. In: Proceedings of 37th Annual Interna-\ntional Symposium on Computer Architecture (ISCA), June 19\u201323, 2010, Saint-\nMalo, France.\nLee, Y., Waterman, A., Cook, H., Zimmer, B., Keller, B., Puggelli, A., Kwak, J., Jevtic, R.,\nBailey, S., Blagojevic, M., Chiu, P.-F., Avizienis, R., Richards, B., Bachrach, J.,\nPatterson, D., Alon, E., Nikolic, B., Asanovic, K., 2016. An agile approach to building\nRISC-V microprocessors. IEEE Micro 36 (2), 8\u201320.\nLeighton, F.T., 1992. Introduction to Parallel Algorithms and Architectures: Arrays, Trees,\nHypercubes. Morgan Kaufmann, San Francisco.\nLeiner, A.L., 1954. System specifications for the DYSEAC. J. ACM 1 (2), 57\u201381.\nLeiner, A.L., Alexander, S.N., 1954. System organization of the DYSEAC. IRE Trans.\nElectron. Comput. 3 (1), 1\u201310.\nLeiserson, C.E., 1985. Fat trees: universal networks for hardware-efficient supercomputing.\nIEEE Trans. Comput. C-34 (10), 892\u2013901.\nLenoski, D., Laudon, J., Gharachorloo, K., Gupta, A., Hennessy, J.L., 1990. The Stanford\nDASH multiprocessor. In: Proceedings of 17th Annual International Symposium on\nComputer Architecture (ISCA), May 28\u201331, 1990, Seattle, WA, pp. 148\u2013159.\nLenoski, D., Laudon, J., Gharachorloo, K., Weber, W.-D., Gupta, A., Hennessy, J.L.,\nHorowitz, M.A., Lam, M., 1992. The Stanford DASH multiprocessor. IEEE Comput.\n25 (3), 63\u201379.\nLevy, H., Eckhouse, R., 1989. Computer Programming and Architecture: The VAX. Digital\nPress, Boston.\nLewis-Kraus, G., 2016. The Great A.I. Awakening. New York Times Magazine..\nLi, K., 1988. IVY: a shared virtual memory system for parallel computing. In: Proceedings\nof 1988 International Conference on Parallel Processing. Pennsylvania State University\nPress, University Park, PA.\nLi, S., Chen, K., Brockman, J.B., Jouppi, N., 2011. Performance Impacts of Non-blocking\nCaches in Out-of-order Processors. HP Labs Tech Report HPL-2011-65 (full text avail-\nable at http://Library.hp.com/techpubs/2011/Hpl-2011-65.html).\nLim, K., Ranganathan, P., Chang, J., Patel, C., Mudge, T., Reinhardt, S., 2008. Understand-\ning and designing new system architectures for emerging warehouse-computing\nenvironments. In: Proceedings of 35th Annual International Symposium on Computer\nArchitecture (ISCA), June 21\u201325, 2008, Beijing, China.\nLincoln, N.R., 1982. Technology and design trade offs in the creation of a modern super-\ncomputer. IEEE Trans. Comput. C-31 (5), 363\u2013376.\nLindholm, T., Yellin, F., 1999. The Java Virtual Machine Specification, 2nd ed. Addi-\nson-Wesley, Reading, MA (Also available online at java.sun.com/docs/books/\nvmspec/).\nReferences\n\u25a0\nR-21"
    },
    {
        "page": 1462,
        "text": "Lipasti, M.H., Shen, J.P., 1996. Exceeding the dataflow limit via value prediction.\nIn: Proceedings of 29th International Symposium on Microarchitecture, December\n2\u20134, 1996, Paris, France.\nLipasti, M.H., Wilkerson, C.B., Shen, J.P., 1996. Value locality and load value prediction.\nIn: Proceedings of Seventh Conference on Architectural Support for Programming Lan-\nguages and Operating Systems (ASPLOS), October 1\u20135, 1996, Cambridge, MA,\npp. 138\u2013147.\nLiptay, J.S., 1968. Structural aspects of the System/360 Model 85, Part II: The cache. IBM\nSyst. J. 7 (1), 15\u201321.\nLo, J., Eggers, S., Emer, J., Levy, H., Stamm, R., Tullsen, D., 1997. Converting thread-level\nparallelism into instruction-level parallelism via simultaneous multithreading. ACM\nTrans. Comput. Syst. 15 (2), 322\u2013354.\nLo, J., Barroso, L., Eggers, S., Gharachorloo, K., Levy, H., Parekh, S., 1998. An analysis of\ndatabase\nworkload\nperformance\non\nsimultaneous\nmultithreaded\nprocessors.\nIn: Proceedings of 25th Annual International Symposium on Computer Architecture\n(ISCA), July 3\u201314, 1998, Barcelona, Spain, pp. 39\u201350.\nLo, D., Cheng, L., Govindaraju, R., Barroso, L.A., Kozyrakis, C., 2014. Towards energy\nproportionality for large-scale latency-critical workloads. In: ACM/IEEE 41st Annual\nInternational Symposium on Computer Architecture (ISCA).\nLoh, G.H., Hill, M.D., 2011. Efficiently enabling conventional block sizes for very large\ndie-stacked DRAM caches. In: Proc. 44th Annual IEEE/ACM International Symposium\non Microarchitecture, ACM, pp. 454\u2013464.\nLovett, T., Thakkar, S., 1988. The symmetry multiprocessor system. In: Proceedings of\n1988\nInternational\nConference\nof\nParallel\nProcessing,\nUniversity\nPark,\nPA,\npp. 303\u2013310.\nLubeck, O., Moore, J., Mendez, R., 1985. A benchmark comparison of three supercom-\nputers: Fujitsu VP-200, Hitachi S810/20, and Cray X-MP/2. Computer 18 (12),\n10\u201324.\nLuk, C.-K., Mowry, T.C., 1999. Automatic compiler-inserted prefetching for pointer-based\napplications. IEEE Trans. Comput. 48 (2), 134\u2013141.\nLunde, A., 1977. Empirical evaluation of some features of instruction set processor archi-\ntecture. Commun. ACM 20 (3), 143\u2013152.\nLuszczek, P., Dongarra, J.J., Koester, D., Rabenseifner, R., Lucas, B., Kepner, J., McCalpin,\nJ., Bailey, D., Takahashi, D., 2005. Introduction to the HPC challenge benchmark suite.\nLawrence Berkeley National Laboratory, Paper LBNL-57493 (April 25), repositories.\ncdlib.org/lbnl/LBNL-57493.\nMaberly, N.C., 1966. Mastering Speed Reading. New American Library, New York.\nMagenheimer, D.J., Peters, L., Pettis, K.W., Zuras, D., 1988. Integer multiplication and\ndivision on the HP precision architecture. IEEE Trans. Comput. 37 (8), 980\u2013990.\nMahlke, S.A., Chen, W.Y., Hwu, W.-M., Rau, B.R., Schlansker, M.S., 1992. Sentinel\nscheduling for VLIW and superscalar processors. In: Proceedings of Fifth International\nConference on Architectural Support for Programming Languages and Operating\nSystems (ASPLOS), October 12\u201315, 1992, Boston, pp. 238\u2013247.\nMahlke, S.A., Hank, R.E., McCormick, J.E., August, D.I., Hwu, W.W., 1995. A comparison\nof full and partial predicated execution support for ILP processors. In: Proceedings of\n22nd Annual International Symposium on Computer Architecture (ISCA), June 22\u201324,\n1995, Santa Margherita, Italy, pp. 138\u2013149.\nMajor, J.B., 1989. Are queuing models within the grasp of the unwashed? In: Proceedings of\nInternational Conference on Management and Performance Evaluation of Computer\nSystems, December 11\u201315, 1989, Reno, Nev, pp. 831\u2013839.\nMarkstein, P.W., 1990. Computation of elementary functions on the IBM RISC System/\n6000 processor. IBM J. Res. Dev. 34 (1), 111\u2013119.\nR-22\n\u25a0\nReferences"
    },
    {
        "page": 1463,
        "text": "Mathis, H.M., Mercias, A.E., McCalpin, J.D., Eickemeyer, R.J., Kunkel, S.R., 2005. Char-\nacterization of the multithreading (SMT) efficiency in Power5. IBM J. Res. Dev.\n49 (4/5), 555\u2013564.\nMcCalpin, J., 2005. STREAM: Sustainable Memory Bandwidth in High Performance\nComputers. www.cs.virginia.edu/stream/.\nMcCalpin, J., Bailey, D., Takahashi, D., 2005. Introduction to the HPC Challenge Bench-\nmark Suite, Paper LBNL-57493. Lawrence Berkeley National Laboratory, University\nof California, Berkeley, repositories.cdlib.org/lbnl/LBNL-57493.\nMcCormick, J., Knies, A., 2002. A brief analysis of the SPEC CPU2000 benchmarks on the\nIntel Itanium 2 processor. In: Paper Presented at Hot Chips 14, August 18\u201320, 2002,\nStanford University, Palo Alto, CA.\nMcFarling, S., 1989. Program optimization for instruction caches. In: Proceedings of Third\nInternational Conference on Architectural Support for Programming Languages and\nOperating Systems (ASPLOS), April 3\u20136, 1989, Boston, pp. 183\u2013191.\nMcFarling, S., 1993. Combining Branch Predictors, WRL Technical Note TN-36, Digital\nWestern Research Laboratory, Palo Alto, CA.\nMcFarling, S., Hennessy, J., 1986. Reducing the cost of branches. In: Proceedings of 13th\nAnnual International Symposium on Computer Architecture (ISCA), June 2\u20135, 1986,\nTokyo, pp. 396\u2013403.\nMcGhan, H., O\u2019Connor, M., 1998. PicoJava: a direct execution engine for Java bytecode.\nComputer 31 (10), 22\u201330.\nMcKeeman, W.M., 1967. Language directed computer design. In: Proceedings of AFIPS\nFall Joint Computer Conference, November 14\u201316, 1967, Washington, DC, pp. 413\u2013417.\nMcMahon, F.M., 1986. The Livermore FORTRAN Kernels: A Computer Test of Numerical\nPerformance Range, Tech. Rep. UCRL-55745. Lawrence Livermore National Labora-\ntory, University of California, Livermore.\nMcNairy, C., Soltis, D., 2003. Itanium 2 processor microarchitecture. IEEE Micro 23 (2),\n44\u201355.\nMead, C., Conway, L., 1980. Introduction to VLSI Systems. Addison-Wesley, Reading, MA.\nMellor-Crummey, J.M., Scott, M.L., 1991. Algorithms for scalable synchronization on\nshared-memory multiprocessors. ACM Trans. Comput. Syst. 9 (1), 21\u201365.\nMenabrea, L.F., 1842. Sketch of the analytical engine invented by Charles Babbage.\nBiblioth\u00e8que Universelle de Gen\u00e8ve. 82.\nMenon, A., Renato Santos, J., Turner, Y., Janakiraman, G., Zwaenepoel, W., 2005. Diag-\nnosing performance overheads in the xen virtual machine environment. In: Proceedings\nof First ACM/USENIX International Conference on Virtual Execution Environments,\nJune 11\u201312, 2005, Chicago, pp. 13\u201323.\nMerlin, P.M., Schweitzer, P.J., 1980. Deadlock avoidance in store-and-forward networks.\nPart I. Store-and-forward deadlock. IEEE Trans. Commun. 28 (3), 345\u2013354.\nMetcalfe, R.M., 1993. Computer/network interface design: lessons from Arpanet and Ether-\nnet. IEEE J. Sel. Area. Commun. 11 (2), 173\u2013180.\nMetcalfe, R.M., Boggs, D.R., 1976. Ethernet: distributed packet switching for local\ncomputer networks. Commun. ACM 19 (7), 395\u2013404.\nMetropolis, N., Howlett, J., Rota, G.C. (Eds.), 1980. A History of Computing in the\nTwentieth Century. Academic Press, New York.\nMeyer, R.A., Seawright, L.H., 1970. A virtual machine time sharing system. IBM Syst. J.\n9 (3), 199\u2013218.\nMeyers, G.J., 1978. The evaluation of expressions in a storage-to-storage architecture.\nComput. Architect. News 7 (3), 20\u201323.\nMeyers, G.J., 1982. Advances in Computer Architecture, second ed. Wiley, New York.\nMicron, 2004. Calculating Memory System Power for DDR2. http://download.micron.com/\npdf/pubs/designline/dl1Q04.pdf.\nReferences\n\u25a0\nR-23"
    },
    {
        "page": 1464,
        "text": "Micron, 2006. The Micron System-Power Calculator. http://www.micron.com/-systemcalc.\nMIPS, 1997. MIPS16 Application Specific Extension Product Description. www.sgi.com/\nMIPS/arch/MIPS16/mips16.pdf.\nMiranker, G.S., Rubenstein, J., Sanguinetti, J., 1988. Squeezing a Cray-class supercomputer\ninto a single-user package. In: Proceedings of IEEE COMPCON, February 29\u2013March 4,\n1988, San Francisco, pp. 452\u2013456.\nMitchell, D., 1989. The transputer: the time is now. Comput. Des. (RISC suppl.) 40\u201341.\nMitsubishi, 1996. Mitsubishi 32-Bit Single Chip Microcomputer M32R Family Software\nManual. Mitsubishi, Cypress, CA.\nMiura,\nK.,\nUchida,\nK.,\n1983.\nFACOM\nvector\nprocessing\nsystem:\nVP100/200.\nIn: Proceedings of NATO Advanced Research Workshop on High-Speed Computing,\nJune 20\u201322, 1983, J\u20aculich, West Germany. Also appears in Hwang, K. (Ed.), 1984.\nSuperprocessors: Design and Applications. IEEE (August), pp. 59\u201373.\nMiya, E.N., 1985. Multiprocessor/distributed processing bibliography. Comput. Architect.\nNews 13 (1), 27\u201329.\nMoney, M.S.N., 2005. Amazon Shares Tumble after Rally Fizzles. http://moneycentral.\nmsn.com/content/CNBCTV/Articles/Dispatches/P133695.asp.\nMontoye, R.K., Hokenek, E., Runyon, S.L., 1990. Design of the IBM RISC System/6000\nfloating-point execution. IBM J. Res. Dev. 34 (1), 59\u201370.\nMoore, G.E., 1965. Cramming more components onto integrated circuits. Electronics 38 (8),\n114\u2013117.\nMoore, B., Padegs, A., Smith, R., Bucholz, W., 1987. Concepts of the System/370 vector\narchitecture. In: 14th Annual International Symposium on Computer Architecture\n(ISCA), June 2\u20135, 1987, Pittsburgh, PA, pp. 282\u2013292.\nMorgan, T., 2014. A rare peek into the massive scale of AWS. Enterprise Tech. https://www.\nenterprisetech.com/2014/11/14/rare-peek-massive-scale-aws/.\nMorgan, T., 2016. How long can AWS keep climbing its steep growth curve? https://www.\nnextplatform.com/2016/02/01/how-long-can-aws-keep-climbingits-steep-growth-curve/.\nMorse, S., Ravenal, B., Mazor, S., Pohlman, W., 1980. Intel microprocessors\u20148080 to\n8086. Computer 13, 10.\nMoshovos, A., Sohi, G.S., 1997. Streamlining inter-operation memory communication\nvia data dependence prediction. In: Proceedings of 30th Annual International\nSymposium on Microarchitecture, December 1\u20133, Research Triangle Park, NC,\npp. 235\u2013245.\nMoshovos, A., Breach, S., Vijaykumar, T.N., Sohi, G.S., 1997. Dynamic speculation and\nsynchronization of data dependences. In: 24th Annual International Symposium on\nComputer Architecture (ISCA), June 2\u20134, 1997, Denver, CO.\nMoussouris, J., Crudele, L., Freitas, D., Hansen, C., Hudson, E., Przybylski, S., Riordan, T.,\nRowen, C., 1986. A CMOS RISC processor with integrated system functions.\nIn: Proceedings of IEEE COMPCON, March 3\u20136, 1986, San Francisco, p. 191.\nMowry, T.C., Lam, S., Gupta, A., 1992. Design and evaluation of a compiler algorithm for\nprefetching. In: Proceedings of Fifth International Conference on Architectural Support\nfor Programming Languages and Operating Systems (ASPLOS), October 12\u201315, 1992,\nBoston (SIGPLAN Notices 27:9 (September), pp. 62\u201373.\nMuchnick, S.S., 1988. Optimizing compilers for SPARC. Sun Technol. 1 (3), 64\u201377.\nMueller, M., Alves, L.C., Fischer, W., Fair, M.L., Modi, I., 1999. RAS strategy for IBM S/\n390 G5 and G6. IBM J. Res. Dev. 43 (5-6), 875\u2013888.\nMukherjee, S.S., Weaver, C., Emer, J.S., Reinhardt, S.K., Austin, T.M., 2003. Measuring\narchitectural vulnerability factors. IEEE Micro 23 (6), 70\u201375.\nMurphy, B., Gent, T., 1995. Measuring system and software reliability using an automated\ndata collection process. Qual. Reliab. Eng. Int. 11 (5), 341\u2013353.\nR-24\n\u25a0\nReferences"
    },
    {
        "page": 1465,
        "text": "Myer, T.H., Sutherland, I.E., 1968. On the design of display processors. Commun. ACM\n11 (6), 410\u2013414.\nNarayanan, D., Thereska, E., Donnelly, A., Elnikety, S., Rowstron, A., 2009. Migrating\nserver storage to SSDs: analysis of trade-offs. In: Proceedings of 4th ACM European\nConference on Computer Systems, April 1\u20133, 2009, Nuremberg, Germany.\nNational Research Council, 1997. The Evolution of Untethered Communications. Computer\nScience and Telecommunications Board, National Academy Press, Washington, DC.\nNational Storage Industry Consortium, 1998. Tape Roadmap. www.nsic.org.\nNelson, V.P., 1990. Fault-tolerant computing: fundamental concepts. Computer 23 (7), 19\u201325.\nNgai, T.-F., Irwin, M.J., 1985. Regular, area-time efficient carry-lookahead adders.\nIn: Proceedings of Seventh IEEE Symposium on Computer Arithmetic, June 4\u20136,\n1985, University of Illinois, Urbana, pp. 9\u201315.\nNicolau, A., Fisher, J.A., 1984. Measuring the parallelism available for very long instruction\nword architectures. IEEE Trans. Comput. C33 (11), 968\u2013976.\nNielsen,\nM.,\n2016.\nNeural\nNetworks\nand\nDeep\nLearning.\nhttp://neuralnetwork\nsanddeeplearning.com/.\nNikhil, R.S., Papadopoulos, G.M., Arvind, 1992. *T: a multithreaded massively parallel\narchitecture. In: Proceedings of 19th Annual International Symposium on Computer\nArchitecture (ISCA), May 19\u201321, 1992, Gold Coast, Australia, pp. 156\u2013167.\nNoordergraaf, L., van der Pas, R., 1999. Performance experiences on Sun\u2019s WildFire\nprototype. In: Proceedings of ACM/IEEE Conference on Supercomputing, November\n13\u201319, 1999, Portland, Ore.\nNvidia, 2016. Tesla GPU Accelerators For Servers. http:// www.nvidia.com/object/tesla-\nservers.html.\nNyberg, C.R., Barclay, T., Cvetanovic, Z., Gray, J., Lomet, D., 1994. AlphaSort: a RISC\nmachinesort. In:Proceedings ofACMSIGMOD, May24\u201327,1994,Minneapolis, Minn.\nOka, M., Suzuoki, M., 1999. Designing and programming the emotion engine. IEEE Micro\n19 (6), 20\u201328.\nOkada, S., Okada, S., Matsuda, Y., Yamada, T., Kobayashi, A., 1999. System on a chip for\ndigital still camera. IEEE Trans. Consum. Electron. 45 (3), 584\u2013590.\nOliker, L., Canning, A., Carter, J., Shalf, J., Ethier, S., 2004. Scientific computations on\nmodern parallel vector systems. In: Proceedings of ACM/IEEE Conference on\nSupercomputing, November 6\u201312, 2004, Pittsburgh, Penn, p. 10.\nOlofsson, A., 2011. Debunking the myth of the $100M ASIC. EE Times. http://www.\neetimes.com/author.asp?section_id\u00bc36&doc_id\u00bc1266014.\nOvtcharov, K., Ruwase, O., Kim, J.Y., Fowers, J., Strauss, K., Chung, E.S., 2015a. Accel-\nerating deep convolutional neural networks using specialized hardware. Microsoft Research\nWhitepaper.\nhttps://www.microsoft.com/en-us/research/publication/accelerating-deep-\nconvolutional-neural-networks-using-specialized-hardware/.\nOvtcharov, K., Ruwase, O., Kim, J.Y., Fowers, J., Strauss, K., Chung, E.S., 2015b. Toward\naccelerating deep learning at scale using specialized hardware in the datacenter. In: 2015\nIEEE Hot Chips 27 Symposium.\nPabst, T., 2000. Performance Showdown at 133 MHz FSB\u2014The Best Platform for\nCoppermine. www6.tomshardware.com/mainboard/00q1/000302/.\nPadua, D., Wolfe, M., 1986. Advanced compiler optimizations for supercomputers.\nCommun. ACM 29 (12), 1184\u20131201.\nPalacharla, S., Kessler, R.E., 1994. Evaluating stream buffers as a secondary cache replace-\nment. In: Proceedings of 21st Annual International Symposium on Computer Architec-\nture (ISCA), April 18\u201321, 1994, Chicago, pp. 24\u201333.\nPalmer, J., Morse, S., 1984. The 8087 Primer. John Wiley & Sons, New York, p. 93.\nReferences\n\u25a0\nR-25"
    },
    {
        "page": 1466,
        "text": "Pan, S.-T., So, K., Rameh, J.T., 1992. Improving the accuracy of dynamic branch prediction\nusing branch correlation. In: Proceedings of Fifth International Conference on Architec-\ntural Support for Programming Languages and Operating Systems (ASPLOS), October\n12\u201315, 1992, Boston, pp. 76\u201384.\nPartridge, C., 1994. Gigabit Networking. Addison-Wesley, Reading, MA.\nPatterson, D., 1985. Reduced instruction set computers. Commun. ACM 28 (1), 8\u201321.\nPatterson, D., 2004. Latency lags bandwidth. Commun. ACM 47 (10), 71\u201375.\nPatterson, D.A., Ditzel, D.R., 1980. The case for the reduced instruction set computer. ACM\nSIGARCH Comput. Architect. News 8 (6), 25\u201333.\nPatterson, D.A., Hennessy, J.L., 2004. Computer Organization and Design: The Hardware/\nSoftware Interface, third ed. Morgan Kaufmann, San Francisco.\nPatterson, D., Nikoli\u0001c, B., 7/25/2015, Agile Design for Hardware, Parts I, II, and III. EE\nTimes, http://www.eetimes.com/author.asp?doc_id\u00bc1327239.\nPatterson, D.A., Garrison, P., Hill, M., Lioupis, D., Nyberg, C., Sippel, T., Van Dyke, K.,\n1983. Architecture of a VLSI instruction cache for a RISC. In: 10th Annual International\nConference on Computer Architecture Conf. Proc., June 13\u201316, 1983, Stockholm,\nSweden, pp. 108\u2013116.\nPatterson, D.A., Gibson, G.A., Katz, R.H., 1987. A Case for Redundant Arrays of Inexpen-\nsive Disks (RAID), Tech. Rep. UCB/CSD 87/391, University of California, Berkeley.\nAlso appeared in Proc. ACM SIGMOD, June 1\u20133, 1988, Chicago, pp. 109\u2013116.\nPavan, P., Bez, R., Olivo, P., Zanoni, E., 1997. Flash memory cells\u2014an overview. Proc.\nIEEE 85 (8), 1248\u20131271.\nPeh, L.S., Dally, W.J., 2001. A delay model and speculative architecture for pipe-lined\nrouters. In: Proceedings of 7th International Symposium on High-Performance Com-\nputer Architecture, January 22\u201324, 2001, Monterrey, Mexico.\nPeng, V., Samudrala, S., Gavrielov, M., 1987. On the implementation of shifters, multi-\npliers, and dividers in VLSI floating point units. In: Proceedings of 8th IEEE Sympo-\nsium on Computer Arithmetic, May 19\u201321, 1987, Como, Italy, pp. 95\u2013102.\nPfister, G.F., 1998. In Search of Clusters, second ed. Prentice Hall, Upper Saddle River, NJ.\nPfister,\nG.F.,\nBrantley,\nW.C.,\nGeorge,\nD.A.,\nHarvey,\nS.L.,\nKleinfekder,\nW.J.,\nMcAuliffe, K.P., Melton, E.A., Norton, V.A., Weiss, J., 1985. The IBM research\nparallel processor prototype (RP3): introduction and architecture. In: Proceedings of\n12th Annual International Symposium on Computer Architecture (ISCA), June\n17\u201319, 1985, Boston, MA, pp. 764\u2013771.\nPinheiro, E., Weber, W.D., Barroso, L.A., 2007. Failure trends in a large disk drive popu-\nlation. In: Proceedings of 5th USENIX Conference on File and Storage Technologies\n(FAST \u201907), February 13\u201316, 2007, San Jose, CA.\nPinkston, T.M., 2004. Deadlock characterization and resolution in interconnection net-\nworks. In: Zhu, M.C., Fanti, M.P. (Eds.), Deadlock Resolution in Computer-Integrated\nSystems. CRC Press, Boca Raton, FL, pp. 445\u2013492.\nPinkston, T.M., Shin, J., 2005. Trends toward on-chip networked microsystems. Int. J. High\nPerform. Comput. Netw. 3 (1), 3\u201318.\nPinkston, T.M., Warnakulasuriya, S., 1997. On deadlocks in interconnection networks.\nIn: 24th Annual International Symposium on Computer Architecture (ISCA), June\n2\u20134, 1997, Denver, CO.\nPinkston, T.M., Benner, A., Krause, M., Robinson, I., Sterling, T., 2003. InfiniBand: the \u2018de\nfacto\u2019 future standard for system and local area networks or just a scalable replacement\nfor PCI buses?\u201d. Cluster Comput. 6 (2), 95\u2013104 (Special issue on communication archi-\ntecture for clusters).\nPostiff, M.A., Greene, D.A., Tyson, G.S., Mudge, T.N., 1999. The limits of instruction level\nparallelism in SPEC95 applications. Comput. Architect. News 27 (1), 31\u201340.\nR-26\n\u25a0\nReferences"
    },
    {
        "page": 1467,
        "text": "Prabhakar, R., Koeplinger, D., Brown, K.J., Lee, H., De Sa, C., Kozyrakis, C., Olukotun, K.,\n2016. Generating configurable hardware from parallel patterns. In: Proceedings of the\nTwenty-First International Conference on Architectural Support for Programming Lan-\nguages and Operating Systems. ACM, pp. 651\u2013665.\nPrakash, T.K., Peng, L., 2008. Performance characterization of spec cpu2006 benchmarks\non intel core 2 duo processor. ISAST Trans. Comput. Softw. Eng. 2 (1), 36\u201341.\nPrzybylski, S.A., 1990. Cache Design: A Performance-Directed Approach. Morgan Kauf-\nmann, San Francisco.\nPrzybylski, S.A., Horowitz, M., Hennessy, J.L., 1988. Performance trade-offs in cache\ndesign. In: 15th Annual International Symposium on Computer Architecture, May\n30\u2013June 2, 1988, Honolulu, Hawaii, pp. 290\u2013298.\nPuente, V., Beivide, R., Gregorio, J.A., Prellezo, J.M., Duato, J., Izu, C., 1999. Adaptive\nbubble router: a design to improve performance in torus networks. In: Proceedings\nof the 28th International Conference on Parallel Processing, September 21\u201324, 1999,\nAizu-Wakamatsu, Fukushima, Japan.\nPutnam, A., Caulfield, A.M., Chung, E.S., Chiou, D., Constantinides, K., Demme, J.,\nEsmaeilzadeh, H., Fowers, J., Gopal, G.P., Gray, J., Haselman, M., Hauck, S., Heil, S.,\nHormati, A., Kim, J.-Y., Lanka, S., Larus, J., Peterson, E., Pope, S., Smith, A.,\nThong, J., Xiao, P.Y., Burger, D., 2014. A reconfigurable fabric for accelerating large-\nscale datacenter services. In: 41st International Symposium on Computer Architecture.\nPutnam, A., Caulfield, A.M., Chung, E.S., Chiou, D., Constantinides, K., Demme, J.,\nEsmaeilzadeh, H., Fowers, J., Gopal, G.P., Gray, J., Haselman, M., Hauck, S.,\nHeil, S., Hormati, A., Kim, J.-Y., Lanka, S., Larus, J., Peterson, E., Pope, S.,\nSmith, A., Thong, J., Xiao, P.Y., Burger, D., 2015. A reconfigurable fabric for accel-\nerating large-scale datacenter services. IEEE Micro. 35(3).\nPutnam, A., Caulfield, A.M., Chung, E.S., Chiou, D., Constantinides, K., Demme, J.,\nEsmaeilzadeh, H., Fowers, J., Gopal, G.P., Gray, J., Haselman, M., Hauck, S., Heil, S.,\nHormati, A., Kim, J.-Y., Lanka, S., Larus, J., Peterson, E., Pope, S., Smith, A.,\nThong,J.,Xiao,P.Y.,Burger,D.,2016.Areconfigurablefabricforacceleratinglarge-scale\ndatacenter services. Commun. ACM. 59 (11), 114\u2013122.\nQadeer, W., Hameed, R., Shacham, O., Venkatesan, P., Kozyrakis, C., Horowitz, M.A.,\n2015. Convolution engine: balancing efficiency & flexibility in specialized computing.\nCommun. ACM 58(4).\nQureshi, M.K., Loh, G.H., 2012. Fundamental latency trade-off in architecting dram caches:\nOutperforming impractical sram-tags with a simple and practical design. In: Proc. 2012\n45th Annual IEEE/ACM International Symposium on Microarchitecture, IEEE Com-\nputer Society, pp. 235\u2013246.\nRadin, G., 1982. The 801 minicomputer. In: Proceedings of Symposium Architectural\nSupport for Programming Languages and Operating Systems (ASPLOS), March\n1\u20133, 1982, Palo Alto, CA, pp. 39\u201347.\nRagan-Kelley, J., Barnes, C., Adams, A., Paris, S., Durand, F., Amarasinghe, S., 2013.\nHalide: a language and compiler for optimizing parallelism, locality, and recomputation\nin image processing pipelines. ACM SIGPLAN Not. 48 (6), 519\u2013530.\nRamacher, U., Beichter, J., Raab, W., Anlauf, J., Bruels, N., Hachmann, A., Wesseling, M.,\n1991. Design of a 1st generation neurocomputer. VLSI Design of Neural Networks.\nSpringer, USA.\nRamamoorthy, C.V., Li, H.F., 1977. Pipeline architecture. ACM Comput. Surv. 9 (1),\n61\u2013102.\nRanganathan, P., Leech, P., Irwin, D., Chase, J., 2006. Ensemble-level power management\nfor dense blade servers. In: Proceedings of 33rd Annual International Symposium on\nComputer Architecture (ISCA), June 17\u201321, 2006, Boston, MA, pp. 66\u201377.\nReferences\n\u25a0\nR-27"
    },
    {
        "page": 1468,
        "text": "Rau, B.R., 1994. Iterative modulo scheduling: an algorithm for software pipelining loops.\nIn: Proceedings of 27th Annual International Symposium on Microarchitecture,\nNovember 30\u2013December 2, 1994, San Jose, CA, pp. 63\u201374.\nRau, B.R., Fisher, J.A., 1993. Instruction-level parallelism. J. Supercomput. 235, Springer\nScience & Business Media.\nRau, B.R., Glaeser, C.D., Picard, R.L., 1982. Efficient code generation for horizontal\narchitectures: compiler techniques and architectural support. In: Proceedings of Ninth\nAnnual International Symposium on Computer Architecture (ISCA), April 26\u201329,\n1982, Austin, TX, pp. 131\u2013139.\nRau, B.R., Yen, D.W.L., Yen, W., Towle, R.A., 1989. The Cydra 5 departmental\nsupercomputer: design philosophies, decisions, and trade-offs. IEEE Comput. 22 (1),\n12\u201334.\nReddi, V.J., Lee, B.C., Chilimbi, T., Vaid, K., 2010. Web search using mobile cores:\nquantifying and mitigating the price of efficiency. In: Proceedings of 37th Annual Inter-\nnational Symposium on Computer Architecture (ISCA), June 19\u201323, 2010, Saint-Malo,\nFrance.\nRedmond, K.C., Smith, T.M., 1980. Project Whirlwind\u2014The History of a Pioneer\nComputer. Digital Press, Boston.\nReinhardt, S.K., Larus, J.R., Wood, D.A., 1994. Tempest and typhoon: user-level shared\nmemory. In: 21st Annual International Symposium on Computer Architecture (ISCA),\nApril 18\u201321, 1994, Chicago, pp. 325\u2013336.\nReinman, G., Jouppi, N.P., 1999. Extensions to CACTI. research.compaq.com/wrl/people/\njouppi/CACTI.html.\nRettberg, R.D., Crowther, W.R., Carvey, P.P., Towlinson, R.S., 1990. The Monarch parallel\nprocessor hardware design. IEEE Comput. 23 (4), 18\u201330.\nRiemens, A., Vissers, K.A., Schutten, R.J., Sijstermans, F.W., Hekstra, G.J., La Hei, G.D.,\n1999. Trimedia CPU64 application domain and benchmark suite. In: Proceedings of\nIEEE International Conference on Computer Design: VLSI in Computers and Proces-\nsors (ICCD\u201999), October 10\u201313, 1999, Austin, TX, pp. 580\u2013585.\nRiseman, E.M., Foster, C.C., 1972. Percolation of code to enhance parallel dispatching and\nexecution. IEEE Trans. Comput. C-21 (12), 1411\u20131415.\nRobin, J., Irvine, C., 2000. Analysis of the Intel Pentium\u2019s ability to support a secure virtual\nmachine monitor. In: Proceedings of USENIX Security Symposium, August 14\u201317,\n2000, Denver, CO.\nRobinson, B., Blount, L., 1986. The VM/HPO 3880-23 Performance Results, IBM Tech.\nBulletin GG66-0247-00. IBM Washington Systems Center, Gaithersburg, MD.\nRopers, A., Lollman, H.W., Wellhausen, J., 1999. DSPstone: Texas Instruments\nTMS320C54x, Tech. Rep. IB 315 1999/9-ISS-Version 0.9. Aachen University of Tech-\nnology, Aachen, Germany (www.ert.rwth-aachen.de/Projekte/Tools/coal/dspstone_\nc54x/index.html).\nRosenblum, M., Herrod, S.A., Witchel, E., Gupta, A., 1995. Complete computer simulation:\nthe SimOS approach. IEEE Parallel Distrib. Technol. 4 (3), 34\u201343.\nRowen, C., Johnson, M., Ries, P., 1988. The MIPS R3010 floating-point coprocessor. IEEE\nMicro 8 (3), 53\u201362.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A.,\nKhosla, A., Bernstein, M., Berg, A.C., 2015. Imagenet large scale visual recognition\nchallenge. Int. J. Comput. Vis. 115(3).\nRussell, R.M., 1978. The Cray-1 processor system. Commun. ACM 21 (1), 63\u201372.\nRymarczyk, J., 1982. Coding guidelines for pipelined processors. In: Proceeding of Sym-\nposium Architectural Support for Programming Languages and Operating Systems\n(ASPLOS), March 1\u20133, 1982, Palo Alto, CA, pp. 12\u201319.\nR-28\n\u25a0\nReferences"
    },
    {
        "page": 1469,
        "text": "Saavedra-Barrera, R.H., 1992. CPU Performance Evaluation and Execution Time Predic-\ntion Using Narrow Spectrum Benchmarking (Ph.D. dissertation). University of Califor-\nnia, Berkeley.\nSalem, K., Garcia-Molina, H., 1986. Disk striping. In: Proceedings of 2nd International\nIEEE Conference on Data Engineering, February 5\u20137, 1986, Washington, DC,\npp. 249\u2013259.\nSaltzer, J.H., Reed, D.P., Clark, D.D., 1984. End-to-end arguments in system design. ACM\nTrans. Comput. Syst. 2 (4), 277\u2013288.\nSamples, A.D., Hilfinger, P.N., 1988. Code Reorganization for Instruction Caches, Tech.\nRep. UCB/CSD 88/447, University of California, Berkeley.\nSantoro, M.R., Bewick, G., Horowitz, M.A., 1989. Rounding algorithms for IEEE multi-\npliers. In: Proceedings of Ninth IEEE Symposium on Computer Arithmetic, September\n6\u20138, Santa Monica, CA, pp. 176\u2013183.\nSatran, J., Smith, D., Meth, K., Sapuntzakis, C., Wakeley, M., Von Stamwitz, P., Haagens,\nR., Zeidner, E., Dalle Ore, L., Klein, Y., 2001. \u201ciSCSI,\u201d IPS Working Group of IETF,\nInternet draft. www.ietf.org/internet-drafts/draft-ietf-ips-iscsi-07.txt.\nSaulsbury, A., Wilkinson, T., Carter, J., Landin, A., 1995. An argument for simple COMA.\nIn: Proceedings of First IEEE Symposium on High-Performance Computer Architec-\ntures, January 22\u201325, 1995, Raleigh, NC, pp. 276\u2013285.\nSchneck, P.B., 1987. Superprocessor Architecture. Kluwer Academic Publishers, Norwell,\nMA.\nSchroeder, B., Gibson, G.A., 2007. Understanding failures in petascale computers. J. Phys.\nConf. Ser. 78 (1), 188\u2013198.\nSchroeder, B., Pinheiro, E., Weber, W.-D., 2009. DRAM errors in the wild: a large-scale\nfield study. In: Proceedings of Eleventh International Joint Conference on Measure-\nment and Modeling of Computer Systems (SIGMETRICS), June 15\u201319, 2009, Seat-\ntle, WA.\nSchurman, E., Brutlag, J., 2009. The user and business impact of server delays.\nIn: Proceedings of Velocity: Web Performance and Operations Conference, June 22\u2013\n24, 2009, San Jose, CA.\nSchwartz, J.T., 1980. Ultracomputers. ACM Trans. Program. Lang. Syst. 4 (2), 484\u2013521.\nScott, N.R., 1985. Computer Number Systems and Arithmetic. Prentice Hall, Englewood\nCliffs, NJ.\nScott, S.L., 1996. Synchronization and communication in the T3E multiprocessor.\nIn: Seventh International Conference on Architectural Support for Programming\nLanguages and Operating Systems (ASPLOS), October 1\u20135, 1996, Cambridge, MA.\nScott, S.L., Goodman, J., 1994. The impact of pipelined channels on k-ary n-cube networks.\nIEEE Trans. Parallel Distrib. Syst. 5 (1), 1\u201316.\nScott, S.L., Thorson, G.M., 1996. The Cray T3E network: adaptive routing in a high per-\nformance 3D torus. In: Proceedings of IEEE HOT Interconnects \u201996, August 15\u201317,\n1996, Stanford University, Palo Alto, CA, pp. 14\u2013156.\nScranton, R.A., Thompson, D.A., Hunter, D.W., 1983. The Access Time Myth. Tech. Rep.\nRC 10197 (45223). IBM, Yorktown Heights, NY.\nSeagate, 2000. Seagate Cheetah 73 Family: ST173404LW/LWV/LC/LCV Product Manual,\nvol. 1. Seagate, Scotts Valley, CA. www.seagate.com/support/disc/manuals/scsi/\n29478b.pdf.\nSeitz, C.L., 1985. The Cosmic Cube (concurrent computing). Commun. ACM 28 (1), 22\u201333.\nSenior, J.M., 1993. Optical Fiber Communications: Principles and Practice, second ed.\nPrentice Hall, Hertfordshire, UK.\nSergio Guadarrama, 2015. BVLC googlenet. https://github.com/BVLC/caffe/tree/ master/\nmodels/bvlc_googlenet.\nReferences\n\u25a0\nR-29"
    },
    {
        "page": 1470,
        "text": "Seznec, A., Michaud, P., 2006. A case for (partially) TAgged GEometric history length\nbranch prediction. J. Instruction Level Parallel. 8, 1\u201323.\nShao, Y.S., Brooks, D., 2015. Research infrastructures for hardware accelerators. Synth.\nLect. Comput. Architect. 10 (4), 1\u201399.\nSharangpani, H., Arora, K., 2000. Itanium processor microarchitecture. IEEE Micro 20 (5),\n24\u201343.\nShurkin, J., 1984. Engines of the Mind: A History of the Computer. W.W. Norton,\nNew York.\nShustek, L.J., 1978. Analysis and Performance of Computer Instruction Sets (Ph.D. disser-\ntation). Stanford University, Palo Alto, CA.\nSilicon Graphics, 1996. MIPS V Instruction Set. http://www.sgi.com/MIPS/arch/ISA5/\n#MIPSV_indx.\nSilver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche, G.,\nSchrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S.,\n2016. Mastering the game of Go with deep neural networks and tree search. Nature\n529(7587).\nSingh, J.P., Hennessy, J.L., Gupta, A., 1993. Scaling parallel programs for multiprocessors:\nmethodology and examples. In: Computer, 2. 7, pp. 22\u201333.\nSingh, A., Ong, J., Agarwal, A., Anderson, G., Armistead, A., Bannon, R., Boving, S.,\nDesai, G., Felderman, B., Germano, P., Kanagala, A., Provost, J., Simmons, J., Eiichi\nTanda, E., Wanderer, J., H\u20acolzle, U., Stuart, S., Vahdat, A., 2015. Jupiter rising: a decade\nof CLOS topologies and centralized control in Google\u2019s datacenter network. ACM SIG-\nCOMM Comput. Commun. Rev. 45 (4), 183\u2013197.\nSinharoy, B., Koala, R.N., Tendler, J.M., Eickemeyer, R.J., Joyner, J.B., 2005. POWER5\nsystem microarchitecture. IBM J. Res. Dev. 49 (4\u20135), 505\u2013521.\nSites, R., 1979. Instruction Ordering for the CRAY-1 Computer, Tech. Rep. 78-CS-023.\nDept. of Computer Science, University of California, San Diego.\nSites, R.L. (Ed.), 1992. Alpha Architecture Reference Manual. Digital Press, Burlington,\nMA.\nSites, R.L., Witek, R. (Eds.), 1995. Alpha Architecture Reference Manual, second ed. Dig-\nital Press, Newton, MA.\nSkadron, K., Clark, D.W., 1997. Design issues and tradeoffs for write buffers.\nIn: Proceedings of Third International Symposium on High-Performance Computer\nArchitecture, February 1\u20135, 1997, San Antonio, TX, pp. 144\u2013155.\nSkadron, K., Ahuja, P.S., Martonosi, M., Clark, D.W., 1999. Branch prediction, instruction-\nwindow size, and cache size: performance tradeoffs and simulation techniques. IEEE\nTrans. Comput. 48(11).\nSlater, R., 1987. Portraits in Silicon. MIT Press, Cambridge, MA.\nSlotnick, D.L., Borck, W.C., McReynolds, R.C., 1962. The Solomon computer.\nIn: Proceedings of AFIPS Fall Joint Computer Conference, December 4\u20136, 1962,\nPhiladelphia, PA, pp. 97\u2013107.\nSmith, B.J., 1978. A pipelined, shared resource MIMD computer. In: Proceedings of Inter-\nnational Conference on Parallel Processing (ICPP), August, Bellaire, MI, pp. 6\u20138.\nSmith, B.J., 1981a. Architecture and applications of the HEP multiprocessor system. Real\nTime Signal Process. IV 298, 241\u2013248.\nSmith, J.E., 1981b. A study of branch prediction strategies. In: Proceedings of Eighth\nAnnual International Symposium on Computer Architecture (ISCA), May 12\u201314,\n1981, Minneapolis, MN, pp. 135\u2013148.\nSmith, A.J., 1982a. Cache memories. Comput. Surv., 14, 3, pp. 473\u2013530.\nSmith, J.E., 1982b. Decoupled access/execute computer architectures. In: Proceedings of\nthe 11th International Symposium on Computer Architecture.\nR-30\n\u25a0\nReferences"
    },
    {
        "page": 1471,
        "text": "Smith, J.E., 1984. Decoupled access/execute computer architectures. ACM Trans. Comput.\nSyst. 2 (4), 289\u2013308.\nSmith, J.E., 1988. Characterizing computer performance with a single number. Commun.\nACM 31 (10), 1202\u20131206.\nSmith, J.E., 1989. Dynamic instruction scheduling and the Astronautics ZS-1. Computer\n22 (7), 21\u201335.\nSmith, J.E., Goodman, J.R., 1983. A study of instruction cache organizations and replace-\nment policies. In: Proceedings of 10th Annual International Symposium on Computer\nArchitecture (ISCA), June 5\u20137, 1982, Stockholm, Sweden, pp. 132\u2013137.\nSmith, A., Lee, J., 1984. Branch prediction strategies and branch-target buffer design. Com-\nputer 17 (1), 6\u201322.\nSmith, J.E., Pleszkun, A.R., 1988. Implementing precise interrupts in pipelined processors.\nIEEE Trans. Comput. 37 (5), 562\u2013573. (This paper is based on an earlier paper that\nappeared in Proceedings of the 12th Annual International Symposium on Computer\nArchitecture (ISCA), June 17\u201319, 1985, Boston, MA.\nSmith, J.E., Dermer, G.E., Vanderwarn, B.D., Klinger, S.D., Rozewski, C.M.,\nFowler, D.L., Scidmore, K.R., Laudon, J.P., 1987. The ZS-1 central processor.\nIn: Proceedings of Second International Conference on Architectural Support for\nProgramming Languages and Operating Systems (ASPLOS), October 5\u20138, 1987,\nPalo Alto, CA, pp. 199\u2013204.\nSmith, M.D., Johnson, M., Horowitz, M.A., 1989. Limits on multiple instruction issue.\nIn: Proceedings of Third International Conference on Architectural Support for\nProgramming Languages and Operating Systems (ASPLOS), April 3\u20136, 1989, Boston,\npp. 290\u2013302.\nSmith, M.D., Horowitz, M., Lam, M.S., 1992. Efficient superscalar performance through\nboosting. In: Proceedings of Fifth International Conference on Architectural Support\nfor Programming Languages and Operating Systems (ASPLOS), October 12\u201315,\n1992, Boston, pp. 248\u2013259.\nSmotherman, M., 1989. A sequencing-based taxonomy of I/O systems and review of\nhistorical machines. Comput. Architect. News 17 (5), 5\u201315. Reprinted in Computer\nArchitecture Readings, Hill, M.D., Jouppi, N.P., Sohi, G.S. (Eds.), 1999. Morgan\nKaufmann, San Francisco, pp. 451\u2013461.\nSodani, A., Sohi, G., 1997. Dynamic instruction reuse. In: Proceedings of 24th Annual Inter-\nnational Symposium on Computer Architecture (ISCA), June 2\u20134, 1997, Denver, CO.\nSohi, G.S., 1990. Instruction issue logic for high-performance, interruptible, multiple\nfunctional unit, pipelined computers. IEEE Trans. Comput. 39 (3), 349\u2013359.\nSohi, G.S., Vajapeyam, S., 1989. Tradeoffs in instruction format design for horizontal archi-\ntectures. In: Proceedings of Third International Conference on Architectural Support for\nProgramming Languages and Operating Systems (ASPLOS), April 3\u20136, 1989, Boston,\npp. 15\u201325.\nSony/Toshiba, 1999. \u2018Emotion Engine\u2019 in PS2 (\u201cIPU is basically an MPEG2 decoder\u2026\u201d).\nhttp://www.cpu-collection.de/?l0\u00bcco&l1\u00bcSony&l2\u00bcEmotion+Engine http://arstechnica.\ncom/gadgets/2000/02/ee/3/.\nSoundararajan,\nV.,\nHeinrich,\nM.,\nVerghese,\nB.,\nGharachorloo,\nK.,\nGupta,\nA.,\nHennessy, J.L., 1998. Flexible use of memory for replication/migration in cache-\ncoherent DSM multiprocessors. In: Proceedings of 25th Annual International Sympo-\nsium on Computer Architecture (ISCA), July 3\u201314, 1998, Barcelona, Spain,\npp. 342\u2013355.\nSPEC, 1989. SPEC Benchmark Suite Release 1.0 (October 2).\nSPEC, 1994. SPEC Newsletter (June).\nReferences\n\u25a0\nR-31"
    },
    {
        "page": 1472,
        "text": "Sporer, M., Moss, F.H., Mathais, C.J., 1988. An introduction to the architecture of the Stellar\nGraphics supercomputer. In: Proceedings of IEEE COMPCON, February 29\u2013March 4,\n1988, San Francisco, p. 464.\nSpurgeon, C., 2001. Charles Spurgeon\u2019s Ethernet Web Site. www.host.ots.utexas.edu/\nethernet/ethernet-home.html.\nSteinberg, D., 2015. Full-Chip Simulations, Keys to Success. In: Proceedings of the\nSynopsys Users Group (SNUG) Silicon Valley 2015.\nStenstr\u20acom, P., Joe, T., Gupta, A., 1992. Comparative performance evaluation of cache-\ncoherent NUMA and COMA architectures. In: Proceedings of 19th Annual Interna-\ntional Symposium on Computer Architecture (ISCA), May 19\u201321, 1992, Gold Coast,\nAustralia, pp. 80\u201391.\nSterling, T., 2001. Beowulf PC Cluster Computing with Windows and Beowulf PC Cluster\nComputing with Linux. MIT Press, Cambridge, MA.\nStern, N., 1980. Who invented the first electronic digital computer? Ann. Hist. Comput.\n2 (4), 375\u2013376.\nStevens, W.R., 1994\u20131996. TCP/IP Illustrated (three volumes). Addison-Wesley, Reading,\nMA.\nStokes, J., 2000. Sound and Vision: A Technical Overview of the Emotion Engine.\narstechnica.com/reviews/1q00/playstation2/ee-1.html.\nStone, H., 1991. High Performance Computers. Addison-Wesley, New York.\nStrauss, W., 1998. DSP Strategies 2002. www.usadata.com/market_research/spr_05/\nspr_r127-005.htm.\nStrecker, W.D., 1976. Cache memories for the PDP-11? In: Proceedings of Third Annual\nInternational Symposium on Computer Architecture (ISCA), January 19\u201321, 1976,\nTampa, FL, pp. 155\u2013158.\nStrecker, W.D., 1978. VAX-11/780: a virtual address extension of the PDP-11 family.\nIn: Proceedings of AFIPS National Computer Conference, June 5\u20138, 1978, Anaheim,\nCA. vol. 47, pp. 967\u2013980.\nSugumar, R.A., Abraham, S.G., 1993. Efficient simulation of caches under optimal replace-\nment with applications to miss characterization. In: Proceedings of ACM SIG-\nMETRICS Conference on Measurement and Modeling of Computer Systems, May\n17\u201321, 1993, Santa Clara, CA, pp. 24\u201335.\nSun Microsystems, 1989. The SPARC Architectural Manual, Version 8, Part No. 8001399-\n09. Sun Microsystems, Santa Clara, CA.\nSussenguth, E., 1999. IBM\u2019s ACS-1 machine. IEEE Comput. 22, 11.\nSwan, R.J., Bechtolsheim, A., Lai, K.W., Ousterhout, J.K., 1977a. The implementation of\nthe Cm* multi-microprocessor. In: Proceedings of AFIPS National Computing Confer-\nence, June 13\u201316, 1977, Dallas, TX, pp. 645\u2013654.\nSwan, R.J., Fuller, S.H., Siewiorek, D.P., 1977b. Cm*\u2014a modular, multi-microprocessor.\nIn: Proceedings of AFIPS National Computing Conference, June 13\u201316, 1977, Dallas,\nTX, pp. 637\u2013644.\nSwartzlander, E. (Ed.), 1990. Computer Arithmetic. IEEE Computer Society Press, Los\nAlamitos, CA.\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,\nVanhoucke,\nV.,\nRabinovich,\nA.,\n2015.\nGoing\ndeeper\nwith\nconvolutions.\nIn: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\nTakagi, N., Yasuura, H., Yajima, S., 1985. High-speed VLSI multiplication algorithm with a\nredundant binary addition tree. IEEE Trans. Comput. C-34 (9), 789\u2013796.\nTalagala, N., 2000. Characterizing Large Storage Systems: Error Behavior and Performance\nBenchmarks (Ph.D. dissertation). Computer Science Division, University of California,\nBerkeley.\nR-32\n\u25a0\nReferences"
    },
    {
        "page": 1473,
        "text": "Talagala, N., Patterson, D., 1999. An Analysis of Error Behavior in a Large Storage System,\nTech. Report UCB//CSD-99-1042. Computer Science Division, University of Califor-\nnia, Berkeley.\nTalagala, N., Arpaci-Dusseau, R., Patterson, D., 2000a. Micro-Benchmark Based Extraction\nof Local and Global Disk Characteristics, CSD-99-1063. Computer Science Division,\nUniversity of California, Berkeley.\nTalagala, N., Asami, S., Patterson, D., Futernick, R., Hart, D., 2000b. The art of massive\nstorage: a case study of a Web image archive. Computer 33 (11), 22\u201328.\nTamir, Y., Frazier, G., 1992. Dynamically-allocated multi-queue buffers for VLSI commu-\nnication switches. IEEE Trans. Comput. 41 (6), 725\u2013734.\nTanenbaum, A.S., 1978. Implications of structured programming for machine architecture.\nCommun. ACM 21 (3), 237\u2013246.\nTanenbaum, A.S., 1988. Computer Networks, second ed. Prentice Hall, Englewood Cliffs, NJ.\nTang, C.K., 1976. Cache design in the tightly coupled multiprocessor system.\nIn: Proceedings of AFIPS National Computer Conference, June 7\u201310, 1976, New York,\npp. 749\u2013753.\nTanqueray, D., 2002. The Cray X1 and supercomputer road map. In: Proceedings of 13th\nDaresbury Machine Evaluation Workshop, December 11\u201312, 2002, Daresbury Labora-\ntories, Daresbury, Cheshire, UK.\nTarjan, D., Thoziyoor, S., Jouppi, N., 2005. HPL Technical Report on CACTI 4.0. www.\nhpl.hp.com/techeports/2006/HPL\u00bc2006+86.html.\nTaylor, G.S., 1981. Compatible hardware for division and square root. In: Proceedings of\n5th IEEE Symposium on Computer Arithmetic, May 18\u201319, 1981, University of Mich-\nigan, Ann Arbor, MI, pp. 127\u2013134.\nTaylor, G.S., 1985. Radix 16 SRT dividers with overlapped quotient selection stages.\nIn: Proceedings of Seventh IEEE Symposium on Computer Arithmetic, June 4\u20136,\n1985, University of Illinois, Urbana, IL, pp. 64\u201371.\nTaylor, G., Hilfinger, P., Larus, J., Patterson, D., Zorn, B., 1986. Evaluation of the SPUR\nLISP architecture. In: Proceedings of 13th Annual International Symposium on Com-\nputer Architecture (ISCA), June 2\u20135, 1986, Tokyo.\nTaylor, M.B., Lee, W., Amarasinghe, S.P., Agarwal, A., 2005. Scalar operand networks.\nIEEE Trans. Parallel Distrib. Syst. 16 (2), 145\u2013162.\nTendler, J.M., Dodson, J.S., Fields Jr., J.S., Le, H., Sinharoy, B., 2002. Power4 system\nmicroarchitecture. IBM J. Res. Dev. 46 (1), 5\u201326.\nTensorFlow Tutorials, 2016. https://www.tensorflow.org/versions/r0.12/tutorials/index.html.\nTexas Instruments, 2000. History of Innovation: 1980s. www.ti.com/corp/docs/company/\nhistory/1980s.shtml.\nTezzaron Semiconductor, 2004. Soft Errors in Electronic Memory, White Paper. Tezzaron\nSemiconductor, Naperville, IL http://www.tezzaron.com/about/papers/soft_errors_1_\n1_secure.pdf.\nThacker, C.P., McCreight, E.M., Lampson, B.W., Sproull, R.F., Boggs, D.R., 1982. Alto: a\npersonal computer. In: Siewiorek, D.P., Bell, C.G., Newell, A. (Eds.), Computer Struc-\ntures: Principles and Examples. McGraw-Hill, New York, pp. 549\u2013572.\nThadhani, A.J., 1981. Interactive user productivity. IBM Syst. J. 20 (4), 407\u2013423.\nThekkath, R., Singh, A.P., Singh, J.P., John, S., Hennessy, J.L., 1997. An evaluation of a\ncommercial\nCC-NUMA\narchitecture\u2014the\nCONVEX\nExemplar\nSPP1200.\nIn: Proceedings of 11th International Parallel Processing Symposium (IPPS), April\n1\u20137, 1997, Geneva, Switzerland.\nThorlin, J.F., 1967. Code generation for PIE (parallel instruction execution) computers.\nIn: Proceedings of Spring Joint Computer Conference, April 18\u201320, 1967, Atlantic City,\nNJ, p. 27.\nReferences\n\u25a0\nR-33"
    },
    {
        "page": 1474,
        "text": "Thornton,J.E.,1964.ParalleloperationintheControlData6600.In:ProceedingsofAFIPSFall\nJoint Computer Conference, Part II, October 27\u201329, 1964, San Francisco. 26, pp. 33\u201340.\nThornton, J.E., 1970. Design of a Computer, the Control Data 6600. Scott Foresman,\nGlenview, IL.\nTjaden, G.S., Flynn, M.J., 1970. Detection and parallel execution of independent instruc-\ntions. IEEE Trans. Comput. C-19 (10), 889\u2013895.\nTomasulo, R.M., 1967. An efficient algorithm for exploiting multiple arithmetic units. IBM\nJ. Res. Dev. 11 (1), 25\u201333.\nTorrellas, J., Gupta, A., Hennessy, J., 1992. Characterizing the caching and synchroniza-\ntion performance of a multiprocessor operating system. In: Proceedings of Fifth Inter-\nnational Conference on Architectural Support for Programming Languages and\nOperating Systems (ASPLOS), October 12\u201315, 1992, Boston (SIGPLAN Notices\n27:9 (September), pp. 162\u2013174.\nTouma, W.R., 1993. The Dynamics of the Computer Industry: Modeling the Supply of\nWorkstations and Their Components. Kluwer Academic, Boston.\nTuck, N., Tullsen, D., 2003. Initial observations of the simultaneous multithreading Pentium\n4 processor. In: Proceedings of 12th International Conference on Parallel Architectures\nand\nCompilation\nTechniques\n(PACT\u201903),\nSeptember\n27\u2013October\n1,\n2003,\nNew Orleans, LA, pp. 26\u201334.\nTullsen, D.M., Eggers, S.J., Levy, H.M., 1995. Simultaneous multithreading: Maximizing\non-chip parallelism. In: Proceedings of 22nd Annual International Symposium on\nComputer\nArchitecture\n(ISCA),\nJune\n22\u201324,\n1995,\nSanta\nMargherita,\nItaly,\npp. 392\u2013403.\nTullsen, D.M., Eggers, S.J., Emer, J.S., Levy, H.M., Lo, J.L., Stamm, R.L., 1996. Exploiting\nchoice: instruction fetch and issue on an implementable simultaneous multithreading\nprocessor. In: Proceedings of 23rd Annual International Symposium on Computer\nArchitecture (ISCA), May 22\u201324, 1996, Philadelphia, PA, pp. 191\u2013202.\nTung, L., 2016. Google Translate: \u2018This landmark update is our biggest single leap in\n10\nyears\u2019,\nZDNet.\nhttp://www.zdnet.com/article/google-translate-this-landmark-\nupdate-is-our-biggest-single-leap-in-10years/.\nUngar, D., Blau, R., Foley, P., Samples, D., Patterson, D., 1984. Architecture of SOAR:\nSmalltalk on a RISC. In: Proceedings of 11th Annual International Symposium on\nComputer Architecture (ISCA), June 5\u20137, 1984, Ann Arbor, MI, pp. 188\u2013197.\nUnger, S.H., 1958. A computer oriented towards spatial problems. Proc. Inst. Radio Eng.\n46 (10), 1744\u20131750.\nVahdat, A., Al-Fares, M., Farrington, N., Niranjan Mysore, R., Porter, G., Radhakrishnan, S.,\n2010. Scale-out networking in the data center. IEEE Micro 30 (4), 29\u201341.\nVaidya, A.S., Sivasubramaniam, A., Das, C.R., 1997. Performance benefits of virtual chan-\nnels and adaptive routing: an application-driven study. In: Proceedings of ACM/IEEE\nConference on Supercomputing, November 16\u201321, 1997, San Jose, CA.\nVajapeyam, S., 1991. Instruction-Level Characterization of the Cray Y-MP Processor\n(Ph.D. thesis). Computer Sciences Department, University of Wisconsin-Madison.\nvan Eijndhoven, J.T.J., Sijstermans, F.W., Vissers, K.A., Pol, E.J.D., Tromp, M.I.A.,\nStruik, P., Bloks, R.H.J., van der Wolf, P., Pimentel, A.D., Vranken, H.P.E., 1999.\nTrimedia CPU64 architecture. In: Proceedings of IEEE International Conference on\nComputer Design: VLSI in Computers and Processors (ICCD\u201999), October 10\u201313,\n1999, Austin, TX, pp. 586\u2013592.\nVan Vleck, T., 2005. The IBM 360/67 and CP/CMS. http://www.multicians.org/thvv/\n360-67.html.\nVanhoucke, V., Senior, A., Mao, M.Z., 2011. Improving the speed of neural networks\non CPUs. https://static.googleusercontent.com/media/research.google.com/en//pubs/\narchive/37631.pdf.\nR-34\n\u25a0\nReferences"
    },
    {
        "page": 1475,
        "text": "von Eicken, T., Culler, D.E., Goldstein, S.C., Schauser, K.E., 1992. Active messages: a\nmechanism for integrated communication and computation. In: Proceedings of 19th\nAnnual International Symposium on Computer Architecture (ISCA), May 19\u201321,\n1992, Gold Coast, Australia.\nWaingold, E., Taylor, M., Srikrishna, D., Sarkar, V., Lee, W., Lee, V., Kim, J., Frank, M.,\nFinch, P., Barua, R., Babb, J., Amarasinghe, S., Agarwal, A., 1997. Baring it all to soft-\nware: raw machines. IEEE Comput. 30, 86\u201393.\nWakerly, J., 1989. Microcomputer Architecture and Programming. Wiley, New York.\nWall, D.W., 1991. Limits of instruction-level parallelism. In: Proceedings of Fourth\nInternational Conference on Architectural Support for Programming Languages and\nOperating Systems (ASPLOS), April 8\u201311, 1991, Palo Alto, CA, pp. 248\u2013259.\nWall, D.W., 1993. Limits of Instruction-Level Parallelism, Research Rep. 93/6, Western\nResearch Laboratory. Digital Equipment Corp., Palo Alto, CA.\nWalrand, J., 1991. Communication Networks: A First Course. Aksen Associates/Irwin,\nHomewood, IL.\nWang, W.-H., Baer, J.-L., Levy, H.M., 1989. Organization and performance of a two-\nlevel virtual-real cache hierarchy. In: Proceedings of 16th Annual International\nSymposium on Computer Architecture (ISCA), May 28\u2013June 1, 1989, Jerusalem,\npp. 140\u2013148.\nWatanabe, T., 1987. Architecture and performance of the NEC supercomputer SX system.\nParallel Comput. 5, 247\u2013255.\nWaters, F. (Ed.), 1986. IBM RT Personal Computer Technology, SA 23-1057. IBM, Austin,\nTX.\nWatson, W.J., 1972. The TI ASC\u2014a highly modular and flexible super processor architec-\nture. In: Proceedings of AFIPS Fall Joint Computer Conference, December 5\u20137, 1972,\nAnaheim, CA, pp. 221\u2013228.\nWeaver, D.L., Germond, T., 1994. The SPARC Architectural Manual, Version 9. Prentice\nHall, Englewood Cliffs, NJ.\nWeicker, R.P., 1984. Dhrystone: a synthetic systems programming benchmark. Commun.\nACM 27 (10), 1013\u20131030.\nWeiss, S., Smith, J.E., 1984. Instruction issue logic for pipelined supercomputers.\nIn: Proceedings of 11th Annual International Symposium on Computer Architecture\n(ISCA), June 5\u20137, 1984, Ann Arbor, MI, pp. 110\u2013118.\nWeiss, S., Smith, J.E., 1987. A study of scalar compilation techniques for pipelined super-\ncomputers. In: Proceedings of Second International Conference on Architectural\nSupport for Programming Languages and Operating Systems (ASPLOS), October\n5\u20138, 1987, Palo Alto, CA, pp. 105\u2013109.\nWeiss, S., Smith, J.E., 1994. Power and PowerPC. Morgan Kaufmann, San Francisco.\nWendel, D., Kalla, R., Friedrich, J., Kahle, J., Leenstra, J., Lichtenau, C., Sinharoy, B.,\nStarke,W., Zyuban, V., 2010. The Power7 processorSoC.In: Proceedingsof International\nConference on IC Design and Technology, June 2\u20134, 2010, Grenoble, France, pp. 71\u201373.\nWeste, N., Eshraghian, K., 1993. Principles of CMOS VLSI Design: A Systems Perspective,\n2nd ed. Addison-Wesley, Reading, MA.\nWiecek, C., 1982. A case study of the VAX 11 instruction set usage for compiler execution.\nIn: Proceedings of Symposium on Architectural Support for Programming Languages\nand Operating Systems (ASPLOS), March 1\u20133, 1982, Palo Alto, CA, pp. 177\u2013184.\nWilkes, M., 1965. Slave memories and dynamic storage allocation. IEEE Trans. Electron.\nComput. EC-14 (2), 270\u2013271.\nWilkes, M.V., 1982. Hardware support for memory protection: capability implementa-\ntions. In: Proceedings of Symposium on Architectural Support for Programming\nLanguages and Operating Systems (ASPLOS), March 1\u20133, 1982, Palo Alto, CA,\npp. 107\u2013116.\nReferences\n\u25a0\nR-35"
    },
    {
        "page": 1476,
        "text": "Wilkes, M.V., 1985. Memoirs of a Computer Pioneer. MIT Press, Cambridge, MA.\nWilkes, M.V., 1995. Computing Perspectives. Morgan Kaufmann, San Francisco.\nWilkes, M.V., Wheeler, D.J., Gill, S., 1951. The Preparation of Programs for an Electronic\nDigital Computer. Addison-Wesley, Cambridge, MA.\nWilliams, T.E., Horowitz, M., Alverson, R.L., Yang, T.S., 1987. A self-timed chip for divi-\nsion. In: Losleben, P. (Ed.), 1987 Stanford Conference on Advanced Research in VLSI.\nMIT Press, Cambridge, MA.\nWilliams, S., Waterman, A., Patterson, D., 2009. Roofline: an insightful visual performance\nmodel for multicore architectures. Commun. ACM 52 (4), 65\u201376.\nWilson Jr., A.W., 1987. Hierarchical cache/bus architecture for shared-memory multipro-\ncessors. In: Proceedings of 14th Annual International Symposium on Computer\nArchitecture (ISCA), June 2\u20135, 1987, Pittsburgh, PA, pp. 244\u2013252.\nWilson, R.P., Lam, M.S., 1995. Efficient context-sensitive pointer analysis for C programs.\nIn: Proceedings of ACM SIGPLAN\u201995 Conference on Programming Language Design\nand Implementation, June 18\u201321, 1995, La Jolla, CA, pp. 1\u201312.\nWolfe, A., Shen, J.P., 1991. A variable instruction stream extension to the VLIW architec-\nture. In: Proceedings of Fourth International Conference on Architectural Support for\nProgramming Languages and Operating Systems (ASPLOS), April 8\u201311, 1991, Palo\nAlto, CA, pp. 2\u201314.\nWood, D.A., Hill, M.D., 1995. Cost-effective parallel computing. IEEE Comput. 28 (2), 69\u201372.\nWu, Y., Schuster, M., Chen, Z., Le, Q., Norouzi, M., Macherey, W., Krikun, M., Cao, Y.,\nGao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, \u0141.,\nGouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N.,\nWang, W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O., Corrado, G.,\nHughes, M., Dean, J., 2016. Google\u2019s Neural Machine Translation System: Bridging\nthe Gap between Human and Machine Translation. http://arxiv.org/abs/1609.08144.\nWulf, W., 1981. Compilers and computer architecture. Computer 14 (7), 41\u201347.\nWulf, W., Bell, C.G., 1972. C.mmp\u2014a multi-mini-processor. In: Proceedings of AFIPS Fall\nJoint Computer Conference, December 5\u20137, 1972, Anaheim, CA, pp. 765\u2013777.\nWulf, W., Harbison, S.P., 1978. Reflections in a pool of processors\u2014an experience report\non C.mmp/Hydra. In: Proceedings of AFIPS National Computing Conference, June 5\u2013\n8, 1978, Anaheim, CA, pp. 939\u2013951.\nWulf, W.A., McKee, S.A., 1995. Hitting the memory wall: implications of the obvious.\nACM SIGARCH Comput. Architect. News 23 (1), 20\u201324.\nWulf, W.A., Levin, R., Harbison, S.P., 1981. Hydra/C.mmp: An Experimental Computer\nSystem. McGraw-Hill, New York.\nYamamoto, W., Serrano, M.J., Talcott, A.R., Wood, R.C., Nemirosky, M., 1994. Perfor-\nmance estimation of multistreamed, superscalar processors. In: Proceedings of 27th\nAnnual Hawaii International Conference on System Sciences, January 4\u20137, 1994, Maui,\npp. 195\u2013204.\nYang, Y., Mason, G., 1991. Nonblocking broadcast switching networks. IEEE Trans.\nComput. 40 (9), 1005\u20131015.\nYeager, K., 1996. The MIPS R10000 superscalar microprocessor. IEEE Micro 16 (2), 28\u201340.\nYeh, T., Patt, Y.N., 1993a. Alternative implementations of two-level adaptive branch\nprediction. In: Proceedings of 19th Annual International Symposium on Computer\nArchitecture (ISCA), May 19\u201321, 1992, Gold Coast, Australia, pp. 124\u2013134.\nYeh, T., Patt, Y.N., 1993b. A comparison of dynamic branch predictors that use two levels\nof branch history. In: Proceedings of 20th Annual International Symposium on Com-\nputer Architecture (ISCA), May 16\u201319, 1993, San Diego, CA, pp. 257\u2013266.\nR-36\n\u25a0\nReferences"
    },
    {
        "page": 1477,
        "text": "Index\nPage references in bold represent\nfigures, tables and boxes.\nA\nAbsolute addressing mode, K-34\nAccelerated Strategic Computing\nInitiative (ASCI)\nASCI Red, F-104\u2013105\nASCI White, F-71, F-105\nAccess 1/Access 2 stages, TI 320C55\nDSP, E-7\nAccess bit, B-52\nAccess time. See also Average memory\naccess time (AMAT)\nDRAM/magnetic disk, D-3\nDSM, 372\u2013373\nmemory hierarchy design, 85\nslowdown causes, B-3, B-3\nSMPs, 371\nAccess time gap, D-3\nAccumulator, 557\u2013558\narchitecture, A-3\nextended, A-3\nAcknowledgment, packets, F-17\nACM. See Association for Computing\nMachinery (ACM)\nACS project, M-29\u201330\nActivation hardware, 557\u2013558\nAda language, integer division/\nremainder, J-12\nAdaptive routing\ndefinition, F-47\u201348\nvs. deterministic routing,\nF-53\u201356\nnetwork fault tolerance, F-98\nand overhead, F-97\nAdders\ncarry-lookahead, J-37\u201341\nchip comparison, J-61\nfull, J-2\u20133, J-3\nhalf, J-2\u20133\ninteger division speedup, J-54\u201357\ninteger multiplication speedup\neven/odd array, J-52\nmany adders, J-50\u201354, J-50\nmultipass array multiplier, J-51\nsigned-digit addition table, J-54\nsingle adder, J-47\u201349, J-48\u201349\nWallace tree, J-53\nradix-2 division, J-55\nradix-4 division, J-56\nradix-4 SRT division, J-57\nripple-carry, J-3, J-3\ntime/space requirements, J-44\nAddition operations\nchip comparison, J-61\nfloating point\ndenormals, J-26\u201327\noverview, J-21\u201325\nrules, J-24\nspeedup, J-25\u201326\ninteger, speedup\ncarry-lookahead, J-37\u201341\ncarry-lookahead circuit, J-38\ncarry-lookahead tree, J-40\ncarry-lookahead tree adder,\nJ-41\ncarry-select adder, J-43\u201344,\nJ-43\u201344\ncarry-skip adder, J-41\u201343, J-42\noverview, J-37\nripply-carry addition, J-2\u20133, J-3\nAddress aliasing prediction, 239\u2013240\nAddress fault, B-42\nAddressing modes\nabsolute, K-34\nbased indexed addressing, K-34\nbase plus scaled indexed, K-34\nfor control flow instructions,\nA-17\u201318\ndata addressing modes, K-32\u201335\ndisplacement, A-11\u201312\nand instruction formats, K-6\u20139\ninstruction set architecture, 13\nmemory addressing, A-8\u201311, A-10\nregister indirect, K-34\nRISC-V, A-36\nAddress offset, B-55\u201356\nAddress space\nglobal, B-52\nlocal, B-52\nmemory hierarchy, B-57\nshared memory, 373\nvirtual memory, B-12, B-40, B-41,\nB-44, B-55\nAddress specifier, A-21, K-54\nAddress stage, TI 320C55 DSP, E-7\nAddress trace, B-4\nAddress translation, B-42\nAMD64 paged virtual memory,\nB-55\nduring indexing, B-36\u201340, 83\nOpteron data TLB, B-47\ntranslation lookaside buffers, B-37,\nB-46, B-47\nvirtual memory, B-46, B-47, 120\nAdvanced load address table (ALAT)\nIA-64 ISA, H-40\nvector sparse matrices, G-12\u201313\nAdvanced loads, IA-64 ISA, H-40\nI-1"
    },
    {
        "page": 1478,
        "text": "Advanced mobile phone service\n(AMPS), cell phones,\nE-25\nAdvanced Research Project Agency\n(ARPA), F-102\u2013103\nAdvanced RISC Machine (ARM), 12\narchitecture, K-22\nGPU computing history, M-53\nAdvanced Simulation and Computing\n(ASC) program, F-106\nAdvanced Switching Interconnect\n(ASI), F-107\nAdvanced Switching SAN, F-71\nAdvanced Technology Attachment\n(ATA) disks\nBerkeley\u2019s Tertiary Disk project,\nD-12\u201313\ndisk power, D-5\ndisk storage, D-4\nhistorical background, M-88\nRAID 6, D-8\u20139\nAdvanced vector extensions (AVX),\n282, 305, 306\nAffine, loop-level parallelism\ndependences, H-6\nAfter rounding rule, J-36\nAggregate bandwidth\ndefinition, F-13\neffective bandwidth calculations,\nF-18\u201319\nshared- vs. switched-media\nnetworks, F-23, F-25\nswitched-media networks, F-24\u201325\nswitch microarchitecture, F-56\nAI. See Artificial intelligence (AI)\nAiken, Howard, M-3\u20134\nALAT. See Advanced load address\ntable (ALAT)\nAlewife machine, M-62\nALGOL, M-17\u201318\nAliases, address translation, B-38\nAllen, Fran, M-29\u201330\nAlliant processors, G-26\nAlloy cache, 115\nAlloyed predictors, 184\nAlpha 21164\ncache hierarchy, characteristics,\n395\nL1 caches, 395\nAlphaServer, 395\u2013396\nAlphaServer 4100, 395\nAltaVista search, cluster history, M-63,\nM-74\u201375\nALUs. See Arithmetic-logical units\n(ALUs)\nAMAT. See Average memory access\ntime (AMAT)\nAmazon\nDynamo key-value storage system,\n485\u2013486\nElastic Computer Cloud, 491\u2013492\nSimple Storage Service, 491\u2013492\nwarehouse-scale computers, 10\nAmazon Elastic Computer Cloud\n(EC2), utility\ncomputing, M-75\u201376\nAmazon Web Services (AWS)\navailability zones, 497\u2013501, 497\ncloud computing, 491\u2013497\nEC2 computer unit, 493\u2013494\ngrowth, 500\nguarantee of service, 492\nlow cost, 492\nreliance on open source software,\n492\nvirtual machines, 491\nXen virtual machine, 126\nAmdahl, Gene, M-29\u201330\nAmdahl\u2019s Law, 5\ncomputer design principles, 49\u201352\ncomputer system power\nconsumption case study,\n69\u201371\nexecution time, 50\nmulticore scaling, 436, 438, 442\nparallel processing calculations,\n373\u2013377\npitfall, 61\nsoftware overhead, F-96\nspeedup, 374\u2013375\nVMIPS on Linpack, G-18\nAMD Athlon 64, Itanium 2\ncomparison, H-43\nAMD Fusion, M-53\nAMD K-5, M-35\nAMD Opteron, 387\u2013388\naddress translation, B-38\ndata cache example, B-12\u201315, B-13\nimplementation, 391\nmicroprocessor, 27\nmisses per instruction, B-15\nNetApp FAS6000 filer, D-42\npaged virtual memory example,\nB-54\u201357\nvs. Pentium protection, B-57\nprocessors, 403\nTLB during address translation,\nB-47\ntournament predictors, 185\u2013187\nAMD processors\nGPU computing history, M-53\npower consumption, F-89\nrecent advances, M-35\nRISC history, M-23\nAmortization of overhead, D-64\u201367\nAmple parallelism, 467\nAndreessen, Marc, F-102\nAndrew benchmark, 399\nAnnual failure rate, 62\nAntenna, radio receiver, E-23\nAntialiasing, B-38\nAntidependence\ncompiler history, M-32\ndefinition, 172\nfinding, H-7\u20138\nregister renaming, 196\nApollo DN 10000, M-32\nApplication layer, F-84\nApplied Minds, M-76\nArbitration algorithm\ncollision detection, F-23\u201324\ncommercial interconnection\nnetworks, F-57\ninterconnection networks, F-21\u201322,\nF-49\u201351\nnetwork impact, F-52\u201353\nSAN characteristics, F-76\u201377\nswitched-media networks, F-24\u201325\nswitch microarchitecture, F-56\npipelining, F-65\u201366\nsystem area network, F-104\u2013105\nArchitecturally visible registers, 234\nArchitectural Support for Compilers\nand Operating Systems\n(ASPLOS), M-12\nArchitecture. See also Computer\narchitecture; Instruction\nset architecture (ISA)\nand compiler writer, A-30\u201331\nmicroarchitecture, 266\u2013273\nAreal density, D-2\nArgument pointer, K-57\nArithmetic intensity, 307\u2013308\nI-2\n\u25a0\nIndex"
    },
    {
        "page": 1479,
        "text": "Arithmetic-logical units (ALUs)\ndata forwarding, C-36\u201337\ndata hazards stall minimization,\nC-14\u201317\nDSP media extensions, E-10\neffective address cycle, C-5\nIA-64 instructions, H-35\ninteger division, J-54\ninteger multiplication, J-48\ninteger operations, C-46\u201348\ninteger shifting over zeros, J-45\nlatency, C-46\u201348\nload interlocks, C-35\nmicro-op fusion, 254\nMIPS R4000 pipeline, C-59\nmulticycle implementation, C-29\noperation, C-27\u201328\npipeline branch issues, C-35\u201336\nRISC classic pipeline, C-8\nRISC instruction set, C-5\nRISC pipeline, C-31\u201332, C-35\nTX-2, M-50\nARM. See Advanced RISC Machine\n(ARM)\nARM AMBA, OCNs, F-3\nARM Cortex-A53\ncharacteristics, 259\nclock cycles per instruction,\n251\u2013252, 252\ndata miss rate, 132\nmemory hierarchy design,\n129\u2013131, 130\nmisprediction rate, 250\nmultiple-issue processors, 247\u2013252\npipeline performance, 249,\n250\u2013252\nvirtual address, physical and data\nblocks, 131\nARMv8, K-4, K-9, 13, K-15, K-16,\nK-22\nARPANET, F-102\nArray\nFFT kernel, I-7\nocean application, I-9\u201310\nrecurrences, H-12\nArray multiplier\nexample, J-50\nintegers, J-50\nmultipass system, J-51\nArtificial intelligence (AI), 546\nASC Purple, F-71, F-105\nASPLOS. See Architectural Support\nfor Compilers and\nOperating Systems\n(ASPLOS)\nAssembly language, 2\nAssociation for Computing Machinery\n(ACM), M-3\nAssociativity. See also Set associativity\nOpteron data cache, B-13\u201314, B-13\nsizes and, B-10\nAstronautics ZS-1, M-31\nAsynchronous events, exception, C-39\nAsynchronous I/O, D-35\u201336\nAsynchronous Transfer Mode (ATM)\ninterconnection networks,\nF-102\u2013103\nLAN, F-93\u201394\npacket format, F-79\ntotal time statistics, F-94\nVOQs, F-60\u201361\nWANs, F-4, F-84\u201385, F-102\u2013103\nATA disks. See Advanced Technology\nAttachment (ATA) disks\nAtanasoff Berry Computer (ABC), M-5\nAtanasoff, John, M-5\nATI Radeon 9700, M-51\u201352\nAtlas computer, M-9\nATM system, TP benchmarks, D-18\nAtom 230, 258, 259\nAtomic exchange, 413\nAtomic instructions, barrier\nsynchronization, I-14\nAtomic operations, 386\nAttributes field, B-52\nAutoincrement deferred addressing,\nK-52\u201353\nAutonet, F-49\nAutonomous instruction fetch units, 127\nAvailability\ncomputer systems, D-43\nI/O system design/evaluation,\nD-36\u201337\nAverage instruction execution time,\nM-6\nAverage memory access time (AMAT)\nblock size calculations, B-26\u201328,\nB-28\ncache optimizations, B-22,\nB-26\u201328\ncache performance, B-15\u201317, B-22\nmemory hierarchy design, 82\nmiss rate, B-29\u201330, B-30\nout-of-order computer, B-21\nand processor performance,\nB-17\u201320\nusing miss rates, B-30\nAverage reception factor\ncentralized switched networks,\nF-33\nmulti-device interconnection\nnetworks, F-26\u201327\nAWS. See Amazon Web Services\n(AWS)\nB\nBack-off time, shared-media networks,\nF-103\nBackpressure, congestion\nmanagement, F-69\nBackpropagation, 548\nBackside bus, 377\nBalanced systems, D-64\u201367\nBalanced tree, MINs with nonblicking,\nF-34\nBandwidth. See also Cache bandwidth;\nThroughput\narbitration, F-49\u201350\nbisection, F-39\u201340, F-93, 478\nand cache miss, B-2\ncommunication mechanism, I-3\ncompute, 350\ncongestion management, F-68\nCray Research T3D, F-91\ndefinition, F-13\ndisparity, F-29\nFP arithmetic, J-62\ngap, disk storage, D-3\ninstruction fetch, 228\u2013232,\n229\u2013230\nlatency and effective, F-25\u201330\nlog-log plot, 21\nmemory, 350, 356\nnetwork performance and topology,\nF-41\nover latency, 20\npoint-to-point links and switches,\nD-34\nshared- vs. switched-media\nnetworks, F-23, F-25\nsnooping, 389\u2013390\ntwo-device networks, F-13\u201320\nfor vector load/store units, 298\u2013299\nIndex\n\u25a0\nI-3"
    },
    {
        "page": 1480,
        "text": "Banerjee, Uptal, M-32\nBank busy time, vector memory\nsystems, G-9\nBanked memory, 346\nvector architectures, G-10\nBarcelona Supercomputer Center, F-80\nBarnes\ncharacteristics, I-8\u20139\ndistributed-memory\nmultiprocessor, I-32\nsymmetric shared-memory\nmultiprocessors,\nI-21\u201322, I-23, I-25\u201326\nBarnes-Hut n-body algorithm, I-8\u20139\nBarriers\nCray X1, G-23\nfetch-and-increment, I-20\u201321\nlarge-scale multiprocessor\nsynchronization,\nI-20\u201321\nlarge-scale multiprocessor,\nsynchronization,\nI-13\u201316, I-14, I-16,\nI-19, I-20\nBased indexed addressing mode,\nK-34\nBase field, B-52\nBase plus scaled indexed addressing\nmode, K-34\u201335\nBase station, E-22\u201323\nBatches, DNNs, 556\nBatch processing workloads, 467\nBay Area Research Network\n(BARRNet), F-83\nBBN Butterfly, M-61\nBBN Monarch, M-61\nBefore rounding rule, J-36\nBenchmarks. See also Thread Block;\nspecific benchmark\ndesktop benchmarks, 41\u201343\ndistribution of data accesses by,\nA-14\nEEMBC, E-12, E-12\nembedded applications\nbasic considerations, E-12\npower consumption and\nefficiency, E-13,\nE-13\u201314\nfallacy, 61\nperformance measurement,\n40\u201345\nresponse time restrictions, D-18\nsorting case study, D-64\u201367\nsuite, 41\nBene\u015d topology, F-33, F-34\nBerkeley\u2019s Tertiary Disk project\nfailures of components, D-12\noverview, D-12\u201313\nsystem log, D-43\nBerners-Lee, Tim, F-102\nBertram, Jack, M-29\u201330\nBest-case lower bounds, F-26\nBest-case upper bounds, F-26\nBetween instructions exception, C-39,\nC-45\nBiased exponent, J-15\u201316\nBidirectional multistage\ninterconnection\nnetworks, F-33\u201334\nBidirectional rings, F-36\nBig Endian\nbyte order, A-7\ninterconnection networks, F-12\nBINAC, M-5\nBinary code compatibility, embedded\nsystems, E-15\nBinary-coded decimal, A-14\nBinary-to-decimal conversion, FP\nprecisions, J-33\u201334\nBing search engine\nnegative impact, 486\nWSCs, 485\nBisection bandwidth, 478\nas network cost constraint, F-93\nnetwork performance and topology,\nF-93\nNEWS communication, F-42\ntopology, F-39\nBisection traffic fraction, F-41\u201342\nBit error rate (BER), wireless networks,\nE-21\nBit rot, case study, D-61\u201364\nBit selection, B-8\nBlack box network\nbasic concept, F-5\u20136\neffective bandwidth, F-18\nperformance, F-13\nswitched-media networks, F-24\u201325\nswitched network topologies, F-41\nBlock. See also Cache block\naddressing, B-7\u20139\ncache optimization, 107\u2013109\ncentralized switched networks,\nF-33\ndefinition, B-2\ndisk array deconstruction,\nD-51\u201354\ndisk deconstruction case study,\nD-48\u201350\nfactor, 108\nglobal code scheduling, H-15\u201316\nhead-of-line, F-59\u201360\nidentification\nmemory hierarchy, B-8\u20139\nvirtual memory, B-44\u201345\nLU kernel, I-8\nmemory hierarchy, 81\nmultithreading, M-35\u201336\nnetwork performance and topology,\nF-41\noffset, B-8\u20139\nblock identification, B-8\u20139\ncache optimization, B-38\nOpteron data cache, B-13, B-14\nplacement\nmemory hierarchy, B-7\u20138, B-7\nvirtual memory, B-44\nRAID performance prediction,\nD-57\u201359\nreplacement\nmemory hierarchy, B-9\u201310\nvirtual memory, B-45\nsize, miss rate and, B-26\u201328,\nB-27\nTI TMS320C55 DSP, E-8\nBlocked floating point arithmetic, DSP,\nE-6\nBlock servers, vs. filers, D-34\u201335\nBlock transfer engine (BLT), F-91\nBoggs, David, F-103\nBOMB, M-4\nBooth recoding, J-8\u201310, J-9, J-17\ninteger multiplication, J-49\nBose-Einstein formula, 34\nBounds checking, B-52\nBranch(es)\ncompletion cycle, C-28\ndelayed, C-20, C-20\nfolding, 231\nhistory table, C-23\nRISC instruction set, C-5\nVAX, K-57\nWCET, E-4\nI-4\n\u25a0\nIndex"
    },
    {
        "page": 1481,
        "text": "Branch byte, K-57\nBranch hazards, C-18\u201322\npenalty reduction, C-19\u201320\npipeline issues, C-35\u201337\nscheme performance, C-21\u201322,\nC-22\nsimple scheme examples, C-21\u201322\nBranch penalty\nbranch-target buffers, 230\u2013231,\n231\nprediction schemes, C-21\nreduction, C-19\u201320\nBranch prediction\naccuracy, C-25\u201326\nbuffers, C-23\u201325, C-24\u201326\ncorrelation, 182\u2013184\ncost reduction, C-22, 182\u2013191\ndynamic, C-23\u201325\nearly schemes, M-29\ninstruction-level parallelism\ncorrelating branch predictors,\n182\u2013184\nIntel Core i7, 190\u2013191\nspecialized branch prediction,\n232\u2013234\ntagged hybrid predictors,\n188\u2013190, 188, 190\ntournament predictors,\n184\u2013188, 186\nintegrated, 233\nstatic, C-22, C-23\ntrace scheduling, H-19\nBranch registers, IA-64, H-34\nBranch stalls, C-61, C-64\nBranch-target address\nbranch hazards, C-38\npipeline branch issues, C-35\u201336\nRISC instruction set, C-5\nBranch-target buffers\nbranch penalty, 230\u2013231, 231\nhandling an instruction with, 230\ninstruction fetch bandwidth,\n228\u2013232, 229\u2013230\nprogram counter, 228\u2013229, 229\nBranch-target cache, 228\nBranch word, K-57\nBrewer, Eric, M-74\u201375\nBridges, F-82, F-83\nBubbles, F-47\u201348, F-54\nBuckets, D-26\nBuffered crossbar switch, F-66\nBuffered wormhole switching, F-52\nBuffers. See also Branch-target buffers\nbranch-prediction, C-23\u201325,\nC-24\u201326\nDSM multiprocessor cache\ncoherence, I-38\u201340\nintegrated instruction fetch units,\n234\nIntel Core i7, 256\ninterconnection networks,\nF-10\u201311\nnetwork interface functions, F-7\norganizations, F-59\u201361\ntranslation lookaside, B-37, B-46,\nB-47\nwrite, B-11, B-14\nBundles\nexample calculations, H-35\u201336\nIA-64, H-34\u201335, H-37\nItanium 2, H-41\nBurks, Arthur, M-3\nBurroughs B5000, M-17\u201318\nBus-based coherent multiprocessors,\nM-59\u201360\nBuses\nbarrier synchronization, I-16\nI/O bus replacements, D-34\nlarge-scale multiprocessor\nsynchronization,\nI-12\u201313\nNEWS communication, F-42\nscientific workloads on symmetric\nshared-memory\nmultiprocessors, I-25\nSony PlayStation 2 Emotion\nEngine, E-18\nvs. switched networks, F-2\nswitch microarchitecture, F-56\nBypassing, C-14\nSAN example, F-77\nByte displacement addressing, K-52\nByte/word/long displacement deferred\naddressing, K-52\u201353\nC\nCache(s). See also Memory hierarchy\nAMD Opteron example, B-12\u201315,\nB-13, B-15\nbenefits, 350\nblock frames and memory, B-7\nconcept, M-11\ndefinition, B-2\nembedded systems, E-4\nItanium 2, H-42\u201343\nparameters, B-42\nSony PlayStation 2 Emotion\nEngine, E-18\nvector processors, G-25\nand virtual memory, B-42\u201343,\nB-42, B-48\u201349, B-48\nCache bandwidth\nblock addressing, 100\nincreasing, 94\nmultibanked caches, 99\u2013100\nnonblocking caches, 100\u2013104\npitfall, 143\nCache block\ncache coherence protocol, 382\u2013383,\n384\u2013385\ndefinition, B-2\nmiss rate reduction, B-26\u201328\nshared state, 386\nsymmetric shared-memory\nmultiprocessors, I-22,\nI-25\u201326, I-25\nwrite strategy, B-10\u201312\nCache coherence\natomic operations, 386\nof cached data, 128\u2013129\nCray X1, G-22\ndefinition, 377\u2013379\ndirectory-based (see Directory-\nbased cache coherence)\nenforcement, 379\u2013380\nexample protocol, 383\u2013387, 384\nextensions, 388\nimplementing locks using,\n414\u2013417, 416\nlarge-scale multiprocessors,\nI-34\u201336\ndeadlock and buffering,\nI-38\u201340\ndirectory controller, I-40\u201341\nDSM implementation,\nI-36\u201337\nlarge-scale multiprocessors history,\nM-61\nmechanism, 384\nmultiprocessor, 377\u2013379, 387\nnonatomic operations, 386\nproblem, 377, 378\nprogram order, 378\u2013379\nIndex\n\u25a0\nI-5"
    },
    {
        "page": 1482,
        "text": "Cache coherence (Continued)\nsnooping (see Snooping cache\ncoherence)\nstate diagram, 385, 387\nCache hit, B-2\nexample calculation, B-5\nOpteron data cache, B-14\nCache miss\nblock replacement, B-10\ndefinition, B-2\ndistributed-memory\nmultiprocessor, I-32\ninterconnection network,\nF-91\u201392\nlarge-scale multiprocessors,\nI-34\u201335\nWCET, E-4\nCache-only memory architecture\n(COMA), M-61\u201362\nCache optimization, B-22\u201340\nadvancement, 117\ncache misses, 112\u2013113\ncase study, 148\u2013164\ncompiler-controlled prefetching,\n111\u2013114\ncompiler optimizations, 107\u2013109\ncritical word first, 104\u2013105\nearly restart, 104\u2013105\nenergy consumption, 97\nfallacy, 142\nfloating-point programs, 101\u2013102\nhardware prefetching, 109\u2013111\nHBM packaging, 114\u2013117\nhit time reduction, B-36\u201340,\n95\u201398\nimpact, B-40, 148\u2013150\nmiss categories, B-23\u201325\nmiss penalty reduction\nread misses vs. writes,\nB-35\u201336\nvia multilevel caches, B-30\u201335\nmiss rate reduction\nvia associativity, B-28\u201330\nvia block size, B-26\u201328, B-27\nvia cache size, B-28\nmultibanked caches, 99\u2013100\nnonblocking caches, 100\u2013104\npipelined access, 99\u2013100\npower reduction, 95\u201398\nway prediction, 98\u201399\nwrite buffer merging, 105\u2013106, 106\nCache organization\nblock placement, B-7\u20138, B-7\nOpteron data cache, B-12\u201315,\nB-13\nCache performance, B-3\u20136, B-15\u201316\naverage memory access time,\nB-17\u201320\ncache optimizations impact on,\nB-40\nequations, B-22\nexample calculation, B-16\u201317\nmiss penalty, B-20\u201321\nout-of-order execution, B-20\u201321\nCache prefetch, 111\nCache size, B-13\nmiss rate vs., B-24\u201325, B-28, B-33,\nB-37\nscientific workloads\ndistributed-memory\nmultiprocessors,\nI-29\u201331\nsymmetric shared-memory\nmultiprocessors,\nI-22\u201324, I-24\nvirtually addressed, B-37\nCaching locks, 415\nCACTI\nenergy consumption, 97\nfirst-level caches, 95\u201398, 96\nCallee/caller saving, A-19\u201320\nCall gate, B-53, B-54\nCanonical form, B-55\nCapabilities, protection schemes, M-9\nCapacity misses\ncache size, B-24\ndefinition, B-23\nmemory hierarchy design, 81\nscientific workloads on symmetric\nshared-memory\nmultiprocessors, I-22,\nI-24, I-24\nCapital expenditures (CAPEX), 36,\n486\u2013490\nCarrier sensing, F-23\u201324\nCarrier signal, wireless networks,\nE-21\nCarry-in, carry-skip adder, J-41\u201342\nCarry-lookahead adder (CLA)\nchip comparison, J-61\ncircuit, J-38\nearly computer arithmetic, J-63\nexample calculations, J-39\ninteger addition speedup,\nJ-37\u201341\nwith ripple-carry adder, J-42\ntree, J-40\u201341\nCarry-out\ncarry-lookahead circuit, J-38\nfloating-point addition speedup,\nJ-25\nCarry-propagate adder (CPA)\ninteger multiplication, J-48, J-51\nmultipass array multiplier, J-51\nCarry-save adder (CSA)\ninteger division, J-54\u201355\ninteger multiplication, J-47\u201348,\nJ-48\nCarry-select adder\ncharacteristics, J-43\u201344\nchip comparison, J-61\nexample, J-43\u201344\nCarry-skip adder (CSA)\ncharacteristics, J-41\u201343\nexample, J-42, J-44\nCase statements, A-17\nCatapult\nboard design, 568\nCNNs on, 570\u2013572, 571\u2013572\nevaluating, 601\u2013602\nguidelines, 577\u2013579\nimplementation and architecture,\n568\u2013569\nsearch acceleration on, 573\u2013574\nsoftware, 569\nversion 1 deployment, 574\nversion 2 deployment, 575\u2013577,\n576\u2013577\nC/C++ language\ndependence analysis, H-6\nGPU computing history,\nM-52\u201353\ninteger division/remainder, J-12\nCDB. See Common data bus (CDB)\nCedar project, M-61\nCell, Barnes-Hut n-body algorithm, I-9\nCell phones\nblock diagram, E-23\nembedded system case study\ncharacteristics, E-22\u201324\nNokia circuit board, E-24\noverview, E-20\nradio receiver, E-23\nI-6\n\u25a0\nIndex"
    },
    {
        "page": 1483,
        "text": "standards and evolution, E-25\nwireless networks, E-21\u201322\nFlash memory, D-3\u20134\nNokia circuit board, E-24\nwireless communication\nchallenges, E-21\nwireless networks, E-21\u201322\nCentralized shared-memory\nmultiprocessor,\n371, 377\ncache coherence protocol, 377\u2013379,\n378, 384\nenforcement, 379\u2013380\nexample protocol, 383\u2013387,\n384\nextensions, 388\nstate diagram, 385, 387\ninvalidate protocol implementation,\n382\u2013383\nlocal memory, 377\nSMP and snooping limitations,\n389\u2013392\nsnooping coherence protocols, 380,\n381\nexample protocol, 383\u2013387,\n384\nimplementation, 392\u2013393\ninvalidate protocol, 381\nlimitations, 389\u2013392\nmaintenance, 380\u2013381\nstructure, 372\nCentralized switched networks,\nF-31\u201335, F-31\nCentrally buffered switch, F-56\nCentral processing unit (CPU)\naverage memory access time,\nB-18\u201320\nDNN and GPUs vs., 595\u2013602\nearly pipelined versions,\nM-27\u201328\nexecution time, B-3, B-5, B-22\nGPU computing history, M-53\nperformance measurement history,\nM-6\nSony PlayStation 2 Emotion\nEngine, E-17\u201318\ntime, 39\nTI TMS320C55 DSP, E-8\nvector memory systems, G-10\nCerf, Vint, F-102\nCFM. See Current frame pointer (CFM)\nChaining\nconvoys, DAXPY code, G-16\nvector processor performance,\nG-11\u201312, G-12\nChannels, cell phones, E-24\nCharge-coupled device (CCD), Sanyo\nVPC-SX500 digital\ncamera, E-19\nChecksum\ndirty bits, D-61\u201364\npacket format, F-7\nChime, 291\nvector chaining, G-11\u201312\nvector execution time, G-4\nvector performance, G-2\u20134\nChip-crossing wire delay, F-3, F-74,\nF-108\nChip fabrication cost, 67\u201368\nChipkill, 94\nChoke packets, F-69\u201370\nChunk\ndisk array deconstruction,\nD-51\u201354\nShear algorithm, D-51\u201354\nCIFS. See Common Internet File\nSystem (CIFS)\nCircuit switching, F-51\nCirculating water system (CWS), 483\nCISC. See Complex Instruction Set\nComputer (CISC)\nCLA. See Carry-lookahead adder\n(CLA)\nClean block, B-11\nClock cycle\nfloating-point operations, C-50\npipeline scheduling, 177\u2013179\nRISC classic pipeline, C-6\nRISC exception, C-42\u201343\nRISC implementation, C-30\nswitch microarchitecture\npipelining, F-66\nvector architectures, G-4\nClock cycles per instruction (CPI), 53,\n559\nARM Cortex-A53, 251\u2013252, 252\nbranch scheme, C-21\u201322, C-22\ncache behavior impact, B-19\u201320\ncache hit calculation, B-5\ncalculation, 375\u2013376\nclock rate, 261\ndata hazards requiring stalls, C-17\ninstruction-level parallelism,\n168\u2013169\nIntel Core i7 6700, 256, 257\nmicroprocessor advances, M-35\npipelined processor, 168\u2013169\npipeline with stalls, C-11\npipelining concept, C-3\nprocessor performance equation, 52\nRISC history, M-22\nSPEC92 benchmarks, C-64\nSPECCPUint2006 benchmarks,\n256, 257\nstalls, C-64\nClock cycle time, 53\nand associativity, B-29\u201330\ncache optimization, B-19\u201320\ncache performance, B-3\npipeline performance, C-11\npipelining, C-3\nRISC implementation, C-30\nshared- vs. switched-media\nnetworks, F-25\nClock rate, 261, 261\nmicroprocessor advances, M-35\nClock skew, C-8\u201310\nClos network, F-33, F-34, 510\u2013511,\n510\u2013511\nCloud computing\nadvantages, 490\nAWS (see Amazon Web Services\n(AWS))\neconomies of scale, 491\nfallacy, 514\nproviders, 518\nutility computing history, M-75\u201376\nClusters, 9\u201310, 369, 478\ncharacteristics, I-45\ncontainers, M-76\nCray X1, G-22\nhistory background, M-62\u201365\nIBM Blue Gene/L, I-41\u201344,\nI-43\u201344\ninterconnection network domains,\nF-3\nlarge-scale multiprocessors, I-6\nlarge-scale multiprocessor trends,\nM-63\u201364\npower consumption, F-89\nutility computing, M-75\u201376\nas WSC forerunners, M-74\u201375\nCm*, M-57\nIndex\n\u25a0\nI-7"
    },
    {
        "page": 1484,
        "text": "C.mmp, M-57\nCMOS\ncell phone, E-24\nfirst vector computers, M-48\nripply-carry addition, J-3\nscaling, 442\u2013443\nvector processors, G-25\u201327\nCNN. See Convolutional neural\nnetwork (CNN)\nCoarse-grained multithreading\ndefinition, 243\u2013244\nsuperscalar processor, 245\nCocke, John, M-20, M-29\u201331\nCode division multiple access\n(CDMA), cell phones,\nE-25\nCode scheduling\nexample, H-16\nparallelism, H-15\u201323\nsuperblock scheduling, H-21\u201323,\nH-22\ntrace scheduling, H-19\u201321, H-20\nCoefficient of variance, D-27\nCoerced exception, C-39\nCoherence. See Cache coherence\nCoherence misses, I-22, 82, 393\nCold aisles, 506, 507\nCold-start misses, B-23\nCollision detection, shared-media\nnetworks, F-23\u201324\nCollision misses, B-23\nCollision, shared-media networks,\nF-23\u201324\nCollocation sites, interconnection\nnetworks, F-89\nCOLOSSUS, M-4\nColumn access strobe (CAS), 85\u201386\nColumn major order, 107\nCOMA. See Cache-only memory\narchitecture (COMA)\nCombining tree, large-scale\nmultiprocessor\nsynchronization, I-18\nCommand queue depth, vs. disk\nthroughput, D-4\nCommercial interconnection networks\ncongestion management,\nF-68\u201370\nconnectivity, F-67\ncross-company interoperability,\nF-67\u201368\nDECstation 5000 reboots, F-73\nfault tolerance, F-70\u201372\nCommit stage, 211\nCommoditization, and cost, 30\u201331\nCommodity cluster, characteristics,\nI-45\nCommon data bus (CDB), 197, 201\nperformance, 207\nreservation stations and register\ntags, 202\nwrite result, 211\nCommon Internet File System (CIFS),\nD-35\nNetApp FAS6000 filer, D-41\u201342\nCommunication bandwidth, I-3\nCommunication latency, I-3\nhiding, I-4\nCommunication mechanism, 375\u2013376\ninternetworking, F-85\u201389\nlarge-scale multiprocessors\nadvantages, I-4\u20136\nmetrics, I-3\u20134\nnetwork interfaces, F-7\u20138\nNEWS communication, F-42\u201344\nCommunication protocol, F-8\nCompare-select-store unit (CSSU), TI\nTMS320C55 DSP, E-8\nCompiler(s)\nconstants, A-31\ndefinition, C-65\ninteraction, A-27\u201330\nfor multimedia instructions,\nA-31\u201332\nphase, 399\nprimitives, A-30\nregularity, A-30\nrole, A-24\u201333\nstructure, A-25\u201326\ntrade-offs, A-30\nwriter, A-30\u201331\nCompiler-controlled prefetching,\n111\u2013114\nCompiler optimization\nfor caches, 107\u2013109, 148\u2013164\ninstruction-level parallelism,\n176\u2013182\nmemory consistency, 422\nCompiler scheduling, hardware\nsupport, M-32\u201333\nCompiler speculation, hardware\nsupport\nexample calculations, H-29\nmemory references, H-32\noverview, H-27\npreserving exception behavior,\nH-28\u201332\nCompiler techniques\nCray X1, G-21\u201322\ndependence analysis, H-7\u20138\nglobal code scheduling, H-17\u201318\nvectorization, G-12\u201314\nvector sparse matrices, G-12\nComplex Instruction Set Computer\n(CISC), K-51\nRISC history, M-23\nCompulsory misses, B-23\ncache size, B-24\nmemory hierarchy design, 81\nComputation-to-communication ratios\nparallel programs, I-10\u201312\nscaling, I-11\nCompute bandwidth, 350\nCompute-optimized processors, F-92\nComputer architecture\ndefinition, 11\u201312, M-18\u201319\nfloating-point addition, rule, J-24\nfunctional requirements, 17\u201318, 18\ngoals, 17\u201318\nhigh-level language, M-19\u201320\ninstruction set architecture, 12\u201317\nlimits of energy, 28\u201329\nof warehouse-scale computers,\n477\u2013482\nComputer arithmetic\nchip comparison, J-57\u201361,\nJ-58\u201360\nfloating point\ndenormals, J-14\u201315\nexceptions, J-34\u201335\nfused multiply-add, J-32\u201333\nIEEE 754, J-16\niterative division, J-27\u201331\nand memory bandwidth, J-62\nnumber representation, J-15\u201316\noverview, J-13\u201314\nprecisions, J-33\u201334\nremainder, J-31\u201332\nspecial values, J-14\u201315, J-16\nunderflow, J-36\u201337, J-62\nfloating-point addition\ndenormals, J-26\u201327\noverview, J-21\u201325\nI-8\n\u25a0\nIndex"
    },
    {
        "page": 1485,
        "text": "rules, J-24\nspeedup, J-25\u201326\nfloating-point multiplication\ndenormals, J-20\u201321\nexamples, J-19\noverview, J-17\u201320\nrounding, J-18, J-19\ninteger addition speedup\ncarry-lookahead, J-37\u201341\ncarry-lookahead circuit, J-38\ncarry-lookahead tree, J-40\ncarry-lookahead tree adder,\nJ-41\ncarry-select adder, J-43\u201344,\nJ-43\u201344\ncarry-skip adder, J-41\u201343, J-42\noverview, J-37\ninteger arithmetic\nlanguage comparison, J-12\noverflow, J-11\nRadix-2 multiplication/division,\nJ-4\u20137, J-4\nrestoring/nonrestoring division,\nJ-5, J-6\nripply-carry addition, J-2\u20133\nsigned numbers, J-7\u201310\nsystems issues, J-10\u201313\ninteger division\nradix-2 division, J-55\nradix-4 division, J-56\nradix-4 SRT division, J-57\nwith single adder, J-54\u201357\nSRT division, J-45\u201347, J-46,\nJ-55\u201357\ninteger-FP conversions, J-62\ninteger multiplication\narray multiplier, J-50\nBooth recoding, J-49\neven/odd array, J-52\nmany adders, J-50\u201354, J-50\nmultipass array multiplier, J-51\nsigned-digit addition table,\nJ-54\nwith single adder, J-47\u201349,\nJ-48\u201349\nWallace tree, J-53\ninteger multiplication/division,\nshifting over zeros, J-45\noverview, J-2\nrounding modes, J-14, J-17\u201320,\nJ-18, J-20\nComputer chip fabrication, Cray X1E,\nG-24\nComputer classes\nclusters, 9\u201310\ndesktop computing, 8\nembedded computers, 6\u20137\ninternet of things, 6\u20137\nparallel architectures, 10\u201311\nparallelism, 10\u201311\npersonal mobile device, 7\u20138\nservers, 8\u20139\nand system characteristics, E-4\nwarehouse-scale computers, 9\u201310\nComputer clusters, 470\nComputer design principles\nAmdahl\u2019s law, 49\u201352\ncommon case, 49\nlocality, 48\u201349\nparallelism, 48\nprocessor performance equation,\n52\u201355\nComputer room air-conditioning\n(CRAC), 482\nComputer technology, improvements,\n2\u20136\nCompute tiles, OCNs, F-3\nCompute Unified Device Architecture\n(CUDA), 311\nCUDA Thread, 311\nGPU programming, 320\nSIMD instructions, 325\u2013326\nGPU computing history, M-52\u201353\nComputing efficiently, at low\nutilization, 468\nConditional branching\nglobal code scheduling, H-16, H-16\ngraphics processing units,\n323\u2013326\noptions, A-18\u201319, A-19\nstatic branch prediction, C-22\nConditional instructions\nexample calculations, H-23\u201324\nexposing parallelism, H-23\u201327\nlimitations, H-26\u201327\nCondition codes, K-11, C-44, K-57\nConflict misses\ncache optimizations, B-23\ncache size, B-24\nmemory hierarchy design, 82\nCongestion control, F-68, F-70\nCongestion management, F-68\u201370\nConnectedness, F-30, F-48\nConnection Machine CM-5, F-96\nConnection Multiprocessor 2, M-46,\nM-56\nConsistency. See Memory consistency\nConstellation, characteristics, I-45\nContainers, cluster history, M-76\nContention delay, F-26\nContext switching, B-49, 119\nControl bits, messages, F-7\nControl Data Corporation (CDC)\nCDC 6600, C-66\u201367\ncomputer architecture\ndefinition, M-19\nearly computer arithmetic,\nJ-64\u201365\nfirst dynamic scheduling,\nM-28\u201329\nmultiple-issue processor\ndevelopment, M-28\u201329\nmultithreading history, M-35\nRISC history, M-28\u201329\nfirst vector computers, M-47\nSTAR-100, first vector computers,\nM-47\nSTAR processor, G-26\nControl dependences\nconditional instructions, H-24\nglobal code scheduling, H-16\nhardware-based speculation, 208\ninstruction-level parallelism,\n174\u2013176\nmaintenance, 175\nControl flow instructions, 14\naddressing modes for, A-17\u201318\nclasses, A-17\ncompilers\nrole, A-24\u201333\nstructure, A-25\u201326, A-25\nconditional branch options,\nA-18\u201319, A-19\nconditional instructions, H-27\nprocedure invocation options,\nA-19\u201320\nRISC-V, A-39\u201340\ntypes, A-16\nControl hazards, C-11\nControllers, historical background,\nM-88\u201389\nConvex Exemplar, M-62\nConvex processors, G-26\nIndex\n\u25a0\nI-9"
    },
    {
        "page": 1486,
        "text": "Convolutional neural network (CNN),\n550\u2013552, 551\non Catapult, 570\u2013572, 571\nprocessing element of, 572\nConvolution, DSP, E-5\nConvoy, 290\u2013292\nchained, DAXPY code, G-16\nDAXPY on VMIPS, G-20\u201321\nstrip-mined loop, G-5\nvector starting times, G-4\nConway, Lynn, M-29\u201330\nCooling systems, 483\nCooling towers, 508\nCopper wiring\nEthernet, F-82\ninterconnection networks, F-9\u201310\nCopy propagation, H-10\nCore, 17\nCore i7, 346\u2013353\nCore plus ASIC, embedded systems,\nE-3\nCortex-A53\nARM, 129\u2013131, 130\nperformance, 132\nCosmic Cube, M-60\u201361\nCost\nbranch prediction, C-22\ndisk storage, D-2\nDRAM/magnetic disk, D-3\ninterconnecting node calculations,\nF-32\u201333\nInternet Archive Cluster, D-38\u201340\nI/O system design/evaluation, D-36\nmagnetic storage history,\nM-85\u201386\nSIMD supercomputer development,\nM-45\nCost-performance, 467\nDSAs, 600\u2013601, 601\nextensive pipelining, C-70\nIBM eServer p5 multiprocessor,\n440, 441\nsorting case study, D-64\u201367\nCost trends\ncost-sensitive designs, 29\nintegrated circuit, 31\u201335\nmanufacturing vs. operation, 36\nvs. price, 35\ntime, volume, and\ncommoditization, 30\u201331\nCounter register, K-25\nCPA. See Carry-propagate adder\n(CPA)\nCPI. See Clock cycles per instruction\n(CPI)\nCP-67 program, M-10\nCPU. See Central processing unit\n(CPU)\nCray-1\nfirst vector computers, M-47\u201348\npipeline depths, G-4\nRISC history, M-20\nvector performance measures, G-16\nCray-2\nDRAM, G-25\nfirst vector computers, M-47\u201348\ntailgating, G-20\u201321\nCray-3, G-27\nCray-4, G-27\nCray C90\nfirst vector computers, M-48\nvector performance calculations,\nG-8\nCray J90, M-48\nCray Research programmers, 303\nCray Research T3D, F-91\nCray, Seymour, G-25, G-27,\nM-47\u201348\nCray supercomputers, early computer\narithmetic, J-63\u201364\nCray T90, 299\nCray T3D, M-61, F-104\u2013105\nCray T3E, M-49, M-61, F-71, F-98\u201399\nCray X, 370\nCray X1, M-64\ncluster history, M-64\nfirst vector computers, M-49\nMSP module, G-22, G-23\u201324\noverview, G-21\u201323\nCray X1E, G-24, F-91, F-95\nCray X2, first vector computers, M-49\nCray X-MP, M-47\u201348\nCray XT3, M-59, M-63\nCray XT3 SeaStar, F-67\nCray Y-MP\nfirst vector computers, M-48\nparallel processing debates, M-58\nCreate vector index instruction (CVI),\nG-13\nCredit-based control flow, F-10, F-18,\nF-69, F-75\nCRISP, M-29\nCritical path\nglobal code scheduling, H-16\ntrace scheduling, H-19\u201321, H-20\nCritical word first, cache optimization,\n104\u2013105\nCrossbars, F-31, F-31\nConvex Exemplar, M-62\nCrossbar switch\ncentralized switched networks,\nF-31\ninterconnecting node calculations,\nF-32\u201333\nCross-company interoperability,\nF-67\u201368\nCrusoe, M-32\u201333\nCryptanalysis, M-4\nCSA. See Carry-save adder (CSA)\nCUDA. See Compute Unified Device\nArchitecture (CUDA)\nCurrent frame pointer (CFM), H-33\u201334\nCustom cluster\ncharacteristics, I-45\nIBM Blue Gene/L, I-41\u201344,\nI-43\u201344\nCut-through packet switching, F-51\u201353\nCVI. See Create vector index\ninstruction (CVI)\nCYBER 205, M-47\nvector processor history,\nG-26\u201327\nCycle time. See also Clock cycle time\nCPI calculation, 375\u2013376\nmemory hierarchy design, 85\nCyclic redundancy check (CRC)\nIBM Blue Gene/L 3D torus\nnetwork, F-76\nnetwork interface, F-8\nCydrome, M-31\u201333\nD\nDAG. See Directed acyclic graph\n(DAG)\nDark silicon, 28\nDASH multiprocessor, M-61\nData addressing modes, K-32\u201335\nData cache\ncache performance, B-16\u201317\nTLB, B-46\nData cache miss\napplications vs. OS, B-59\nOpteron, B-12\u201315, B-13\nI-10\n\u25a0\nIndex"
    },
    {
        "page": 1487,
        "text": "sizes and associativities, B-10\nwrites, B-10\nDatacenters, containers, M-76\nData dependences\nconditional instructions, H-24\ndefinition, 170\u2013172\nexample calculations, H-3\u20134\ninstruction-level parallelism,\n170\u2013172\nmaintenance, 175\nData fetch (DF), C-58\u201359\nData flow\ncontrol dependence, 174\u2013176\nexecution, hardware-based\nspeculation, 209\nglobal code scheduling, H-17\u201318\nlimit, M-34\nData hazards\ndefinition, C-11\ndynamic scheduling, 191\u2013201\ninstruction set complications, C-45\nmicroarchitectural techniques case\nstudy, 266\u2013273\npipelined execution of instructions,\nC-13\nprogram order, 173\u2013174\nstall minimization by forwarding,\nC-14\u201315, C-15\u201316\nstall requirements, C-16\u201317\ntypes, C-12\nData integration (DI), 44\nData-level parallelism (DLP), 5, 10\u201311\ncomputer design principles, 48\ncross-cutting issues\nbanked memory and graphics\nmemory, 346\nenergy and DLP, 345\nstrided accesses and TLB\nmisses, 346\nenergy and, 345\ngraphics processing units\nconditional branching,\n323\u2013326\nmultimedia SIMD computers\nvs., 335\nNVIDIA computational\nstructures, 313\u2013320\nNVIDIA GPU instruction set\narchitecture, 320\u2013323\nNVIDIA GPU memory\nstructures, 326\u2013328, 327\nPascal GPU architecture,\n328\u2013331\nprogramming, 310\u2013313\nquick guide, 314\nvector architectures vs.,\n331\u2013334, 332\nloop-level parallelism\nanalysis, 337\u2013339\nCUDA/NVIDIA term, 337\u2013338\ndependent computations,\n344\u2013345\nfinding dependences, 339\u2013344\nSIMD Multimedia Extensions,\n304\u2013310\nvector architecture, 282\nexecution time, 290\u2013293\nvs. graphics processing units,\n331\u2013334\nmemory banks, 298\u2013299\nmultidimensional arrays,\n299\u2013301\nmultiple lanes, 293\u2013294\npredicate registers, 296\u2013298\nprocessor example, 288\u2013290\nprogramming, 302\u2013304\nRV64V extension, 283\u2013287,\n284\nsparse matrices, 301\u2013302\nvector-length registers,\n294\u2013296\nWSCs, 467\nData link layer\ndefinition, F-84\ninterconnection networks, F-10\nData parallelism, M-56\nData-race-free, 419\nData races, 419\nData transfers\ncache miss rate calculations, B-16\nRISC-V, A-36\nData trunks, C-70\nData types, dependence analysis, H-10\nDauber, Phil, M-29\u201330\nDAXPY loop\nchained convoys, G-16\non enhanced VMIPS, G-19\u201321\nvector performance measures, G-16\nVMIPS, G-19\u201321\ncalculations, G-18\non Linpack, G-18\npeak performance, G-17\nD-cache, way prediction, 98\u201399\nDDR. See Double data rate (DDR)\nDDR3 memory systems, 153\u2013155\nDeadlock, F-45\u201346, 386\navoidance, F-46\nlarge-scale multiprocessor cache\ncoherence, I-34\u201335,\nI-38\u201340\nrecovery, F-46\nDead time, vector pipeline, G-8, G-8\nDEC Alpha processor, K-3\nDecoder, radio receiver, E-23\nDecode stage, TI 320C55 DSP, E-7\nDEC PDP-11, address space,\nB-57\u201358\nDECstation 5000, F-73\nDEC VAX\naddress space, B-57\u201358\ncluster history, M-62, M-74\ncomputer architecture definition,\nM-19\nearly computer arithmetic,\nJ-63\u201364\nearly pipelined CPUs, M-28\nfailures, D-13\u201315\ninteger overflow, J-11\nRISC history, M-20\nDEC VAX-11/780, M-6\u20137, M-11,\nM-19\nDEC VAX 8700\nvs. MIPS M2000, M-22\nRISC history, M-22\nDedicated link network\nblack box network, F-5\u20136\neffective bandwidth, F-18\nexample, F-6\nDeep neural networks (DNNs)\nacceleration, 606\u2013613\nactivation, 546\napplications, 547, 595\nbatches, 556\nconvolutional neural network,\n550\u2013552\nCPUs and GPUs vs., 595\u2013602\nmultilayer perceptron, 549\u2013550\nneurons of, 546\u2013547\nperformance summary, 603\nquantization, 556\nrecurrent neural network,\n553\u2013555\ntraining set sizes/time, 548\nIndex\n\u25a0\nI-11"
    },
    {
        "page": 1488,
        "text": "Deep neural networks (DNNs)\n(Continued)\ntraining vs. inference, 547\u2013549\nweights/parameters, 546\nDefect tolerance, 67\u201368\nDelayed branch\nbehavior, C-20\ncompiler history, M-33\ndefinition, C-20\nDell PowerEdge servers, 55\u201358, 56\nDell Poweredge Thunderbird, F-80\nDemand access, memory hierarchy\ndesign, 138\nDemodulator, radio receiver, E-23\nDennard scaling, 4\u20135, 58, 368\u2013369,\n442\nDenormals, J-14\u201315, J-20\u201321\nfloating point addition, J-26\u201327\nfloating-point underflow, J-36\nDense matrix multiplication, LU\nkernel, I-8\nDensity-optimized processors, vs.\nSPEC-optimized, F-89\nDependability\nbenchmark examples, D-21\u201323\ndefinition, D-10\u201311\ndisk operators, D-13\u201315\nintegrated circuits, 36\u201338\nInternet Archive Cluster, D-38\u201340\nin memory systems, 93\u201394\nvia redundancy, 467\nDependence analysis\nbasic approach, H-5\nexample calculations, H-7\nlimitations, H-8\u20139\nDependence distance, loop-carried\ndependences, H-6\nDependences\ncontrol, 174\u2013176\ndata, 170\u2013172\nfinding, H-6\u201310\nloop-level parallelism, H-3\nname, 172\u2013173\nsparse matrices, G-12\u201313\ntypes, 170\u2013171\nDependent computations, H-10\u201312,\n344\u2013345\nDescriptor privilege level (DPL), B-53\nDescriptor tables, B-52\nDesign faults, D-11\nDesktop benchmarks, 41\u201343\nDesktop computers\ninterconnection networks, F-72\nmultimedia support, E-11\nRAID history, M-87\nRISC architectures survey for,\nK-3\u201329\nsystem characteristics, E-4\nDesktop computing, A-2, 8\nDesktop/server RISC architectures,\ninstruction formats for,\nK-8\nDestination offset, IA-32 segment, B-53\nDeterministic routing algorithm,\nF-46\u201347\nDF. See Data fetch (DF)\nDies, 31\nembedded systems, E-15\nIntel Core i7 microprocessor, 32\nRISC-V, 33\nyield, 33\u201334\nDigital Alpha\nconditional instructions, H-27\nDigital Alpha 21064, M-48\nprocessors\nMAX, multimedia support,\nE-11\nrecent advances, M-35\nsynchronization history, M-64\u201365\nDigital Equipment Vax, 2\nDigital Linear Tape, M-85\nDigital signal processor (DSP)\ncell phones, E-23\u201324, E-23\ndefinition, E-3\ndesktop multimedia support, E-11\nembedded RISCs, K-28\nexamples and characteristics, E-6\nmedia extensions, E-10\u201311\noverview, E-5\u20137\nTI TMS320C55, E-6\u20137, E-7\u20138\nTI TMS320C6x, E-8\u201310\nTI TMS320C64x, E-9\nTI TMS320C6x instruction packet,\nE-10\nDimension-order routing (DOR),\nF-46\u201347\nDirect attached disks, D-35\nDirected acyclic graph (DAG), 582\nDirect-mapped cache, B-7, B-8\naddress translation, B-38\nearly work, M-11\nmemory hierarchy, B-48, 81\nDirect memory access (DMA)\nhistorical background, M-89\nnetwork interface functions, F-7\nSanyo VPC-SX500 digital camera,\nE-19\nSony PlayStation 2 Emotion\nEngine, E-18\nTI TMS320C55 DSP, E-8\nzero-copy protocols, F-95\nDirect networks, F-35, F-37, F-96\nDirectory-based cache coherence, 380,\n391\u2013392\ncase study, 451\u2013452\nhome node, 406\nlarge-scale multiprocessors history,\nM-61\nlocal node, 406\noperations, 406\nprotocol example, 408\u2013412\nremote node, 406\u2013407\nstate transition diagram, 408,\n409\u2013410\nDirectory-based multiprocessor\ncharacteristics, I-31\nscientific workloads, I-26, I-29\nsynchronization, I-16, I-19\u201320\nDirectory controller, cache coherence,\nI-40\u201341\nDirectory protocol, 404\nDirectX 9, M-51\u201352\nDirectX 10 generation, M-52\u201353\nDirty bit, B-11, B-46, D-61\u201364\nDirty block, B-11, B-36\nDiscrete cosine transform, DSP, E-5\nDisk arrays\ndeconstruction case study, D-51\u201354\nRAID 6, D-8\u20139\nRAID levels, D-6\u201310\nDisk layout, RAID performance\nprediction, D-57\u201359\nDisk power, D-5\nDisk storage, D-2\u201310, D-48\u201350\nDisk system\nperformance milestones, 22\nsubsystem, failure rates of,\n51\u201352\nworkload measurements, 400\nDispatch stage, 266\u2013273\nDisplacement addressing, K-52\nDisplacement-style addressing mode,\nA-11\u201312\nI-12\n\u25a0\nIndex"
    },
    {
        "page": 1489,
        "text": "Display lists, Sony PlayStation 2\nEmotion Engine, E-17\nDistributed routing, F-49\nDistributed shared memory (DSM),\n371, 373\naccess time, 372\u2013373\narchitecture, 373\ncharacteristics, I-45\ndirectory-based cache coherence,\n404\u2013412, 405\ndisadvantages, 372\u2013373\nmulticore processor, 373, 405, 452\nDistributed shared-memory\nmultiprocessors\ncache coherence implementation,\nI-36\u201337\nscientific application performance,\nI-26\u201332, I-28\u201332\nDistributed switched networks,\nF-35\u201340\nDivide operations\nchip comparison, J-61\nfloating-point iterative, J-27\u201331\ninteger shifting over zeros, J-45\nintegers, speedup\nradix-2 division, J-55\nradix-4 division, J-56\nradix-4 SRT division, J-57\nwith single adder, J-54\u201357\nSRT division, J-45\u201347, J-46,\nJ-55\u201357\nlanguage comparison, J-12\nn-bit unsigned integers, J-4\nRadix-2, J-4\u20137, J-4\nrestoring/nonrestoring, J-5, J-6\nSRT division, J-45\u201347, J-46\nDLP. See Data-level parallelism (DLP)\nDLX, integer arithmetic, J-11\u201312\nDNNs. See Deep neural networks\n(DNNs)\nDomain-specific architectures (DSAs),\n5\narchitecture renaissance, 605\u2013606\ncost-performance, 600\u2013601\nCPUs and GPUs vs. DNN\naccelerators, 595\u2013602\ncustom chip, 602\ndeep neural networks\nactivation, 546\napplications, 547\nbatches, 556\nconvolutional neural network,\n550\u2013552\nmultilayer perceptron, 549\u2013550\nneurons of, 546\u2013547\nquantization, 556\nrecurrent neural network,\n553\u2013555\ntraining set sizes/time, 548\ntraining vs. inference, 547\u2013549\nweights/parameters, 546\ndesigning, 604\nguidelines for, 543\u2013544, 543\nheterogeneity, 592\u2013594\nIntel Crest, 579\nISPs, 580\u2013582\nMicrosoft Catapult\nboard design, 568\nCNNs on, 570\u2013572, 571\u2013572\nevaluating, 601\u2013602\nguidelines, 577\u2013579\nimplementation and\narchitecture, 568\u2013569\nsearch acceleration on, 573\u2013574\nsoftware, 569\nversion 1 deployment, 574\nversion 2 deployment, 575\u2013577,\n576\u2013577\nopen instruction set, 594\nperformance counters, 603\nperformance/watt, 600\u2013601\nPixel Visual Core\narchitecture philosophy,\n583\u2013584\nevaluating, 601\u2013602\nexample, 588\nfloor plan, 592\nHalo, 584\u2013585\nimplementation, 590\u2013591\ninstruction set architecture,\n587\u2013588\nline buffers in, 590\nprocessing element, 588\u2013589\nprocessor, 585\u2013587\nsoftware, 582\ntwo-dimensional array, 586\ntwo-dimensional line buffers,\n589\u2013590\nresponse time, 596\u2013600\nrooflines, 596\u2013600\nsystem on a chip, 592\u2013594\nsystolic array, 561\nTCO, 600\u2013601\ntensor processing unit\narchitecture, 557\u2013558\nblock diagram, 558\ncase study, 606\u2013617\ndie, 562\nguidelines, 566\u2013567\nimplementation, 560\u2013563\nimproving, 564\u2013566\ninstruction set architecture, 559\nmicroarchitecture, 559\u2013560\norigin, 557\nprinted circuit board, 563\nsoftware, 563\nTensorFlow program, 564\nthroughput, 596\u2013600\nDouble data rate (DDR), 87, 399\nIBM Blue Gene/L, I-43\nInfiniBand, F-81\nDouble-extended floating-point\narithmetic, J-33\u201334\nDouble failures, RAID reconstruction,\nD-55\u201357\nDouble-precision floating point, C-63,\n329\nchip comparison, J-58\nDSP media extensions, E-10\nDouble rounding\nFP precisions, J-34\nFP underflow, J-36\u201337\nDouble words, A-7, A-8, A-14, K-35,\n300\nDPL. See Descriptor privilege level\n(DPL)\nDRAM. See Dynamic random-access\nmemory (DRAM)\nDRDRAM, Sony PlayStation 2,\nE-16\nDriver domains, Xen virtual machine,\n126\nDSAs. See Domain-specific\narchitectures (DSAs)\nDSM. See Distributed shared memory\n(DSM)\nDSP. See Digital signal processor\n(DSP)\nDual inline memory modules\n(DIMMs), F-74, 89\nDynamically allocatable multi-queues\n(DAMQs), F-56\nDynamically shared libraries, A-18\nIndex\n\u25a0\nI-13"
    },
    {
        "page": 1490,
        "text": "Dynamic branch prediction, C-23\u201325\nDynamic energy, 25\nDynamic network reconfiguration,\nF-71\u201373\nDynamic power, 80\nDynamic programming feature (DPF),\n577\nDynamic random-access memory\n(DRAM)\narithmetic operations and energy\ncost, 29\nclock rates, bandwidth, and names,\n89\ncost vs. access time, D-3\nCray X1, G-22\ndependability, 516\ndie stacking, 91\ndisk storage, D-3\nembedded benchmarks, E-13\nerrors and faults, D-11\nfirst vector computers, M-47\u201349\nIBM Blue Gene/L, I-43\u201344\ninternal organization, 86\nmagnetic storage history, M-86\nmemory hierarchy design, 85\u201387\nmemory performance\nimprovement, 87\u201390\nPlayStation 2, E-16, E-17\nprice pressures, 34\nsemiconductor, 19\nstacked/embedded, 91\ntiming parameters, 153\u2013155\nvector memory systems, G-9\u201310\nvector processor, G-9\u201310, G-25\nDynamic register typing, 287\nDynamic scheduling\nadvantages, 191\u2013192\ndata hazards, 191\u2013201\ndefinition, C-65\u201366\nfirst use, M-28\u201329\nout-of-order execution,\n193\u2013194\nwith scoreboard, C-66\u201370,\nC-68\nTomasulo\u2019s algorithm, 195\u2013204\nloop-based example, 204\u2013208,\n206\nsteps, 205\nunoptimized code, C-70\nDynamic voltage-frequency scaling\n(DVFS), 27\nE\nEarly restart, cache optimization,\n104\u2013105\nEarth Simulator, M-48\u201349, M-63\nECC. See Error-Correcting Code\n(ECC)\nEckert, J. Presper, M-2\u20135, M-20\nEckert-Mauchly Computer\nCorporation, M-5, M-57\nECL minicomputer, M-20\nEEMBC. See Electronic Design News\nEmbedded\nMicroprocessor\nBenchmark Consortium\n(EEMBC)\nEffective address, A-8\u20139\nRISC classic pipeline, C-8\nRISC instruction set, C-5\nsimple RISC implementation, C-27\nTLB, B-49\nEffective bandwidth\ndefinition, F-13\nexample calculations, F-18\u201319\nvs. packet size, F-19\ntwo-device networks, F-13\u201320\nEfficiency factor, F-53, F-55\u201356\nEight-way set associativity\ncache optimization, B-28\u201329\nconflict misses, B-23\ndata cache misses, B-10\nElapsed time, 39\nElectronically erasable programmable\nread-only memory\n(EEPROM), 92\nElectronic Design News Embedded\nMicroprocessor\nBenchmark Consortium\n(EEMBC), 41\nbenchmark classes, E-12\nkernel suites, E-12\npower consumption and efficiency\nmetrics, E-13,\nE-13\u201314\nElectronic Discrete Variable Automatic\nComputer (EDVAC),\nM-2\u20133\nElectronic Numerical Integrator and\nCalculator (ENIAC),\nM-2\u20133, M-85\nEmbedded applications, A-2\nEmbedded computer, 6\u20137\nRISC architectures survey for,\nK-3\u201329\nEmbedded DRAM, 91\nEmbedded multiprocessors,\ncharacteristics, E-14\u201315\nEmbedded systems\nbenchmarks\nbasic considerations, E-12\npower consumption and\nefficiency, E-13,\nE-13\u201314\ncell phone case study\nblock diagram, E-23\ncharacteristics, E-22\u201324\nNokia circuit board, E-24\noverview, E-20\nradio receiver, E-23\nstandards and evolution, E-25\nwireless networks, E-21\u201322\ncharacteristics, E-4\ndigital signal processor\ncell phones, E-23\u201324, E-23\ndefinition, E-3\ndesktop multimedia support,\nE-11\nexamples and characteristics, E-6\nmedia extensions, E-10\u201311\noverview, E-5\u20137\nTI TMS320C55, E-6\u20137, E-7\u20138\nTI TMS320C6x, E-8\u201310\nTI TMS320C64x, E-9\nTI TMS320C6x instruction\npacket, E-10\nEEMBC benchmark suite, E-12\noverview, E-2\nperformance, E-13\u201314\nreal-time processing, E-3\u20134\nSanyo digital cameras, SOC, E-20\nSanyo VPC-SX500 digital camera\ncase study, E-19\nSony PlayStation 2 case study,\nE-15\u201318\nblock diagram, E-16\norganization, E-18\nEMC, M-87\u201388\nEmotion Engine\norganization modes, E-18\nSony PlayStation 2 case study,\nE-15\u201318\nempowerTel Networks, MXP\nprocessor, E-14\u201315\nI-14\n\u25a0\nIndex"
    },
    {
        "page": 1491,
        "text": "Enclaves, instruction set extensions,\n125\nEncore Multimax, M-59\u201360\nEnd-to-end flow control, F-69\nEnergy\nand DLP, 345\nlimits of, 28\u201329\nwithin microprocessor, 25\u201328\nproportionality, 503\nsystems perspective, 23\u201324\nEnergy efficiency, 434\u2013437, 467.\nSee also Power\nconsumption\nembedded benchmarks, E-13\nEngineering Research Associates\n(ERA), M-4\u20135\nENIAC. See Electronic Numerical\nIntegrator and Calculator\n(ENIAC)\nEnigma coding machine, M-4\nEntry time, transactions, D-16, D-17\nEnvironmental faults, storage systems,\nD-11\nEPIC approach\nhistorical background, M-33\nIA-64, H-33\nE-24 RF. See Register fetch (RF)\nError correcting codes (ECCs),\n93\u201394\ndisk storage, D-11\nhardware dependability, D-15\nRAID 2, D-6\nError handling, interconnection\nnetworks, F-9\u201312\nErrors, definition, D-10\nEscape resource set, F-47\u201348\nETA processor, G-26\u201327\nEthernet, 478\nand bandwidth, F-82, F-93\nLANs, F-4, F-82\u201384, F-103\u2013104\npacket format, F-79\nshared-media networks, F-23\u201324\nshared- vs. switched-media\nnetworks, F-23\nswitch vs. NIC, F-90\nsystem area networks, F-76\u201377\ntotal time statistics, F-94\nWAN, F-84\u201385\nEugene, Miya, M-65\nEuropean Center for Particle Research\n(CERN), F-102\nEven/odd array\nexample, J-52\ninteger multiplication, J-51\u201352\nEVEN-ODD scheme development,\nD-10\nException\narithmetic-logical units, C-5\ncategories, C-40\ncontrol dependence, 174\u2013175\nfloating-point, C-41\u201342\nfloating-point arithmetic, J-34\u201335\nimprecise, 194\nmemory protection, 175\nprecise, C-41\u201344\npreservation via hardward support,\nH-28\u201332\nRISC V, C-42\u201343, C-42\nstopping/restarting, C-41\u201342\ntypes and requirements, C-38\u201341,\nC-40\nunexpected sequences, C-70\nExecute step\nItanium 2, H-42\nTI 320C55 DSP, E-7\nExecution, C-69, 198, 211\nExecution address cycle (EX)\ndata hazards requiring stalls, C-18\ndata hazards stall minimization,\nC-14\ndynamic scheduling pipelines, C-66\nexception stopping/restarting, C-41\nfloating point pipeline, C-46\nlonger latency pipelines, C-51\nMIPS R4000 pipeline, C-58\u201359\npipeline branch issues, C-35\u201336\nRISC exception, C-42\u201343, C-43\nRISC instruction set, C-5, C-6\nRISC pipeline, C-32\u201335\nsimple RISC implementation, C-27\nExecution time, 39\nAmdahl\u2019s law, 50\napplication/OS misses, B-59\ncache performance, B-3\ncentral processing unit, B-3, B-5,\nB-22\ncomponents, 400\nmultiprocessor, 438\nmultiprogrammed parallel \u201cmake\u201d\nworkload, 400\npipelining performance, C-3,\nC-8\u201310\nsecond-level cache size, B-32, B-34\nand stall time, B-21\nvector length, G-7\nExpand-down field, B-53\nExplicit parallelism, H-34\u201337\nExplicit unit-stride, 333\nExponential back-off\nlarge-scale multiprocessor\nsynchronization,\nI-17\u201318\nspin lock, I-17\nExponential distribution, D-27\nExtended accumulator, A-3, K-30\nExtended stack architecture, K-30\nF\nFabrication cost, 67\u201368\nFabrication yield, 67\u201368\nFailure. See also Mean time between\nfailures (MTBF); Mean\ntime to failure (MTTF)\nBerkeley\u2019s Tertiary Disk project,\nD-12\ndefinition, D-10\ndependability, 37\u201338\ndirty bits, D-61\u201364\nRAID\nreconstruction, D-55\u201357\nrow-diagonal parity, D-9\nrates of disk subsystem, 51\u201352\nstorage system, D-6\u201310\ncomponents, D-43\nTertiary Disk, D-13\nFailures in time (FIT), 37\nFalse sharing, 393\u2013394, 398\nFast Fourier transformation (FFT)\ncharacteristics, I-7\ndistributed-memory\nmultiprocessor, I-32\nexample calculations, I-27\u201329\nsymmetric shared-memory\nmultiprocessors, I-22,\nI-23, I-25\u201326\nFat trees, F-34, F-38\nFault, 111\ndefinition, D-10\ndependability benchmarks, D-21\nprogramming mistakes, D-11\nTandem Computers, D-13\nFault detection, 64\nFault-induced deadlock, F-45\u201346\nIndex\n\u25a0\nI-15"
    },
    {
        "page": 1492,
        "text": "Fault tolerance, F-70\u201372, F-98\ndependability benchmarks, D-21\nRAID, D-7\nFault-tolerant routing, F-70\u201371,\nF-98\u201399\nFC. See Fibre Channel (FC)\nFC-AL. See Fibre Channel Arbitrated\nLoop (FC-AL)\nFeature Extraction, 573, 574\nFeature functional unit (FFU), 576\nFeature maps, two-dimensional, 550\nFeature size, 21\nFEC. See Forward error correction\n(FEC)\nFederal Communications Commission\n(FCC), D-15\nFENCE in RISC V, 420\u2013422\nFetch-and-increment, 413\u2013414\nlarge-scale multiprocessor\nsynchronization, I-20\u201321\nsense-reversing barrier, I-21\nFetch stage, TI 320C55 DSP, E-7\nFFT. See Fast Fourier transformation\n(FFT)\nFibre Channel (FC), F-106\nfile system benchmarking, D-20\nNetApp FAS6000 filer, D-41\u201342\nFibre Channel Arbitrated Loop (FC-\nAL), M-88, F-106\nblock servers vs. filers, D-35\nFibre Channel Switched (FC-SW),\nF-106\nFilers\nvs. block servers, D-34\u201335\nNetApp FAS6000 filer, D-41\u201343\nservers, SPEC benchmarking,\nD-20\u201321\nFilters, radio receiver, E-23\nFine-grained multithreading, 243\u2013244\nFingerprint, storage system, D-48\nFinite-state machine, F-49, F-56\u201358\nFirmware, network interfaces, F-7\u20138\nFirst-in first-out (FIFO), B-9, B-10, 197\ndefinition, D-26\nFirst-level caches\ncache optimization, B-30\u201335\nhit time/power reduction, 95\u201398\ninterconnection network, F-74\nItanium 2, H-41\nmemory hierarchy, B-48\u201349, B-48\nparameter ranges, B-42\nFirst-reference misses, B-23\nFixed-field decoding, C-5\nFixed length, 14\nFixed-point arithmetic, DSP, E-5\u20136\nFlash memory\ndisk storage, D-3\u20134\nembedded benchmarks, E-13\nmemory hierarchy design, 92\u201393\ntechnology trends, 19\nFLASH multiprocessor, M-62\nFlexible chaining, 290\u2013291\nvector processor, G-11\nFlex point, 579\nFloating-point (FP) operations,\nK-38\u201340\naddition\ndenormals, J-26\u201327\noverview, J-21\u201325\nrules, J-24\nspeedup, J-25\u201326\nchip comparison, J-58\nCPI, C-64\ndata dependences, 171\ndenormals, J-14\u201315, J-20\u201321,\nJ-26\u201327\ndouble-precision, C-63\nDSP media extensions, E-10\u201311\nearly computer arithmetic, J-64\u201365\nexceptions, J-34\u201335, C-41\u201342\nfused multiply-add, J-32\u201333\nIEEE 754, J-16\ninteger conversions, J-62\nItanium 2, H-41\niterative division, J-27\u201331\nlatency, C-61, C-63, 177\nand memory bandwidth, J-62\nmicro-op fusion, 254\nMIPS R4000 pipeline, C-60\u201361,\nC-60\nmispeculation, 239\nmultiplication\ndenormals, J-20\u201321\nexamples, J-19\noverview, J-17\u201320\nrounding, J-18, J-19\nmultiplication precision, J-21\nmultiply and add operation, C-62\nnumber representation, J-15\u201316\noverflow, J-11\noverview, J-13\u201314\nperformance, 308\npipeline scheduling, 178\nprecisions, J-33\u201334\nprograms, 101\u2013102\nregister file, C-50\nremainder, J-31\u201332\nresult stalls, C-61, C-64\nRISC exception, C-43\nRISC multicycle operations,\nC-45\u201355\nRISC pipeline, C-45\u201355, C-47\u201348,\nC-57\nRISC-V, A-40\u201341\nspecial values, J-14\u201315, J-16\nsquare root, 51\nstatic branch prediction, C-23\nstructural stalls, C-61, C-64\nTomasulo\u2019s algorithm, 198\nunderflow, J-36\u201337, J-62\nvector chaining, G-11\nFloating-point registers (FPRs)\nIA-64, H-34\nIBM Blue Gene/L, I-42\nFloating Point Systems AP-120B,\nM-30\nFloppy disks, M-85\u201386\nFlow-balanced state, D-24\nFlow control\nand arbitration, F-22\ninterconnection networks, F-9\u201312\nFluent, F-80\u201381\nFlush, branch penalty reduction, C-19\nForget gate, 553\nForm factor, interconnection networks,\nF-9\u201310\nFORTRAN\ncompiler vectorization, G-14, G-15\ndependence analysis, H-6\ninteger division/remainder, J-12\nperformance measurement history,\nM-6\nForward error correction (FEC), E-5\u20137\nForwarding, C-14\narithmetic-logical units, C-36\u201337\ndata hazards stall minimization,\nC-14\u201315, C-15\u201316\nload instruction, C-17\nlonger latency pipelines, C-49\u201352\ntable, F-56\u201358\nForward path, cell phones, E-24\nFourier-Motzkin algorithm, M-32\nFourier transform, DSP, E-5\nI-16\n\u25a0\nIndex"
    },
    {
        "page": 1493,
        "text": "Four-way set associativity, B-23\nFPGA, 568\u2013569\nCatapult, 567\nFeature Extraction, 574\nFP operations. See Floating-point (FP)\noperations\nFragmentation problem, 114\nFrame pointer, K-57\nFree-form expressions, 573\u2013574\nFreeze, branch penalty reduction, C-19\nFrequency modulation (FM), wireless\nneworks, E-21\nFront-end stage, Itanium 2, H-42\nFU. See Functional unit (FU)\nFujitsu Primergy BX3000 blade server,\nF-89\nFujitsu SPARC64 X+, 389, 426, 429\nfeature, 427\nperformance, 429\u2013431, 432\nFujitsu VP100, M-48\nFujitsu VP200, M-48\nFull access\ndimension-order routing, F-47\u201348\ninterconnection network topology,\nF-30\nFull adders, J-2\u20133, J-3\nFull-duplex mode, F-23\nFully associative cache, B-7\u20139, B-12,\n81\nFully connected layer, 549\nFully connected topology, F-35\u201336,\nF-35\u201336\nFunctional hazards, 266\u2013273\nFunctional unit (FU), C-46\nexecution slots, superscalar\nprocessors, 244\u2013245, 244\nItanium 2, H-41\u201343\nlatency, C-47, 177\nOCNs, F-3\nFunction pointers, A-18\nFused multiply-add, floating point,\nJ-32\u201333\nFuture file approach, C-54\nG\nGates, 553\nGateways, Ethernet, F-83\nGather-scatter, A-31\u201332, 301\u2013302, 352\nsparse matrices, G-13\u201314\nGE 645, M-9\u201310\nGeForce 8800, M-52\nGeneral-Purpose Computing on GPUs\n(GPGPU), M-52\nGeneral-purpose electronic computers,\nM-2\u20134\nGeneral-purpose registers (GPRs)\narchitectures, A-3\nIA-64, H-38\nGeometric mean, 46\nGibson mix, M-6\nGlobal address space, B-52\nGlobal code scheduling\nexample, H-16\nparallelism, H-15\u201323\nsuperblock scheduling, H-21\u201323,\nH-22\ntrace scheduling, H-19\u201321, H-20\nGlobal common subexpression\nelimination, A-26\nGlobal data area, A-29\nGlobal Environment for Network\nInnovation (GENI),\nF-102\nGlobal miss rate, B-31\nGlobal optimizations, A-26\nGlobal Positioning System, CDMA,\nE-25\nGlobal predictors, 184\u2013188\nGlobal scheduling algorithms, 219\u2013220\nGlobal system for mobile\ncommunication (GSM),\ncell phones, E-25\nGoldschmidt\u2019s division algorithm,\nJ-29\u201330\nGoldstine, Herman, M-2\u20133\nGoogle\nclusters history, M-63\ncontainers, M-76\nGoogle App Engine, M-75\u201376\nGoogle Clusters, 94\npower consumption, F-89\nGoogle File System (GFS), 474\nGoogle Translate, 4, 7, 40\u201345\nGoogle WSCs\nairflow, 506\navailability zones, 498\ncooling, 506\u2013508\ngenerators, 505\nnetworking, 510\u2013511\nnetwork switches, 502\nnetwork traffic, 501\non-site substation, 504\npower distribution, 504\u2013506\npower utilization efficiency of, 485\nracks, 509\u2013510, 509, 512\nservers, 505, 512\u2013513, 513\nswitch gear, 505\ntransformers, 505\nGordon Bell Prize, M-58\nGPGPU. See General-Purpose\nComputing on GPUs\n(GPGPU)\nGPRs. See General-purpose registers\n(GPRs)\nGradual underflow, J-15, J-36\nGrain size, 370\nGrant phase, arbitration, F-49\u201350\nGraph coloring, A-27\nGraphical Processor Units (GPUs)\ncomputing history, M-52\u201353\nhistorical background, M-50\u201351\nscalable, M-51\nGraphics data RAMs (GDRAMs), 90\nGraphics-intensive benchmarks, 41\nGraphics memory, 346\nGraphics pipelines, M-51\u201352\nGraphics processing unit (GPU), 10\nconditional branching, 323\u2013326\nDNN and CPUs vs., 595\u2013602\nembedded vs. server, 346\u2013353\nfallacy, 353\nmultimedia SIMD and MIMD vs.,\n347\u2013353\nmultimedia SIMD computers vs.,\n335\nNVIDIA computational structures,\n313\u2013320\nNVIDIA GPU instruction set\narchitecture, 320\u2013323\nNVIDIA GPU memory structures,\n326\u2013328, 327\nPascal GPU architecture, 328\u2013331\nprogramming, 310\u2013313\nquick guide, 314\nvector architectures vs., 331\u2013334,\n332\nvector kernel implementation,\n357\u2013359\nGraphics synchronous DRAMs\n(GSDRAMs), 90\nGraphics Synthesizer, Sony\nPlayStation 2, E-16\u201318,\nE-16\nIndex\n\u25a0\nI-17"
    },
    {
        "page": 1494,
        "text": "Greatest common divisor (GCD),\n342\u2013343\ntest, loop-level parallelism\ndependences, H-7\nGrid computing, M-75\nGrid mapping, 315, 316\nGrid topology, F-36\u201338\nGshare predictors\ntagged hybrid vs., 190\n2-bit predictor, 184, 186, 262\nGuest domains, 126\nGuest virtual machine, 121\nH\nHalf adders, J-2\u20133\nHalf-duplex mode, F-23\nHalf-precision floating-point\narithmetic, 329\nHalo, 584\u2013585\nHAMR, 19\nHandshaking, interconnection\nnetworks, F-10\nHard cores, Cortex-A53, 130\nHard errors, memory hierarchy design,\n93\nHard real-time systems, definition,\nE-3\u20134\nHardware, 17\ncompiler scheduling support,\nM-32\u201333\ncompiler speculation support\nmemory references, H-32\noverview, H-27\npreserving exception behavior,\nH-28\u201332\ndesigning, 17\u201318\nfor exposing parallelism, H-23\u201327\nfaults, D-11\ninterconnection networks, F-8\npipeline hazard detection, C-34\nHardware-based speculation, 208\u2013217\ndata flow execution, 209\ndefinition, 208\ndisadvantage, 241\ninstruction execution step, 211\u2013212\nkey ideas, 208\nreorder buffer, 209\u2013212, 214\u2013215\nvs. software speculation, 240\u2013241\nwrite result, 217\nHardware prefetching, 109\u2013111,\n148\u2013164\nHardware primitives, 412\u2013414\nlarge-scale multiprocessor\nsynchronization,\nI-18\u201321\nHarvard architecture, M-3\u20134\nHazards. See also Data hazards\ncontrol hazards, C-11\ndata (see Data hazards)\ndefinition, C-10\u201311\ndetection, hardware, C-34\nfunctional, 266\u2013273\ninstruction set complications, C-45\nlonger latency pipelines, C-49\u201352\nread after write, C-12\u201314\nstructural (see Structural hazards)\nwrite after read, C-12\nwrite after write, C-12\nHeader\nmessages, F-7\npacket format, F-7\nswitch microarchitecture\npipelining, F-64\nTCP/IP, F-87\u201389\nHead-of-line (HOL) blocking,\nF-59\u201361, F-60\nHeap, A-29\nHEP processor, M-35\nHeterogeneity, DSAs, 592\u2013594\nHeterogeneous architecture, 282\nHewlett-Packard AlphaServer,\nF-104\u2013105\nHewlett-Packard PA-RISC\nEPIC approach, M-33\nfloating-point precisions, J-33\nMAX2, multimedia support, E-11\nHewlett-Packard RISC\nmicroprocessors, G-26\nHewlett Packard server, WSCs, 476\nHewlett-Packard\u2019s PA-RISC, K-3\nHidden layers, 546\u2013547\nHigh bandwidth memory (HBM), 346\ncache optimization, 114\u2013117\nmemory hierarchy design, 91\nPascal GPU architecture, 329\nHigher-radix division, J-54\u201355\nHigher-radix multiplication, integer,\nJ-48\nHigh-level language computer\narchitecture (HLLCA),\nM-20\nHigh-level optimizations, A-26\nHighly parallel memory systems,\n150\u2013153\nHigh-order functions, A-18\nHigh-performance computing (HPC),\n466\nvector processor history,\nG-27\u201328\nHigh Performance Fortran (HPF)\u2014\nprograms, 422\nHigh-speed chip-to-chip interconnect,\n329\nHillis, Danny, M-46, M-56, M-58\u201359,\nM-76\nHistogram, D-26\nHistory file approach, C-54\nHitachi S810, M-48\nHit time, B-15\u201316\naddress translation, 83\nfirst-level caches, 95\u201398\nlatency, 115\nmemory hierarchy design, 82\nreducing, 94\nreduction, B-36\u201340\nway prediction, 98\u201399\nHLLCA. See High-level language\ncomputer architecture\n(HLLCA)\nHome node, 406\nHop count, F-30\nHops, F-36, F-40\nHost\nNVIDIA GPU memory structures,\n327\nvirtual machine, 121\nHost channel adapters (HCAs), F-90\nhistorical background, M-89\nHot aisles, 506, 506\nHot swapping, F-71\u201373\nHPC. See High-performance\ncomputing (HPC)\nHP Precision Architecture, integer\narithmetic, J-11\u201312\nHP ProLiant BL10e G2 blade server,\nF-89\nHPSm, M-31\nHybrid predictors, 184\nHypercube networks, F-44, F-96\nHyperTransport, F-67\nNetApp FAS6000 filer, D-42\nHypervisor. See Virtual machine\nmonitor (VMM)\nI-18\n\u25a0\nIndex"
    },
    {
        "page": 1495,
        "text": "I\nIAS machine, M-3, M-5\nIBM\nBlueGene, 370\nChipkill, 94\ncluster history, M-62, M-74\ncomputer history, M-5\nearly VM work, M-10\nIBM 360, address space, B-58\nIBM 370 architecture, 124\nmagnetic storage, M-85\u201386\nmultiple-issue processor\ndevelopment, M-29\u201330\nRAID history, M-87\nIBM 360\narchitects, M-10\ncomputer architecture definition,\nM-18\nI/O bus history, M-89\nmemory hierarchy development,\nM-9\u201310\nparallel processing debates, M-58\nIBM 360/85, M-11, M-29\nIBM 360/91\nearly computer arithmetic, J-63\nhistory, M-29\nspeculation concept origins, M-31\nIBM 370\nearly computer arithmetic,\nJ-63\u201364\ninteger overflow, J-11\nvector processor history, G-27\nIBM 370/158, M-6\u20137\nIBM 650, M-5\nIBM 701, M-5\nIBM 702, M-5\nIBM 704, M-5, M-27\u201328\nIBM 705, M-5\nIBM 801, M-20\nIBM 3081, M-61\nIBM 7030, M-27\u201328\nIBM 360/370 architecture, K-69\u201370\nbranches and special loads and\nstores\u2014RX format,\nK-72\nbranches and status setting R-R\ninstructions, K-71\nbranches/logical and floating-point\ninstructions\u2014RX\nformat, K-71\ndefinition, K-69\n360 detailed measurements,\nK-70\u201374\nhistorical perspective and\nreferences, K-75\ninteger/logical and floating-point R-\nR instructions, K-70\nmeasurements, K-70\u201374\nRS and SI format instructions, K-72\nSS format instructions, K-73\nIBM AS/400, M-87\nIBM Blue Gene/L, F-4, I-44\ncluster history, M-64\ncomputing node, I-42\u201344, I-43\nas custom cluster, I-41\u201344, I-43\u201344\ndeterministic vs. adaptive routing,\nF-53\u201356\nparallel processing debates, M-59\nsystem area network, F-76\u201377\n3D torus network, F-39\nIBM 3840 cartridge, M-85\nIBM 9840 cartridge, M-85\nIBM CoreConnect\ncross-company interoperability,\nF-68\nOCNs, F-3\nIBM eServer p5 multiprocessor\nbenchmarks, 440\ncost-performance, 440, 441\nIBM Federation network interfaces,\nF-18\nIBM Power 1, M-31\nIBM Power 2, M-31\nIBM Power 4\nmultithreading history, M-36\nrecent advances, M-35\nIBM Power 5, 424\nItanium 2 comparison, H-43\nmultithreading history, M-36\nIBM Power 8, 371, 389\u2013390, 426\ndesign, 429\nfeature, 427\non-chip organizations, 428\nperformance, 431\u2013432, 432\nIBM Power processors\nbranch-prediction buffer, C-25\ncharacteristics, 265\nIBM Pulsar processor, M-35\u201336\nIBM RP3, M-61\nIBM RS/6000, M-58\nIBM RT-PC, M-21\nIBM SAGE, M-89\nIBM Stretch, M-6\nIBM 3090 Vector Facility, G-27\nIBM zSeries, G-27\nIC. See Instruction count (IC)\nI-cache, way prediction, 98\u201399\nID. See Instruction decode (ID)\nIdeal pipeline CPI, 169\nIDE disks, Berkeley\u2019s Tertiary Disk\nproject, D-12\nIdle Control Register (ICR), TI\nTMS320C55 DSP, E-8\nIdle domains, TI TMS320C55 DSP,\nE-8\nIEEE arithmetic\nfloating point, J-13\u201314\naddition, J-21\u201327\nexceptions, J-34\u201335\nmultiplication, J-17\u201321\nremainder, J-31\u201332\nunderflow, J-36\u201337\nhistorical background, J-63\u201365\niterative division, J-30\nNaN, J-14\nrounding modes, J-20\nsingle-precision numbers, J-15\n\u2013x vs. 0 \u2013x, J-62\nIEEE 754 floating-point standard, J-16\nIEEE 1394, Sony PlayStation 2\nEmotion Engine case\nstudy, E-15\nIEEE standard 802.3 (Ethernet), F-82\nLAN history, F-82\nIF cycle. See Instruction fetch (IF) cycle\nIlliac IV, M-45, M-55, F-104\nILP. See Instruction-level parallelism\n(ILP)\nImage processing units (IPUs),\n580\u2013582\nImage signal processors (ISPs),\n580\u2013582\nhardwired predecessors of,\n580\u2013581\ninterconnection of, 582\nImmediate addressing, K-52\nIMPACT, M-33\nImplicit unit stride, 333\nImprecise exceptions, 194\nInclusion, 383\ndrawback, B-35\nimplementation, 423\u2013424\nL1 caches, 423\u2013424\nIndex\n\u25a0\nI-19"
    },
    {
        "page": 1496,
        "text": "Inclusion (Continued)\nL2 caches, 423\u2013424\nL3 caches, 424\nmemory hierarchy history, M-12\nmultilevel, B-34, 423\nproperty, 78\nIndexed addressing, K-34, K-53\nIndexes\naddress translation during, B-36\u201340\nOpteron data cache, B-13\nrecurrences, H-12\nIndex field, B-8\u20139\nIndex vector, 301\u2013302\nIndirect addressing, K-52\nIndirect jumps, branch prediction,\n232\u2013234\nIndirect networks, F-32\u201333\nInexact exception\nfloating-point arithmetic, J-35\nfloating-point underflow, J-36\nInfiniBand, F-62, F-68, F-77\u201381, F-79\ncluster history, M-64\nInfinite population model, D-30\nInitiation rate, 290\nInktomi, M-63, M-74\u201375\nIn-order commit, speculation concept\norigins, M-31\nIn-order execution\naverage memory access time, B-18\ncache miss, B-2\nIBM Power processors, 265\nInput buffered switch, F-56\nInput gate, 553\nInput-output buffered switch, F-56,\nF-58, F-65\nInstruction cache\nAMD Opteron example, B-15,\nB-15\napplication/OS misses, B-59\nbranch prediction, C-24\nTI TMS320C55 DSP, E-8\nInstruction commit, C-43\u201344, 209, 235\nInstruction count (IC), B-4, 53\ncache performance, B-15\u201316\nprocessor performance equation, 52\nRISC history, M-23\nInstruction decode (ID)\nbranch hazards, C-18\ndata hazards requiring stalls, C-18\ndynamic scheduling, C-66, 193\nlonger latency pipelines, C-50\u201351\nMIPS R4000 pipeline, C-56\npipeline branch issues, C-35\u201336\nRISC classic pipeline, C-8\nRISC instruction set, C-5, C-6\nRISC pipeline, C-32\u201334, C-35\nsimple RISC implementation,\nC-27\nInstruction delivery, 228\u2013240\nstage, Itanium 2, H-42\nInstruction fetch (IF), 253\nbandwidth, 228\u2013232, 229\u2013230\ncycle\nARM Cortex-A53, 249\u2013250\nbranch hazards, C-18\nbranch-prediction buffer, C-24\ndata hazards requiring stalls,\nC-18\nexception stopping/restarting,\nC-41\nMIPS R4000 pipeline, C-56\nRISC exception, C-42\u201343, C-43\nRISC instruction set, C-4, C-6\nRISC pipeline, C-31\u201333\nsimple RISC implementation,\nC-27\nunits, integrated, 233\u2013234\nInstruction formats\naddressing modes and, K-6\u20139\nhigh-level language computer\narchitecture, M-20\nIA-64 ISA, H-34\u201338, H-39\nInstruction groups, IA-64, H-34\nInstruction issue, C-33\u201334\nItanium 2, H-41\u201343\nInstruction-level parallelism (ILP), 5,\n10, 368, 370\naggressive compiler-based\napproaches, 168\napproaches, 168\nbranch prediction\ncorrelating branch predictors,\n182\u2013184\nIntel Core i7, 190\u2013191\nspecialized, 232\u2013234\ntagged hybrid predictors,\n188\u2013190, 188, 190\ntournament predictors,\n184\u2013188, 186\nbranch-prediction buffer, C-25,\nC-25\nclock cycles per instruction,\n168\u2013169\ncompiler scheduling, M-32\ncompiler techniques, 176\u2013182\nconcepts, 169\u2013170\ncontrol dependences, 174\u2013176\ndata dependences, 170\u2013172\ndata flow limit, M-34\ndata hazards, 173\u2013174\ndefinition, 168\ndynamic scheduling, 222\u2013227\nadvantages, 191\u2013192\ndata hazards, 191\u2013201\nout-of-order execution,\n193\u2013194\nTomasulo\u2019s algorithm,\n195\u2013208, 205\u2013206\nearly studies, M-33\u201334\nexploitation methods, H-21\u201323\nexploitation of, 2\nexploitation statically, H-2\nexposing with hardware support,\nH-23\u201327\nIA-64, H-32\nloop unrolling, 177\u2013182\nmicroarchitectural techniques case\nstudy, 266\u2013273\nmultiple-issue processors, M-31,\n218\u2013227\nadvantages, 221\u2013222\nchallenges, 182, 221\u2013222\ncharacteristics, 219\ndynamically scheduled\nprocessor, 222, 224\nEPIC approach, 221\nmicroarchitectural techniques\ncase study, 266\u2013273\nwith speculation, 223\nsuperscalar, 218, 223\nVLIW approach, 218\u2013222, 220\nmultithreading history, M-36\nname dependences, 172\u2013173\npipeline scheduling, 177\u2013182\nscaling, 442\nspeculation, 222\u2013227\naddress aliasing prediction,\n239\u2013240\nadvanced techniques, 228\u2013240\nadvantages, 237\u2013238\nchallenge of issues per clock,\n236\u2013237\nI-20\n\u25a0\nIndex"
    },
    {
        "page": 1497,
        "text": "control dependence, 175\u2013176\ndisadvantages, 238\nand energy efficiency, 238\u2013239\nexception handling, 199\nexecution, 241\nhardware vs. software, 240\u2013241\nmicroarchitectural techniques\ncase study, 266\u2013273\nmultiple branches, 238\nregister renaming vs. ROB,\n234\u2013236\nstatic scheduling, 218\u2013222\nTI 320C6x DSP, E-8\nInstruction path length, 52\nInstruction prefetch, 234\nInstruction register (IR)\nRISC pipeline, C-31\u201332\nsimple RISC implementation, C-27\nInstruction set architecture (ISA),\n12\u201317. See also Intel\n80x86 processors;\nReduced Instruction Set\nComputer (RISC)\nbyte-addressed computers, A-8\nchanges, A-46\u201347\nclasses, A-4\nclassifying, A-3\u20136\nclass of, 12\ncomplications, C-43\u201345\ncomputer architecture definition,\nM-18\u201319\nCray X1, G-21\u201322\ncross-cutting issues, 126\u2013127\nencoding, 14, A-21\u201324, A-22\nfallacies and pitfalls, A-42\u201355\nfirst vector computers, M-49\ngeneral-purpose register computers,\nA-6\nhigh-level language computer\narchitecture, M-20\nIA-64\ninstruction formats, H-34\u201337,\nH-39\ninstructions, H-35\u201337\ninstruction set basics, H-38\noverview, H-32\u201340\npredication and speculation,\nH-38\u201340\nregister model, H-33\u201334\nmemory addressing, A-7\u201313\nmemory and total operands, A-5\nMIPS\nRISC history, M-20\u201323, M-22\nstack architectures, M-17\u201318\noperands, A-4, A-13\u201315\noperations in, A-15\u201316\noptimizations impact on\nperformance, A-27\nperformance and energy efficiency,\n258\nPixel Visual Core, 587\u2013588\nregister allocation, A-27\nRISC-V, A-34\nTPU, 559\nvirtual machine, 120\u2013121\nfor virtual machine, 122\u2013123\nInstruction set, extension, 124\u2013125\nInstructions per clock (IPC), 52, 169\nInteger arithmetic\naddition speedup\ncarry-lookahead, J-37\u201341\ncarry-lookahead circuit, J-38\ncarry-lookahead tree, J-40\ncarry-lookahead tree adder,\nJ-41\ncarry-select adder, J-43\u201344,\nJ-43\u201344\ncarry-skip adder, J-41\u201343, J-42\noverview, J-37\ndivision\nradix-2 division, J-55\nradix-4 division, J-56\nradix-4 SRT division, J-57\nwith single adder, J-54\u201357\nSRT division, J-45\u201347, J-46,\nJ-55\u201357\nFP conversions, J-62\nlanguage comparison, J-12\nmultiplication\narray multiplier, J-50\nBooth recoding, J-49\neven/odd array, J-52\nmany adders, J-50\u201354, J-50\nmultipass array multiplier,\nJ-51\nsigned-digit addition table,\nJ-54\nwith single adder, J-47\u201349,\nJ-48\u201349\nWallace tree, J-53\nmultiplication/division, shifting\nover zeros, J-45\noverflow, J-11\nRadix-2 multiplication/division,\nJ-4\u20137, J-4\nrestoring/nonrestoring division, J-5,\nJ-6\nripply-carry addition, J-2\u20133\nsigned numbers, J-7\u201310\nSRT division, J-45\u201347, J-46\nsystems issues, J-10\u201313\nInteger operations\nARM Cortex-A53, 249\ndata dependences, 171\nItanium 2, H-41\nmispeculation, 239\nRISC pipeline, C-45\u201355\nstalls, C-55\nstatic branch prediction, C-22, C-23\nInteger registers, IA-64, H-33\u201334\nIntegrated branch prediction, 233\nIntegrated circuits\nbasics, cell phones, E-24, E-24\ncost of, 31\u201335\ndependability, 36\u201338\nlogic technology, 19\npower and energy in, 23\u201329\nIntel 80286, M-9\u201310\nIntel Core i7, 100\nbranch prediction, 190\u2013191\nbuffers and queues, 256\nhardware prefetching, 110\nIntel Core i7 920\ncharacteristics, 259\nclock cycles per instruction, 256,\n257\nmisprediction rate, 192\nrelative performance and energy\nefficiency, 260\nIntel Core i7 6700, 55\u201356\nclock cycles per instruction, 256,\n257\nmemory hierarchy design,\n133\u2013142, 134\u2013135\nmisprediction rate, 192\nmultiple-issue processors, 247,\n252\u2013257\nperformance, 138\u2013142, 255\u2013257\npipeline structure, 254\nIntel Core i7 microprocessor\ndie, 32\nfallacy, 61\nfloorplan, 32\nIndex\n\u25a0\nI-21"
    },
    {
        "page": 1498,
        "text": "Intel Core i7 920 multicore computer,\n309\nIntel Crest, 579, 580\nIntel 8087, floating point remainder,\nJ-31\nIntel Haswell CPU Roofline, 599\nIntel i7, 388, 395\nIntel i7 920\nperformance and energy efficiency,\n434\u2013437\nsimultaneous multithreading, 246\nIntel i860, M-30, M-32, M-50,\nM-60\u201361\nIntel IA-32 architecture\ncall gate, B-53, B-54\ndescriptor table, B-52\ninstruction set complications, C-45\nOCNs, F-3\nsegment descriptor, B-52, B-53\nsegmented virtual memory,\nB-51\u201354\nIntel IA-64 architecture\ncompiler scheduling history,\nM-32\u201333\nconditional instructions, H-27\nexplicit parallelism, H-34\u201337\nhistorical background, M-33\nISA\ninstruction formats, H-34\u201337,\nH-39\ninstructions, H-35\u201337\ninstruction set basics, H-38\noverview, H-32\u201340\npredication and speculation,\nH-38\u201340\nItanium 2 processor\ninstruction latency, H-41\noverview, H-40\u201341\nperformance, H-43, H-43\nparallelism exploitation statically,\nH-2\nregister model, H-33\u201334\nRISC history, M-23\nsoftware pipelining, H-14\u201315\nsynchronization history, M-64\u201365\nIntel iPSC 860, M-60\u201361\nIntel Itanium, 168\ninstruction-level parallelism,\n261\u2013262\nsparse matrices, G-13\nspeculation, 241\nIntel Itanium 2\nIA-64\nfunctional units and instruction\nissue, H-41\u201343\ninstruction latency, H-41\noverview, H-40\u201341\nperformance, H-43, H-43\nIntellectual property, DSAs, 593\nIntelligent devices, historical\nbackground, M-88\nIntel Paragon, M-60\u201361, F-96\nIntel Pentium 4, 110, 261\u2013262\nExtreme, M-35\nItanium 2 comparison, H-43\nmultithreading history, M-36\nIntel Pentium II, M-35\nIntel Pentium III, power consumption,\nF-89\nIntel Pentium M, F-89\nIntel Pentium MMX, multimedia\nsupport, E-11\nIntel Pentium Pro, M-35\nIntel Pentium processors\nearly computer arithmetic, J-65\nvs. Opteron memory protection,\nB-57\nsegmented virtual memory,\nB-51\u201354\nIntel processor, 261\ninstruction set extensions, 125\nmultiple processors, 5\npower consumption, F-89\nIntel Teraflops processors, OCNs, F-3\nIntel Thunder Tiger 4 QsNetII, F-67\nIntel 80x86\ncomparative operation\nmeasurements, K-45\u201348\nfloating-point operations, K-38\u201340\ninstruction encoding, K-40\u201343\ninteger operations, K-35\u201337\nmeasurements of instruction set\nusage, K-44\u201348\noperand addressing, K-44\u201345\nprocessors\naddress space, B-58\ninteger overflow, J-11\nmemory hierarchy\ndevelopment, M-9\u201310\nprotection structure, B-50\nregisters and data addressing\nmodes, K-32\u201335\nSPECint92 programs, K-44, K-46,\nK-48\u201350\nIntel x86, conditional instructions,\nH-27\nIntel Xeon, F-80, 354, 387\nIntel Xeon E7, 426\nInteractive workloads, 467\nInterarrival times, D-30\nInterconnection networks\nadaptive routing, F-97\nadaptive routing and fault tolerance,\nF-98\narbitration, F-49\u201351\nbasic characteristics, F-2, F-21\nbisection bandwidth, F-93\ncommercial\ncongestion management,\nF-68\u201370\nconnectivity, F-67\ncross-company interoperability,\nF-67\u201368\nDECstation 5000 reboots,\nF-73\nfault tolerance, F-70\u201372\ncommunication bandwidth, I-3\ncompute-optimized processors vs.\nreceiver overhead, F-92\ndefinition, F-2\ndensity- vs. SPEC-optimized\nprocessors, F-89\ndevice example, F-3\ndirect vs. high-dimensional, F-96\ndomains, F-3\u20134, F-3\nEthernet, F-82\u201384\nexamples, F-73\u201385\nHOL blocking, F-59\u201361\nIBM Blue Gene/L, I-43\nInfiniBand, F-77\u201381\nLAN, F-82\u201384\nlink bandwidth, F-94\nmemory hierarchy interface,\nF-91\u201392\nmesh network routing, F-47\nMIN vs. direct network costs, F-96\nmulti-device connections\nbasic considerations, F-20\u201321\neffective bandwidth vs. nodes,\nF-29\nlatency vs. nodes, F-28\nperformance characterization,\nF-25\u201330\nI-22\n\u25a0\nIndex"
    },
    {
        "page": 1499,
        "text": "shared-media networks, F-23\u201324\nshared- vs. switched-media\nnetworks, F-23, F-25\nswitched-media networks,\nF-24\u201325\ntopology, routing, arbitration,\nswitching, F-21\u201322\nOCN, F-27\u201329\nprotection, F-91\nrouting, F-22, F-44\u201356\nrouting/arbitration/switching\nimpact, F-21\u201322\nSAN characteristics, F-27\u201329\nsoftware overhead, F-96\nstorage area networks, F-106\u2013108\nswitching, F-51\u201352\nswitch microarchitecture, F-56\u201366\nswitch vs. NIC, F-90\nsystem area networks, F-104\u2013106\nsystem/storage area network,\nF-77\u201381\ntop-level architecture, F-75\ntopology, F-30\u201344\ntwo-device interconnections\nbasic considerations, F-6\neffective bandwidth vs. packet\nsize, F-19\nexample, F-6\ninterface functions, F-6\u20139\nperformance, F-13\u201320\nstructure and functions, F-9\u201312\nvirtual channels and throughput,\nF-47\u201348\nWAN, F-84\u201385\nwormhole switching\nperformance, F-52\nzero-copy protocols, F-95\nIntermittent faults, D-11\nInternal fragmentation, B-47\nInternational Computer Architecture\nSymposium (ISCA),\nM-12\nInternational Mobile Telephony 2000\n(IMT-2000), cell phone\nstandards, E-25\nInternational Technology Roadmap for\nSemiconductors (ITRS),\n58\u201359, 59\nInternet Archive Cluster, D-36\u201341\ncontainers, M-76\nInternet of Things (IoT), 6\u20137\nInternet Protocol (IP), F-85\u201389\ncores, OCNs, F-3\nrouters, VOQs, F-31, F-103\nInternetworking, F-2, F-84, F-85\u201389\nInterprocedural analysis, H-10\nInterprocessor communication, large-\nscale multiprocessors,\nI-3\u20136\nInterrupt Enable (IE) flag, 127\nInvalidate protocol, 380\nexample, 385, 385\nimplementation, 382\u2013383\nsnooping coherence, 381\nInvalid exception, floating-point\narithmetic, J-35\nInverted page table, B-44\u201345\nI/O bandwidth, D-15\u201316\nI/O benchmarks, response time\nrestrictions, D-18\nI/O-bound, 121, 123\u2013124\nI/O bus\nhistorical background, M-88\u201389\npoint-to-point replacement, D-34\nSony PlayStation 2 Emotion Engine\ncase study, E-15\nI/O cache coherency, 128\u2013129\nI/O devices\naddress translation, B-38\nhistorical background, M-88\u201389\nperformance, D-15\u201323\nSANs, F-4\nshared-media networks, F-23\nswitched networks, F-2\nswitch vs. NIC, F-90\nwrite strategy, B-11\nI/O interfaces, storage area network\nhistory, F-107\nI/O network, F-67\nI/O processor (IOP)\nfirst dynamic scheduling, M-28\u201329\nSony PlayStation 2 Emotion Engine\ncase study, E-15\nI/O subsystems\ndesign, D-59\u201361\ninterconnection network speed, F-92\nvs. NIC, F-95\nzero-copy protocols, F-95\nI/O systems\nasynchronous, D-35\u201336\nas black box, D-24\ndirty bits, D-61\u201364\nmultithreading history, M-35\nqueing theory, D-23\nqueue calculations, D-29\nrandom variable distribution, D-26\nIP block, DSAs, 593\nIPC. See Instructions per clock (IPC)\nIPoIB, F-81\nIR. See Instruction register (IR)\nISA. See Instruction set architecture\n(ISA)\niSCSI\nNetApp FAS6000 filer, D-41\u201342\nstorage area network, F-106\u2013107\nISPs. See Image signal processors\n(ISPs)\nIssue logic, 236\nIssue stage\nID pipe stage, 194\ninstruction step, 197, 211\nIterative division, floating point,\nJ-27\u201331\nJ\nJava benchmark, 435\u2013437, 435, 437\nJava language, dependence analysis,\nH-10\nJava Virtual Machine (JVM), early\nstack architectures,\nM-18\nJohnson, Reynold B., M-85\nJumps, VAX, K-57\nJust-in-time (JIT), M-18\nK\nKahle, Brewster, M-76\nKahn, Robert, F-102\nk-ary n-cubes, F-38\u201339\nKendall Square Research KSR-1,\nM-61\u201362\nKernels\nEEMBC benchmarks, E-12\nFFT, I-7\nFORTRAN, compiler\nvectorization, G-15\nLU, I-8\nprocess, 40\nDAG of, 582\nDriver, 563\nsegmented virtual memory,\nB-51\nIndex\n\u25a0\nI-23"
    },
    {
        "page": 1500,
        "text": "Kernels (Continued)\nthroughput computing, 350\nvector kernel implementation,\n357\u2013359\nvirtual memory, 119\nL\nLabVIEW, embedded benchmarks,\nE-13\nLampson, Butler, F-103\nLarge-scale multiprocessors\ncache coherence implementation,\nI-34\u201335\ndeadlock and buffering, I-38\u201340\ndirectory controller, I-40\u201341\nDSM multiprocessor, I-36\u201337\ncharacteristics, I-45\ncluster history, M-63\u201364\nexample calculations, I-12\u201313\nhistorical background, M-60\u201362\nIBM Blue Gene/L, I-41\u201344,\nI-43\u201344\ninterprocessor communication, I-3\u20136\nfor parallel programming, I-2\nscientific applications, I-6\u201312\ndistributed-memory\nmultiprocessors,\nI-26\u201332, I-28\u201332\nparallel processing, I-33\u201334\nsymmetric shared-memory\nmultiprocessor, I-21\u201326,\nI-23\u201326\nspace and relation of classes, I-46\nsynchronization\nmechanisms, I-17\u201321\nperformance, I-12\u201316\nLatency, 20. See also Response time\nALU, C-46\u201348\nbandwidth, F-25\u201330\nbarrier synchronization, I-16\nand cache miss, B-2\ncluster history, M-74\ncommunication mechanism, I-3\ndefinition, D-15\u201316, C-46\u201348\ndeterministic vs. adaptive routing,\nF-53\u201356\ndistributed-memory\nmultiprocessors, I-30,\nI-32\nFlash memory, D-3\nFP operations, C-61, C-63, 177\nfunctional units, C-47\nhazards and forwarding, C-49\u201352\ninterconnection networks, F-13\u201320\nItanium 2 instructions, H-41\nmicroarchitectural techniques case\nstudy, 266\u2013273\nOCNs vs. SANs, F-28\nout-of-order execution, B-20\u201321\npackets, F-13, F-13\nperformance trends, 20, 21\nsnooping cache coherence, 447, 448\nSony PlayStation 2 Emotion\nEngine, E-17\nthroughput vs. response time, D-16\nutility computing, M-75\nvector memory systems, G-9\nvector start-up, G-8\nLatency-hiding techniques, 418\nL1 cache. See also First-level caches\naddress translation, B-46\nAlpha 21164, 395\nARM Cortex-A53, 251\u2013252\ndata cache size, 402\nfirst-level caches, 95\u201398\ninclusion, 423\u2013424\nIntel i7, 395\nmemory hierarchy, B-39, B-48\u201349,\nB-48\nmiss rate, 402\nOpteron memory, B-57\nL2 cache. See also Second-level caches\nAlpha 21164, 395\nARM Cortex-A53, 251\u2013252\ncache optimization, B-34, B-35\nIBM Blue Gene/L, I-42\ninclusion, 423\u2013424\nIntel i7, 395\nmemory hierarchy, B-39, B-48\u201349,\nB-48, B-57\nmemory system, 241\nL3 cache. See also Third-level caches\nAlpha 21164, 395\nIBM Blue Gene/L, I-42\nIBM Power8, 371\nIBM Power processors, 265\ninclusion, 424\nIntel i7, 395\nmemory access cycle shift,\n396\u2013397, 397\nmiss rate, 397\u2013399, 398\nsnoop bandwidth, 390\nLearning curve, 30\nLeast common ancestor (LCA),\nF-48\u201349\nLeast recently used (LRU)\nAMD Opteron data cache, B-14\nblock replacement, B-9\u201310, B-10\nmemory hierarchy history, M-11\nvirtual memory block replacement,\nB-45\nLimit field, B-52\nLinear speedup, multiprocessor,\n438\u2013439, 440\nLine Buffer Pool (LBP), 590\nLine locking, embedded systems, E-4\nLine, memory hierarchy, 81\nLink injection bandwidth\ncalculation, F-17\ninterconnection networks, F-26\u201327\nLink pipelining, F-16\u201317\nLink reception bandwidth, calculation,\nF-17\nLink register, K-25\nLinpack benchmark\ncluster history, M-64\nparallel processing debates, M-59\nVMIPS performance, G-17\u201319\nLinux operating system, RAID\nbenchmarks, D-22\nLiquid crystal display (LCD), Sanyo\nVPC-SX500 digital\ncamera, E-19\nLISP, K-21\u201322\nRISC history, M-20\u201321\nLittle Endian\nbyte, A-7\ninterconnection networks, F-12\nLittle\u2019s Law, 328\ndefinition, D-24\nserver utilization calculation, D-29\nLivelock, network routing, F-45\u201346\nLiveness, control dependences, 176\nLivermore Fortran Kernels, M-6\nLoad instruction\ncontrol dependence, 175\ndata hazards requiring stalls, C-17,\nC-17\nRISC instruction set, C-5\nLoad interlock, C-33\u201334, C-35\nLoad memory data (LMD), C-28\u201329\nLoad reserved, 413\u2013414, 416\nLoads instruction, 199\nI-24\n\u25a0\nIndex"
    },
    {
        "page": 1501,
        "text": "Load stalls, C-61, C-64\nLoad-store architecture, A-3\nLoad-store instruction set architecture,\nC-5, 12, C-28\nRISC history, M-20\nLocal address space, B-52\nLocal area networks (LANs)\ncharacteristics, F-4\ncross-company interoperability,\nF-67\u201368\neffective bandwidth, F-18\u201319\nEthernet as, F-82\nfault tolerance calculations, F-72\nInfiniBand, F-77\u201378\ninterconnection network domain\nrelationship, F-4, F-5\nlatency and effective bandwidth,\nF-27\u201329\noffload engines, F-8\npacket latency, F-13\u201316, F-13\nshared-media networks, F-23\ntime of flight, F-14\nLocal memory\ncentralized shared-memory\narchitectures, 377\nmultiprocessor architecture,\n371\u2013373\nNVIDIA GPU memory structures,\n326\u2013327\nLocal miss rate, B-31\nLocal node, 406\nLocal optimizations, A-26\nLocal predictors, 184\u2013188\nLocal scheduling techniques, 219\u2013220\nLocal state, 377\nLocation counts, WSCs, 468\nLocks\ncaching, 415\nlarge-scale multiprocessor\nsynchronization, I-18\u201321\nspin, 414\u2013416\nusing coherence, 414\u2013417, 416\nLockup-free cache, 100\u2013104\nLogical units, storage systems, D-34\nLogical volumes, D-34\nLong displacement addressing, K-52\nLong Instruction Word (LIW)\nEPIC approach, M-33\nmultiple-issue processor\ndevelopment, M-30,\nM-32\nLong mode, K-32\nLong short-term memory (LSTM)\ncells, 553, 554\u2013555\nLoop branches prediction, 232\u2013234\nLoop-carried dependence, 289, 312,\n337\u2013339\ndependence distance, H-6\nexample calculations, H-4\u20135\nloop-level parallelism, H-3\nrecurrence form, H-5\nLoop interchange, cache optimization,\n107\nLoop-level parallelism, H-2\u201312\nanalysis, 337\u2013339\nCUDA/NVIDIA term, 337\u2013338\ndefinition, 169\u2013170\ndependent computations, 344\u2013345\ndetection and enhancement,\nH-2\u201312\ndependence analysis, H-6\u201310\ndependent computation\nelimination, H-10\u201312\nfinding dependences, 339\u2013344\nhistory, M-32\u201333\nSIMD, 170\nLoop stream detection, 254\nLoop unrolling\nlimitation, 181\nand scheduling, 181\u2013182\nsoftware pipelining, H-12\u201315,\nH-13, H-15\nLossless networks\ndefinition, F-12\nswitch buffer organizations,\nF-59\u201360\nLossy networks, definition, F-12\nLRU. See Least recently used (LRU)\nLU kernel\ncharacteristics, I-8\ndistributed-memory\nmultiprocessor, I-32\nsymmetric shared-memory\nmultiprocessors, I-22,\nI-23, I-25\u201326\nM\nMAC. See Multiply-accumulate\n(MAC)\nMachine language programmer, M-18\nMachine learning, 546, 573\nMachine memory, virtual machine, 123\nMacro-op fusion, 253\nMagnetic disk technology, 19\nMagnetic storage\naccess time, D-3\ncost vs. access time, D-3\nhistorical background, M-85\u201386\nMail servers, D-20\u201321\nMain memory, B-2\u20133, 377, 400\nblock identification, B-44\u201345\nblock placement, B-44\ncache function, B-2\nvector processor, G-25\nvs. virtual memory, B-41\nwrite strategy, B-45\u201346\nMapReduce\nAWS, 495, 495\nWSCs, 471\u2013476, 472\nMark-I, M-3\u20134, M-6\nMark-II, M-3\u20134\nMark-III, M-3\u20134\nMark-IV, M-3\u20134\nMasPar, M-46, M-56\nMassively parallel processors (MPPs)\ncharacteristics, I-45\ncluster history, M-62\u201363, M-74\nsystem area network, F-104\nMatrix300 kernel, prediction buffer,\nC-25\nMatrix multiplication, LU kernel, I-8\nMatrix multiply unit, 557\u2013558, 558, 562\nMauchly, John, M-2\u20133, M-5, M-20\nMaximum transfer unit, network\ninterfaces, F-8\nMcCreight, Ed, F-103\nMcFarling\u2019s gshare predictor, 184\nMCP operating system, M-17\u201318\nMean time between failures (MTBF),\n37\nMean time to failure (MTTF)\ncomputer system power\nconsumption case study,\n69\u201371\ndependability, 37\u201338\nbenchmarks, D-21\ndisk arrays, D-6\nfallacy, 62\nI/O subsystem design, D-59\u201361\nRAID, M-86\u201387\nRAID reconstruction, D-55\u201357\nTB-80 cluster, D-41\nWSCs, 468\nIndex\n\u25a0\nI-25"
    },
    {
        "page": 1502,
        "text": "Mean time to repair (MTTR), 37\ndependability benchmarks, D-21\ndisk arrays, D-6\nRAID 6, D-8\u20139\nRAID reconstruction, D-55\u201356\nMean time until data loss (MTDL),\nD-55\u201357\nMedia extensions, DSPs, E-10\u201311\nMedia, interconnection networks,\nF-9\u201312\nMellanox MHEA28-XT, F-80\nMemory access\nALUs, data forwarding, C-36\u201337\ncache hit calculation, B-5\nCray Research T3D, F-91, F-91\ncross-cutting issues, 127\u2013128\ndata hazards stall minimization,\nC-14, C-16\ndistributed-memory\nmultiprocessor, I-32\nexception stopping/restarting, C-41\ninstruction set complications,\nC-43\u201344\nintegrated instruction fetch units,\n234\nlonger latency pipelines, C-51\nMIPS R4000 pipeline, C-59\nmulticycle FP operations, C-52\nRISC classic pipeline, C-8\nRISC exception, C-42\u201343, C-43\nRISC instruction set, C-5, C-6\nRISC pipeline, C-32\u201335, C-36\nsimple RISC implementation, C-28\nvector architectures, G-10\nMemory addressing, 13\naddressing modes, A-8\u201311, A-10\ncompiler-based speculation, H-32\ndisplacement addressing mode,\nA-11\u201312\nimmediate/literal, A-12, A-12\ninterpreting, A-7\u20138\nvector architectures, G-10\nMemory bandwidth, 350, 356\nMemory banks. See also Banked\nmemory\nexample, 301\nvector architecture, 298\u2013299\nvector systems, G-9\u201311\nMemory bus (M-bus), 377\ninterconnection networks,\nF-91\u201392\nMemory consistency, 377\u2013378,\n417\u2013422\ncase study, 456\u2013458\ncompiler optimization, 422\ndevelopment of models, M-64\u201365\nprogrammer\u2019s view, 418\u2013419\nrelaxed consistency models,\n419\u2013422, 421\nsequential consistency, 417\nspeculation to hide latency,\n422\u2013423\nMemory-constrained scaling, I-33\u201334\nMemory hierarchy, F-91\u201392\naddress space, B-57\nblock identification, B-8\u20139\nblock placement, B-7\u20138, B-7\nblock replacement, B-9, B-10\ncache optimizations, B-22\u201340, B-40\nhit time reduction, B-36\u201340\nmiss categories, B-23\u201325\nmiss penalty reduction,\nB-30\u201336\nmiss rate reduction, B-26\u201330,\nB-27\ncache performance, B-3\u20136,\nB-15\u201316, B-40\naverage memory access time,\nB-17\u201320\nequations, B-22\nexample calculation, B-16\u201317\nmiss penalty, B-20\u201321\nout-of-order execution,\nB-20\u201321\ndevelopment, M-9\u201312\nlevels in slow down, B-3\nOpteron data cache example,\nB-12\u201315, B-13, B-15\nOpteron L1/L2, B-57\nOS and page size, B-58\nquestions, B-6\u201312\nterminology, B-2\nvirtual address to L2 cache, B-39\nvirtual memory, B-2\u20133, B-40\u201349\naddress space, B-12, B-41,\nB-44, B-55\naddress translation, B-46, B-47\ncaches and, B-42\u201343, B-42,\nB-48\u201349, B-48\nclasses, B-43\nIntel Pentium vs. AMD\nOpteron, B-57\npaged example, B-54\u201357\npage size selection, B-46\u201347\nparameter ranges, B-42\nPentium vs. Opteron protection,\nB-57\nprotection, B-49\u201350\nquestions, B-44\u201346\nsegmented example, B-51\u201354\nwrite strategy, B-45\u201346\nWSCs, 479\u2013482, 479\nMemory hierarchy design\nARM Cortex-A53, 129\u2013131, 130\nbasics of, 81\u201384\ncache optimization\nadvancement, 117\ncache misses, 112\u2013113\ncompiler-controlled\nprefetching, 111\u2013114\ncompiler optimizations,\n107\u2013109\ncritical word first, 104\u2013105\nearly restart, 104\u2013105\nenergy consumption, 97\nfloating-point programs,\n101\u2013102\nhardware prefetching, 109\u2013111\nHBM packaging, 114\u2013117\nhit time/power reduction, 95\u201398\nmultibanked caches, 99\u2013100\nnonblocking caches, 100\u2013104\npipelined access, 99\u2013100\nway prediction, 98\u201399\nwrite buffer merging, 105\u2013106,\n106\nCortex-A53 performance, 132\nC program evaluation, 151\ncross-cutting issues\nautonomous instruction fetch\nunits, 127\ncase study, 150\u2013153\ncoherency of cached data,\n128\u2013129\nprotection, virtualization, and\ninstruction set\narchitecture, 126\u2013127\nspecial instruction caches, 128\nspeculation and memory access,\n127\u2013128\nIntel Core i7 6700, 133\u2013142, 134\nin personal mobile device, 79\npitfall, 143\nI-26\n\u25a0\nIndex"
    },
    {
        "page": 1503,
        "text": "technology and optimizations\ndependability, 93\u201394\nDRAM technology, 85\u201387\nFlash memory, 92\u201393\nGDRAMs, 90\nphase-change memory\ntechnology, 93\nSRAM technology, 85\nstacked/embedded DRAMs, 91\nsynchronous DRAM, 87\u201390\nvirtual machine\nhardware management, 121\nimpact on virtual memory,\n123\u2013124\ninstruction set architecture for,\n122\u2013123\nprotection via, 120\u2013122\nsoftware management, 121\nvirtual machine monitor\ninstruction set extension,\n124\u2013125\nrequirements, 122\nXen virtual machine, 126\nvirtual memory\ninstruction set extension,\n124\u2013125\nprotection via, 119\u2013120\nvirtual machines impact on,\n123\u2013124\nMemory latency, 85\nMemoryless, D-27\u201328\nMemory mapping, B-52\nMemory-memory architecture, A-3\nMemory protection\nexception, 175\nPentium vs. Opteron, B-57\nprocesses, B-49\u201350\nsafe calls, B-54\nsegmented virtual memory,\nB-51\u201354\nMemory stall cycles\naverage memory access time, B-18\ndefinition, B-3\u20134, B-22\nmiss rate calculation, B-6\nout-of-order execution, B-20\nMemory system, 377\u2013378\ncoherency, 378\nIntel Core i7 6700 pipeline\nstructure, 254\nmultiprocessor architecture, 369,\n371\u2013373\npage size changes, B-58\nspeculative execution, 241\nvector architectures, G-9\u201311\nvector chaining, G-11\nvirtual (see Virtual memory)\nMemristor, 93\nMesh interface unit (MIU), F-74\nMesh network, F-43, F-47\nMesh topology, F-74\nMESIF protocol, 388\nMESI protocol, 388, 449\nMessage ID, packet header,\nF-8, F-17\nMessage-passing communication\nadvantages, I-5\u20136\nhistorical background, M-60\u201361\nMessage Passing Interface (MPI)\nfunction, F-8\nInfiniBand, F-80\u201381\nlack in shared-memory\nmultiprocessors, I-5\nMessage-passing protocols, 373\nMessages\nadaptive routing, F-64\ninterconnection networks,\nF-6\u20139\nzero-copy protocols, F-95\nMFLOPS. See Millions of floating-\npoint operations per\nsecond (MFLOPS)\nMicroarchitecture, 17, 272\ncase study, 266\u2013273\nCray X1, G-21\u201322\nOCNs, F-3\nTPU, 559\u2013560\nMicrobenchmarks\ndisk array deconstruction,\nD-51\u201354\ndisk deconstruction, D-48\u201350\nMicroinstructions, complications, C-45\nMicroMIPS64, K-3\n16-bit instruction, K-7, K-10, K-17\nregister encodings, K-7\nMicro-op decode, 253\nMicro-op fusion, ALU, 254\nMicroprocessor\nAMD Opteron, 27\nclock rate, 26\ndesign for typical case, 27\ndo nothing well, 27\nenergy and power within, 25\u201328\ngrowth rate, 2, 3\ninside disks, D-4\nperformance milestones, 22\nrecent advances, M-35\nVAX 11/780, 3\nMicroprocessor without Interlocked\nPipeline Stages (MIPS)\nearly pipelined CPUs, M-28\nmultiple-issue processor, M-30\nperformance measurement history,\nM-6\u20137\nRISC history, M-20\nMicrosoft\navailability zones, 499\ncontainers, M-76\nMicrosoft Azure, M-75\u201376\nMicrosoft Catapult\nBing search engine, 573\nboard design, 568\nCNNs on, 570\u2013572, 571\u2013572\nevaluating, 601\u2013602\nguidelines, 577\u2013579\nimplementation and architecture,\n568\u2013569\nsearch acceleration on, 573\u2013574\nsoftware, 569\nversion 1 deployment, 574\nversion 2 deployment, 575\u2013577,\n576\u2013577\nMicrosoft\u2019s DirectX 8, M-51\u201352\nMicrosoft Windows, RAID\nbenchmarks, D-22\nMicrosoft XBox, M-51\u201352\nMigration, 379\nMillion instructions per second (MIPS)\nconditional instructions, H-27\nembedded systems, E-15\nMIPS16, K-4, K-5\nMIPS M2000, M-22, M-22\nMIPS M2000 vs. VAX 8700,\nM-22\nMIPS64 R6, K-19, K-19\nMIPS R2000, M-21\nMIPS R10000, 423\nMIPS R4000 pipeline, C-55\u201364\nfloating-point operations,\nC-60\u201361, C-60\nperformance, C-61\u201364\nRISC instruction set, C-3\u20134\nSony PlayStation 2 Emotion\nEngine, E-17\nIndex\n\u25a0\nI-27"
    },
    {
        "page": 1504,
        "text": "Millions of floating-point operations\nper second (MFLOPS)\nearly performance measures, M-7\nparallel processing debates,\nM-58\u201359\nSIMD computer history, M-56\nSIMD supercomputer development,\nM-46\nvector performance measures,\nG-15\u201316\nMinibatches, DNNs, 556\nMinicomputers, 4\nMIPS. See Million instructions per\nsecond (MIPS)\nMIPS R3000\ninteger division/remainder,\nJ-11\u201312\ninteger overflow, J-11\nMIPS R3010\narithmetic functions, J-57\u201361\nchip comparison, J-58\nchip layout, J-59\u201360\nfloating-point exceptions, J-35\nMIPS R4000, early pipelined CPUs,\nM-28\nMisprediction rate\nARM Cortex-A53, 250\nbranch-prediction buffers, C-25\nprofile-based predictor, C-23\nSPEC89 benchmark, C-26\nSPEC89 vs. predictor size, 187\nstatic branch prediction, C-22, C-23\ntagged hybrid vs. gshare predictors,\n190\nMisses per instruction\nadvantage, B-6\napplication/OS statistics, B-59\nand block size, 397\u2013399, 398\ncache performance, B-5\u20136\nmemory hierarchy design, 82\nMiss penalty, B-20\u201321\ncache optimization, B-30\u201336,\n105\u2013106, 106\ncompiler-controlled prefetching,\n111\u2013114\ncritical word first, 104\u2013105\nearly restart, 104\u2013105\nhardware prefetching, 109\u2013111\nmemory hierarchy design, 82\nmultilevel caches reducing, 83\nreducing, 95\nreduction via multilevel caches,\nB-30\u201335\nwrite buffers, 83\nMiss rate\nAMD Opteron example, B-15\naverage memory access time,\nB-29\u201330, B-30\nbigger caches reducing, 83\ncache optimization\nand associativity, B-28\u201330\nand block size, B-26\u201328, B-27\nand cache size, B-24\u201325, B-28,\nB-33, B-37\nand virtually addressed cache\nsize, B-37\ncache performance, B-16\u201317\nand cache size, B-24\u201325, B-28,\nB-33, B-37\ncompiler-controlled prefetching,\n111\u2013114\ndata, B-16\nand data cache size, 402\nearly IBM computers, M-11\nexample calculations, B-6\nformula, B-16\nglobal, B-31\nhardware prefetching, 109\u2013111\nhigher associativity reducing, 83\ninstruction, B-16\nlarger blocks reducing, 82\nL1 caches, 402\nL3 caches, 397\u2013399, 398\nlocal, B-31\nmeasurement, B-4\u20135\nmemory hierarchy, 81\nmemory stall clock cycles, B-4\nreducing, 95\nscientific workloads\ndistributed-memory\nmultiprocessors,\nI-28\u201332\nsymmetric shared-memory\nmultiprocessors, I-22,\nI-23\u201325\ntotal and distribution, B-25\nunified, B-16\nMiss Status Handling Registers\n(MSHRs), 104\nMIT Raw, F-78\nMixed cache, B-15\nMixer, radio receiver, E-23\nM/M/1 model, D-30, D-32, D-57\nM/M/2 model, D-57\nModified, shared, invalid (MSI)\nprotocol, 388\nModified state, 383\u2013384, 406\nlarge-scale multiprocessor cache\ncoherence, I-35\u201336\nModula-3, integer division/remainder,\nJ-12\nModule availability, 37\nModule reliability, 37\nMOESI, 388\nMoore\u2019s Law, 19\ninterconnection networks, F-74\npitfall, 58\npoint-to-point links and switches,\nD-34\nRISC history, M-23\nsemiconductor manufacturing, 4\nswitch size, F-29\ntransistors, 5, 540\nMotion JPEG encoder, Sanyo VPC-\nSX500 digital camera,\nE-19\nMotorola 68882, floating-point\nprecisions, J-33\nMotorola 68000, memory protection,\nM-10\nMove address, K-55\u201356\nMPEG\nmultimedia SIMD extensions\nhistory, M-50\nSanyo VPC-SX500 digital camera,\nE-19\nSony PlayStation 2 Emotion\nEngine, E-17\nMPPs. See Massively parallel\nprocessors (MPPs)\nMTTF. See Mean time to failure\n(MTTF)\nMTTR. See Mean time to repair\n(MTTR)\nMultibanked caches, 99\u2013100\nMultichip modules, OCNs, F-3\nMulticomputers, 370\ncluster history, M-65\ndefinition, M-59\nhistorical background, M-64\u201365\nMulticore processor, 17, 369, 371\u2013372,\n382, 408\napproaches, 389\nI-28\n\u25a0\nIndex"
    },
    {
        "page": 1505,
        "text": "architecture, 430\ncoherence, 387\nCray X1E, G-24\ndevelopment, 404\nDSM, 373, 405, 452\nIntel i7 performance and energy\nefficiency, 434\u2013437\non multiprogrammed workload,\n426\u2013432\nOCN, F-101\nperformance, 426\u2013437, 432\npoint-to-point, 446\nscaling, 432, 442\u2013444\nsingle chip, 382, 391, 446\u2013451\nand SMT, 436\u2013437\nMultics protection software, M-9\u201310\nMulticycle operations, RISC pipeline,\nC-45\u201349\nFP pipeline performance, C-55\nhazards and forwarding, C-49\u201352\nmaintaining precise exceptions,\nC-53\u201355\nMultidimensional arrays, 299\u2013301\nMultiflow processor, M-31\u201333\nMultigrid methods, ocean application,\nI-9\u201310\nMultilayer perceptrons (MLPs), 549\u2013550\nMultilevel caches\ncentralized shared-memory\narchitectures, 377\nmemory hierarchy history, M-12\nmiss penalty reduction, B-30\u201335,\nB-33\nwrite process, B-11\nMultilevel exclusion, B-35\nMultilevel inclusion, B-34, 423\nmemory hierarchy history, M-12\nMultimedia applications, desktop\nprocessor support, E-11\nMultimedia Extensions (MMX), K-31,\nM-49\u201350\nMultimedia instruction sets, 10\nMultimedia SIMD extensions, M-49\u201350\nDSPs, E-11\nMultimode fiber, interconnection\nnetworks, F-9\u201310\nMultipass array multiplier, J-51\nMultiple instruction streams, multiple\ndata streams (MIMD),\n11, 282, 369\u2013370,\n438\u2013439\nearly computers, M-57\nfirst vector computers, M-49\nmultimedia SIMD and GPU vs.,\n347\u2013353\nMultiple instruction streams, single\ndata stream (MISD), 11\nMultiple-issue processors\nadvantages, 221\u2013222\nchallenges, 182, 221\u2013222\ncharacteristics, 219\ndynamically scheduled processor,\n222, 224\nearly development, M-29\u201332\nEPIC approach, 221\ninstruction-level parallelism,\n218\u2013227\nmicroarchitectural techniques case\nstudy, 266\u2013273\nwith speculation, 223\nsuperscalar, 218, 223\nVLIW approach, 218\u2013222, 220\nMultiple lanes technique\nvector architecture, 293\u2013294, 294\nvector performance, G-7\u20139\nMultiple-precision addition, J-13\nMultiply-accumulate (MAC), 589\nDSP, E-5\nTI TMS320C55 DSP, E-8\nMultiply operations\nchip comparison, J-61\nfloating point\ndenormals, J-20\u201321\nexamples, J-19\nmultiplication, J-17\u201321\noverview, J-17\u201320\nprecision, J-21\nrounding, J-18, J-19\ninteger arithmetic\narray multiplier, J-50\nBooth recoding, J-49\neven/odd array, J-52\nissues, J-11\nmany adders, J-50\u201354, J-50\nmultipass array multiplier, J-51\nn-bit unsigned integers, J-4\nRadix-2, J-4\u20137\nsigned-digit addition table, J-54\nwith single adder, J-47\u201349,\nJ-48\u201349\nWallace tree, J-53\ninteger shifting over zeros, J-45\nMultiprocessor\napplication, 373\u2013374\narchitecture issues and approach,\n370\u2013373\nbus-based coherent, M-59\u201360\ncache coherence, 377\u2013379\ncluster history, M-62\u201365\ncoining of term, M-59\ndefinition, 369\nearly computers, M-57\nearly machines, M-57\nembedded systems, E-14\u201315\nexecution time, 438\nfactors, 368\nfallacy, 60\nIntel, 5\nlarge-scale (see Large-scale\nmultiprocessors)\nlinear speedup, 438\u2013439, 440\nparallel processing challenges,\n373\u2013377\nparallel processing debates,\nM-57\u201359\nperformance gains, 424\u2013426\nprocessor performance, 371\u2013372\nrecent advances and developments,\nM-59\u201360\nshared-memory (see Shared-\nmemory multiprocessors\n(SMPs))\nSIMD computers, M-55\u201357\nsynchronization and consistency\nmodels, M-64\u201365\nXeon E7 MP scalability, 433\u2013434\nMultiprogramming, 369\nvirtual memory, B-49, 119\nworkload, 399\u2013404, 426\u2013432\nMultistage interconnection networks\n(MINs)\nbidirectional, F-34\ncrossbar switch calculations,\nF-32\u201333\nvs. direct network costs, F-96\ntopology, F-32\nMultistage switch fabrics, F-31\nMulti-Streaming Processor (MSP)\nCray X1, G-21\u201324, G-22\nCray X1E, G-24\nfirst vector computers, M-49\nMultithreaded SIMD Processor, 311,\n315, 317\nIndex\n\u25a0\nI-29"
    },
    {
        "page": 1506,
        "text": "Multithreading, 369\nhardware approaches, 243\u2013244\nhistorical background, M-35\u201336\ninstruction-level parallelism,\n242\u2013247\nparallel benchmarks, 247\nperformance gains, 424\u2013426\nsimultaneous (see Simultaneous\nmultithreading (SMT))\nspeedup from, 248\nsuperscalar processors, 245\u2013247\nMVAPICH, F-81, F-81\nM-way set associative, B-8\nMXP processor, components, E-14\u201315\nMyrinet SAN, F-49, M-63, M-74, F-90,\nF-90\nN\nNAK. See Negative acknowledge\n(NAK)\nName dependences\ninstruction-level parallelism,\n172\u2013173\nregister renaming, 196\nNameplate power rating, 482\nNaN. See Not a Number (NaN)\nNASA Ames Research Center, M-46,\nM-65\nNAS parallel benchmarks, vector\nprocessor history,\nG-27\u201328\nNational Science Foundation, F-102\nNatural parallelism embedded systems,\nE-15\nn-bit adder, carry-lookahead, J-38\nn-bit number representation, J-7\u201310\nn-bit unsigned integers division, J-4\nN-body algorithms, Barnes application,\nI-8\u20139\nNBS DYSEAC, M-89\nN-cube topology, F-36\u201338\nNEC SX/2, M-48\nNEC SX/5, M-48\u201349\nNEC SX/6, M-48\u201349\nNEC SX-8, M-48\u201349\nNEC SX-9, M-49\nfirst vector computers, M-49\nNEC VR 4122, embedded benchmarks,\nE-13\nNegative acknowledge (NAK)\ncache coherence, I-39\ndirectory controller, I-41\nDSM multiprocessor cache\ncoherence, I-37\nNegative-first routing, F-48\nNested page tables, 146\nNetscape, F-102\nNetwork Appliance (NetApp), D-9,\nD-41\u201343\nNetwork-attached storage (NAS),\nM-88, 478\nNetwork bandwidth, interconnection\nnetwork, F-18\nNetwork-Based Computer Laboratory\n(Ohio State), F-80\u201381\nNetwork buffers, network interfaces,\nF-8\nNetwork fabric, F-24\u201325\nNetwork File System (NFS)\nbenchmarking, D-20, D-20\nblock servers vs. filers, D-35\ninterconnection networks, F-93\u201394\nTCP/IP, F-86\nNetworking\nGoogle WSCs, 510\u2013511\nperformance milestones, 22\nNetwork injection bandwidth\ninterconnection network, F-19\u201320\nmulti-device interconnection\nnetworks, F-26\nNetwork interface\nfault tolerance, F-67\nfunctions, F-6\nmessage composition/processing,\nF-6\u20139\nNetwork interface card (NIC)\nfunctions, F-8\nI/O subsystem, F-95\nvs. switches, F-90, F-90\nzero-copy protocols, F-95\nNetwork I/O, 467\nNetwork layer, F-84\nNetwork nodes, F-23, F-35, F-36\ndistributed switched networks, F-35\nNetwork of Workstations, M-63,\nM-74\u201375\nNetwork on chip (NoC), F-3\nNetwork ports, F-30\nNetwork protocol layer, F-10\nNetwork reception bandwidth,\nF-19\u201320\nNetwork reconfiguration, F-70\u201371\nNetwork technology, 20\npersonal computers, F-2\nNewton\u2019s iteration, J-27\u201330, J-28\nNicely, Thomas, J-65\nNodes\ncommunication bandwidth, I-3\ndirect network topology, F-37\ndistributed switched networks,\nF-35\u201340\nIBM Blue Gene/L, I-42\u201344, I-43\nIBM Blue Gene/L 3D torus\nnetwork, F-76\nnetwork topology performance and\ncosts, F-40\npoints-to analysis, H-9\nNokia cell phone, circuit board, E-24,\nE-24\nNonatomic operations, cache\ncoherence, 386\nNonbinding prefetch, 111\nNonblocking caches\ncache bandwidth, 100\u2013104\ncase study, 148\u2013164\neffectiveness, 102\nimplementing, 103\u2013104\nmemory hierarchy history,\nM-11\u201312\nNonblocking crossbar, F-33\nNonfaulting prefetches, 111\nNonoptimal replacement algorithm,\nM-11\nNon-overlapped latency, B-20\nNonrecurring engineering (NRE) costs,\n542\nNonrestoring division, J-5, J-6\nNonuniform cache access (NUCA),\n371, 390, 426\nNonuniform memory access (NUMA),\n372\u2013373, 391, 426\u2013429\nlarge-scale multiprocessors history,\nM-61\nNon-unit strides, 300\nvector processor, G-25\nNorth-last routing, F-48\nNot a Number (NaN), J-14\u201316, J-21,\nJ-34\u201335\nNotifications, interconnection\nnetworks, F-10\nNOW project, M-74\u201375\nNo-write allocate, B-11\u201312\nNSFNET, F-102\nI-30\n\u25a0\nIndex"
    },
    {
        "page": 1507,
        "text": "NTSC/PAL encoder, Sanyo VPC-\nSX500 digital camera,\nE-19\nNUMA. See Nonuniform memory\naccess (NUMA)\nNVIDIA GeForce, M-51\u201352\nNVIDIA K80 GPU die Roofline, 599\nNVIDIA system\ncomputational structures, 313\u2013320\nGPU computational structures,\n313\u2013320\nGPU computing history, M-52\u201353\nGPU instruction set architecture,\n320\u2013323\nGPU memory structures, 326\u2013328,\n327\ngraphics pipeline history, M-51\u201352\ninstruction set architecture,\n320\u2013323\nNVIDIA P100, 354\nscalable GPUs, M-51\nTegra Parker system vs. Core i7,\n346\u2013353\nN-way set associative, B-8\nconflict misses, B-23\nmemory hierarchy, 81\nTLB, B-49\nNYU Ultracomputer, M-61\nO\nOccupancy, communication\nbandwidth, I-3\nOcean application\ncharacteristics, I-9\u201310\ndistributed-memory multiprocessor,\nI-30, I-32\nexample calculations, I-11\u201312\nmiss rates, I-28\nsymmetric shared-memory\nmultiprocessors, I-23\nOffline reconstruction, RAID, D-55\nOffload engines\nnetwork interfaces, F-8\nTCP/IP reliance, F-100\nOffset, B-8\u20139\naddress, B-55\u201356\nblock identification, B-8\u20139\ncache optimization, B-38\ndestination, IA-32 segment, B-53\nIA-32 segment, B-53\nOpteron data cache, B-13, B-14\npage, B-38\nsign-extended, C-5\nvirtual memory, B-43\u201344, B-46,\nB-55\u201356\nword, C-28\nOLTP. See Online transaction\nprocessing (OLTP)\nOmega, F-31, F-32\u201333\nOn-chip memory, embedded systems,\nE-4\nOn-chip networks (OCNs)\nbasic considerations, F-3\ncommercial implementations,\nF-73\u201374\ncommercial interconnection\nnetworks, F-63\ncross-company interoperability,\nF-68\neffective bandwidth, F-19\ninterconnection network domain\nrelationship, F-4, F-5\nlatency vs. nodes, F-28\npacket latency, F-13, F-14\u201316\ntime of flight, F-14\ntopology, F-30\nwormhole switching, F-52\nOne\u2019s complement, J-7\u20138\nOne-way set associativity, conflict\nmisses, B-23\nOnline reconstruction, RAID, D-55\nOnline transaction processing (OLTP),\n44, 395\u2013396, 396, 401\nstorage system benchmarks, D-18\nOpcode, A-21\nOpenGL, M-51\u201353\nOpen instruction set, DSAs, 594\nOpen Systems Interconnect (OSI)\nEthernet, F-86\nlayers, F-84\nOperand delivery stage, Itanium 2,\nH-42\nOperands\nDSP, E-6\ninstruction set architecture, A-4,\nA-13\u201315\nread, C-68\ndynamic scheduling pipelines,\nC-66\nID pipe stage, 194\nTMS320C55 DSP, E-6\ntype and size, A-13\u201315\nOperating systems (general)\naddress translation, B-38\ncommunication performance, F-8\ndisk access scheduling, D-44\nmemory protection performance,\nB-58\nmiss statistics, B-59\nand page size, B-58\nsegmented virtual memory, B-54\nstorage systems, D-35\u201336\nvendor-independent, 2\nworkload, 399\u2013404\nOperational costs count, 468\nOperational expenditures (OPEX), 36,\n486\u2013490, 488\nOperation faults, D-11\nOperations, 14 See also specific types of\noperations\natomic, 386\nin instruction set, A-15\u201316, A-15\nOperator dependability, disks,\nD-13\u201315\nOpteron Data Cache, B-11\nOptical media, interconnection\nnetworks, F-9\u201310\nOracle database, miss statistics, B-59\nOrdering, and deadlock, F-47\u201348\nOrganizations, 17\nblock placement, B-7\u20138, B-7\nbuffer, F-59\u201361\ndata dependences, 172\ndesigning, 17\u201318\ndynamic random-access memory\n(DRAM), 86\non-chip\nIBM Power8, 428\nXeon E7, 428\nOpteron data cache, B-12\u201315, B-13\nSony PlayStation Emotion Engine,\nE-18\nSPEC benchmarks, 433\u2013434\nOut-of-order completion\ndefinition, C-53\ndynamic scheduling pipelines, C-66\nOut-of-order execution\nand cache miss, B-2\ndynamic scheduling, 193\u2013194\ndynamic scheduling pipelines, C-66\nmemory hierarchy, B-2\nmicroarchitectural techniques case\nstudy, 266\u2013273\nIndex\n\u25a0\nI-31"
    },
    {
        "page": 1508,
        "text": "Out-of-order execution (Continued)\nmiss penalty, B-20\u201321\nTomasulo\u2019s scheme, 208\nOut-of-order processors, memory\nhierarchy history, M-12\nOutput buffered switch, F-56, F-58,\nF-61, F-65\nOutput dependence, 173\ncompiler history, M-32\nfinding, H-7\u20138\nOutput gate, 553\nOverclocking, 28\nOverflow, integer arithmetic, J-8,\nJ-10\u201311, J-11\nOverhead\nAmdahl\u2019s law, F-96\ncalculating, F-94\ncommunication latency, I-4\ninterconnection networks, F-27\u201329\nOCNs vs. SANs, F-28\nprocessor, G-4\nsoftware, F-96\nsorting case study, D-64\u201367\ntime of flight, F-14\nOverlapping triplets, integer\nmultiplication, J-49\nOversubscription, 478\nP\nPacked decimal, A-14\nPackets\nATM, F-79\nbidirectional rings, F-36\ncentralized switched networks,\nF-33\ndiscarding, F-69\neffective bandwidth vs. packet size,\nF-19\nformat example, F-7\nInfiniBand, F-79\nlatency issues, F-13, F-13\nlossless vs. lossy networks, F-12\nnetwork interfaces, F-8\nnetwork routing, F-22\nswitching, F-51\nswitch microarchitecture, F-56\u201359\npipelining, F-64\u201366\nTI TMS320C6x DSP, E-10\ntopology, F-21\u201322\ntransport, interconnection\nnetworks, F-9\u201312\nPage(s)\ncoloring, B-38\ndefinition, B-43\nvs. segments, B-43\nPaged segments, B-43\u201344, B-43\nPaged virtual memory, B-54\u201357\nPage fault\ndefinition, B-2\u20133, B-42, B-45\nexception stopping/restarting, C-41\nPage offset\ndefinition, B-38\nvirtual memory, B-43\u201344, B-46\nPage size, B-56\noperating systems and, B-58\nselection, B-46\u201347\nvirtual memory, B-46\u201347\nPage table entry (PTE)\ndefinition, B-44, 136\nfields in, B-52\nPage tables\nAMD64 paged virtual memory,\nB-55\ndescriptor tables as, B-52\nmain memory block, B-44\u201345\nnested, 146\nprotection processes, B-50\nsegmented virtual memory,\nB-51\u201352\nshadow, 123\nsize, B-47\nvirtual address to physical address,\nB-45\nPaging support, 330\nPaired single operations, DSP media\nextensions, E-10\u201311\nPalt, definition, B-2\u20133\nPapadopolous, Greg, M-76\nParallel architectures, classes of,\n10\u201311\nParallelism\nchallenges, 373\u2013377\nclasses of, 10\u201311\ncomputer design principles, 48\ndependence analysis, H-8\u20139\nEthernet, F-74, F-82\u201383\nexploitation statically, H-2\nexposing with hardware support,\nH-23\u201327\nglobal code scheduling, H-15\u201323,\nH-16\nIA-64 instruction format, H-34\u201337\nILP (see Instruction-level\nparallelism (ILP))\nrequest-level, 369\nsoftware pipelining, H-12\u201315\nsuperblock scheduling, H-21\u201323,\nH-22\ntaking advantage of, 48\nthread-level (see Thread-level\nparallelism (TLP))\ntrace scheduling, H-19\u201321, H-20\nParallel memory systems, highly,\n150\u2013153\nParallel processing, 369\nParallel processors\nareas of debate, M-57\u201359\nbus-based coherent\nmultiprocessors,\nM-59\u201360\ncluster history, M-62\u201365\nlarge-scale multiprocessors history,\nM-60\u201362\nrecent advances and developments,\nM-59\u201360\nscientific applications, I-33\u201334\nSIMD computers history, M-55\u201357\nsynchronization and consistency\nmodels, M-64\u201365\nvirtual memory history, M-65\nParallel programming\ncomputation communication,\nI-10\u201312\nwith large-scale multiprocessors,\nI-2\nParallel Thread Execution (PTX),\n320\u2013323, 322\nParavirtualization, 126\nPA-RISC, K-3\nParity, dirty bits, D-61\u201364\nPARSEC benchmark\nsimultaneous multithreading, 246\nwithout SMT, 435\u2013437, 435, 437\nPartial disk failure, dirty bits, D-61\u201364\nPartial store order (PSO), 420, 421, 457\nPartitioned add operation, DSP media\nextensions, E-10\nPartitioning, 480\u2013482\nPascal GPU architecture, 328\u2013331, 329\nfull-chip block diagram, 318\nSIMD Processor, 330\nPascal programs, integer division/\nremainder, J-12\nI-32\n\u25a0\nIndex"
    },
    {
        "page": 1509,
        "text": "Pattern, disk array deconstruction,\nD-51\nPayload\nmessages, F-6\npacket format, F-7\np-bits, J-21\u201323, J-25, J-36\u201337\nPC. See Program counter (PC)\nPCI bus, historical background, M-88\nPCI-Express (PCIe), storage area\nnetwork, F-29\nPCI-X, M-88\nstorage area network, F-29\nPCI-X 2.0, F-67\nPCMCIA slot, Sony PlayStation 2\nEmotion Engine case\nstudy, E-15\nPC-relative addressing, A-9, K-52\nPDP-11, M-10\u201311, M-19, M-57, M-88\nPeak performance\nCray X1E, G-24\nDAXPY on VMIPS, G-21\nfallacy, 63, 63\nin vector architectures, 355\nVMIPS on DAXPY, G-17\nPeer-to-peer, wireless networks, E-22\nPegasus, M-17\nPennySort competition, D-66\nPentium, K-31\u201332\nPerfect Club benchmarks\nvectorization, 303, 303\nvector processor history, G-27\u201328\nPerfect-shuffle exchange, F-32\nPerformability, RAID reconstruction,\nD-55\u201357\nPerformance. See also Cost-\nperformance; Peak\nperformance\nbenchmarks, 40\u201345\nbranch scheme, C-21\u201322, C-22\ncache (see Cache performance)\ncommon data bus, 207\ndeep neural networks, 603\ndirty bits, D-61\u201364\ndisk array deconstruction, D-51\u201354\ndisk deconstruction, D-48\u201350\nDSAs, 600\u2013601, 601\nembedded computers, E-13\u201314\nFujitsu SPARC64 X+, 429\u2013431,\n432\nIBM Power8, 431\u2013432, 432\ninstruction set architecture, 258\nIntel Core i7 920, 434\u2013437\nIntel Core i7 6700, 138\u2013142,\n255\u2013257\nInternet Archive Cluster, D-36\u201341\ninterprocessor communication, I-3\nI/O devices, D-15\u201323\nI/O subsystem design, D-59\u201361\nI/O system design/evaluation,\nD-36\u201341\nItanium 2, H-43, H-43\nlarge-scale multiprocessors,\nscientific application\ndistributed-memory\nmultiprocessors,\nI-26\u201332, I-28\u201332\nparallel processors, I-33\u201334\nsymmetric shared-memory\nmultiprocessor, I-21\u201326,\nI-23\u201326\nsynchronization, I-12\u201316\nlatency, 20, 21\nmeasuring, 39\u201347\nmicroprocessor, 22\nmulticore processor, 426\u2013437, 432\nquantitative measures, M-6\u20137\nreporting, 39\u201347\nsorting case study, D-64\u201367\nsummarizing, 39\u201347\nsymmetric shared-memory\nmultiprocessor, I-21\u201326,\nI-23\u201326\ntrends, 20\nvector processor, G-2\u20139\nchaining, G-11\u201312, G-12\nDAXPY on VMIPS, G-19\u201321\nsparse matrices, G-12\u201314\nstart-upandmultiplelanes, G-7\u20139\nunchaining, G-12\nVMIPS on Linpack, G-17\u201319\nPermanent failure, commercial\ninterconnection\nnetworks, F-70\nPermanent faults, D-11, 93\nPersonal computers, 4, 6\nLANs, F-4\nnetworks, F-2\nPCIe, F-67\nPersonal mobile device (PMD), A-2,\n7\u20138\nimage-processing unit for, 542\nmemory hierarchy in, 79\nPetaBox GB2000, Internet Archive\nCluster, D-37\nPhase-change memory (PCM)\nmemory hierarchy design, 93\nXpoint memory chips, 93\nPhase-ordering problem, A-26\nPhysical address\nAMD Opteron data cache, B-12\u201313\nAMD64 paged virtual memory,\nB-55\nsafe calls, B-54\ntranslation, B-36\u201340\nvirtual address, B-45\nvirtual memory, B-42, B-51\nPhysical cache, B-36\u201337\nPhysical channels, F-47\u201348\nPhysical layer, F-84\nPhysical memory\ncentralized shared-memory\nmultiprocessor, 372\ndirectory-based cache coherence,\n380\nmemory hierarchy, B-40\u201342\nvirtual machine, 123\nPhysical register\ninstruction, 237\nregister renaming, 235\nSIMD instructions, 320\nuses of, 234\u2013235\nPhysical transfer units (phits), F-64\nPhysical volumes, D-35\nPicoJoules, 541\nPID. See Process-identifier tag (PID)\nPin-out bandwidth, topology, F-39\nPipelined circuit switching, F-51\nPipelined CPUs, early versions,\nM-27\u201328\nPipeline delays, C-44\nPipeline interlock, C-17\nPipeline latches, C-30\u201331\nPipeline organization, data\ndependences, 172\nPipeline registers\ndata hazards stall minimization, C-14\ndefinition, C-30\u201332\npipelining performance, C-8\u201310\nPipeline scheduling\ninstruction-level parallelism,\n177\u2013182\nmicroarchitectural techniques case\nstudy, 266\u2013273\nIndex\n\u25a0\nI-33"
    },
    {
        "page": 1510,
        "text": "Pipeline stall cycles\nARM Cortex-A53, 250\u2013251\nbranch scheme performance, C-21\nPipelining\nbranch cost reduction, C-22\nbranch hazards, C-18\u201322\nbranch issues, C-35\u201337\nbranch penalty reduction, C-19\u201320\nbranch-prediction buffers,\nC-23\u201325, C-24\u201326\nbranch scheme performance,\nC-21\u201322, C-22\ncache access, 99\u2013100\nclassic stages for RISC processor,\nC-6\u20138, C-7\ncompiler scheduling, M-32\ncomputer design principles, 48\nconcept, C-2\u20133\ndata hazards, C-12\u201317\ndefinition, C-11\ninstruction set complications,\nC-45\npipelined execution of\ninstructions, C-13\nstall minimization by\nforwarding, C-14\u201315,\nC-15\u201316\nstall requirements, C-16\u201317\ntypes, C-12\ndefinition, C-2\ndetection of hazard, C-34\nexample, C-7\nexception\narithmetic-logical units, C-5\ncategories, C-40\nfloating-point, C-41\u201342\nprecise, C-41\u201344\nRISC V, C-42\u201343, C-42\nstopping/restarting, C-41\u201342\ntypes and requirements,\nC-38\u201341, C-40\nunexpected sequences, C-70\nfloating-point addition speedup,\nJ-25\ngraphics pipeline history, M-51\u201352\nhazards, C-10\u201325\ninstruction set complications,\nC-43\u201345\ninterconnection networks, F-12\nMIPS R4000, C-55\u201364\nperformance issues, C-8\u201310\nperformance with stalls, C-11\u201312\npredicted-not-taken scheme,\nC-19\u201320, C-19\nRISC V, C-30\u201333\nclassic pipeline stages, C-6\u20138\ncontrol, C-33\u201335\nexception, C-42\u201343, C-42\nFP pipeline, C-45\u201355, C-57\ninstruction set, C-3\u20134, C-65\ninstruction set complications,\nC-43\u201345\ninteger pipeline to handle\nmulticycle operations,\nC-45\u201355\nmulticycle FP operations,\nC-45\u201355\npipeline control, C-33\u201335\nsimple implementation, C-4\u20136,\nC-6, C-26\u201329, C-30\nsimple implementation, C-26\u201337\nspeedup from, C-11\u201312\nstatic branch prediction, C-22, C-23\nswitch microarchitecture,\nF-64\u201366\nPipe segment, C-3\nPipe stage\ndefinition, C-3\nexception stopping/restarting, C-41\nperformance issues, C-8\u201310\nprogram counter, C-31\u201332\nregister additions, C-31\nRISC processor, C-8\nPixel Visual Core\narchitecture philosophy, 583\u2013584\nevaluating, 601\u2013602\nexample, 588\nfloor plan, 592\nHalo, 584\u2013585\nimplementation, 590\u2013591\ninstruction set architecture,\n587\u2013588\nline buffers in, 590\nprocessing element, 588\u2013589\nprocessor, 585\u2013587\nprogrammer view of, 589\nsoftware, 582\ntwo-dimensional array, 586\ntwo-dimensional line buffers,\n589\u2013590\nPLA, early computer arithmetic, J-65\nPoints-to analysis, H-9\nPoint-to-point links\nbus replacement, D-34\nEthernet, F-30\nstorage systems, D-34\nswitched-media networks, F-24\u201325\nPoison bits, compiler-based speculation,\nH-28, H-30\u201331\nPoisson distribution\nbasic equation, D-28\nrandom variables, D-26\u201334\nPoisson, Sim\u0001eon, D-28\nPolycyclic scheduling, M-32\nPortable computers, interconnection\nnetworks, F-89\nPortable mobile devices (PMDs),\n580\u2013581\nPort number, network interfaces, F-7\u20138\nPosition independence, A-17\nPower, 442\u2013443\nfirst-level caches, 95\u201398\nwithin microprocessor, 25\u201328\nsystems perspective, 23\u201324\nPower 3, K-25\nadditional instructions, K-24\nbranch registers, K-23\u201325\nPower consumption. See also Energy\nefficiency\ncase study, 69\u201371\nembedded benchmarks, E-13\ninterconnection networks, F-61,\nF-89\nsimultaneous multithreading, 246\nspeculation, 238\u2013239\nTI TMS320C55 DSP, E-8\nPower gating, 28\nPowerPC, K-6, K-11, K-25\ncluster history, M-64\nconditional instructions, H-27\nIBM Blue Gene/L, I-41\u201342\nRISC history, M-21\nPowerPC AltiVec, multimedia support,\nE-11\nPower-performance\nDell PowerEdge servers, 55\u201358, 56\nof servers, 57\nPower utilization effectiveness (PUE)\npitfall, 515\nWSCs, 483, 484\u2013485\nPrecise exceptions\ndefinition, C-41\u201344\nmaintaining, C-53\u201355\nI-34\n\u25a0\nIndex"
    },
    {
        "page": 1511,
        "text": "Precisions, floating-point arithmetic,\nJ-33\u201334\nPredicated instructions\nexample calculations, H-25\nexposing parallelism, H-23\u201327\nIA-64, H-38\u201340\nPredicate registers, 296\u2013298\nIA-64, H-34\nPredication, TI TMS320C6x DSP, E-10\nPredicted-not-taken scheme, C-19\u201320,\nC-19\nPrediction. See also Misprediction rate\nbranch\naccuracy, C-25\u201326\ncost reduction, C-22\ndynamic, C-23\u201325\ninstruction-level parallelism,\n182\u2013191\nstatic, C-22, C-23\nbranch-prediction buffers,\nC-23\u201325, C-24\u201326\nreturn address, 232\u2013234\nsize, 187\n2-bit scheme, C-24, 182, 184, 185\nPrediction by Partial Matching (PPM),\n188\nPrefetching, 376\ninstruction, 234\nItanium 2, H-42\nmemory hierarchy design, 138\nsoftware and hardware, 148\u2013164\nPrefix, Intel 80x86 integer operations,\nK-35\nPresentation layer, F-84\nPresent bit, B-52\nPrice, cost vs., 35\nPrice-performance, 8\nPrimitives, synchronization, M-64\u201365\nPrinciple of locality\ncoining of term, M-11\ncomputer design principles, 48\u201349\ndefinition, B-2\nscientific workloads on symmetric\nshared-memory\nmultiprocessors, I-25\u201326\nPrivate data, 377\nPrivate memory, NVIDIA GPU\nmemory structures, 326\nProcedure calls\nIA-64 register model, H-33\u201334\nVAX, K-57\nProcedure invocation options, A-19\u201320\nProcess-complexity factor, 34\nProcess concept, B-49, 119\nProcess-identifier tag (PID), B-37\u201338,\nB-37\nProcessing element (PE) array, 570,\n572, 580, 584\u2013585, 587,\n592\nProcessor consistency, 420\nProcessor cycle\ncache performance, B-3\ndefinition, C-3\nProcessor-dependent optimizations,\nA-26\nProcessor-intensive benchmarks, 41\nProcessor performance\naverage memory access time and,\nB-17\u201320\nequation, 52\u201355\nmultiprocessors, 371\u2013372\nProcess switch\ndefinition, B-37, B-49\nvirtual memory, B-49, 119\nProducer-server model, D-15\u201316, D-16\nProfile-based predictor, misprediction\nrate, C-23\nProgram counter (PC), A-17\nbranch hazards, C-18\nbranch-target buffers, 228\u2013229, 229\ndynamic branch prediction,\nC-23\u201324\nexception stopping/restarting,\nC-41\u201342\npipeline branch issues, C-35\u201336\npipe stage, C-31\u201332\nprecise exceptions, C-54\nRISC V instruction set, C-4\nsimple RISC implementation,\nC-27\u201329\ntagged hybrid predictors, 188\u2013189\nProgrammer\u2019s view, memory\nconsistency, 418\u2013419\nProgramming models, warehouse-scale\ncomputers, 471\u2013476\nProgram order\ncache coherence, 378\u2013379\ncontrol dependence, 174\u2013175\ndata hazards, 173\u2013174\ndefinition, 173\nProtection schemes\ncross-cutting issues, 126\u2013127\ndevelopment, M-9\u201312\nnetwork interfaces, F-8\nPentium vs. Opteron, B-57\nprocesses, B-50\nsafe calls, B-54\nsegmented virtual memory,\nB-51\u201354\nvirtual memory, B-41\nProtocol deadlock, routing, F-45\u201346\nProtocol stack, F-86\u201387\nPSO. See Partial store order (PSO)\nPTE. See Page table entry (PTE)\nPulse amplitude modulation (PAM-4),\n59\nQ\nQCDOD, M-64\nQPI. See QuickPath Interconnect (QPI)\nQsNetII, F-67, F-80\nQuadrics SAN, F-80\nQuality of service (QoS)\ndependability benchmarks, D-21\nWAN, F-102\u2013103\nQuantitative performance measures,\ndevelopment, M-6\u20137\nQuantization, DNNs, 556\nQueue\ndefinition, D-25\ndiscipline, D-26\nIntel Core i7, 256\nwaiting time calculations, D-26\nQueuing locks, large-scale\nmultiprocessor\nsynchronization,\nI-18\u201321\nQueuing theory, D-23\u201334\nQuickPath Interconnect (QPI),\n426\u2013429\nR\nRace-to-halt, 28\nRadio frequency amplifier, radio\nreceiver, E-23\nRadio receiver, components, E-23\nRadio waves, wireless networks, E-21\nRadix-8 multiplication, J-49\nRadix-2 multiplication/division, J-4\u20137,\nJ-4, J-55\nRadix-4 multiplication/division, J-49,\nJ-49, J-56, J-56\u201358, J-61\nIndex\n\u25a0\nI-35"
    },
    {
        "page": 1512,
        "text": "RAID. See Redundant array of\ninexpensive disks\n(RAID)\nRandom access memory (RAM), F-56\nRandom Access Method of Accounting\nControl (RAMAC),\nM-85, M-88\u201389\nRandom replacement, B-9\u201310, B-10\nRandom variables, distribution,\nD-26\u201334\nRanking, 573\nRansomware, 491\nRay casting (RC), 350\nRead after write (RAW), C-12\u201314\ncheck for, C-52\nfirst vector computers, M-47\u201348\ninstruction set complications, C-44\nprogram order, 173\nRISC pipeline control, C-34\nstalls, C-49, C-50, C-51\nTI TMS320C55 DSP, E-8\nTomasulo\u2019s algorithm, 195, 217\nRead miss, 382\u2013383, 410\ncache coherence, 384\u2013385, 386,\n388\ndirectory-based cache coherence\nprotocol, 410\u2013411\nmemory stall clock cycles, B-4\nmiss penalty reduction, B-35\u201336\nOpteron data cache, B-14\nRead operand, C-68\ndynamic scheduling pipelines,\nC-66\nID pipe stage, 194\nReal addressing mode, K-31\nReal memory, 123, 126\nReal mode, K-36\nReal numbers, K-39\nReal-time constraints, E-2\nReal-time performance, 7\u20138\nrequirement, definition, E-3\u20134\nReal-time processing, embedded\nsystems, E-3\u20134\nRearrangeably nonblocking, F-33\nReceiving overhead, F-28, F-28, F-41,\nF-67\nReceiving overhead, communication\nlatency, I-3\nReconfiguration deadlock, F-45\u201346\nRecovery time, vector processor, G-8\nRectified linear unit (ReLU), 546\nRecurrences\nbasic approach, H-11\nloop-carried dependences, H-5\nRecurrent neural network (RNN),\n553\u2013555\nRed-black Gauss-Seidel, I-9\u201310\nReduced Instruction Set Computer\n(RISC), 413\u2013414, 423\ncache performance, B-6\ncompiler history, M-33\ncorrelating predictors, 183\ndevelopment, 2\nearly pipelined CPUs, M-28\nFENCE in, 420\u2013422\nhistorical background, M-20\u201323,\nM-22\nmultimedia SIMD extensions\nhistory, M-50\npipeline scheduling and loop\nunrolling, 177\u2013178\nreduced code size in, A-23\u201324\nRISC-I, M-20\u201321\nRISC-II, M-20\u201321\nSanyo VPC-SX500 digital camera,\nE-19\nvector processor history, G-26\nReduced Instruction Set Computer\n(RISC) architectures,\nA-33\u201342\naddressing modes and instruction\nformats, K-6\u20139\nARM architecture, K-22\n16-bit instructions, K-3, K-4, K-5,\nK-10\ncompare and conditional branch,\nK-11\u201316\nconditional branches, K-17\ncontrol instructions, K-18\ndata transfer instructions, K-18\ndigital signal-processing\nextensions, K-28\nextensions beyond RV64G,\nK-18\u201319\nfor lower-end applications, K-3\nMIPS64 R6, K-19\nmultimedia and graphics\noperations, K-25\u201327\nPower3, K-23\u201325\nRISC-V integer ISA, K-13\nRV64GC core 16-bit instructions,\nK-16\u201317\nRV64G core instructions, K-11\nRV64G instruction, K-5, K-10,\nK-12\nSIMD extensions, K-25\u201327\nSPARC v.9, K-20\u201322\nsurvey of, K-3\u201329\nReduced Instruction Set Computer\n(RISC) V, 12\u201317, 13,\n413\u2013414, 423\naddressing modes, A-36\ncontrol flow instructions, A-39\u201340\ndata types for, A-35\u201336\ndies, 33\nFENCE in, 420\u2013422\nfloating point instructions for, 16\nfloating-point operations, A-40\u201341\ninstruction format, A-36\u201337\ninstruction set architecture formats,\n16\ninstruction set organization, A-34\nload and store instructions, A-38\noperations, A-37\u201339\npipelining, C-30\u201333\nclassic pipeline stages, C-6\u20138\ncontrol, C-33\u201335\nexception, C-42\u201343, C-42\nFP pipeline, C-45\u201355, C-57\ninstruction set, C-3\u20134, C-65\ninstruction set complications,\nC-43\u201345\ninteger pipeline to handle\nmulticycle operations,\nC-45\u201355\nmulticycle FP operations,\nC-45\u201355\npipeline control, C-33\u201335\nsimple implementation,\nC-4\u20136, C-6, C-26\u201329,\nC-30\nregisters for, A-34\u201335\nscalar architecture, RV64V, 284\nSIMD Processor, 306\u2013307\nSPECint2006 programs, A-42\nsubset of instructions in, 15\nTomasulo\u2019s algorithm\nfloating-point operation, 198\ninstruction set, 195\nReductions, 344\u2013345\nRedundancy\nchip fabrication cost case study,\n67\u201368\nI-36\n\u25a0\nIndex"
    },
    {
        "page": 1513,
        "text": "computer system power\nconsumption case study,\n69\u201371\nindex checks, B-9\nsimple RISC implementation, C-29\nRedundant array of inexpensive disks\n(RAID)\ndependability benchmarks,\nD-21\u201323\ndisk array deconstruction case\nstudy, D-51\u201354\ndisk deconstruction case study,\nD-48\u201350\nhardware dependability, D-15\nhistorical background, M-86\u201388\nI/O subsystem design, D-59\u201361\nlogical units, D-35\nNetApp FAS6000 filer, D-41\u201343\noverview, D-6\u20138\nperformance prediction, D-57\u201359\nRAID 0, D-6\nRAID 1, D-6, M-87\nRAID 2, D-6, M-87\nRAID 3, D-6, M-87\nRAID 4, D-7, M-87\nRAID 5, D-8, M-87\nRAID 6, D-8\u201310\nRAID 10, D-8\nrow-diagonal parity, D-9\u201310, D-9,\nD-41\u201342\nRedundant multiplication, integers,\nJ-47\nReference bit, B-45, B-52\nRegional explicit congestion\nnotification (RECN),\nF-70\nRegister(s)\nDSP examples, E-6\nIA-64, H-33\u201334\ninstructions and hazards, C-13\nnetwork interface functions, F-7\npipe stage, C-31\ntag, 202\nRegister addressing, K-52\nRegister allocation, A-26\u201327\nRegister deferred addressing,\nK-52\u201353\nRegister fetch (RF)\ncycle, C-5\nMIPS R4000 pipeline, C-56\nsimple RISC implementation, C-27\nRegister file, C-27, 200\u2013201\ndata hazards, C-16\u201317\nfloating-point operations, C-50\nOCNs, F-3\nprecise exceptions, C-54\nRISC instruction set, C-5, C-7\u20138\nsimple RISC implementation, C-29\nRegister indirect addressing mode,\nK-34\nRegister management software-\npipelined loops, H-14\nRegister-memory\narchitecture, A-3\nISAs, 12\nRegister prefetch, 111\nRegister pressure, 182\nRegister renaming\nantidependence, 196\ndeallocating registers, 235\ndefinition, 173, 195\u2013196\nexpected output, 269\ninitial state table, 270\nmicroarchitectural techniques case\nstudy, 266\u2013273\nname dependences, 196\nvs. reorder buffers, 234\u2013236\nreservation stations, 196\u2013197,\n199\u2013200\nsample code, 269\u2013270\nRegister stack engine, IA-64, H-34\nRegister Transfer Level (RTL) code,\n569\nRegularity, bidirectional MINs,\nF-33\u201334\nReinforcement learning (RL), 549\nRelaxed consistency models, 419\u2013422,\n421\nRelease consistency (RC), 420\u2013422,\n421, 457\nReliability\ncommercial interconnection\nnetworks, F-37\nI/O subsystem design, D-59\u201361\nstorage systems, D-44\nRelocation, virtual memory, B-41\u201342\nRemainder, floating point, J-31\u201332\nRemington-Rand, M-5\nRemote direct memory access\n(RDMA), F-80\nRemote node, 406\u2013407\nRenaming map, 235\nReorder buffer (ROB)\ncompiler-based speculation,\nH-31\u201332\nhardware-based speculation,\n209\u2013212, 214\u2013215\nissue with, 236\nregister renaming vs., 234\u2013236\nReplication\ndefinition, 377, 379\nvirtual memory, B-49\nReply, messages, F-6\nReproducibility, 45\nRequest\nmessages, F-6\nswitch microarchitecture,\nF-58\u201359\nRequested protection level, B-54\nRequest-level parallelism (RLP)\ndefinition, 5, 10\u201311, 369\nWSCs, 467\nRequest phase, F-49\u201350\nRequest-reply deadlock, F-45\u201346\nReservation stations\ncommon data bus, 202\nfields, 199\u2013200\nregister renaming, 196\u2013197,\n199\u2013200\nReserved register, 414\nResource sparing, F-70\u201371\nResponse time. See also Latency\ndefinition, 20, 39\nDNN applications, 596\u2013600\nI/O benchmarks, D-18\nproducer-server model, D-16\nvs. throughput, D-16\u201318, D-17\nRestartable pipeline\ndefinition, C-40\u201341\nexception, C-41\u201342\nRestorations, dependability, 37\nRestoring division, J-5, J-6\nResume event, exception, C-40\nReturn address, predictors, 232\u2013234\nReturns, cache coherence, 378\u2013379\nReverse path, cell phones, E-24\nRF. See Register fetch (RF)\nRings, F-43\nprotection processes, B-50\nRipple-carry adder, J-2\u20133, J-3\ncarry-lookahead adder with, J-42\nchip comparison, J-61\nRipply-carry addition, J-2\u20133\nIndex\n\u25a0\nI-37"
    },
    {
        "page": 1514,
        "text": "RISC. See Reduced Instruction Set\nComputer (RISC)\nROB. See Reorder buffer (ROB)\nRole code, 569, 570\nRoofline model, 349\nCPUs vs. GPUs, 355\nDNN applications, 596\u2013600, 597\nRound digit, J-18\nRounding modes, J-14, J-17\u201320, J-18,\nJ-20\nFP precisions, J-34\nfused multiply-add, J-33\nRound-robin (RR), F-49\nRouters, F-64, F-83\nRouting algorithm, F-21\u201322, F-45\u201349\nRow access strobe (RAS), memory\nhierarchy design, 85\u201386\nRow-diagonal parity, D-9\u201310, D-9,\nD-41\u201342\nRow major order, blocking, 107\nRV64c, 16-bit instruction formats, K-7\nRV32E, K-4\nRV64GC, K-3\nALU instructions in, K-17\n16-bit instructions, K-10\ncore 16-bit instructions, K-16\u201317\nregister encodings, K-7\nRV64G core instructions, K-11\nRV64G, extensions beyond, K-18\u201319\nRV64G instruction, K-12\nRV64V extension\ndata sizes, 287\nvector architecture, 283\u2013287, 284\nvector instructions, 286\nRV64V instruction set, 293\nS\nSanyo digital cameras, SOC, E-20\nSanyo VPC-SX500 digital camera,\nembedded system case\nstudy, E-19\nSASI, M-88\nSATA disks. See Serial Advanced\nTechnology Attachment\n(SATA) disks\nSaturating arithmetic, DSP media\nextensions, E-11\nScalability\ncomputer design principles, 48\nserver systems, 9\nScalable GPUs, M-51\nScalar lane (SCL), 586\u2013588\nScalar processors, 310, 326, 332, 334.\nSee also Superscalar\nprocessors\nearly pipelined CPUs, M-28\nvs. vector, G-19\nScalar registers\nCray X1, G-21\u201322\nset of, 285\nScaled speedup. See Weak scaling\nScaling\nCMOS, 442\u2013443\ncomputation-to-communication\nratios, I-11\ninstruction-level parallelism, 442\nmulticore processor, 432,\n442\u2013444\nscientific applications on parallel\nprocessing, I-34\nSPECintRate benchmarks,\n429\u2013431, 431\nstrong, 439\nweak, 439\nScan Line Interleave (SLI), M-51\nScatter store, 301\u2013302\nSchorr, Herb, M-29\u201330\nScientific applications\nBarnes, I-8\u20139\ncharacteristics, I-6\u201312\ncluster history, M-62\u201363\ndistributed-memory\nmultiprocessors,\nI-26\u201332, I-28\u201332\nFFT kernel, I-7\nLU kernel, I-8\nocean, I-9\u201310\nparallel processors, I-33\u201334\nparallel program computation/\ncommunication,\nI-10\u201312, I-11\nparallel programming, I-2\nsymmetric shared-memory\nmultiprocessor, I-21\u201326,\nI-23\u201326\nScoreboarding\ndefinition, 194\u2013195\ndynamic scheduling with, C-66\u201370,\nC-68\nSCSI. See Small Computer System\nInterface (SCSI)\nSDRWAVE, J-62\nSecond-level caches\ncache optimization, B-30\u201335, B-34\nexecution time, B-32, B-34\ninterconnection network, F-74\nItanium 2, H-41\nmemory hierarchy, B-48\u201349, B-48\nmiss rate calculations, B-30\u201335,\nB-34\nSecure Virtual Machine (SVM), 146\nSeek distance, D-46, D-46\u201347\nSeek time, storage disks, D-45\u201346, D-46\nSegment descriptor, B-52, B-53\nSegmented virtual memory\nbounds checking, B-52\nIntel Pentium processors, B-51\u201354\nmemory mapping, B-52\nsafe calls, B-54\nsharing and protection, B-52\u201353\nSegments\ndefinition, B-43\npages vs., B-43\nSelf-correction, Newton\u2019s algorithm,\nJ-28\u201330\nSelf-draining pipelines, M-30\nSelf-routing, MINs, F-48\u201349\nSemiconductor\nDRAM, 19\nflash, 19\nITRS, 58\u201359, 59\nmanufacturing, 4\nSending overhead\nOCNs vs. SANs, F-27\u201329\ntime of flight, F-14\nSending overhead, communication\nlatency, I-3\nSense-reversing barrier\ncode example, I-15, I-21\nlarge-scale multiprocessor,\nsynchronization, I-14\u201316\nSequency number, packet header, F-8\nSequential consistency (SC), 417, 421,\n457\nimplementation, 418, 423\nprogrammer\u2019s view, 418\u2013419\nSequential interleaving, 100\nSequent Symmetry, M-59\u201360\nSerial Advanced Technology\nAttachment (SATA)\ndisks\nNetApp FAS6000 filer, D-42\npower consumption, D-5\nI-38\n\u25a0\nIndex"
    },
    {
        "page": 1515,
        "text": "RAID 6, D-8\u20139\nvs. SAS drives, D-5, D-5\nSerial Attach SCSI (SAS) drive\nhistorical background, M-88\npower consumption, D-5\nvs. SATA drives, D-5\nSerialization\nbarrier synchronization, I-16\ncache coherence, 378\u2013381\ndefinition, 380\u2013382, 413\nDSM multiprocessor cache\ncoherence, I-37\nSerpentine recording, M-85\nServe-longest-queue (SLQ) scheme\narbitration, F-49\nServer(s), A-2, 8\u20139.\nSee also Warehouse-\nscale computer (WSC)\nbenchmarks, 43\u201345\nCPU utilization, 475\ndefinition, D-25\nGoogle WSCs, 512\u2013513, 513\nsingle-server model, D-25\nsystem characteristics, E-4\nServer computer, RISC architectures\nsurvey for, K-3\u201329\nServerless Computing, 496\nServerNet interconnection network,\nF-70\u201371\nServer utilization, D-25, D-28\u201329\nService accomplishment, 36\nService interruption, 36\nService level agreements (SLAs), 36\u201337\nService level objectives (SLOs), 36,\n485\u2013486\nSession layer, F-84\nSet associativity, 81\nAMD Opteron data cache, B-13\ncache block, B-8, B-8\ncache misses, B-10\nSet, definition, B-8\nSettle time, D-46\nSFS benchmark, NFS, D-20\u201321\nShadow page table, 123\nSharding, 480\u2013482\nShared data, 377\nShared-media networks, F-23\u201325\nShared memory, 373, 379, 406\naddress space, 373\ndistributed (see Distributed shared\nmemory (DSM))\nShared-memory communication, large-\nscale multiprocessors,\nI-4\u20135\nShared-memory multiprocessors\n(SMPs), 371, 373\naccess time, 371\ndefinition, M-64\nhistory background, M-61\nsnooping coherence protocols, 380\nShared state\ncache block, 386\ndefinition, 383\u2013384\nSharing addition, segmented virtual\nmemory, B-52\u201353\nShear algorithms, disk array,\nD-51\u201354\nSheet Generator (SHG), 585\nShell code, 569, 570\nShifting over zeros, integer\nmultiplication/division,\nJ-45\u201347\nSiFive, 33\nSignals, definition, E-2\nSignal-to-noise ratio (SNR), wireless\nnetworks, E-21\nSigned-digit representation\nexample, J-54\ninteger multiplication, J-53\nSigned number arithmetic, J-7\u201310\nSign-extended offset, RISC, C-5\nSignificand, J-15\nSign magnitude, J-7\u20138\nSilicon Graphics Altix, M-64\nSilicon Graphics Challenge, M-60\nSilicon Graphics 4D/240, M-59\u201360\nSilicon Graphics Origin, M-62, M-64\nSilicon Graphics systems (SGI), vector\nprocessor history, G-27\nSimultaneous multithreading (SMT),\n424\u2013426, 425, 435\ndefinition, 244\nhistorical background, M-35\u201336\nimplementations, 245\nJava and PARSEC benchmark\nwithout, 435\u2013437, 435,\n437\nmulticore processor and, 436\u2013437\nsuperscalar processors, 245\u2013247\nSingle chip multicore processor, 382,\n391, 446\u2013451\nSingle-event upsets (SEUs), 569\nSingle-extended precision floating-\npoint arithmetic, J-33\u201334\nSingle instruction multiple data\n(SIMD), 11, 170, 282\nhistorical overview, M-55\u201357\ninstruction\nDSP media extensions, E-10\nIBM Blue Gene/L, I-42\nSony PlayStation 2, E-16\nIntel Core i7 920 multicore\ncomputer, 309\nloop-level parallelism, 170\nmultimedia extensions\n256-bit-wide operations, 304\ndata-level parallelism, 304\u2013310\nGPU and MIMD vs., 347\u2013353\nvs. GPUs, 335, 335\nIntel Core i7 920 multicore\ncomputer, 309\nNEC SX-9 vector processor,\n309\nprogramming, 307\nRISC-V, 306\u2013307\nroofline visual performance\nmodel, 307\u2013310\nNEC SX-9 vector processor, 309\nprocessors\nmultithreaded, 311, 315, 317\nPascal GPU architecture, 330\nRISC V, 306\u2013307\nsupercomputer development,\nM-45\u201346\nsystem area network history, F-104\nthread instructions, 315\u2013317, 316,\n319\nthread schedule, 315\u2013317\nTI 320C6x DSP, E-9\nSingle instruction, multiple thread\n(SIMT), 311\nSingle instruction stream, single data\nstream (SISD), 11, M-56\nSIMD, M-46\nSingle-precision floating point\narithmetic, J-33\u201334\nrepresentation, J-15\nSingle-precision floating-point\narithmetic, 329\nSingle-Streaming Processor (SSP)\nCray X1, G-21\u201324\nCray X1E, G-24\nSkippy algorithm, D-49, D-50\nIndex\n\u25a0\nI-39"
    },
    {
        "page": 1516,
        "text": "Small Computer System Interface\n(SCSI)\nBerkeley\u2019s Tertiary Disk project,\nD-4\ndependability benchmarks, D-21\ndisk storage, D-4\nhistorical background, M-88\nI/O subsystem design, D-59\u201361\nRAID reconstruction, D-56\nstorage area network history,\nF-106\u2013107\nSmall form factor (SFF) disk, M-86\nSmalltalk, K-21\u201322\nSmart interface cards, vs. smart\nswitches, F-90\nSmart switches, vs. smart interface\ncards, F-90\nSMPs. See Shared-memory\nmultiprocessors (SMPs)\nSMT. See Simultaneous multithreading\n(SMT)\nSnooping bandwidth, 389\u2013390\nSnooping cache coherence, 380, 381\nexample protocol, 383\u2013387, 384\nimplementation, 392\u2013393\ninvalidate protocol, 381\nlarge-scale multiprocessors,\nI-34\u201335, M-61\nlatencies, 447, 448\nlimitations, 389\u2013392\nmaintenance, 380\u2013381\nsample types, M-60\nSoC. See System-on-chip (SoC)\nSoft cores, 130\nSoft errors, 93\nSoft real-time, 7\u20138\ndefinition, E-3\u20134\nSoftware as a service (SaaS)\ngrowth of, 9\nWSCs, 467\nSoftware guard extensions (SGX), 125\nSoftware pipelining\nexample calculations, H-13\u201314\nloops, execution pattern, H-15\ntechnique, H-12\u201315, H-13\nSoftware prefetching, 148\u2013164\nSoftware speculation\ndefinition, 176\nhardware-based vs., 240\u2013241\nSoftware technology\nlarge-scale multiprocessor, I-6\nsynchronization, I-17\u201318\nnetwork interfaces, F-7\u20138\nSolaris, RAID benchmarks, D-21,\nD-22, D-23\nSonic Smart Interconnect, OCNs, F-3\nSony PlayStation 2\nblock diagram, E-16\nembedded multiprocessors,\nE-14\u201315\nEmotion Engine case study,\nE-15\u201318\nEmotion Engine organization, E-18\nSort procedure, VAX\ncode example, K-62\u201364\nfull procedure, K-65\nregister allocation, K-62\nregister preservation, K-64\u201365\nSource routing, F-49\nSPARC \u201cannulling\u201d branch, K-18\u201319\nSPARCLE processor, M-35\u201336\nSPARC v.9\nadditional instructions, K-22\nfast traps, K-20\u201321\ninteger arithmetic, K-21\nLISP, K-21\u201322\nmisaligned trap, K-21\nregister windows, K-20\nSmalltalk, K-21\u201322\nSPARC VIS, K-25\u201326, K-27\nSPARC64 X+, 389, 426, 429\nfeature, 427\nperformance, 429\u2013431, 432\nSparse matrices, vector architecture,\nG-12\u201314, 301\u2013302\nSpatial locality, B-26\ncoining of term, M-11\ncomputer design principles, 49\ndefinition, B-2\nSPEC benchmark\nactive benchmarks, 44\ncorrelating predictors, 182\ndesktop performance, 41\u201343, 42\nearly performance measures, M-7\norganization, 433\u2013434\nserver performance, 43\u201345\nstatic branch prediction, C-22, C-23\nstorage systems, D-20\u201321\nvector processor history, G-27\u201328\nSPEC89 benchmark, 41\nbranch-prediction buffer, C-24\u201325,\nC-25\nmisprediction rate, 187\nmispredictions rate, C-26\ntournament predictors, 187\nSPEC92 benchmarks\nCPI, C-64\nstalls, C-61\u201362\nSPEC95 benchmarks\nprocedure returns, 232\nreturn address buffer, 232, 233\nSPEC2000 benchmarks\ncompulsory miss rate, B-23\nperl benchmark, 144\nspeculation, 238\u2013239\nSPEC2006Cint execution times, 47\nSPECCPU2006 benchmark\nIntel Core i7 920/6700, 192\nnonblocking caches, 101\u2013102\nvirtual machine, 121\nSPEC CPU95 benchmark, return\naddress buffer, 232, 233\nSPECCPUint2006 benchmark, clock\ncycles per instruction,\n256, 257\nSPECfp benchmark\nIntel Core i7, 253\ninterconnection network, F-91\u201392\nItanium 2, H-43\nstalls, C-55, C-56\u201357\nSPECfpRate benchmark\ncost-performance, 440, 441\nspeedup, 440, 440\nSPEChpc96 benchmark, G-27\u201328\nSpecial instruction caches, 128\nSpecial-purpose machines\nhistorical background, M-4\u20135\nSIMD computer history, M-56\u201357\nSpecial-purpose register computer, A-3\nSpecial values, floating point, J-14\u201315,\nJ-16\nSPECInt2006 benchmark\nARM Cortex-A53, 132, 132\u2013133,\n250\nL1 data cache miss rate, 139\nSPECINT92 benchmark, nonblocking\ncaches, 101\u2013102\nSPECINT benchmarks\ninterconnection network, F-91\u201392\nItanium 2, H-43\nSPECint95 benchmarks, F-92\nSPECintRate benchmarks\ncost-performance, 440, 441\nI-40\n\u25a0\nIndex"
    },
    {
        "page": 1517,
        "text": "performance scaling, 429\u2013431, 431\nspeedup, 440, 440\nSPEC Mail benchmark, D-20\u201321\nSPEC-optimized processors, vs.\ndensity-optimized,\nF-89\nSPECpower benchmark, WSCs,\n475\u2013476\nSPECRate benchmark, 439\nfor memory-intensive benchmarks,\n116, 116\nserver performance, 43\nSPECRatios, 46\u201347\nSPEC SFS benchmarks, D-20\nSpeculation\naddress aliasing prediction,\n239\u2013240\nadvanced techniques, 228\u2013240\nadvantages, 237\u2013238\nchallenge of issues per clock,\n236\u2013237\nconcept origins, M-31\ncontrol dependence, 175\u2013176\ncross-cutting issues, 127\u2013128\ndisadvantages, 238\nand energy efficiency, 238\u2013239\nexception handling, 199\nexecution, 241\nhardware-based, 208\u2013217\ndata flow execution, 209\ndefinition, 208\ndisadvantage, 241\ninstruction execution step,\n211\u2013212\nkey ideas, 208\nreorder buffer, 209\u2013212,\n214\u2013215\nvs. software speculation,\n240\u2013241\nwrite result, 217\nIA-64, H-38\u201340\nILP studies, M-33\u201334\nmemory reference, hardware\nsupport, H-32\nmicroarchitectural techniques case\nstudy, 266\u2013273\nmultiple branches, 238\nregister renaming vs. ROB,\n234\u2013236\nsoftware, 176, 240\u2013241\nSPEC Web benchmarks, D-20\u201321\nSpeedup\nAmdahl\u2019s Law, 374\u2013375\ncomputer design principles, 49\u201352\nfloating-point addition, J-25\u201326\ninteger addition\ncarry-lookahead, J-37\u201341\ncarry-lookahead circuit, J-38\ncarry-lookahead tree, J-40\ncarry-lookahead tree adder,\nJ-41\ncarry-select adder, J-43\u201344,\nJ-43\u201344\ncarry-skip adder, J-41\u201343, J-42\noverview, J-37\ninteger division\nradix-2 division, J-55\nradix-4 division, J-56\nradix-4 SRT division, J-57\nwith single adder, J-54\u201357\nSRT division, J-45\u201347, J-46,\nJ-55\u201357\ninteger multiplication\narray multiplier, J-50\nBooth recoding, J-49\neven/odd array, J-52\nmany adders, J-50\u201354, J-50\nmultipass array multiplier, J-51\nsigned-digit addition table, J-54\nwith single adder, J-47\u201349,\nJ-48\u201349\nWallace tree, J-53\ninteger multiplication/division,\nshifting over zeros, J-45\ninteger SRT division, J-45\u201347, J-46\nlinear, 438\u2013439, 440\nfrom multithreading, 248\npipeline with stalls, C-11\u201312\nSPECfpRate benchmarks, 440, 440\nSPECintRate benchmarks, 440, 440\nswitch buffer organizations,\nF-59\u201360\nTPC-C benchmarks, 440, 440\nSperry-Rand, M-4\u20135\nSpin locks, 414\u2013416\nlarge-scale multiprocessor\nsynchronization\nbarrier synchronization, I-16\nexponential back-off, I-17\nSPRAM, Sony PlayStation 2 Emotion\nEngine organization,\nE-18\nSprowl, Bob, F-103\nSquared coefficient of variance, D-27\nSRAM. See Static random-access\nmemory (SRAM)\nSRT division\nchip comparison, J-61\ncomplications, J-45\u201347\nearly computer arithmetic, J-65\nexample, J-46\nhistorical background, J-63\nintegers, with adder, J-55\u201357\nradix-4, J-56\u201357, J-57\nStack, A-3, A-28\narchitecture, historical background,\nM-17\u201318\nStacked DRAM, 91\nStack frame, K-57\nStack pointer, K-57\nStale copy, cache coherency, 128\nStall\ncontrol dependences, 176\ncycles\naverage memory access time,\nB-18\nbranch scheme performance,\nC-21\ndefinition, B-3\u20134, B-6, B-22\nmiss rate calculation, B-6\nout-of-order execution, B-20\ndata hazards minimization, C-13,\nC-14\u201315\ndata hazards requiring, C-16\u201317\nlonger latency pipelines, C-49,\nC-50\npipelining performance with,\nC-11\u201312\nRAW, C-49, C-50, C-51\nSPEC92 benchmarks, C-61\u201362\nSPECfp benchmarks, C-55,\nC-56\u201357\nStandardization, commercial\ninterconnection\nnetworks, F-67\u201368\nStandard Performance Evaluation\nCorporation (SPEC), 41\nStart-up time\nDAXPY on VMIPS, G-20\u201321\ndefinition, 292\npage size selection, B-47\nvector architectures, G-4, G-4, G-8\nvector convoys, G-4\nIndex\n\u25a0\nI-41"
    },
    {
        "page": 1518,
        "text": "Start-up time (Continued)\nvector performance, G-2\u20134, G-16\nvector processor, G-7\u20139, G-25\nVMIPS, G-5\nStatically based exploitation, ILP, H-2\nStatic power, 80\nStatic random-access memory (SRAM)\narithmetic operations and energy\ncost, 29\nmemory hierarchy design, 85\nprice pressures, 34\nvector memory systems, G-9\u201310\nvector processor, G-9\u201310, G-25\nStatic scheduling\ndefinition, C-65\ninstruction-level parallelism,\n218\u2013222\nunoptimized code, C-70\nStencil computation, 550\nSticky bit, J-18\nStochastic gradient descent, 548\nStorage area networks, F-77\u201381,\nF-106\u2013108\ndependability benchmarks,\nD-21\u201323\nStorage systems\nAmazon, Dynamo key-value,\n485\u2013486\nasynchronous I/O and operating\nsystems, D-35\u201336\nBerkeley\u2019s Tertiary Disk project,\nD-12\u201313\nblock servers vs. filers, D-34\u201335\nbus replacement, D-34\ncomponent failure, D-43\ncomputer system availability, D-43\ndependability benchmarks,\nD-21\u201323\ndirty bits, D-61\u201364\ndisk array deconstruction, D-51\u201354\ndisk arrays, D-6\u201310\ndisk deconstruction, D-48\u201350\ndisk power, D-5\ndisk seeks, D-45\ndisk storage, D-2\u201310\nfile system benchmarking, D-20\u201321\nInternet Archive Cluster\n(see Internet Archive\nCluster)\nI/O performance, D-15\u201323\nI/O system design/evaluation,\nD-59\u201361\nmail server benchmarking,\nD-20\u201321\nNetApp FAS6000 filer, D-41\u201343\noperator dependability, D-13\u201315\nOS-scheduled disk access, D-44\npoint-to-point links, D-34\nqueuing theory, D-23\u201334\nRAID performance prediction,\nD-57\u201359\nRAID reconstruction case study,\nD-55\u201357\nreal faults and failures, D-10\u201315\nreliability, D-15\u201323\nresponse time restrictions for\nbenchmarks, D-18\nseek distance comparison, D-47\nseek time vs. distance, D-46\nserver utilization calculation,\nD-28\u201329\nsorting case study, D-64\u201367\nTandem Computers, D-13\nthroughput vs. response time,\nD-16\u201318\nTP benchmarks, D-18\u201320\ntransactions components, D-17\nweb server benchmarking, D-20\u201321\nWSCs, 478\nStore-and-forward packet switching,\nF-51\nStore conditional\nadvantage, 414, 416\ndefinition, 413\u2013414\nStore instructions, C-5\u20136, 199.\nSee also Load-store\ninstruction set\narchitecture\nStore unit\nbandwidth for, 298\u2013299\ndefinition, 285\nStreaming SIMD Extensions (SSE), 305\nStride, 300\nvector memory systems, G-10\u201311\nStrided accesses, 346\nStrided addressing, A-31\u201332.\nSee also Unit stride\naddressing\nStriping, D-51\ndisk arrays, D-6\nRAID, D-8\nStrip-mined vector loop\nconvoys, G-5\nDAXPY on VMIPS, G-20\u201321\nStrip mining, 180, 296, 297\nDAXPY on VMIPS, G-20\u201321\nStrong scaling, 439\nStructural hazards\ncheck for, C-52\ndefinition, C-11\nSubblocking, cache optimization, 114\nSubset property, 423\nSun Microsystems, B-38\nSun Microsystems Enterprise, M-60\nSun Microsystems Niagara (T1/T2),\nmultithreading history,\nM-35\nSun Microsystems SPARC\nconditional instructions, H-27\ninteger arithmetic, J-11\u201312\ninteger overflow, J-11\nRISC history, M-21\nsynchronization history, M-64\u201365\nSun Microsystems SPARCCenter,\nM-60\nSun Microsystems SPARCstation-2,\nF-92\nSun Microsystems SPARCstation-20,\nF-92\nSun Microsystems SPARC V8,\nfloating-point precisions,\nJ-33\nSun Microsystems SPARC VIS,\nmultimedia support,\nE-11\nSun Microsystems UltraSPARC,\nM-63, M-74\nSun Microsystems UltraSPARC T1\nprocessor, F-78\nSun Modular Datacenter, M-76\nSUN servers, 94\nSun Ultra 5, 47\nSuperblock scheduling\nbasic process, H-21\u201323, H-22\ncompiler history, M-33\nSupercomputers, 10\nclusters, F-80\ncommercial interconnection\nnetworks, F-37\ndirect network topology, F-37\nSAN characteristics, F-30\u201331\nSIMD, development, M-45\u201346\nSuperpipelining, C-55\nSuperscalar processors, 223\nannouncement, M-35\ncoarse-grained multithreading, 245\nI-42\n\u25a0\nIndex"
    },
    {
        "page": 1519,
        "text": "coining of term, M-31, M-34\ndynamically scheduled, M-36, 224\nfunctional unit execution slots,\n244\u2013245, 244\nILP, M-33\u201334\nrecent advances, M-35\nsimultaneous multithreading,\n245\u2013247\nSupervised learning, 547\u2013548\nSupervisor process, virtual memory,\n119\nSussenguth, Ed, M-29\u201330\nSutherland, Ivan, M-35\nSwap procedure, VAX, K-57\ncode example, K-59\u201360\nfull procedure, K-61\nregister allocation for, K-59\nregister preservation, K-60\u201361\nSwitched-media networks, F-2,\nF-24\u201325\nSwitched networks\ncentralized, F-31\u201335\ndistributed, F-35\u201340\nSwitches\ncontext, B-49\nearly LANs and WANs, F-29\ninterconnecting node calculations,\nF-32\u201333\nvs. NIC, F-90\nprocess switch, B-49\nstatements, A-17\nstorage systems, D-34\nswitched-media networks, F-24\u201325\nSwitch fabric, switched-media\nnetworks, F-24\u201325\nSwitching, F-21\u201322, F-44\u201356\nSwitch microarchitecture, basic\nmicroarchitecture\nSwitch ports, F-30\nSyllable, IA-64, H-35\nSymbolic loop unrolling, software\npipelining, H-12\u201315,\nH-13\nSymmetric multiprocessors (SMP),\nF-106\ncharacteristics, I-45\nfirst vector computers, M-49\nSymmetric shared-memory\nmultiprocessors, 371\nlimitations, 389\u2013392\nperformance, 393\u2013404\ncommercial workload, 394\u2013399\nmultiprogramming and OS\nworkload, 399\u2013404\nscientific workloads, I-21\u201326,\nI-23\u201326\nSynapse N+1, M-59\u201360\nSynchronization, 352, 412\nCray X1, G-23\nfetch-and-increment, 413\u2013414\nhardware primitives, 412\u2013414\nhistorical background, M-64\u201365\nlarge-scale multiprocessors\nbarrier synchronization,\nI-13\u201316, I-14, I-16, I-19,\nI-20\nhardware primitives, I-18\u201321\nperformance challenges,\nI-12\u201316\nsense-reversing barrier, I-21\nsoftware implementations,\nI-17\u201318\ntree-based barrier, I-19\nlocks using coherence, 414\u2013417,\n416\nmessage-passing communication,\nI-5\nSynchronous dynamic random-access\nmemory (SDRAM)\ncapacity and access times, 88\nIBM Blue Gene/L, I-42\u201343\nmemory hierarchy design, 87\u201390\npower consumption reduction,\n89\u201390\nSynchronous events, exception, C-39\nSynchronous I/O, definition, D-35\nSynonyms, address translation, B-38\nSynthetic benchmarks, 40\nSystem area networks, F-76\u201377, F-80,\nF-104\u2013106\nSystem call, virtual memory, 119\nSystem interface controller (SIF), F-74\nSystem-on-chip (SoC)\ncell phone, E-24\ncost trends, 31\ncross-company interoperability,\nF-23\nDSAs, 592\u2013594\nembedded systems, E-3\nSanyo digital cameras, E-20\nSanyo VPC-SX500 digital camera,\nE-19\nSystem response time, D-16\nSystems software, 503\nSystem/storage area networks (SANs)\ncharacteristics, F-3\ncommunication protocols, F-8\ncongestion management, F-68\u201370\ncross-company interoperability,\nF-67\u201368\neffective bandwidth, F-19\nfat trees, F-34\u201335\nfault tolerance, F-71\nInfiniBand, F-77\u201381\ninterconnection network domain\nrelationship, F-4, F-5\nlatency and effective bandwidth,\nF-29\u201330\npacket latency, F-13, F-14\u201316\ntime of flight, F-14\nSystem virtual machines, 120\u2013121\nSystolic array, 560\nT\nTag, 383\nAMD Opteron data cache, B-13\u201314\nmemory hierarchy, 81\nregisters, 202\nvirtual memory fast address\ntranslation, B-46\nwrite strategy, B-10\nTag check\nMIPS R4000 pipeline, C-58\u201359\nwrite strategy, B-10\nTag field, B-8\u20139\nTagged hybrid predictors, 188\u2013190,\n188, 190\nTail duplication, superblock\nscheduling, H-21\nTailgating, G-20\u201321\nTail latency, 473\nTail tolerant systems, 486\nTandem Computers, D-13\ncluster history, M-62, M-74, M-87\nTarget address\nbranch hazards, C-18\u201319\nbranch penalty reduction, C-19\u201320\nbranch-target buffers, 231\npipeline branch issues, C-35\u201336\nRISC instruction set, C-5\nTarget channel adapters (TCAs), F-90\nTarget instructions\nbranch-target buffers, 231\nGPU conditional branching, 323\nTask-level parallelism (TLP), 10\nTB-80 VME rack, D-38, D-41\nIndex\n\u25a0\nI-43"
    },
    {
        "page": 1520,
        "text": "Technology trends\nbandwidth over latency, 20\nimplementation technologies,\n19\u201320\nscaling of transistor performance\nand wires, 21\u201323\nTemporal locality, B-26\ncoining of term, M-11\ncomputer design principles, 49\ndefinition, B-2\nTensor processing unit (TPU)\narchitecture, 557\u2013558\nblock diagram, 558\ncase study, 606\u2013617\ndie, 562\nfactors limiting, 598\nguidelines, 566\u2013567\nimplementation, 560\u2013563\nimproving, 564\u2013566\ninstruction set architecture, 559\nmicroarchitecture, 559\u2013560\norigin, 557\nprinted circuit board, 563\nsoftware, 563\nTensorFlow program, 564\nTERA processor, M-35\nTerminate event, exception, C-40\nTertiary Disk project, D-12\u201313\nTesla, M-52\nTest-and-set operation,\nsynchronization, 413\nTexas Instruments 8847\narithmetic functions, J-57\u201362\nchip comparison, J-58\nchip layout, J-59\u201360\nTexas Instruments ASC, first vector\ncomputers, M-47\nTFLOPS, parallel processing debates,\nM-58\nThacker, Chuck, F-103\nThermal design power (TDP), 24\nThin-film transistor (TFT), Sanyo\nVPC-SX500 digital\ncamera, E-19\nThinking Machines, M-46, M-56, M-87\nThinking Multiprocessors CM-5,\nM-60\u201361\nThink time, D-16\nThird-level caches, 166, 262\ninterconnection network, F-91\u201392\nThrash, B-25\u201326\nThread Block, 311, 315\nThread Block Scheduler, 315, 316\nThread-level parallelism (TLP), 5,\n10\u201311, 369\ncentralized shared-memory\nmultiprocessor, 371, 377\nbasic schemes for enforcing\ncoherence, 379\u2013380\ncache coherence protocol,\n377\u2013379, 378, 383\u2013387,\n384\nextensions to coherence\nprotocol, 388\nimplementation techniques,\n382\u2013383\nSMP and snooping limitations,\n389\u2013392\nsnooping coherence protocols,\n380\u2013381, 381, 392\u2013393\nstructure, 372\ndefinition, 242\ndirectory-based cache coherence,\n380\ncase study, 451\u2013452\nprotocol example, 408\u2013412\ndistributed shared memory, 371,\n373\naccess time, 372\u2013373\narchitecture, 373\ndirectory-based cache\ncoherence, 404\u2013412, 405\ndisadvantages, 372\u2013373\nembedded systems, E-15\nmemory consistency, 379, 417\u2013422\ncase study, 456\u2013458\ncompiler optimization, 422\nprogrammer\u2019s view, 418\u2013419\nrelaxed consistency models,\n419\u2013422, 421\nspeculation to hide latency,\n422\u2013423\nmulticore processor, 369, 371\u2013372,\n382, 387, 408\napproaches, 389\narchitecture, 430\ncoherence, 387\ndevelopment, 404\nDSM, 373, 405, 452\nIntel i7 920 performance and\nenergy efficiency,\n434\u2013437\non multiprogrammed workload,\n426\u2013432\nperformance, 426\u2013437, 432\nscalability in Xeon E7 with\ndifferent workloads,\n433\u2013434\nscaling, 432, 442\u2013444\nsingle chip, 382, 391, 446\u2013451\nand SMT, 436\u2013437\nmultiprocessor architecture,\n370\u2013373\nvs. multithreading, M-36\nparallel processing challenges,\n373\u2013377\nsingle-chip multicore processor,\n446\u2013451\nsynchronization, 412\nhardware primitives, 412\u2013414\nlocks using coherence,\n414\u2013417, 416\nThread of SIMD instructions\nGPU programming, 315\u2013317, 316\nscheduling, 319\nThree-dimensional space, direct\nnetworks, F-39\nThrottling packets, F-10\nThroughput, 20, 39.\nSee also Bandwidth\ncomputing kernel, 350, 351\ndefinition, C-3, F-13\ndisk storage, D-4\nDNN applications, 596\u2013600\nproducer-server model, D-15\u201316,\nD-16\nvs. response time, D-16\u201318\nrouting comparison, F-54\nuniprocessor, 242\u2013247\nThumb-2, K-3\n16-bit instructions, K-7, K-10\nregister encodings, K-7\nTilera TILE-Gx processors, OCNs, F-3\nTime, and cost, 30\u201331\nTime-constrained scaling, I-33\u201334\nTime division multiple access\n(TDMA), cell phones,\nE-25\nTime of flight\ncommunication latency, I-3\ninterconnection networks, F-14\nTime-sharing, B-49\u201350\nTiming independent, M-18\nI-44\n\u25a0\nIndex"
    },
    {
        "page": 1521,
        "text": "TI TMS320C55 DSP\narchitecture, E-7\ncharacteristics, E-7\u20138\ndata operands, E-6\nTI TMS320C6x DSP\narchitecture, E-9\ncharacteristics, E-8\u201310\ninstruction packet, E-10\nTLB. See Translation lookaside buffer\n(TLB)\nTLP. See Thread-level parallelism\n(TLP)\nTomasulo\u2019s algorithm\nadvantages, 201\ndefinition, 194\u2013195\ndynamic scheduling, 195\u2013201\nRAW, 217\nRISC-V floating-point unit, 198\nsteps in, 216\nTOP500, M-59\nTop of Rack (ToR) switch, 477\u2013478\nTopology, F-21\u201322, F-30\u201344\nTorus networks, F-53\u201356, F-76\u201377\nTotal cost of ownership (TCO), 577\ncase study, 519\u2013521\nDSAs, 600\u2013601, 601\nresource allocation, 521\u2013522\nTotal store ordering (TSO), 420, 421\nTournament predictors, 184\u2013188, 186\nadvantage, 185\u2013187\nbranch address, 186\nearly schemes, M-29\nlocal/global predictors, 184\u2013188,\n186\nToy programs, 40\nTPC-C benchmarks\ndefinition, 44, 439\nspeedup, 440, 440\nTPC-C, file system benchmarking,\nD-18\u201320\nTPU. See Tensor processing unit (TPU)\nTrace compaction, H-19\nTrace scheduling, H-19\u201321, H-20\nTrace selection, definition, H-19\nTraffic intensity, queuing theory,\nD-26\nTrailer\nmessages, F-6\npacket format, F-7\nTransaction components, D-16,\nI-38\u201339\nTransaction-processing (TP)\nbenchmarks, server performance,\n43\u201344\nstorage system benchmarks,\nD-18\u201320\nTransaction Processing Council (TPC),\n43\u201345\nbenchmarks overview, D-18\u201320\nTransfers, A-16. See also Data transfers\nTransforms, DSP, E-5\nTransient failure, F-70\nTransient faults, D-11, 93\nTransistor performance, scaling, 21\u201323\nTranslation lookaside buffer (TLB)\naddress translation, B-37, B-46,\nB-47\nARM Cortex-A53, 251\u2013252\ncoining of term, M-9\ninterconnection network protection,\nF-91\nmisses, 346\nOpteron, B-47, B-56\u201357\nspeculation, 237\u2013238\nTransmission Control Protocol (TCP),\ncongestion management,\nF-69\nTransmission Control Protocol/Internet\nProtocol (TCP/IP), F-86\nATM, F-102\u2013103\nheaders, F-88\ninternetworking, F-85\u201389\nreliance on, F-99\nWAN, F-102\nTransmission speed, interconnection\nnetwork performance,\nF-13\nTransmission time, F-14\ncommunication latency, I-3\nTransport latency\ntime of flight, F-14\ntopology, F-25\u201326\nTransport layer, F-84\nTransputer, F-105\nTrap-handling routines, C-54\nTree-based barrier, large-scale\nmultiprocessor\nsynchronization, I-19\nTree height reduction, H-11\nTrellis codes, definition, E-6\u20137\nTRIPS Edge processor, F-67\nTrojan horses, B-51\u201353\nTrue dependence, finding, H-7\u20138\nTrue sharing misses, 393\u2013394, 397,\n398\nTSMC, Stratton, F-3\nTSO. See Total store ordering (TSO)\nTSS operating system, M-9\nTurbo mode in 2008, 28\nTuring, Alan, M-4, M-20\nTurn Model routing algorithm, F-48\nTwo-dimensional line buffer, 589\u2013590\nTwo-level predictors, 183, 191\nTwo\u2019s complement, J-7\u20138\nTwo-way set associativity, B-8\naverage memory access time, B-19\nconflict misses, B-23\nOpteron data cache, B-13\u201314, B-13\n2:1 cache rule of thumb, B-29\nTX-2, M-35, M-50\nU\nUltrix, DECstation 5000 reboots, F-73\nUMA. See Uniform memory access\n(UMA)\nUnbiased exponent, J-15\nUncached state, 406\nUnderflow\nfloating-point arithmetic, J-36\u201337,\nJ-62\ngradual, J-15, J-36\nUnicasting, shared-media networks,\nF-24\nUnified buffer, 558\nUnified cache\nAMD Opteron example, B-15,\nB-15\nmiss rate, B-16\nUnified virtual memory, 330\nUniform memory access (UMA), 371\nUninterruptible power supply (UPS),\n504\nUniprocessor, 377\u2013378\ncache coherence mechanism, 384,\n386\nthroughput, 242\u2013247\nUnit stride addressing, A-31\u201332\nUNIVAC I, M-5, M-17\nUNIX systems, B-38\nblock servers vs. filers, D-34\nfloating point remainder, J-32\nmiss statistics, B-59\nseek distance comparison, D-47\nIndex\n\u25a0\nI-45"
    },
    {
        "page": 1522,
        "text": "UNIX systems (Continued)\nvector processor history, G-26\nworkload, 399\nUnoptimized code, C-70\nUnpacked decimal, A-14, J-16\nUnshielded twisted pair (UTP), F-104\nUp*/down* routing, F-49\nUSB, Sony PlayStation 2 Emotion\nEngine case study, E-15\nUse bit, B-45\u201346, B-52\nUser-level communication, F-8\nUser maskable events, exception, C-39\nUser nonmaskable events, exception,\nC-39\nUser requested events, exception, C-39\nUser Space Driver, 563\nUtility computing, M-75\u201376\nUtilization\nI/O system calculations, D-26\nqueuing theory, D-25\nV\nValid bit, B-8, 383, 393\u2013394\naddress translation, B-46\nAMD Opteron data cache, B-14\npage table entry, B-52\nValue prediction, 228, 234\nVAPI, InfiniBand, F-81\nVariable length, 14\nVariables, random, distribution,\nD-26\u201334\nVAX architecture\nfallacies and pitfalls, K-65\u201367\ninstructions encoding, K-54\u201355\noperands and addressing modes,\nK-51\u201354\noperations, K-56\u201357\nsort, K-62\u201365\nswap, K-59\u201361\nVector architecture, 10, A-31, 282\ncomputer development, M-45\u201346\nexecution time, 290\u2013293\nfallacy, 356\nvs. graphics processing units,\n331\u2013334\nmemory banks, 298\u2013299\nmemory systems, G-9\u201311\nmultidimensional arrays, 299\u2013301\nmultiple lanes, 293\u2013294\npitfall, 355\u2013356\npredicate registers, 296\u2013298\nprocessor example, 288\u2013290\nprogramming, 302\u2013304\nRV64V extension, 283\u2013287, 284\nsparse matrices, 301\u2013302\nstart-up latency and dead time, G-8\nvector-length registers, 294\u2013296\nvector-register characteristics, G-3\nVector array, 585\nVector element, 289\nVector functional units, 285\nVector instruction\ndefinition, 289\ninstruction-level parallelism, 170\nVectorized code, 289\nVectorizing compilers\neffectiveness, G-14\nFORTRAN test kernels, G-15\nsparse matrices, G-12\u201313\nVector kernel implementation, case\nstudy, 357\u2013359\nVector-length register (VLR), 294\u2013296\nperformance, G-4\u20135\nVector load\nbandwidth for, 298\u2013299\ndefinition, 285\nVector-mask control, 297\nVector-mask registers\nCray X1, G-21\u201322\nVector processor\nCray X1, G-21\u201324, G-22\u201323\nCray X1E, G-24\nDAXPY on VMIPS, G-17,\nG-19\u201321\ndefinition, 370\nDSP media extensions, E-10\nexecution time, G-7\nhistorical background, G-26\u201328\nmeasures, G-15\u201316\nNEC SX-9 vector processor, 309\noverview, G-25\u201326\nperformance, G-2\u20139\nchaining, G-11\u201312, G-12\nDAXPY on VMIPS, G-17\nsparse matrices, G-12\u201314\nstart-up and multiple lanes,\nG-7\u20139\nunchaining, G-12\nvs. scalar processor, G-19\nSony PlayStation 2 Emotion\nEngine, E-17\u201318\nstart-up overhead, G-4\nvector kernel implementation,\n357\u2013359\nVMIPS on DAXPY, G-17,\nG-19\u201321\nVMIPS on Linpack, G-17\u201319\nVector registers, 284\nVery-large-scale integration (VLSI)\nearly computer arithmetic, J-63\ninterconnection network topology,\nF-30\nRISC history, M-21\nWallace tree, J-52\u201353\nVery long instruction word (VLIW)\ncompiler history, M-32\nEPIC approach, M-33\nIA-64, H-33\ninstruction set, 587, 587\nmultiple issue processors, 218\u2013222,\n220, 271\nmultiple-issue processors, M-30\nmultithreading history, M-36\nTI 320C6x DSP, E-8\u201310\nVGA controller, M-51\nVI interface, M-63, M-74\nVirtual address\nAMD Opteron data cache,\nB-12\u201313\nmemory hierarchy, B-39\nmiss rate vs. cache size, B-37\nOpteron mapping, B-55\nOpteron memory management,\nB-54\u201357, B-55\nand page size, B-58\nphysical address, B-45\ntranslation, B-36\u201340\nvirtual memory, B-41, B-42, B-44\nVirtual cache, B-36\u201338\nVirtual channels (VCs), F-47\u201348\nHOL blocking, F-60\nswitching, F-52\nswitch microarchitecture\npipelining, F-66\nsystem area network, F-105\u2013106\nand throughput, F-97\nVirtual cut-through switching, F-52\nVirtual functions, A-17\nVirtual instruction set architecture\n(VISA), 587\u2013588\nVirtualization\nIntel 80x86 instruction, 145\nmemory hierarchy design, 126\u2013127\nI-46\n\u25a0\nIndex"
    },
    {
        "page": 1523,
        "text": "Virtual Machine Control State\n(VMCS), 146\nVirtual machine monitor (VMM), 121\ninstruction set extension, 124\u2013125\nlaissez faire attitude, 145\npitfall, 145\nrequirements, 122\nXen virtual machine, 126\nVirtual machines (VMs), 491\nearly IBM work, M-10\nhardware management, 121\nimpact on virtual memory, 123\u2013124\ninstruction set architecture for,\n122\u2013123\nprotection via, 120\u2013122\nsoftware management, 121\nVirtual memory, B-2\u20133, B-40\u201349\naddress space, B-12, B-41, B-44,\nB-55\naddress translation, B-46, B-47\ncaches and, B-42\u201343, B-42,\nB-48\u201349, B-48\nclasses, B-43\ninstruction set extension, 124\u2013125\nIntel Pentium vs. AMD Opteron,\nB-57\npaged example, B-54\u201357\npage size selection, B-46\u201347\nparameter ranges, B-42\nPentium vs. Opteron protection,\nB-57\nprotection, B-49\u201350, 119\u2013120\nquestions, B-44\u201346\nsegmented example, B-51\u201354\nvirtual machine, 123\u2013124\nVirtual output queues (VOQs),\nF-60\u201361\nVLIW. See Very Long Instruction\nWord (VLIW)\nVLR. See Vector-length register (VLR)\nVLSI. See Very-large-scale integration\n(VLSI)\nVME rack, D-37\u201338, D-38\nVMIPS\nDAXPY, G-18\u201321\nenhanced, DAXPY performance,\nG-19\u201321\npeak performance on DAXPY,\nG-17\nperformance, G-4\non Linpack, G-17\u201319\nsparse matrices, G-13\nstart-up penalties, G-5\nvector execution time, G-6\u20137\nvector performance measures, G-16\nVoltage regulator controller (VRC),\nF-74\nVolume, and cost, 30\u201331\nVon Neumann computer, M-2\u20133\nVon Neumann, John, M-2\u20135\nVoodoo2, M-51\nW\nWafer\ndefinition, 31\nRISC-V dies, 33\nyield, 34\nWaiting line, D-25\nWallace tree\nexample, J-52\u201353, J-53\nhistorical background, J-63\nWall-clock time, 39\nscientific applications on parallel\nprocessors, I-33\nWAR. See Write after read (WAR)\nWarehouse-scale computer (WSC),\n4\u20135, 9\u201310, 369\u2013370, 466\nactive vs. inactive low power\nmodes, 516\naverage memory latency, 480\ncapital costs, 516\ncase study, 487\ncloud computing\nadvantages, 490\nAWS (see Amazon Web\nServices (AWS))\neconomies of scale, 491\nfallacy, 514\ncluster history, M-74\u201375\ncomputer architecture of, 477\u2013482\ncost, 486\u2013490\ncost-performance, 515, 517\ncost trends, 36\nefficiency\nand cost, 482\u2013490\nenergy, 503\nmeasuring, 483\u2013486\nfault tolerance, 516\nGoogle\ncooling, 506\u2013508\nnetworking, 510\u2013511\npower distribution, 504\u2013506\nracks, 509\u2013510\nservers, 512\u2013513\nhierarchy of switches, 477\nLayer 3 network, 481\nlow-power servers, 519\u2013521\nmemory hierarchy, 479\u2013482\nmicrosecond delays, 517\nopportunities/problems, 468\nperformance, 514\npower utilization effectiveness,\n483, 484\npreventing, 501\u2013503\nprogramming models and\nworkloads, 471\u2013476\nresource allocation, 521\u2013522\nserver cost and power, 519\u2013521\nstorage, 478\ntotal cost of ownership, 519\u2013521\nWarp, M-32\nWater-side economization, 508\nWavelength division multiplexing\n(WDM), F-103\nWAW. See Write after write (WAW)\nWay prediction, hit time, 98\u201399\nWB cycle. See Write-back (WB) cycle\nWCET. See Worst-case execution time\n(WCET)\nWeak ordering, 420, 421\nWeak scaling, 439\nWeb servers\nbenchmarking, D-21\nWAN, F-102\nWeighted arithmetic mean time, D-27\nWeight FIFO, 558\nWeight memory, 558\nWeitek 3364\narithmetic functions, J-57\u201361\nchip comparison, J-58\nchip layout, J-59\u201360\nWest-first routing, F-48\nWet-bulb temperature, 508\nWhirlwind project, M-4\nWide area networks (WANs)\nATM, F-4\ncharacteristics, F-4\ncross-company interoperability,\nF-68\neffective bandwidth, F-19\nfault tolerance, F-71\u201373\nhistorical overview, F-102\u2013103\nInfiniBand, F-77\u201378\nIndex\n\u25a0\nI-47"
    },
    {
        "page": 1524,
        "text": "Wide area networks (WANs)\n(Continued)\ninterconnection network domain\nrelationship, F-4, F-5\nlatency and effective bandwidth,\nF-27\u201329\noffload engines, F-8\npacket latency, F-13, F-14\u201316\nswitching, F-51\ntime of flight, F-14\nWilkes, Maurice, M-3\nWinchester disk design, M-86\nWindow, F-69\nTCP/IP headers, F-88\nWireless networks\nbasic challenges, E-21\nand cell phones, E-21\u201322\nWires, scaling of, 21\u201323\nWithin instructions exception, C-39\ninstruction set complications, C-45\nstopping/restarting exception,\nC-41\nWord(s)\nAMD Opteron data cache, B-14\u201315\ndouble, A-7, A-8, A-14, A-44, 300\nDSP, E-6\nhalf, A-8, A-8, A-14, A-44\nWord count, B-53\nWord displacement addressing, K-52\nWord offset, C-28\nWorking set effect, I-24\nWorkload, 39\u201340\ncommercial, 394\u2013399\nmeasurements, 400\nmultiprogramming and OS,\n399\u2013404\nphases, 399\u2013400\nRAID performance prediction,\nD-57\u201359\nscalability in Xeon E7 with,\n433\u2013434\nsymmetric shared-memory\nmultiprocessors,\nI-21\u201326, I-23\u201326\nwarehouse-scale computers,\n471\u2013476\nWormhole switching, F-52, F-97,\nF-105\nWorst-case execution time (WCET),\nE-4\nWrite after read (WAR)\ndynamic scheduling, 193\nhazard, C-12, C-69\nmultiple-issue processors, M-30\nprogram order, 174\nregister renaming, 196\nTI TMS320C55 DSP, E-8\nTomasulo\u2019s algorithm, 195, 207\nWrite after write (WAW), C-12\ncheck for, C-52\ndynamic scheduling, 193\nlonger latency pipelines, C-49,\nC-51\nmultiple-issue processors, M-30\nprogram order, 173\nregister renaming, 196\nTomasulo\u2019s algorithm, 195\nWrite allocate, B-11\u201312\nWrite-back cache, B-11\u201312\ncache coherence protocol, 385, 385\ndirectory-based cache coherence\nprotocol, 411\nmemory hierarchy, 81\nsnooping coherence, 380\u2013384, 381\nuniprocessor, 386\nWrite-back (WB) cycle\ndata hazards stall minimization,\nC-13\u201314\nMIPS R4000 pipeline, C-58\u201359\nmulticycle FP operations, C-52\nRISC classic pipeline, C-8\nRISC exception, C-43\nRISC instruction set, C-5, C-6\nRISC pipeline, C-33\nRISC pipeline control, C-35, C-36\nsimple RISC implementation, C-29\nWrite broadcast protocol, 381\nWrite buffer, B-11, B-14, 382\nmemory hierarchy, 81\nmerging, 105\u2013106, 106\nWrite hit\ncache coherence, 384\u2013385, 386,\n387\ndefinition, B-11\nWrite invalidate protocol, 380\nexample, 385, 385\nimplementation, 382\u2013383\nsnooping coherence, 381\nWrite miss, 411, 418\nAMD Opteron data cache, B-12\ncache coherence, 384\u2013385, 385,\n387\ndirectory-based cache coherence\nprotocol, 411\nmemory stall clock cycles, B-4\nmiss penalty reduction, B-35\u201336\noperation, 408\nOpteron data cache, B-12, B-14\noptions, B-11\nWrite result, 199\ndynamic scheduling with\nscoreboard, C-69\nhardware-based speculation, 217\ninstruction step, 211\nWrite serialization\ncache coherence, 378\u2013379\ndefinition, 380\u2013382, 413\nWrite stall, B-11\nWrite strategy\nmemory hierarchy, B-45\u201346\nvirtual memory, B-45\u201346\nWrite-through cache, B-11\u201312\naverage memory access time,\nB-16\u201317\ncoherence protocol, 378, 382\u2013384\nmemory hierarchy, 81\nWrite update protocol, 381\nX\nXALANCBMK benchmarks, 138\nXBox, M-51\u201352\nXen virtual machine, 126\nXeon E7, 389, 426\u2013429\nfeature, 427\non-chip organizations, 428\nperformance, 431, 432\nQuickPath Interconnect, 429\nscalability, 433\u2013434, 434\nXerox Palo Alto Research Center,\nF-103\nXIMD architecture, M-36\nXon/Xoff, interconnection networks,\nF-10\u201311, F-18\nZ\nZero-copy protocols, F-8\nZero-load latency, F-75\nZ-80 microcontroller, cell phones, E-24\nZuse, Konrad, M-4\u20135\nI-48\n\u25a0\nIndex"
    },
    {
        "page": 1525,
        "text": "Translation between GPU terms in book and official NVIDIA and OpenCL terms.\nType\nMore Descriptive\nName used in\nthis Book\nOfficial CUDA/\nNVIDIA Term\nBook Definition and OpenCL Terms\nOfficial CUDA/NVIDIA Definition\nProgram Abstractions\nVectorizable\nLoop\nGrid\nA vectorizable loop, executed on the GPU, made\nup of 1 or more \u201cThread Blocks\u201d (or bodies of\nvectorized loop) that can execute in parallel.\nOpenCL name is \u201cindex range.\u201d\nA Grid is an array of Thread Blocks that can\nexecute concurrently, sequentially, or a mixture.\nBody of\nVectorized\nLoop\nThread Block\nA vectorized loop executed on a \u201cStreaming\nMultiprocessor\u201d (multithreaded SIMD\nprocessor), made up of 1 or more \u201cWarps\u201d (or\nthreads of SIMD instructions). These \u201cWarps\u201d\n(SIMD Threads) can communicate via \u201cShared\nMemory\u201d (Local Memory). OpenCL calls a\nthread block a \u201cwork group.\u201d\nA Thread Block is an array of CUDA threads that\nexecute concurrently together and can cooperate\nand communicate via Shared Memory and\nbarrier synchronization. A Thread Block has a\nThread Block ID within its Grid.\nSequence of\nSIMD Lane\nOperations\nCUDA Thread\nA vertical cut of a \u201cWarp\u201d (or thread of SIMD\ninstructions) corresponding to one element\nexecuted by one \u201cThread Processor\u201d (or SIMD\nlane). Result is stored depending on mask.\nOpenCL calls a CUDA thread a \u201cwork item.\u201d\nA CUDA Thread is a lightweight thread that\nexecutes a sequential program and can cooperate\nwith other CUDA threads executing in the same\nThread Block. A CUDA thread has a thread ID\nwithin its Thread Block.\nMachine Object\nA Thread of\nSIMD\nInstructions\nWarp\nA traditional thread, but it contains just SIMD\ninstructions that are executed on a \u201cStreaming\nMultiprocessor\u201d (multithreaded SIMD\nprocessor). Results stored depending on a per\nelement mask.\nA Warp is a set of parallel CUDA Threads (e.g.,\n32) that execute the same instruction together in\na multithreaded SIMT/SIMD processor.\nSIMD\nInstruction\nPTX Instruction\nA single SIMD instruction executed across the\n\u201cThread Processors\u201d (SIMD lanes).\nA PTX instruction specifies an instruction\nexecuted by a CUDA Thread.\nProcessing Hardware\nMultithreaded\nSIMD Processor\nStreaming\nMultiprocessor\nMultithreaded SIMD processor that executes\n\u201cWarps\u201d (thread of SIMD instructions),\nindependent of other SIMD processors. OpenCL\ncalls it a \u201cCompute Unit.\u201d However, CUDA\nprogrammer writes program for one lane rather\nthan for a \u201cvector\u201d of multiple SIMD lanes.\nA Streaming Multiprocessor (SM) is a\nmultithreaded SIMT/SIMD processor that\nexecutes Warps of CUDA Threads. A SIMT\nprogram specifies the execution of one CUDA\nthread, rather than a vector of multiple SIMD\nlanes.\nThread Block\nScheduler\nGiga Thread\nEngine\nAssigns multiple \u201cThread Blocks\u201d (or body of\nvectorized loop) to \u201cStreaming Multiprocessors\u201d\n(multithreaded SIMD processors).\nDistributes and schedules Thread Blocks of a\nGrid to Streaming Multiprocessors as resources\nbecome available.\nSIMD Thread\nScheduler\nWarp\nScheduler\nHardware unit that schedules and issues \u201cWarps\u201d\n(threads of SIMD instructions) when they are\nready to execute; includes a scoreboard to track\n\u201cWarp\u201d (SIMD thread) execution.\nA Warp Scheduler in a Streaming\nMultiprocessor schedules Warps for execution\nwhen their next instruction is ready to execute.\nSIMD Lane\nThread\nProcessor\nHardware SIMD Lane that executes the\noperations in a \u201cWarp\u201d (thread of SIMD\ninstructions) on a single element. Results stored\ndepending on mask. OpenCL calls it a\n\u201cProcessing Element.\u201d\nA Thread Processor is a datapath and register file\nportion of a Streaming Multiprocessor that\nexecutes operations for one or more lanes of a\nWarp.\nMemory Hardware\nGPU Memory\nGlobal Memory\nDRAM memory accessible by all \u201cStreaming\nMultiprocessors\u201d (or multithreaded SIMD\nprocessors) in a GPU. OpenCL calls it \u201cGlobal\nMemory.\u201d\nGlobal Memory is accessible by all CUDA\nThreads in any Thread Block in any Grid.\nImplemented as a region of DRAM, and may be\ncached.\nPrivate\nMemory\nLocal Memory\nPortion of DRAM memory private to each\n\u201cThread Processor\u201d (SIMD lane). OpenCL calls\nit \u201cPrivate Memory.\u201d\nPrivate \u201cthread-local\u201d memory for a CUDA\nThread. Implemented as a cached region of\nDRAM.\nLocal Memory\nShared\nMemory\nFast local SRAM for one \u201cStreaming\nMultiprocessor\u201d (multithreaded SIMD\nprocessor), unavailable to other Streaming\nMultiprocessors. OpenCL calls it \u201cLocal\nMemory.\u201d\nFast SRAM memory shared by the CUDA\nThreads composing a Thread Block, and private\nto that Thread Block. Used for communication\namong CUDA Threads in a Thread Block at\nbarrier synchronization points.\nSIMD Lane\nRegisters\nRegisters\nRegisters in a single \u201cThread Processor\u201d (SIMD\nlane) allocated across full \u201cThread Block\u201d (or\nbody of vectorized loop).\nPrivate registers for a CUDA Thread. Implemented\nas multithreaded register file for certain lanes of\nseveral warps for each thread processor."
    },
    {
        "page": 1526,
        "text": "RV64G Instruction Subset\nMnemonic\nFunction\nData transfer\nMove data to/from GPRs and FPRs\nlb,lbu,lh,lhu,lw,lwu\nLoad byte, half word, or word to lower portion of  GPR  with/without sign extension\nld,sd\nsb,sh,sw\nfld,flw,fsd,fsw\nLoad or store a double word to GPR\nStore a byte, half word, or word from lowest portion of GPR to memory\nLoad or store a double word or word to/from the FPRs\nALU Operations\nRegister-register and register immediate ALU operations\nadd, addi,addw,addiw\nAdd, add immediate, add word, or add word immediate. Word version affects \nlower 32 bits. \nand,andi,or,ori,xor,xori\nAND, AND immediate, or OR immediate, exclusive OR, exclusive OR immediate\nauipc\nAdd upper immediate to PC; puts sum of a shifted immediate and PC in a register\nlui\nLoads an immediate value into the upper portion of a word.\nmul,mulw,mulh,mulhsu,\nmulhu\nMultiply, multiply word, multiply halfword,  multiply upper half, signed and unsigned. \nWord affects lower 32 bits.\ndiv,diw,divu\nDivide, divide word, divide unsigned. \nrem,remw,remu,remuw\nRemainder, remainder word, remainder unsigned. \nsll,slli,srl,srli,sra,srai\nShift left /right logical, right arithmetic, immediate and with shift amount in a GPR. \nsllw,sllwi,srlw,srlwi,\nsraw, sraiw\nWord shifts: affect only the lower 32-bits or a GPR. \nslt,slti,sltiu,sltu\nSet Less then: if first operand less than the second, set destination to 1 else 0; \nimmediate form and signed/unsigned.\nsub,subi,subw,subwi\nSubtract, subtract immediate. Word version affects lower 32 bits. \nControl Transfer\nBranches, jumps, procedure calls\nbeq,bge,bgeu,blt,bltu,bne\nCompare two registers if condition is true branch to PC + offset\njal,jalr\nJump, Jump to register contents. The address of the next instruction is saved in designated \nregister. Unconditional jump without link by setting destination register to x0. \nFloating Point Operations\nFloating point instructions operating of FPRs.\nfadd.*, fsub.*, fmul.*,\nfdiv.* , fsrt.*\nFP add, subtract, multiply,  divide, and square root; single (.s) and double (.d) precision \nversions.\nfmadd.*, fmsub.*, fmnadd.*, \nfmnsub.*\nMultiply-add, multiply-subtract, negte multiply-add, negate multiply-subtract; single (.s) \nand double (.d) precision versions.\nfsgnj.*, sgnjn.*, fsgnjx.*\nCopy sign, inverse sign, or XOR of sign to first operand; single (.s) and double (.d) \nprecision versions.\nfmin.*, fmax.*\nMinimum and maximum of two values; single (.s) and double (.d) precision versions.\nfeq.*, flt.*, fle.*\nFloating point compares; single (.s) and double (.d) precision versions.\nfclass.*\nClassify type of FP value; single (.s) and double (.d) precision versions.\nfmv.*.x,fmv.x.*\nMove from/to GPRs; single (.s) and double (.d) precision versions.\nfcvt.d.s, fcvt.s.d\nConvert SP to DP or DP to SP\nfcvt.*.w, fcvt.*.wu,\nfcvt.*.i, fct.*.lu\nfcvt.i.*, fcvt.lu.*\nConvert from word or double word, signed or unsigned to DP or DP. \nfcvt.w.*, fcvt.wu.*, \nConvert to word or double word, signed or unsigned."
    },
    {
        "page": 1527,
        "text": "[OCR Extracted Text from Image 1]:\nCOMPUTER Sixth Edition\nARCHITECTURE\n\nA Quantitative Approach\n\nJohn L. Hennessy | David A. Patterson\nForeword by Norman P. Jouppi\n\n\u201cThis sixth edition comes at a critical time: Moore\u2019s Law is fading just as deep learning demands unprecedented\ncompute cycles. The new chapter on domain-specific architectures documents a number of promising\napproaches and prophesies a rebirth in computer architecture. Like the scholars of the European Renaissance,\ncomputer architects must understand our own history, and then combine the lessons of that history with new\ntechniques to remake the world.\u201d\n\n\u2014Cliff Young, Google\n\nComputer Architecture: A Quantitative Approach, Sixth Edition has been considered essential reading by\ninstructors, students, and practitioners of computer design for nearly 30 years. The sixth edition of this classic\ntextbook is fully revised with the latest developments in processor and system architecture. It now features\nexamples from the RISC-V (\u201cRISC Five\u201d) instruction set architecture, a modern RISC instruction set developed\nand designed to be a free and openly adoptable standard. It also includes a new chapter on domain-specific\narchitectures and an updated chapter on warehouse-scale computing that features the first public information on\nGoogle\u2019s newest WSC. True to its original mission of demystifying computer architecture, this edition continues\nthe longstanding tradition of focusing on areas where the most exciting computing innovation is happening, while\nalways keeping an emphasis on good engineering design.\n\nFeatures\n\n@ Includes a new chapter on domain-specific architectures, explaining how they are the only path forward\nfor improved performance and energy efficiency given the end of Moore\u2019s Law and Dennard scaling\n\n@ Features the introduction of four DSAs from industry: Google Tensor Processing Unit, Google Pixel Visual Core,\nIntel Nervana Neural Network Processor, and Microsoft Catapult\n\n@ Features extensive updates to the chapter on warehouse-scale computing, with the first public information on\nthe newest Google WSC\n\n\u2122@ Offers updates to other chapters including new material dealing with the use of stacked DRAM; data on the\nperformance of new NVIDIA Pascal GPU vs. new AVX-512 Intel Skylake CPU; and extensive additions to\ncontent covering multicore architecture and organization\n\nAbout the Authors\n\nDavid A. Patterson\n\nDistinguished Engineer, Google\nPardee Chair of Computer Science,\nEmeritus\n\nUniversity of California at Berkeley\n\nJohn L. Hennessy\n\nProfessor of Electrical Engineering\nand Computer Science\n\nPresident Emeritus\n\nStanford University\n\nComputer Systems and Design\nComputer Engineering\n\nISBN 978-0-12-811905-1\n\nMORGAN KAUFMANN PUBLISHERS | |\n9\"780128\"119051\n\nAN IMPRINT OF ELSEVIER\n\nelsevier.com/books-and.journals"
    }
]